[{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$\nTo be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$\nTo be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$\nTo be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$\nTo be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart.\nTo be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases: Training: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; To be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases: Training: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word. To be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases: Training: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word. Inference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy. To be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases: Training: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word. Inference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nTo be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nTo be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nTo be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary. ‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;). ‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;). ‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nTo be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nTo be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives.\nThe WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##).\nTo be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##).\nTo be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##).\nTo be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\nTo be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\nTo be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigrams To be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigrams To be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\nIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigrams To be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\nIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigrams To be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\nIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nTo be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\nIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo; A single word can be tokenized in multiple ways. For example \u0026lsquo;hugs\u0026rsquo; can be tokenized as:\n[\u0026ldquo;hug\u0026rdquo;, \u0026ldquo;s\u0026rdquo;] [\u0026ldquo;h\u0026rdquo;, \u0026ldquo;ug\u0026rdquo;, \u0026ldquo;s\u0026rdquo;] [\u0026ldquo;h\u0026rdquo;, \u0026ldquo;u\u0026rdquo;, \u0026ldquo;g\u0026rdquo;, \u0026ldquo;s\u0026rdquo;] To be continued\u0026hellip; ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\nIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo; A single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] To be continued\u0026hellip; ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\nIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows: 1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text. 3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo; If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed. 4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached. To be continued\u0026hellip; ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\nIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows: 1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text. 3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo; If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed. 4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached. To be continued\u0026hellip; ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\nIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows: 1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text. 3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo; If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed. 4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached. To be continued\u0026hellip; ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\nIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text. 3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo; If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed. 4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached. To be continued\u0026hellip; ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\nIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo; If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed. 4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached. To be continued\u0026hellip; ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\nIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed. 4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached. To be continued\u0026hellip; ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\nIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached. To be continued\u0026hellip; ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\nIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached. Becaus the Unigram allows multiple segmentation for the same text, during inference it relies on a dynamic programming method that maximizes the score on overall path used to recreate the token. It is called Viterbi algorithm. To be continued\u0026hellip; ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\nIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached. Becaus the Unigram allows multiple segmentation for the same text, during inference it relies on a dynamic programming method that maximizes the score on overall path used to recreate the token. It is called Viterbi algorithm. To be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\nIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows: 1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecaus the Unigram allows multiple segmentation for the same text, during inference it relies on a dynamic programming method that maximizes the score on overall path used to recreate the token. It is called Viterbi algorithm. To be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\nIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s]\nEach of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows: 1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecaus the Unigram allows multiple segmentation for the same text, during inference it relies on a dynamic programming method that maximizes the score on overall path used to recreate the token. It is called Viterbi algorithm. To be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\nIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s]\nEach of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows: 1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecaus the Unigram allows multiple segmentation for the same text, during inference it relies on a dynamic programming method that maximizes the score on overall path used to recreate the token. It is called Viterbi algorithm. To be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\nIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecaus the Unigram allows multiple segmentation for the same text, during inference it relies on a dynamic programming method that maximizes the score on overall path used to recreate the token. It is called Viterbi algorithm. To be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\nIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecaus the Unigram allows multiple segmentation for the same text, during inference it relies on a dynamic programming method that maximizes the score on overall path used to recreate the token. It is called Viterbi algorithm. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords. It calculates the probability of every path (segmentation) and unrolls the path with highest segmentation score. This ensures that the tokenization is mathematically optimal based on training data, rather than greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families. To be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\nIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecaus the Unigram allows multiple segmentation for the same text, during inference it relies on a dynamic programming method that maximizes the score on overall path used to recreate the token. It is called Viterbi algorithm. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords. It calculates the probability of every path (segmentation) and unrolls the path with highest segmentation score. This ensures that the tokenization is mathematically optimal based on training data, rather than greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n###4. SentencePiece: The last piece of the puzzle. To be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\nIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecaus the Unigram allows multiple segmentation for the same text, during inference it relies on a dynamic programming method that maximizes the score on overall path used to recreate the token. It is called Viterbi algorithm. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords. It calculates the probability of every path (segmentation) and unrolls the path with highest segmentation score. This ensures that the tokenization is mathematically optimal based on training data, rather than greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The last piece of the puzzle. To be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\nIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to break split text into words and that would be the corpus. During inference (i.e. when we want to generate text), any unknown word would be given out of vocabulary token (\u0026lt;UNK\u0026gt;). Rare words like \u0026lsquo;uninstagrammable\u0026rsquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most importantly, it is a simple bottoms up frequency based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe modern Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces like Japanese or Korean. Unicode contains over 150000 characters, by using UTF bytes, the vocabulary starts with only 256 tokens. Caveat is, more fragmented the tokens, more the number of tokens required to represent a word and higher the tokenization (computational) cost.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecaus the Unigram allows multiple segmentation for the same text, during inference it relies on a dynamic programming method that maximizes the score on overall path used to recreate the token. It is called Viterbi algorithm. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords. It calculates the probability of every path (segmentation) and unrolls the path with highest segmentation score. This ensures that the tokenization is mathematically optimal based on training data, rather than greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of language data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eIn the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and sought widespread attention for its use in BERT model and its derivatives. The WordPiece tokenization strategy is a more recent development that addresses some of the limitations of BPE. Rather than relying for meaning in the frequency of subword tokens, this uses a maximum likelihood strategy in figuring out meaningful vocabulary corpus of tokens rather than raw counts.\nSuperficially WordPiece resembles BPE where a small vocabulary is started with single characters and then iteratively merged until the desired vocabulary size is reached. However, the difference is that WordPiece uses a statistical approach and a threshold of scores that are assigned to individual tokens in training data. The score for potential merge of two terms A and B, is:\n$$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecaus the Unigram allows multiple segmentation for the same text, during inference it relies on a dynamic programming method that maximizes the score on overall path used to recreate the token. It is called Viterbi algorithm. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords. It calculates the probability of every path (segmentation) and unrolls the path with highest segmentation score. This ensures that the tokenization is mathematically optimal based on training data, rather than greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\u003c/p\u003e\n\u003cp\u003eThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula shows that the individual common parts are penalized. This semantic cohesion guarentees that the words that appear together are given more importance (Score) than those parts which occur apart. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE which memorized merge rules for inference, WordPiece is not required to remeber the rules it operates by finding the longest possible subword match (greedy longest-match-first) strategy.\nGiven a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token is a hard cliff and thus it is not possible to figure out tokens for very rare words. Recent research has provided that most of the tokens in WordPiece are start tokens (~70%) and only ~30% are continuation tokens (prefixed with ##). The model does not capture sematic links between words and the tokens are not contextually aware. words like \u0026lsquo;advice\u0026rsquo; and \u0026lsquo;advises\u0026rsquo; will be tokenized differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecaus the Unigram allows multiple segmentation for the same text, during inference it relies on a dynamic programming method that maximizes the score on overall path used to recreate the token. It is called Viterbi algorithm. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords. It calculates the probability of every path (segmentation) and unrolls the path with highest segmentation score. This ensures that the tokenization is mathematically optimal based on training data, rather than greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\u003c/p\u003e\n\u003cp\u003eThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecaus the Unigram allows multiple segmentation for the same text, during inference it relies on a dynamic programming method that maximizes the score on overall path used to recreate the token. It is called Viterbi algorithm. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords. It calculates the probability of every path (segmentation) and unrolls the path with highest segmentation score. This ensures that the tokenization is mathematically optimal based on training data, rather than greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\u003c/p\u003e\n\u003cp\u003eThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\u003c/p\u003e\n\u003cp\u003eThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\u003c/p\u003e\n\u003cp\u003eThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\u003c/p\u003e\n\u003cp\u003eThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\u003c/p\u003e\n\u003cp\u003eThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"}]