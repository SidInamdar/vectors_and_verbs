[{"content":"\nThis is a placeholder for the Feedforward Network post.\n","permalink":"http://localhost:1313/posts/feedforward-networks/","summary":"Exploring the fundamental building blocks of deep learning: Feedforward Neural Networks and Activation Functions.","title":"The Hidden Layers: Understanding Feedforward Networks"},{"content":"\nWe are about to touch the holy grail of modern AI. The 2017 declaration was simple: Attention Is All You Need. But as we‚Äôve raced from GPT-1 to reasoning models, the definition of \u0026lsquo;Attention\u0026rsquo; has quietly transformed. Is the algorithm ruling the world today actually the same one we started with? Let‚Äôs look under the hood.\nüî• The Core Concept At its heart, the attention mechanism is a form of weighted aggregation. It is often described using a database retrieval analogy. Imagine you have a database of information (context).\nQuery ($Q$): What are you currently looking for? (The token being generated). Key ($K$): The indexing tag associated with each piece of information. Value ($V$): The actual information (The context token). The \u0026ldquo;Attention\u0026rdquo; is the process of matching your Query against all the Keys to determine how much \u0026ldquo;weight\u0026rdquo; to put on each Value. Attention scores are calculated in each transformer layer at every step of the sequence. This is a single unifying factor in all transformer models, from BERT to Gemini.\nüî• Standard Multi-Head Attention (MHA) The original mechanism introduced in the paper \u0026ldquo;Attention Is All You Need\u0026rdquo;.\nHere, instead of performing the Key-Query-Value (KQV) operation on the entire context only once, the attention is split into $N$ parallel heads. Each head learns a different aspect of the language (e.g., word meaning, syntax, semantic relationships, etc.). The equation is a simple culmination of dot products aided by a regularizer like Softmax.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nWhere:\n$QK^T$ computes the similarity scores between the Query and Key. $\\sqrt{d_k}$ is a scaling factor to prevent the dot product from growing too large, mitigating the vanishing/exploding gradient problem. The weighted sum is applied to the values of $V$. The Memory Bandwidth Bottleneck: \u0026gt; During inference, the model must store Key and Value matrices for all previous tokens. This is called the KV Cache. In MHA, every single head has its own unique set of Keys and Values. For large models with massive context windows, loading this massive KV cache from memory (HBM) to the computation chip (SRAM) becomes the primary bottleneck, slowing down generation significantly. Earlier models could often only generate ~8 tokens per second at inference due to this constraint.\nüî• Multi-Query Attention (MQA) This approach addresses the memory bottleneck by sharing the same set of Keys and Values across all heads while keeping separate Query heads.\nMHA: $H$ Query heads, $H$ Key heads, $H$ Value heads MQA: $H$ Query heads, 1 Key head, 1 Value head This drastically reduces the memory requirements of the KV cache by a factor of $H$ (often 8x or 16x). The GPU needs to load far less data, significantly speeding up inference. However, it does not reduce the computation (FLOPs)‚Äîso why is it faster?\nThis can be explained with the Kitchen Analogy: Imagine a kitchen (GPU) with 8 chefs (Heads).\nComputation (Math): The Chef chopping vegetables. Memory Access (Bandwidth): The Assistant running to the fridge to get ingredients. In MHA: Each chef demands unique vegetables from the fridge. The assistant runs back and forth 8 times, fetching 8 different crates. Most of the time is spent waiting for the assistant to return. This results in idle cores waiting for data to load from HBM.\nIn MQA: All chefs agree to use the same vegetables. The assistant runs back and forth only once, fetching one giant crate of potatoes and dumping it. This results in minimized memory transfer and no idle cores.\nThe Technical Reality: During text generation, the model is memory-bound, not compute-bound. The time it takes to move the KV Cache from HBM (High Bandwidth Memory) to the chip\u0026rsquo;s SRAM is the bottleneck. MQA reduces the data volume of that move by a factor of $H$.\nThe Capacity Gap: MQA is a trade-off between nuanced response and memory capacity. The model loses the ability to store different types of information, as a single $V$ matrix must now store all nuances of information flow in the language.\nThe Surgery Trick (Uptraining): Google Research figured out a way to take an existing model with MHA and convert it to MQA by averaging the weight matrices of $H$ heads. This is called Uptraining. You then train this \u0026ldquo;Frankenstein\u0026rdquo; model for ~5% of the original training steps for adjustments. This yields an MQA model that is nearly as good as the original MHA but vastly faster.\nüî• Grouped Query Attention (GQA) A \u0026ldquo;best of both worlds\u0026rdquo; solution used in modern models like Llama 2 and Llama 3.\nHow it works: GQA acts as an interpolation between MHA and MQA. Instead of having just 1 Key head (MQA) or $H$ Key heads (MHA), it divides the query heads into $G$ groups. Each group shares a single Key and Value head.\nEquation modification: If you have 8 query heads, you might create 4 groups (2 queries per group). This reduces the KV cache by 2x instead of 8x, preserving more quality than MQA while still speeding up inference.\nMHA: 8 people (heads) are working on a project. Each person has their own unique filing cabinet (KV cache). It takes forever to move 8 cabinets around. MQA: 8 people share one single filing cabinet. It\u0026rsquo;s very fast to move, but they fight over the organization and lose detail. GQA: The 8 people split into 4 teams of 2. Each team shares a cabinet. It\u0026rsquo;s a balance between speed and organizational detail. üî• Multi-Head Latent Attention (MLA) This is the most advanced mechanism for attention, used in models like DeepSeek-V3. It is a sophisticated design for massive efficiency that takes a radical approach: mathematically compressing the memory into a low-rank \u0026ldquo;latent\u0026rdquo; vector.\nKV Cache Explosion: As mentioned before, for a massive model like DeepSeek-V3 (671B parameters), storing the full Keys and Values for every token in a long conversation requires terabytes of high-speed memory (HBM), and chips spend more time waiting than churning.\nMHA (Standard) stores everything. MQA deletes redundancy but hurts quality. MLA keeps the quality of MHA but compresses the data 4x-6x smaller than even GQA.\nThe Zip File Approach: Low-Rank Joint Compression Let\u0026rsquo;s compare the math of MHA and MLA. In MHA, for every single token, we generate a unique Key ($k$) and Value ($v$) matrix for each attention head.\nThe Memory Problem: The $k$ is stored in the KV Cache. For 128 heads, for example, it calculates a massive vector of $128 \\times 128 = 16,384$ floats per token.\nFor MHA, the attention score looks like: $$\\text{Score} = q^T \\cdot k$$\nBut for MLA, instead of storing the full $k$, it projects the input down into a tiny latent vector $c_{KV}$, which is much smaller (e.g., 512 floats). $$c_{KV} = W_{DKV} \\cdot x$$\nNow here is the catch: Naive compression and decompression would mean constructing a small latent matrix for every token and deconstructing it every single time the information is required, which defeats the purpose of compression.\n$$k_{\\text{reconstructed}} = W_{UK} \\cdot c_{KV}$$ $$\\text{Score} = q^T \\cdot k_{\\text{reconstructed}}$$\nThe Matrix Absorption Trick (Optimization) Instead of reconstructing $k$ for every token, we can absorb the Up-Projection matrix ($W_{UK}$) into the Down-Projection matrix ($W_{DKV}$) and absorb the query $q$ into a new $Q$.\nFrom the original equation: $$\\text{Score} = q^T \\cdot (\\underbrace{W_{UK} \\cdot c_{KV}}_{\\text{This is } k})$$\nWe associate differently:\n$$\\text{Score} = (\\underbrace{q^T \\cdot W_{UK}}_{\\text{Absorbed Query}}) \\cdot c_{KV}$$ We change the order in which the matrix multiplication is performed.\nHow is this allowed? In linear algebra, matrix multiplication is associative:\n$$(A \\cdot B) \\cdot C = A \\cdot (B \\cdot C)$$ We can move the parentheses! Instead of grouping $(W_{UK} \\cdot c_{KV})$ to make the Key, we group $(q^T \\cdot W_{UK})$ to make a new Query.\nNote: The positional embedding information is left untouched. That rotational information from algorithms like RoPE is preserved outside the compression; it requires minimal space anyways.\n","permalink":"http://localhost:1313/posts/attention-mechanisms/","summary":"We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek\u0026rsquo;s MLA, how has the definition of \u0026lsquo;Attention\u0026rsquo; transformed?","title":"Attention Is All You Need, but exactly which one?: MHA, GQA and MLA"},{"content":"\nPositional Embeddings are the \u0026ldquo;voice\u0026rdquo; that tell Transformers where words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.\n\u0026ldquo;The dog chased the cat\u0026rdquo; and \u0026ldquo;cat chased the dog\u0026rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.\nüî• Sinusoidal Embeddings: The Odometer of Language Introduced in the seminal \u0026ldquo;Attention Is All You Need\u0026rdquo; paper, this was the industry standard for many years. This approach adds a vector to each word embedding based on its position. The vector is computed out of wavelike patterns that are periodic and smooth. Mathematically, for a position $pos$ and embedding dimension $i$:\n$$ PE_{(pos, i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $$\nThe model uses sine and cosine waves of different frequencies to capture both short and long-range dependencies. The frequency of the waves decreases as the embedding dimension increases, which helps capture long-range dependencies in the input sequence.\nThe creators wanted the model to calculate distance easily, which is why a linear function was used. The model simply rotates the current position vector using a rotation matrix to derive the next position.\n$$ \\begin{pmatrix} \\cos(k) \u0026 \\sin(k) \\\\ -\\sin(k) \u0026 \\cos(k) \\end{pmatrix} \\cdot PE(pos) \\approx PE(pos + k) $$ Low dimensions act like a fast-ticking clock (high frequency, changing rapidly) and high dimensions act like a slow clock (low frequency, changing slowly).\nThe number 10,000 is an arbitrarily chosen high number to reduce Aliasing. By choosing such a high number, the creators ensured that the wavelength of the slowest dimension is incredibly long. Consequently, the pattern practically never repeats, and each position remains unique.\nThis combination works like a mechanical odometer dial. Each subsequent dimension acts as a slower-moving gear. The model learns to read these \u0026ldquo;dials\u0026rdquo; to understand exactly where a word sits in the sequence.\nThis mitigated the challenge of exploding gradients that previous array-based positional embeddings suffered from (e.g., adding integers 1, 2, 3\u0026hellip;). This model keeps values between -1 and +1, adding stability while ensuring every position gets a distinct code.\nThe Problem: This approach pollutes the word embedding matrix (semantic information). While high-dimensional vectors are almost always orthogonal‚Äîmeaning the noise doesn\u0026rsquo;t fully destroy the word\u0026rsquo;s identity‚Äîit does not strictly guarantee purity.\nThis led to a more mature approach.\nüî• ALiBi: The Fading Streetlight Attention with Linear Biases (ALiBi) is a method where, instead of adding a vector to the embeddings at the start, the model injects positional information linearly, directly into the attention scores. If Sinusoidal Encoding asks, \u0026ldquo;Where am I on the map?\u0026rdquo;, ALiBi asks, \u0026ldquo;How much harder should I squint to see you?\u0026rdquo;\nThe biggest problem ALiBi solved is the \u0026ldquo;Invisible Wall\u0026rdquo;. If you train a Transformer on sequences of length 2048 and then feed it a book with 3000 words, it will crash or output garbage at word 2049. This is a failure of Extrapolation.\nALiBi throws away the idea of adding positional vectors entirely. Instead, it devises a \u0026ldquo;penalty\u0026rdquo; system for attention scores:\n$$ \\text{Score} = (q \\cdot k^\\top) - (m \\cdot \\text{distance}) $$\nDistance: If words A and B are neighbors, the distance is 0, so the penalty is 0. Although the distance is linear, it does not cause exploding gradients because the raw attention scores are normalized by a softmax function. This distance prioritizes nearby words (local context) over distant ones.\nSlope ($m$): If we always penalize distance equally, the model becomes myopic. ALiBi mitigates this by assigning a different slope value to each attention head:\nHead 1 (The Historian): Very low slope. The penalty grows slowly, allowing it to see far back. Head 2 (The Reader): Medium slope. Head 3 (The Myopic): Very high slope (e.g., 0.5); it effectively cannot see beyond 2-3 words. This mimics holding a lantern. Words close to you are bright, while words further away naturally fade into darkness.\nExtrapolation: If you walk 5 miles down the road (a longer sequence), your lantern works exactly the same way. The physics of light decay are constant, allowing the model to handle sequences longer than it was trained on.\nHowever, ALiBi forces a \u0026ldquo;sliding window\u0026rdquo; view where distant words eventually become invisible. This brings us to the modern standard.\nüî• RoPE: The Rotary Revolution Rotary Position Embeddings (RoPE) is the current industry standard (used in Llama 3, Mistral, PaLM) because it mathematically unifies \u0026ldquo;absolute position\u0026rdquo; with \u0026ldquo;relative position.\u0026rdquo; This method rotates the vectors in the Query and Key matrices based on their absolute position.\nIf a vector has 512 dimensions, RoPE breaks it into 256 pairs. Each pair is rotated by a different frequency:\n$$ \\theta_i = 10000^{-2i/d} $$\nThe First pair ($i = 0$) is rotated by the highest frequency. The Last pair ($i = 255$) is rotated by the lowest frequency. This spread ensures the model has \u0026ldquo;high precision\u0026rdquo; (fast rotation) to distinguish immediate neighbors, and \u0026ldquo;low precision\u0026rdquo; (slow rotation) to track global position without the pattern repeating.\nDoesn\u0026rsquo;t rotation change meaning? Yes and No. The vector for \u0026ldquo;Apple\u0026rdquo; at position 1 and \u0026ldquo;Apple\u0026rdquo; at position 256 will look completely different in terms of coordinates. However, rotation does not change the semantic strength (magnitude/norm) of the vector.\nThe Relativity Trick: Transformers compare words using dot products (angles). RoPE relies on the fact that the attention mechanism ignores the absolute rotation and only detects the difference in rotation (relative position). Thus, the \u0026ldquo;relative meaning\u0026rdquo; is encoded purely into the angle between words, leaving the semantic \u0026ldquo;magnitude\u0026rdquo; untouched.\nüî• LongRoPE: The Bifocal Lens While RoPE is powerful, it struggles when stretching context windows from 4k to 2 million tokens. The \u0026ldquo;fast\u0026rdquo; dimensions spin so rapidly over long distances that they become random noise. LongRoPE solves this using an evolutionary search algorithm to find a unique scaling factor ($\\lambda$) for each dimension.\nThe Equations Instead of rotating by the standard $\\theta_i$, LongRoPE rotates by a rescaled frequency:\n$$ \\theta\u0026rsquo;_i = \\lambda_i \\theta_i $$\nTo efficiently find these $\\lambda$ values without searching an infinite space, the algorithm enforces a monotonicity constraint based on NTK theory:\n$$ \\lambda_i \\le \\lambda_{i+1} $$\nThis ensures that low-frequency dimensions (global context) are stretched more than high-frequency dimensions (local context), creating a \u0026ldquo;bifocal\u0026rdquo; effect:\nHigh Frequencies (Local): Kept sharp ($\\lambda \\approx 1$) to maintain grammar. Low Frequencies (Global): Stretched ($\\lambda \u0026gt; 1$) to track massive distances without repeating. Not worth perusing more since the next one is much better at long context positional encoding. üî• HoPE: The Hyperbolic Slide Hyperbolic Rotary Positional Encoding (HoPE) moves from the geometry of a circle to the geometry of a hyperbola (inspired by Lorentz transformations). It was designed to fix a subtle flaw in RoPE: the \u0026ldquo;wobbly\u0026rdquo; or oscillatory nature of attention scores.\nThe Core Problem: RoPE\u0026rsquo;s \u0026ldquo;Wobble\u0026rdquo; In standard RoPE, we rotate vectors around a circle.\nThe Issue: Circles repeat. A dial at $361^\\circ$ looks identical to $1^\\circ$. Even with different frequencies, the dot product fluctuates up and down as distance increases. The Consequence: The model gets confused at long distances, potentially mistaking a word 1,000 tokens away for a neighbor just because the rotation cycles aligned, creating \u0026ldquo;noise.\u0026rdquo; The \u0026ldquo;Hyperbolic\u0026rdquo; Shift HoPE replaces trigonometric functions ($\\sin, \\cos$) with hyperbolic functions ($\\sinh, \\cosh$).\nCircular Rotation: Keeps distance from the center constant. Hyperbolic Rotation: Moves points along a hyperbola. Crucially, hyperbolic functions do not repeat; they grow or decay exponentially. By using hyperbolic rotation, HoPE guarantees that as words get further apart, the attention score decays monotonically (smoothly drops).\nThe Equations HoPE uses a hyperbolic matrix $B(\\theta, m)$ defined using the Lorentz Boost structure:\n$$ B(\\theta, m) = \\begin{pmatrix} \\cosh(m\\theta) \u0026 \\sinh(m\\theta) \\\\ \\sinh(m\\theta) \u0026 \\cosh(m\\theta) \\end{pmatrix} $$ Since hyperbolic functions grow to infinity, HoPE introduces a decay penalty ($e^{-m\\theta'}$). The final attention score simplifies beautifully to a function of relative distance: $$ \\text{Score} \\propto e^{-|m-n|(\\theta\u0026rsquo; - \\theta)} $$\nThis ensures the attention score exponentially decays as distance increases, eliminating the wobbles of standard RoPE. This the latest model that is taking up the space in the industry for extremely long context models.\n","permalink":"http://localhost:1313/posts/positional-embeddings/","summary":"From Sinusoidal to RoPE and HoPE: How Transformers learn to process word order and sequence length.","title":"The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE"},{"content":"Imagine trying to teach a computer the difference between \u0026ldquo;Apple\u0026rdquo; the fruit and \u0026ldquo;Apple\u0026rdquo; the company. To us, the distinction is intuitive. To a machine, it‚Äôs just a string of characters. How do we turn these strings into meaningful math? While early attempts like Word2Vec gave us a great start, they missed the forest for the trees‚Äîor in some cases, the twigs for the branches. Enter GloVe and FastText: two algorithms that revolutionized how machines understand the nuances of human language.\nPrevious static embedding models like Word2Vec successfully captured the local semantics of words, but they failed in addressing contexts between words that might not always appear in the same context window. Furthermore, they treated words as indivisible atomic units in continuous vector spaces, missing out on the internal structure of language.\nThe two simple but effective ideas behind GloVe and FastText are subword units and global co-occurrence. Let\u0026rsquo;s dive into them.\nGloVe: A Global Mapping Strategy Global Vectors for Word Representation (GloVe) was created at Stanford. The core idea was to train a model where word vectors are defined by how often they appear around other words across the entire corpus, rather than just locally.\nTraining Steps: Co-occurrence Matrix X: GloVe scans the entire corpus and creates a giant matrix (spreadsheet) recording the counts of how often words appear near each other. Factorization: It compresses this giant matrix into smaller, dense vectors. Objective: The function tries to equate the dot product of two word vectors with the logarithm of their probability of appearing together. The error between these values is then backpropagated to update the vectors. $$w_i \\cdot w_j + b_i + b_j = \\log(X_{ij})$$\n$w$: The word vector (what we want). $X_{ij}$: The count from our matrix. The Loss Function: We can\u0026rsquo;t satisfy the equation above perfectly for every word pair. So, we use a weighting function to minimize the squared error:\n$$J = \\sum_{i,j=1}^V f(X_{ij}) ({w_i}^T w_j + b_i + b_j - \\log(X_{ij}))^2$$\nIf we do not use the weighting function, the loss would be infinite for rare words (since $\\log(0) = \\infty$) and frequent words (like \u0026rsquo;the\u0026rsquo;) would overpower the model.\n$$ f(x) = \\begin{cases} (x/x_{max})^\\alpha \u0026amp; \\text{if } x \u0026lt; x_{max} \\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$\n(Typically $x_{max}=100$ and $\\alpha=0.75$)\nFastText: The Lego Bricks Strategy FastText was created by Facebook AI Research (FAIR). The idea was to treat words not as solid rocks, but as structures made of \u0026ldquo;Lego bricks\u0026rdquo; (subword units).\nWord2Vec struggled to create embeddings for words it hadn\u0026rsquo;t seen before (Out-Of-Vocabulary or OOV words). FastText solves this by breaking words down.\nGloVe treats a word like \u0026ldquo;Apple\u0026rdquo; as a single, atomic unit. FastText breaks \u0026ldquo;Apple\u0026rdquo; into character n-grams: \u0026lt;ap, app, ppl, ple, le\u0026gt; For a complex or new word like \u0026ldquo;Applesauce\u0026rdquo;, the model might not know the whole word, but it recognizes the Lego brick for \u0026ldquo;Apple\u0026rdquo; and the remaining bits from other subwords.\nFastText takes its inspiration from the SkipGram architecture. The vector for a word is simply the sum of the vectors of its n-gram subwords:\n$$v_w = \\sum_{g \\in G_w} z_g$$\nObjective: The objective is to predict context words given a target word.\n$$P(w_c | w_t) = \\frac{e^{s(v_{w_t} \\cdot v_{w_c})}}{\\sum_{w \\in V} e^{s(v_{w_t} \\cdot v_w)}}$$\nAnd the scoring function is defined as:\n$$s(w_t, w_c) = u_{w_t}^T (\\sum_{g \\in G_w} z_g)$$\n$u_{w_c}$: The vector for the context word (output vector). $z_g$: The vector for the n-gram $g$ (input vector). Modifications to Softmax: Calculating the full softmax for every word in the vocabulary is computationally expensive. FastText uses two methods to approximate this efficiently.\nOption A: Negative Sampling\nWe pick a few random \u0026ldquo;wrong\u0026rdquo; words every time to serve as \u0026ldquo;negative samples\u0026rdquo; and calculate the loss only for the correct word and these noise words.\n$$J = - \\log \\sigma(s(w_t, w_c)) - \\sum\\limits_{i=1}^{N} \\mathbb{E}_{n_i \\sim P_n(w)} [\\log \\sigma(-s(w_t, n_i))]$$\nThe sigmoid function $\\sigma$ squashes the score between 0 and 1. The first term pushes the probability of the correct word towards 1. The second term pushes the probability of the noise words towards 0. Option B: Hierarchical Softmax (Faster for infrequent words)\nInstead of a flat list of words, imagine the vocabulary arranged as a binary tree (Huffman Tree).\nRoot: Top of the tree. Leaves: The actual words. To calculate the probability of a word, we trace a path from the root to that leaf. At every branching node, we calculate the probability (sigmoid) of going left or right. The probability of a word $w$ is the product of the probabilities of the turns taken to reach it. Code Example from gensim.models import FastText from gensim.test.utils import common_texts # Initialize the FastText model with specific hyperparameters model = FastText( vector_size=100, # Dimensionality of the word vectors window=5, # Maximum distance between the current and predicted word within a sentence min_count=1, # Ignores all words with total frequency lower than this sg=1, # Training algorithm: 1 for skip-gram; 0 for CBOW hs=0, # If 0, and negative is non-zero, negative sampling will be used negative=5, # Number of \u0026#34;noise words\u0026#34; to be drawn for negative sampling workers=4, # Number of worker threads to train the model epochs=10, # Number of iterations over the corpus min_n=3, # Minimum length of character n-grams max_n=6 # Maximum length of character n-grams ) # Build the vocabulary from the provided text corpus model.build_vocab(common_texts) # Train the model on the corpus model.train( common_texts, total_examples=model.corpus_count, epochs=model.epochs ) # Calculate and print the cosine similarity between two words in the vocabulary print(f\u0026#34;Similarity between \u0026#39;computer\u0026#39; and \u0026#39;human\u0026#39;: {model.wv.similarity(\u0026#39;computer\u0026#39;, \u0026#39;human\u0026#39;)}\u0026#34;) # Demonstrate FastText\u0026#39;s ability to handle Out-Of-Vocabulary (OOV) words # Even if \u0026#39;computation\u0026#39; wasn\u0026#39;t in the training data, FastText constructs a vector using character n-grams oov_vector = model.wv[\u0026#39;computation\u0026#39;] # Check if a specific word exists in the model\u0026#39;s fixed vocabulary index word = \u0026#34;computer\u0026#34; is_in_vocab = word in model.wv.key_to_index print(f\u0026#34;\\nIs the word \u0026#39;{word}\u0026#39; in the model\u0026#39;s vocabulary? {is_in_vocab}\u0026#34;) ","permalink":"http://localhost:1313/posts/glove-and-fasttext/","summary":"How subword units and global co-occurrence matrices allow GloVe and FastText to capture nuances that Word2Vec missed.","title":"The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText"},{"content":" Before we had Large Language Models writing poetry, we had to teach computers that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; are related not just by spelling, but by meaning. This is the story of that breakthrough. It‚Äôs the moment we stopped counting words and started mapping their souls‚Äîturning raw text into a mathematical landscape where math can solve analogies. Welcome to the world of Word2Vec. üîÆ\nLanguage models require vector representations of words to capture semantic relationships. Before the 2010s, models used word count-based vector representations that captured only the frequency of words (e.g., One-Hot Encoding). The Problems: üöß\nThe Curse of Dimensionality Lack of meaning (Synonyms were treated as mathematically unrelated). In 2011, Mikolov et al. at Google introduced Word2Vec, which used a shallow neural network to learn vector representations of words. The best part? It could create much denser relationships between words than prior models, and it was unsupervised.\nThe training (prediction) is essentially \u0026ldquo;fake\u0026rdquo;‚Äîa pretext task. But the weight matrices we get on the side are a gold mine of fine-grained semantic relationships. ‚õèÔ∏è\nThere are two popular variants of Word2Vec:\nSkipGram Continuous Bag Of Words (CBOW) 1. Continuous Bag of Words (CBOW) üß© CBOW is a simple \u0026lsquo;fill in the blanks\u0026rsquo; machine. It takes the surrounding words as inputs and tries to predict the center word.\n\u0026ldquo;Continuous\u0026rdquo;: It operates in continuous vector space (unlike the discrete space of n-gram models).\n\u0026ldquo;Bag of words\u0026rdquo;: The order of the context does not matter. \u0026ldquo;The cat sat on the mat\u0026rdquo; and \u0026ldquo;The mat sat on the cat\u0026rdquo; are the same for CBOW. They produce exactly the same predictions.\nObjective: Maximize the probability of the target word $w_t$ given its context $w_{t-C}, \\dots, w_{t+C}$.\n2. Training Defining Dimensions:\n$V$: Vocabulary size (number of unique words in the corpus, e.g., 10,000) $N$: Dimension of the embedding space (e.g., 100) $C$: Number of context words (e.g., 2 words on left of target and 2 words on right) Step 1: Input Lookup \u0026amp; One-Hot Encoding\nMathematically, we represent each context word as a one-hot vector $x^{(c)}$. We have an Input Matrix $W_{in}$ of size $V \\times N$. For each context word (e.g., \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;), we select its corresponding row from $W_{in}$.\n$$ v_c = W_{in}^T x^{(c)} $$\nWhere $v_c$ is the $N$-dimensional vector for context word $c$.\nStep 2: Projection (The \u0026ldquo;Mixing\u0026rdquo;)\nWe combine the context vectors. In standard CBOW, we simply average them. This creates the hidden layer vector $h$.\n$$ h = \\frac{1}{2C} \\sum_{c=1}^{2C} v_c $$\n$h$ is a single vector of size $N \\times 1$. Note: This operation is linear and contains no activation function (like ReLU or Sigmoid).\nStep 3: Output Scoring (The \u0026ldquo;Dot Product\u0026rdquo;)\nWe have a second matrix, the Output Matrix $W_{out}$ of size $N \\times V$. We compute the raw score (logit) $u_j$ for every word $j$ in the vocabulary by taking the dot product of the hidden vector $h$ and the output vector $v\u0026rsquo;_j$.\n$$ u_j = {v\u0026rsquo;_j}^T h $$\nOr in matrix form:\n$$ u = W_{out}^T h $$\nResult $u$ is a vector of size $V \\times 1$.\nStep 4: Probability Conversion (Softmax)\nWe convert the raw scores $u$ into probabilities using the Softmax function. This tells us the probability that word $w_j$ is the center word.\n$$ y_j = P(w_j | \\text{context}) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 5: Loss Calculation (Cross-Entropy)\nWe compare the predicted distribution $y$ with the actual ground truth $t$ (a one-hot vector where the index of the true word is 1). The loss function $E$ is:\n$$ E = - \\sum_{j=1}^V t_j \\log(y_j) $$\nSince $t_j$ is 0 for all words except the target word (let\u0026rsquo;s call the target index $j^*$), this simplifies to:\n$$ E = - \\log(y_{j^*}) $$\n3. Backpropagation Details We need to update the weights to minimize $E$. We use the Chain Rule.\nA. Gradient w.r.t. Output Matrix ($W_{out}$)\nWe want to know how the error changes with respect to the raw score $u_j$.\n$$ \\frac{\\partial E}{\\partial u_j} = y_j - t_j $$\nLet\u0026rsquo;s call this error term $e_j$.\nIf $j$ is the target word: $e_j = y_j - 1$ (Result is negative; we boost the score). If $j$ is not target: $e_j = y_j - 0$ (Result is positive; we suppress the score). The update rule for the output vector $v\u0026rsquo;_j$ becomes:\n$$ v'_{j}(\\text{new}) = v'_{j}(\\text{old}) - \\eta \\cdot e_j \\cdot h $$ ($\\eta$ is the learning rate)\nB. Gradient w.r.t. Hidden Layer ($h$)\nWe backpropagate the error from the output layer to the hidden layer.\n$$ EH = \\sum_{j=1}^V e_j \\cdot v\u0026rsquo;_j $$\n$EH$ is an $N$-dimensional vector representing the aggregate error passed back to the projection layer.\nC. Gradient w.r.t. Input Matrix ($W_{in}$)\nSince $h$ was just an average of the input vectors, the error $EH$ is distributed to each context word\u0026rsquo;s input vector. For every word $w_c$ in the context:\n$$ v_{w_c}(\\text{new}) = v_{w_c}(\\text{old}) - \\eta \\cdot \\frac{1}{2C} \\cdot EH $$\nLet\u0026rsquo;s build a single pass through the network for a given context.\nimport torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # Define a simple Continuous Bag of Words (CBOW) style model class CBOWModel(nn.Module): def __init__(self, vocab_size, embedding_dim): super(CBOWModel, self).__init__() # The embedding layer stores the word vectors we want to learn self.embeddings = nn.Embedding(vocab_size, embedding_dim) # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word self.linear = nn.Linear(embedding_dim, vocab_size) def forward(self, inputs): # inputs: tensor of word indices for the context embeds = self.embeddings(inputs) # Aggregate context by calculating the mean of the word embeddings h = torch.mean(embeds, dim=1) # Produce logits (raw scores) for each word in the vocabulary logits = self.linear(h) return logits # Setup vocabulary and mappings word_to_ix = {\u0026#34;the\u0026#34;: 0, \u0026#34;quick\u0026#34;: 1, \u0026#34;brown\u0026#34;: 2, \u0026#34;fox\u0026#34;: 3, \u0026#34;jumps\u0026#34;: 4, \u0026#34;over\u0026#34;: 5, \u0026#34;lazy\u0026#34;: 6, \u0026#34;dog\u0026#34;: 7} ix_to_word = {v: k for k, v in word_to_ix.items()} # Configuration constants EMBEDDING_DIM = 5 VOCAB_SIZE = len(word_to_ix) LEARNING_RATE = 0.01 # Initialize the model, loss function, and optimizer model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM) loss_function = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE) # Prepare dummy training data: context words and the target word # Context: [\u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;over\u0026#34;] -\u0026gt; Target: \u0026#34;fox\u0026#34; context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long) target_idxs = torch.tensor([3], dtype=torch.long) # Perform a single optimization step model.zero_grad() # Forward pass: we reshape context to (1, -1) to simulate a batch of size 1 logits = model(context_idxs.view(1, -1)) # Calculate loss against the target word index loss = loss_function(logits, target_idxs) # Backpropagate and update weights loss.backward() optimizer.step() # Output results for the blog post print(\u0026#34;=== Model Training Snapshot ===\u0026#34;) print(f\u0026#34;Calculated Loss: {loss.item():.6f}\u0026#34;) print(f\u0026#34;\\n=== Learned Vector for \u0026#39;jumps\u0026#39; ===\u0026#34;) word_vec = model.embeddings(torch.tensor([word_to_ix[\u0026#39;jumps\u0026#39;]])) print(word_vec.detach().numpy()) print(\u0026#34;\\n=== Embedding Matrix (Weights) ===\u0026#34;) print(model.embeddings.weight.detach().numpy()) print(\u0026#34;\\n=== Linear Layer Weights ===\u0026#34;) print(model.linear.weight.detach().numpy()) Output shows single pass loss and learned vector for \u0026lsquo;jumps\u0026rsquo; word. This is a single training sample result but after multiple such passes both Embedding and linear weights should be very similar.\n=== Model Training Snapshot === Calculated Loss: 1.989656 === Learned Vector for \u0026#39;jumps\u0026#39; === [[-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775]] === Embedding Matrix (Weights) === [[ 1.66039 -0.11371879 0.6246518 0.35860053 -1.2504417 ] [-2.1847186 -0.77199775 -0.17050214 -0.38411248 -0.03913084] [ 0.11852697 0.90073633 0.8847807 0.7404524 0.900149 ] [ 0.07440972 -0.40259898 2.6246994 -0.08851447 0.02660969] [-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775] [-0.9245572 -0.5545908 0.9083091 -1.0755049 0.84047747] [-1.0237687 0.59466314 0.05621134 -0.6202532 1.3664424 ] [ 0.60998917 -1.0549186 1.6103884 0.8724912 -1.2486908 ]] === Linear Layer Weights === [[ 0.02165058 -0.28883642 0.14545658 -0.3442509 0.32704315] [-0.18731792 0.28583744 0.22635977 0.13245736 0.29019794] [ 0.3158916 -0.15826383 -0.03203773 0.16377363 -0.41457543] [-0.05080034 0.4180087 0.11228557 0.30218413 0.3025514 ] [-0.38419306 0.24475925 -0.39210224 -0.38660625 -0.2673145 ] [-0.32321444 0.12200444 -0.03569533 0.2891424 -0.07345333] [-0.33704326 0.2521956 0.31587374 0.22590035 0.29866052] [ 0.4117266 -0.44231793 0.24064957 -0.29684234 0.333821 ]] Interesting Facts About CBOW üß† The \u0026ldquo;Averaging Problem\u0026rdquo;: This is a common issue in CBOW models. The averaging and training is order-agnostic. All variants of word order in the context window will generate the same output. \u0026ldquo;Dog bit the man\u0026rdquo; and \u0026ldquo;Man bit the dog\u0026rdquo; are the same in CBOW\u0026rsquo;s eyes.\nSmoothing Effect: CBOW models are much faster to train than Skip-Gram but are generally smoother.\nRare Words: CBOW models struggle slightly with rare words because the presence of surrounding common words (like articles and prepositions) can skew the embeddings and dissolve their uniqueness.\nComputational Cost: Softmax requires summing over the entire vocabulary size, which can be computationally expensive if $V$ is very large. Solutions include Hierarchical Softmax and Negative Sampling (approximating the denominator by only checking the target word vs. 5 random noise words).\nVectors as Double Agents: Every word has a different representation in the embedding matrix and the output matrix. Most of the time $W_{out}$ is discarded, but some consider averaging both $W_{in}$ and $W_{out}$ for slightly better performance.\nLinear Relationships: The famous analogy King - Man + Woman = Queen emerges from the model because of the linear relationships between the words.\nInitialization: This is a no-brainer but worth mentioning. Initialization to zero vectors would mean no learning. We must initialize randomly.\n2. Skip-Gram (The \u0026ldquo;Burger Flip\u0026rdquo; of CBOW) üîÑ Skip-Gram flips the CBOW logic. While CBOW tries to predict the center word given the context, Skip-Gram tries to predict the scattered context given the center word.\nCore Concept: If a word like \u0026ldquo;monarch\u0026rdquo; can accurately predict words like \u0026ldquo;king\u0026rdquo;, \u0026ldquo;throne\u0026rdquo;, \u0026ldquo;crown\u0026rdquo;, \u0026ldquo;royalty\u0026rdquo;, and \u0026ldquo;power\u0026rdquo;, then it must effectively capture the concept of royalty.\nTraining Let\u0026rsquo;s define dimensions:\n$V$: Vocabulary size $D$: Embedding dimension $C$: Number of context words Step 1: Input Lookup\nUnlike CBOW, our input is just one word vector (the center word $w_t$). We grab the vector $v_c$ from the Input Matrix $W_{in}$.\n$$ h = v_{w_t} $$\nNote: There is no averaging here. The hidden layer $h$ is simply the raw vector of the center word.\nStep 2: Output Scoring (The Broadcast)\nThe model takes this single vector $h$ and compares it against the Output Matrix $W_{out}$ (the entire vocabulary). It computes a score $u_j$ for every word $j$ in the dictionary.\n$$ u = W_{out}^T h $$\nResult $u$: A vector of size $V \\times 1$ containing raw scores.\nStep 3: Probability Conversion (Softmax)\nWe apply Softmax to turn scores into probabilities.\n$$ P(w_j | w_t) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 4: The Multi-Target Loss\nHere is the major difference from CBOW. In CBOW, we had one target (the center). In Skip-Gram, we have $2C$ targets (all the surrounding words). For a sentence \u0026ldquo;The cat sat on mat\u0026rdquo; (Center: \u0026ldquo;sat\u0026rdquo;), the targets are \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;. We want to maximize the probability of all these true context words. The Loss function ($E$) sums the error for each context word:\n$$ E = - \\sum_{w_{ctx} \\in \\text{Window}} \\log P(w_{ctx} | w_t) $$\nStep 5: Backpropagation (Accumulating Gradients)\nSince one input word is responsible for predicting multiple output words, the error signals from all those context words add up.\nAt the Output Layer:\nIf \u0026ldquo;cat\u0026rdquo; was a target, the \u0026ldquo;cat\u0026rdquo; vector in $W_{out}$ gets a signal: \u0026ldquo;Move closer to \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; If \u0026ldquo;apple\u0026rdquo; was NOT a target, the \u0026ldquo;apple\u0026rdquo; vector gets a signal: \u0026ldquo;Move away from \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; At the Hidden Layer ($h$):\nThe center word \u0026ldquo;sat\u0026rdquo; receives feedback from all its neighbors simultaneously. Error signal = (Error from \u0026ldquo;The\u0026rdquo;) + (Error from \u0026ldquo;cat\u0026rdquo;) + (Error from \u0026ldquo;on\u0026rdquo;)\u0026hellip; The vector for \u0026ldquo;sat\u0026rdquo; in $W_{in}$ moves in the average direction of all its neighbors. 3. The \u0026ldquo;Computational Nightmare\u0026rdquo; \u0026amp; Negative Sampling The equations above describe Naive Softmax.\nThe Problem: If $V = 100,000$, computing the denominator $\\sum \\exp(u_k)$ takes 100,000 operations. Doing this for every word in a training set of billions of words is impossibly slow.\nThe Solution: Negative Sampling Instead of updating the entire vocabulary (1 correct word vs 99,999 wrong ones), we approximate the problem.\nPositive Pair: (sat, cat) $\\rightarrow$ Maximize probability (Label 1). Negative Pairs: We pick $K$ (e.g., 5) random words the model didn\u0026rsquo;t see, e.g., (sat, bulldozer), (sat, quantum). $\\rightarrow$ Minimize probability (Label 0). New Equation (Sigmoid instead of Softmax):\n$$ E = - \\log \\sigma({v'_{\\text{pos}}}^T v_{\\text{in}}) - \\sum_{k=1}^K \\log \\sigma(-{v'_{\\text{neg}_{k}}}^T v_{\\text{in}}) $$ Effect: We typically only update 6 vectors per step (1 pos + 5 negs) instead of 100,000.\nLet\u0026rsquo;s see this in code:\nimport torch import torch.nn as nn import torch.optim as optim class SkipGramNegativeSampling(nn.Module): \u0026#34;\u0026#34;\u0026#34; Skip-Gram with Negative Sampling (SGNS) implementation. SGNS approximates the softmax over the entire vocabulary by instead distinguishing between a real context word (positive) and K noise words (negative). \u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embedding_dim): super(SkipGramNegativeSampling, self).__init__() # Input embeddings: used when the word is the center word self.in_embed = nn.Embedding(vocab_size, embedding_dim) # Output embeddings: used when the word is a context or negative sample self.out_embed = nn.Embedding(vocab_size, embedding_dim) # Initialize weights with small values to prevent gradient saturation initrange = 0.5 / embedding_dim self.in_embed.weight.data.uniform_(-initrange, initrange) self.out_embed.weight.data.uniform_(-initrange, initrange) def forward(self, center_words, target_words, negative_words): \u0026#34;\u0026#34;\u0026#34; Computes the negative sampling loss. Args: center_words: (batch_size) target_words: (batch_size) negative_words: (batch_size, K) where K is number of negative samples \u0026#34;\u0026#34;\u0026#34; # Retrieve vectors v_c = self.in_embed(center_words) # (batch_size, embed_dim) u_o = self.out_embed(target_words) # (batch_size, embed_dim) u_n = self.out_embed(negative_words) # (batch_size, K, embed_dim) # 1. Positive Score: log(sigmoid(v_c ¬∑ u_o)) # Compute dot product: (batch, 1, dim) @ (batch, dim, 1) -\u0026gt; (batch, 1) pos_score = torch.bmm(u_o.unsqueeze(1), v_c.unsqueeze(2)).squeeze(2) pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-7) # 2. Negative Score: sum(log(sigmoid(-v_c ¬∑ u_n))) # Compute dot products for all K samples: (batch, K, dim) @ (batch, dim, 1) -\u0026gt; (batch, K) neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2) neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-7), dim=1, keepdim=True) # Total loss is the negative of the objective function loss = -(pos_loss + neg_loss) return torch.mean(loss) # --- Configuration \u0026amp; Mock Data --- VOCAB_SIZE = 100 EMBED_DIM = 10 word_to_ix = {\u0026#39;fox\u0026#39;: 0} # Example vocabulary mapping model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock inputs: 1 center word, 1 target word, 5 negative samples center_id = torch.tensor([0], dtype=torch.long) target_id = torch.tensor([1], dtype=torch.long) negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long) # Training Step model.zero_grad() loss = model(center_id, target_id, negative_ids) loss.backward() optimizer.step() # Output Results print(f\u0026#34;Loss after one step: {loss.item():.6f}\u0026#34;) word_vec = model.in_embed(torch.tensor([word_to_ix[\u0026#39;fox\u0026#39;]])) print(f\u0026#34;Vector for \u0026#39;fox\u0026#39;:\\n{word_vec.detach().numpy()}\u0026#34;) The outputs:\nLoss after one step: 4.158468 Vector for \u0026#39;fox\u0026#39;: [[ 0.04197385 0.02400733 -0.03800093 0.01672485 -0.03872231 -0.0061478 0.0121122 0.04057864 -0.036255 0.03861175]] Interesting Facts About Skip-Gram üí° Makes Rare Words Shine: Skip-Gram is more effective at capturing the meaning of rare words because it focuses on the context of the center word. In CBOW, a rare word\u0026rsquo;s vector is averaged with others, diluting its signal. In Skip-Gram, when the rare word appears as the center, it gets the full, undivided attention of the backpropagation update.\nSlow Training: This method creates more training samples. A window of size 2 creates 4 training pairs per center word, while CBOW creates only 1 training pair per center word.\nSemantic Focus: Skip-Gram puts more emphasis on the semantic relationship between words. It views the context as the primary signal, with the center word as the target. This makes it better at capturing the meaning of words in context. Skip-Gram tends to capture semantic relationships (King/Queen) slightly better, while CBOW captures syntactic ones (walk/walked) slightly better.\n","permalink":"http://localhost:1313/posts/word-embeddings/","summary":"Understanding the mathematics behind Word2Vec, CBOW, and Skip-Gram and how they map language to vector space.","title":"Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n5. Code Examples: We\u0026rsquo;ll use sentencepiece for tokenization and compare results for different models.\nimport sentencepiece as spm import os # Configuration for training the SentencePiece model # SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words. options = { # The source text file used for learning the vocabulary \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # The base name for the generated model (.model) and vocabulary (.vocab) files \u0026#39;model_prefix\u0026#39;: \u0026#39;bpe_model\u0026#39;, # Number of unique tokens in the final vocabulary \u0026#39;vocab_size\u0026#39;: 4000, # \u0026#39;bpe\u0026#39; (Byte Pair Encoding) merges frequent pairs of characters/sequences \u0026#39;model_type\u0026#39;: \u0026#39;bpe\u0026#39;, # Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets \u0026#39;character_coverage\u0026#39;: 0.9995, # When enabled, unknown characters are decomposed into UTF-8 bytes to avoid \u0026#39;unk\u0026#39; tokens \u0026#39;byte_fallback\u0026#39;: True, # Treats digits individually (0-9), preventing large numbers from being treated as single tokens \u0026#39;split_digits\u0026#39;: True, # Prevents adding a whitespace prefix to the first token; useful for fine-grained control \u0026#39;add_dummy_prefix\u0026#39;: False } try: print(\u0026#34;Starting the training process...\u0026#34;) # SentencePieceTrainer.train takes the dictionary of options to build the BPE model spm.SentencePieceTrainer.train(**options) print(\u0026#34;Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created.\u0026#34;) # Initialize the processor and load the newly trained model sp = spm.SentencePieceProcessor() sp.load(\u0026#39;bpe_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Model Metadata:\u0026#34;) # Retrieve the total number of tokens in the vocabulary print(f\u0026#34;Total Vocab Size: {sp.get_piece_size()}\u0026#34;) # Special tokens are used for sequence boundaries and handling unknown characters print(f\u0026#34;BOS (Beginning of Sentence) ID: {sp.bos_id()}\u0026#34;) print(f\u0026#34;EOS (End of Sentence) ID: {sp.eos_id()}\u0026#34;) print(f\u0026#34;UNK (Unknown) ID: {sp.unk_id()}\u0026#34;) print(f\u0026#34;PAD (Padding) ID: {sp.pad_id()}\u0026#34;) # Test the tokenizer on sample strings to see how it breaks down text test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: shows the actual subword units (tokens) print(f\u0026#34;Subword Tokens: {sp.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: shows the numerical mapping for each token print(f\u0026#34;Numerical IDs: {sp.encode_as_ids(text)}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred during training or processing: {e}\u0026#34;) You can see the following output:\nStarting training... Starting the training process... Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created. ------------------------------ Model Metadata: Total Vocab Size: 4000 BOS (Beginning of Sentence) ID: 1 EOS (End of Sentence) ID: 2 UNK (Unknown) ID: 0 PAD (Padding) ID: -1 --- Tokenization Test --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;el\u0026#39;, \u0026#39;lo\u0026#39;, \u0026#39;‚ñÅW\u0026#39;, \u0026#39;orld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 380, 309, 476, 619, 36, 3924, 3964, 3959, 3993, 3978, 3983, 57, 3975, 3976, 60, 3974] --- Tokenization Test --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅmo\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;inst\u0026#39;, \u0026#39;ag\u0026#39;, \u0026#39;ram\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅever\u0026#39;] Numerical IDs: [1894, 304, 3284, 376, 321, 313, 312, 1008, 3313, 1325, 428, 3937, 341, 304, 3284, 1495] The ! is fallen back to UTF bytes. and the word instagrammable is split into multiple subword tokens. Also, original text can be recreated right back by simply concatenating the tokens. And replacing underscore by whitespace. Now we can try Unigram model. and see the same sentences tokenized differently.\nfrom tokenizers import Tokenizer from tokenizers.models import WordPiece from tokenizers.trainers import WordPieceTrainer from tokenizers.pre_tokenizers import Whitespace try: # 1. Initialize the WordPiece Tokenizer # We specify the [UNK] token for handling words not found in the vocabulary. tokenizer = Tokenizer(WordPiece(unk_token=\u0026#34;[UNK]\u0026#34;)) # 2. Configure Pre-tokenization # Before the subword algorithm runs, we need to split the raw text into words. # Whitespace splitting is the standard first step for most English NLP tasks. tokenizer.pre_tokenizer = Whitespace() # 3. Initialize the Trainer # We define our target vocabulary size and the special tokens required for # downstream tasks (like BERT\u0026#39;s [CLS] for classification or [SEP] for separators). trainer = WordPieceTrainer( vocab_size=4000, special_tokens=[\u0026#34;[UNK]\u0026#34;, \u0026#34;[CLS]\u0026#34;, \u0026#34;[SEP]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[MASK]\u0026#34;] ) # 4. Train the Model # The tokenizer scans the training file to build a vocabulary of the most # frequent subword units. tokenizer.train(files=[\u0026#34;train_data.txt\u0026#34;], trainer=trainer) # 5. Persist the Model # Save the configuration and vocabulary to a JSON file for future inference. tokenizer.save(\u0026#34;wordpiece.json\u0026#34;) print(\u0026#34;Training complete. \u0026#39;wordpiece.json\u0026#39; created.\u0026#34;) # 6. Metadata Inspection print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;WordPiece Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {tokenizer.get_vocab_size()}\u0026#34;) # 7. Testing Subword Tokenization # WordPiece shines at handling rare words by breaking them into meaningful chunks. test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (WordPiece) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # Encode converts raw text into a Tokenizer object containing tokens and IDs output = tokenizer.encode(text) # \u0026#39;tokens\u0026#39; shows the subword breakdown (e.g., \u0026#39;un\u0026#39;, \u0026#39;##insta\u0026#39;, etc.) print(f\u0026#34;Subword Tokens: {output.tokens}\u0026#34;) # \u0026#39;ids\u0026#39; are the numerical indices mapped to the vocabulary print(f\u0026#34;Numerical IDs: {output.ids}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred with WordPiece model: {e}\u0026#34;) You can see the following output:\nWordPiece Model Metadata: Total Vocab Size: 2609 --- Tokenization Test (WordPiece) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;H\u0026#39;, \u0026#39;##el\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;##or\u0026#39;, \u0026#39;##ld\u0026#39;, \u0026#39;[UNK]\u0026#39;, \u0026#39;[UNK]\u0026#39;] Numerical IDs: [37, 180, 214, 52, 162, 418, 0, 0] --- Tokenization Test (WordPiece) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;##os\u0026#39;, \u0026#39;##t\u0026#39;, \u0026#39;un\u0026#39;, \u0026#39;##ins\u0026#39;, \u0026#39;##ta\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;##ra\u0026#39;, \u0026#39;##m\u0026#39;, \u0026#39;##ma\u0026#39;, \u0026#39;##ble\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;ever\u0026#39;] Numerical IDs: [691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240] We can see the special character ## in the output. We can also see the breakdowns not having any semantic meaning. They are likelihood based. We can try the Unigram model now.\n# \u0026#39;Unigram\u0026#39; is the default and usually recommended over BPE in SentencePiece. options_unigram = { \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # Path to the raw text file for training \u0026#39;model_prefix\u0026#39;: \u0026#39;unigram_model\u0026#39;, # Prefix for the output .model and .vocab files \u0026#39;vocab_size\u0026#39;: 1200, # Desired size of the final vocabulary \u0026#39;model_type\u0026#39;: \u0026#39;unigram\u0026#39;, # Specifies the Unigram language model algorithm \u0026#39;character_coverage\u0026#39;: 0.9995, # Percentage of characters covered by the model (0.9995 is standard for Latin scripts) \u0026#39;byte_fallback\u0026#39;: True, # Enables mapping unknown characters to UTF-8 bytes to avoid \u0026lt;unk\u0026gt; tokens \u0026#39;split_digits\u0026#39;: True, # Treats each digit as an individual token (useful for numerical data) \u0026#39;add_dummy_prefix\u0026#39;: False # Prevents adding a leading space (SentencePiece default is True) } try: # 1. Train the SentencePiece model using the defined options print(\u0026#34;Starting Unigram training...\u0026#34;) spm.SentencePieceTrainer.train(**options_unigram) print(\u0026#34;Training complete. \u0026#39;unigram_model.model\u0026#39; created.\u0026#34;) # 2. Load the trained model into a processor instance for inference sp_unigram = spm.SentencePieceProcessor() sp_unigram.load(\u0026#39;unigram_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Unigram Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {sp_unigram.get_piece_size()}\u0026#34;) # 3. Define test cases to evaluate how the model handles common and rare words test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] # 4. Iterate through test sentences to visualize subword segmentation for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (Unigram) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: Converts text into subword strings (visual representation) print(f\u0026#34;Subword Tokens: {sp_unigram.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: Converts text into numerical indices for model input print(f\u0026#34;Numerical IDs: {sp_unigram.encode_as_ids(text)}\u0026#34;) except Exception as e: # Handle potential errors during training or loading (e.g., missing input file) print(f\u0026#34;An error occurred with Unigram model: {e}\u0026#34;) The output is as follows:\nUnigram Model Metadata: Total Vocab Size: 1200 --- Tokenization Test (Unigram) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ll\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;ld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311] --- Tokenization Test (Unigram) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;T\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅm\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;sta\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;ra\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ver\u0026#39;] ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"A comprehensive guide to tokenization strategies: BPE, WordPiece, Unigram, and SentencePiece.","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"A brief introduction to Vectors \u0026amp; Verbs and formatting verification.","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"\nFeedforward network (FFN) blocks are the least talked about when in context to LLMs. But that is where more than 70% of the paramters come from. Something as simple and convinient as a couple of linear transforming matrices become a burden when models start scaling in Billions of parameters. So, getting the maximum juice of a lean structure is a challenge. Plus, mostly under represented, because the FFN sub-layer acts as the primary \u0026ldquo;knowledge\u0026rdquo; processing engine, often containing the majority of a model\u0026rsquo;s total parameters.\n","permalink":"http://localhost:1313/posts/feedforward-networks/","summary":"Exploring the hidden layers of trillion-parameter switchboards: Feedforward Neural Networks and Activation Functions.","title":"Anatomy of Trillion-Parameter Switchboards: Understanding Feedforward Networks"},{"content":"\nWe are about to touch the holy grail of modern AI. The 2017 declaration was simple: Attention Is All You Need. But as we‚Äôve raced from GPT-1 to reasoning models, the definition of \u0026lsquo;Attention\u0026rsquo; has quietly transformed. Is the algorithm ruling the world today actually the same one we started with? Let‚Äôs look under the hood.\nüî• The Core Concept At its heart, the attention mechanism is a form of weighted aggregation. It is often described using a database retrieval analogy. Imagine you have a database of information (context).\nQuery ($Q$): What are you currently looking for? (The token being generated). Key ($K$): The indexing tag associated with each piece of information. Value ($V$): The actual information (The context token). The \u0026ldquo;Attention\u0026rdquo; is the process of matching your Query against all the Keys to determine how much \u0026ldquo;weight\u0026rdquo; to put on each Value. Attention scores are calculated in each transformer layer at every step of the sequence. This is a single unifying factor in all transformer models, from BERT to Gemini.\nüî• Standard Multi-Head Attention (MHA) The original mechanism introduced in the paper \u0026ldquo;Attention Is All You Need\u0026rdquo;.\nHere, instead of performing the Key-Query-Value (KQV) operation on the entire context only once, the attention is split into $N$ parallel heads. Each head learns a different aspect of the language (e.g., word meaning, syntax, semantic relationships, etc.). The equation is a simple culmination of dot products aided by a regularizer like Softmax.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nWhere:\n$QK^T$ computes the similarity scores between the Query and Key. $\\sqrt{d_k}$ is a scaling factor to prevent the dot product from growing too large, mitigating the vanishing/exploding gradient problem. The weighted sum is applied to the values of $V$. The Memory Bandwidth Bottleneck: \u0026gt; During inference, the model must store Key and Value matrices for all previous tokens. This is called the KV Cache. In MHA, every single head has its own unique set of Keys and Values. For large models with massive context windows, loading this massive KV cache from memory (HBM) to the computation chip (SRAM) becomes the primary bottleneck, slowing down generation significantly. Earlier models could often only generate ~8 tokens per second at inference due to this constraint.\nüî• Multi-Query Attention (MQA) This approach addresses the memory bottleneck by sharing the same set of Keys and Values across all heads while keeping separate Query heads.\nMHA: $H$ Query heads, $H$ Key heads, $H$ Value heads MQA: $H$ Query heads, 1 Key head, 1 Value head This drastically reduces the memory requirements of the KV cache by a factor of $H$ (often 8x or 16x). The GPU needs to load far less data, significantly speeding up inference. However, it does not reduce the computation (FLOPs)‚Äîso why is it faster?\nThis can be explained with the Kitchen Analogy: Imagine a kitchen (GPU) with 8 chefs (Heads).\nComputation (Math): The Chef chopping vegetables. Memory Access (Bandwidth): The Assistant running to the fridge to get ingredients. In MHA: Each chef demands unique vegetables from the fridge. The assistant runs back and forth 8 times, fetching 8 different crates. Most of the time is spent waiting for the assistant to return. This results in idle cores waiting for data to load from HBM.\nIn MQA: All chefs agree to use the same vegetables. The assistant runs back and forth only once, fetching one giant crate of potatoes and dumping it. This results in minimized memory transfer and no idle cores.\nThe Technical Reality: During text generation, the model is memory-bound, not compute-bound. The time it takes to move the KV Cache from HBM (High Bandwidth Memory) to the chip\u0026rsquo;s SRAM is the bottleneck. MQA reduces the data volume of that move by a factor of $H$.\nThe Capacity Gap: MQA is a trade-off between nuanced response and memory capacity. The model loses the ability to store different types of information, as a single $V$ matrix must now store all nuances of information flow in the language.\nThe Surgery Trick (Uptraining): Google Research figured out a way to take an existing model with MHA and convert it to MQA by averaging the weight matrices of $H$ heads. This is called Uptraining. You then train this \u0026ldquo;Frankenstein\u0026rdquo; model for ~5% of the original training steps for adjustments. This yields an MQA model that is nearly as good as the original MHA but vastly faster.\nüî• Grouped Query Attention (GQA) A \u0026ldquo;best of both worlds\u0026rdquo; solution used in modern models like Llama 2 and Llama 3.\nHow it works: GQA acts as an interpolation between MHA and MQA. Instead of having just 1 Key head (MQA) or $H$ Key heads (MHA), it divides the query heads into $G$ groups. Each group shares a single Key and Value head.\nEquation modification: If you have 8 query heads, you might create 4 groups (2 queries per group). This reduces the KV cache by 2x instead of 8x, preserving more quality than MQA while still speeding up inference.\nMHA: 8 people (heads) are working on a project. Each person has their own unique filing cabinet (KV cache). It takes forever to move 8 cabinets around. MQA: 8 people share one single filing cabinet. It\u0026rsquo;s very fast to move, but they fight over the organization and lose detail. GQA: The 8 people split into 4 teams of 2. Each team shares a cabinet. It\u0026rsquo;s a balance between speed and organizational detail. üî• Multi-Head Latent Attention (MLA) This is the most advanced mechanism for attention, used in models like DeepSeek-V3. It is a sophisticated design for massive efficiency that takes a radical approach: mathematically compressing the memory into a low-rank \u0026ldquo;latent\u0026rdquo; vector.\nKV Cache Explosion: As mentioned before, for a massive model like DeepSeek-V3 (671B parameters), storing the full Keys and Values for every token in a long conversation requires terabytes of high-speed memory (HBM), and chips spend more time waiting than churning.\nMHA (Standard) stores everything. MQA deletes redundancy but hurts quality. MLA keeps the quality of MHA but compresses the data 4x-6x smaller than even GQA.\nThe Zip File Approach: Low-Rank Joint Compression Let\u0026rsquo;s compare the math of MHA and MLA. In MHA, for every single token, we generate a unique Key ($k$) and Value ($v$) matrix for each attention head.\nThe Memory Problem: The $k$ is stored in the KV Cache. For 128 heads, for example, it calculates a massive vector of $128 \\times 128 = 16,384$ floats per token.\nFor MHA, the attention score looks like: $$\\text{Score} = q^T \\cdot k$$\nBut for MLA, instead of storing the full $k$, it projects the input down into a tiny latent vector $c_{KV}$, which is much smaller (e.g., 512 floats). $$c_{KV} = W_{DKV} \\cdot x$$\nNow here is the catch: Naive compression and decompression would mean constructing a small latent matrix for every token and deconstructing it every single time the information is required, which defeats the purpose of compression.\n$$k_{\\text{reconstructed}} = W_{UK} \\cdot c_{KV}$$ $$\\text{Score} = q^T \\cdot k_{\\text{reconstructed}}$$\nThe Matrix Absorption Trick (Optimization) Instead of reconstructing $k$ for every token, we can absorb the Up-Projection matrix ($W_{UK}$) into the Down-Projection matrix ($W_{DKV}$) and absorb the query $q$ into a new $Q$.\nFrom the original equation: $$\\text{Score} = q^T \\cdot (\\underbrace{W_{UK} \\cdot c_{KV}}_{\\text{This is } k})$$\nWe associate differently:\n$$\\text{Score} = (\\underbrace{q^T \\cdot W_{UK}}_{\\text{Absorbed Query}}) \\cdot c_{KV}$$ We change the order in which the matrix multiplication is performed.\nHow is this allowed? In linear algebra, matrix multiplication is associative:\n$$(A \\cdot B) \\cdot C = A \\cdot (B \\cdot C)$$ We can move the parentheses! Instead of grouping $(W_{UK} \\cdot c_{KV})$ to make the Key, we group $(q^T \\cdot W_{UK})$ to make a new Query.\nNote: The positional embedding information is left untouched. That rotational information from algorithms like RoPE is preserved outside the compression; it requires minimal space anyways.\n","permalink":"http://localhost:1313/posts/attention-mechanisms/","summary":"We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek\u0026rsquo;s MLA, how has the definition of \u0026lsquo;Attention\u0026rsquo; transformed?","title":"Attention Is All You Need, but exactly which one?: MHA, GQA and MLA"},{"content":"\nPositional Embeddings are the \u0026ldquo;voice\u0026rdquo; that tell Transformers where words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.\n\u0026ldquo;The dog chased the cat\u0026rdquo; and \u0026ldquo;cat chased the dog\u0026rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.\nüî• Sinusoidal Embeddings: The Odometer of Language Introduced in the seminal \u0026ldquo;Attention Is All You Need\u0026rdquo; paper, this was the industry standard for many years. This approach adds a vector to each word embedding based on its position. The vector is computed out of wavelike patterns that are periodic and smooth. Mathematically, for a position $pos$ and embedding dimension $i$:\n$$ PE_{(pos, i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $$\nThe model uses sine and cosine waves of different frequencies to capture both short and long-range dependencies. The frequency of the waves decreases as the embedding dimension increases, which helps capture long-range dependencies in the input sequence.\nThe creators wanted the model to calculate distance easily, which is why a linear function was used. The model simply rotates the current position vector using a rotation matrix to derive the next position.\n$$ \\begin{pmatrix} \\cos(k) \u0026 \\sin(k) \\\\ -\\sin(k) \u0026 \\cos(k) \\end{pmatrix} \\cdot PE(pos) \\approx PE(pos + k) $$ Low dimensions act like a fast-ticking clock (high frequency, changing rapidly) and high dimensions act like a slow clock (low frequency, changing slowly).\nThe number 10,000 is an arbitrarily chosen high number to reduce Aliasing. By choosing such a high number, the creators ensured that the wavelength of the slowest dimension is incredibly long. Consequently, the pattern practically never repeats, and each position remains unique.\nThis combination works like a mechanical odometer dial. Each subsequent dimension acts as a slower-moving gear. The model learns to read these \u0026ldquo;dials\u0026rdquo; to understand exactly where a word sits in the sequence.\nThis mitigated the challenge of exploding gradients that previous array-based positional embeddings suffered from (e.g., adding integers 1, 2, 3\u0026hellip;). This model keeps values between -1 and +1, adding stability while ensuring every position gets a distinct code.\nThe Problem: This approach pollutes the word embedding matrix (semantic information). While high-dimensional vectors are almost always orthogonal‚Äîmeaning the noise doesn\u0026rsquo;t fully destroy the word\u0026rsquo;s identity‚Äîit does not strictly guarantee purity.\nThis led to a more mature approach.\nüî• ALiBi: The Fading Streetlight Attention with Linear Biases (ALiBi) is a method where, instead of adding a vector to the embeddings at the start, the model injects positional information linearly, directly into the attention scores. If Sinusoidal Encoding asks, \u0026ldquo;Where am I on the map?\u0026rdquo;, ALiBi asks, \u0026ldquo;How much harder should I squint to see you?\u0026rdquo;\nThe biggest problem ALiBi solved is the \u0026ldquo;Invisible Wall\u0026rdquo;. If you train a Transformer on sequences of length 2048 and then feed it a book with 3000 words, it will crash or output garbage at word 2049. This is a failure of Extrapolation.\nALiBi throws away the idea of adding positional vectors entirely. Instead, it devises a \u0026ldquo;penalty\u0026rdquo; system for attention scores:\n$$ \\text{Score} = (q \\cdot k^\\top) - (m \\cdot \\text{distance}) $$\nDistance: If words A and B are neighbors, the distance is 0, so the penalty is 0. Although the distance is linear, it does not cause exploding gradients because the raw attention scores are normalized by a softmax function. This distance prioritizes nearby words (local context) over distant ones.\nSlope ($m$): If we always penalize distance equally, the model becomes myopic. ALiBi mitigates this by assigning a different slope value to each attention head:\nHead 1 (The Historian): Very low slope. The penalty grows slowly, allowing it to see far back. Head 2 (The Reader): Medium slope. Head 3 (The Myopic): Very high slope (e.g., 0.5); it effectively cannot see beyond 2-3 words. This mimics holding a lantern. Words close to you are bright, while words further away naturally fade into darkness.\nExtrapolation: If you walk 5 miles down the road (a longer sequence), your lantern works exactly the same way. The physics of light decay are constant, allowing the model to handle sequences longer than it was trained on.\nHowever, ALiBi forces a \u0026ldquo;sliding window\u0026rdquo; view where distant words eventually become invisible. This brings us to the modern standard.\nüî• RoPE: The Rotary Revolution Rotary Position Embeddings (RoPE) is the current industry standard (used in Llama 3, Mistral, PaLM) because it mathematically unifies \u0026ldquo;absolute position\u0026rdquo; with \u0026ldquo;relative position.\u0026rdquo; This method rotates the vectors in the Query and Key matrices based on their absolute position.\nIf a vector has 512 dimensions, RoPE breaks it into 256 pairs. Each pair is rotated by a different frequency:\n$$ \\theta_i = 10000^{-2i/d} $$\nThe First pair ($i = 0$) is rotated by the highest frequency. The Last pair ($i = 255$) is rotated by the lowest frequency. This spread ensures the model has \u0026ldquo;high precision\u0026rdquo; (fast rotation) to distinguish immediate neighbors, and \u0026ldquo;low precision\u0026rdquo; (slow rotation) to track global position without the pattern repeating.\nDoesn\u0026rsquo;t rotation change meaning? Yes and No. The vector for \u0026ldquo;Apple\u0026rdquo; at position 1 and \u0026ldquo;Apple\u0026rdquo; at position 256 will look completely different in terms of coordinates. However, rotation does not change the semantic strength (magnitude/norm) of the vector.\nThe Relativity Trick: Transformers compare words using dot products (angles). RoPE relies on the fact that the attention mechanism ignores the absolute rotation and only detects the difference in rotation (relative position). Thus, the \u0026ldquo;relative meaning\u0026rdquo; is encoded purely into the angle between words, leaving the semantic \u0026ldquo;magnitude\u0026rdquo; untouched.\nüî• LongRoPE: The Bifocal Lens While RoPE is powerful, it struggles when stretching context windows from 4k to 2 million tokens. The \u0026ldquo;fast\u0026rdquo; dimensions spin so rapidly over long distances that they become random noise. LongRoPE solves this using an evolutionary search algorithm to find a unique scaling factor ($\\lambda$) for each dimension.\nThe Equations Instead of rotating by the standard $\\theta_i$, LongRoPE rotates by a rescaled frequency:\n$$ \\theta\u0026rsquo;_i = \\lambda_i \\theta_i $$\nTo efficiently find these $\\lambda$ values without searching an infinite space, the algorithm enforces a monotonicity constraint based on NTK theory:\n$$ \\lambda_i \\le \\lambda_{i+1} $$\nThis ensures that low-frequency dimensions (global context) are stretched more than high-frequency dimensions (local context), creating a \u0026ldquo;bifocal\u0026rdquo; effect:\nHigh Frequencies (Local): Kept sharp ($\\lambda \\approx 1$) to maintain grammar. Low Frequencies (Global): Stretched ($\\lambda \u0026gt; 1$) to track massive distances without repeating. Not worth perusing more since the next one is much better at long context positional encoding. üî• HoPE: The Hyperbolic Slide Hyperbolic Rotary Positional Encoding (HoPE) moves from the geometry of a circle to the geometry of a hyperbola (inspired by Lorentz transformations). It was designed to fix a subtle flaw in RoPE: the \u0026ldquo;wobbly\u0026rdquo; or oscillatory nature of attention scores.\nThe Core Problem: RoPE\u0026rsquo;s \u0026ldquo;Wobble\u0026rdquo; In standard RoPE, we rotate vectors around a circle.\nThe Issue: Circles repeat. A dial at $361^\\circ$ looks identical to $1^\\circ$. Even with different frequencies, the dot product fluctuates up and down as distance increases. The Consequence: The model gets confused at long distances, potentially mistaking a word 1,000 tokens away for a neighbor just because the rotation cycles aligned, creating \u0026ldquo;noise.\u0026rdquo; The \u0026ldquo;Hyperbolic\u0026rdquo; Shift HoPE replaces trigonometric functions ($\\sin, \\cos$) with hyperbolic functions ($\\sinh, \\cosh$).\nCircular Rotation: Keeps distance from the center constant. Hyperbolic Rotation: Moves points along a hyperbola. Crucially, hyperbolic functions do not repeat; they grow or decay exponentially. By using hyperbolic rotation, HoPE guarantees that as words get further apart, the attention score decays monotonically (smoothly drops).\nThe Equations HoPE uses a hyperbolic matrix $B(\\theta, m)$ defined using the Lorentz Boost structure:\n$$ B(\\theta, m) = \\begin{pmatrix} \\cosh(m\\theta) \u0026 \\sinh(m\\theta) \\\\ \\sinh(m\\theta) \u0026 \\cosh(m\\theta) \\end{pmatrix} $$ Since hyperbolic functions grow to infinity, HoPE introduces a decay penalty ($e^{-m\\theta'}$). The final attention score simplifies beautifully to a function of relative distance: $$ \\text{Score} \\propto e^{-|m-n|(\\theta\u0026rsquo; - \\theta)} $$\nThis ensures the attention score exponentially decays as distance increases, eliminating the wobbles of standard RoPE. This the latest model that is taking up the space in the industry for extremely long context models.\n","permalink":"http://localhost:1313/posts/positional-embeddings/","summary":"From Sinusoidal to RoPE and HoPE: How Transformers learn to process word order and sequence length.","title":"The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE"},{"content":"Imagine trying to teach a computer the difference between \u0026ldquo;Apple\u0026rdquo; the fruit and \u0026ldquo;Apple\u0026rdquo; the company. To us, the distinction is intuitive. To a machine, it‚Äôs just a string of characters. How do we turn these strings into meaningful math? While early attempts like Word2Vec gave us a great start, they missed the forest for the trees‚Äîor in some cases, the twigs for the branches. Enter GloVe and FastText: two algorithms that revolutionized how machines understand the nuances of human language.\nPrevious static embedding models like Word2Vec successfully captured the local semantics of words, but they failed in addressing contexts between words that might not always appear in the same context window. Furthermore, they treated words as indivisible atomic units in continuous vector spaces, missing out on the internal structure of language.\nThe two simple but effective ideas behind GloVe and FastText are subword units and global co-occurrence. Let\u0026rsquo;s dive into them.\nGloVe: A Global Mapping Strategy Global Vectors for Word Representation (GloVe) was created at Stanford. The core idea was to train a model where word vectors are defined by how often they appear around other words across the entire corpus, rather than just locally.\nTraining Steps: Co-occurrence Matrix X: GloVe scans the entire corpus and creates a giant matrix (spreadsheet) recording the counts of how often words appear near each other. Factorization: It compresses this giant matrix into smaller, dense vectors. Objective: The function tries to equate the dot product of two word vectors with the logarithm of their probability of appearing together. The error between these values is then backpropagated to update the vectors. $$w_i \\cdot w_j + b_i + b_j = \\log(X_{ij})$$\n$w$: The word vector (what we want). $X_{ij}$: The count from our matrix. The Loss Function: We can\u0026rsquo;t satisfy the equation above perfectly for every word pair. So, we use a weighting function to minimize the squared error:\n$$J = \\sum_{i,j=1}^V f(X_{ij}) ({w_i}^T w_j + b_i + b_j - \\log(X_{ij}))^2$$\nIf we do not use the weighting function, the loss would be infinite for rare words (since $\\log(0) = \\infty$) and frequent words (like \u0026rsquo;the\u0026rsquo;) would overpower the model.\n$$ f(x) = \\begin{cases} (x/x_{max})^\\alpha \u0026amp; \\text{if } x \u0026lt; x_{max} \\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$\n(Typically $x_{max}=100$ and $\\alpha=0.75$)\nFastText: The Lego Bricks Strategy FastText was created by Facebook AI Research (FAIR). The idea was to treat words not as solid rocks, but as structures made of \u0026ldquo;Lego bricks\u0026rdquo; (subword units).\nWord2Vec struggled to create embeddings for words it hadn\u0026rsquo;t seen before (Out-Of-Vocabulary or OOV words). FastText solves this by breaking words down.\nGloVe treats a word like \u0026ldquo;Apple\u0026rdquo; as a single, atomic unit. FastText breaks \u0026ldquo;Apple\u0026rdquo; into character n-grams: \u0026lt;ap, app, ppl, ple, le\u0026gt; For a complex or new word like \u0026ldquo;Applesauce\u0026rdquo;, the model might not know the whole word, but it recognizes the Lego brick for \u0026ldquo;Apple\u0026rdquo; and the remaining bits from other subwords.\nFastText takes its inspiration from the SkipGram architecture. The vector for a word is simply the sum of the vectors of its n-gram subwords:\n$$v_w = \\sum_{g \\in G_w} z_g$$\nObjective: The objective is to predict context words given a target word.\n$$P(w_c | w_t) = \\frac{e^{s(v_{w_t} \\cdot v_{w_c})}}{\\sum_{w \\in V} e^{s(v_{w_t} \\cdot v_w)}}$$\nAnd the scoring function is defined as:\n$$s(w_t, w_c) = u_{w_t}^T (\\sum_{g \\in G_w} z_g)$$\n$u_{w_c}$: The vector for the context word (output vector). $z_g$: The vector for the n-gram $g$ (input vector). Modifications to Softmax: Calculating the full softmax for every word in the vocabulary is computationally expensive. FastText uses two methods to approximate this efficiently.\nOption A: Negative Sampling\nWe pick a few random \u0026ldquo;wrong\u0026rdquo; words every time to serve as \u0026ldquo;negative samples\u0026rdquo; and calculate the loss only for the correct word and these noise words.\n$$J = - \\log \\sigma(s(w_t, w_c)) - \\sum\\limits_{i=1}^{N} \\mathbb{E}_{n_i \\sim P_n(w)} [\\log \\sigma(-s(w_t, n_i))]$$\nThe sigmoid function $\\sigma$ squashes the score between 0 and 1. The first term pushes the probability of the correct word towards 1. The second term pushes the probability of the noise words towards 0. Option B: Hierarchical Softmax (Faster for infrequent words)\nInstead of a flat list of words, imagine the vocabulary arranged as a binary tree (Huffman Tree).\nRoot: Top of the tree. Leaves: The actual words. To calculate the probability of a word, we trace a path from the root to that leaf. At every branching node, we calculate the probability (sigmoid) of going left or right. The probability of a word $w$ is the product of the probabilities of the turns taken to reach it. Code Example from gensim.models import FastText from gensim.test.utils import common_texts # Initialize the FastText model with specific hyperparameters model = FastText( vector_size=100, # Dimensionality of the word vectors window=5, # Maximum distance between the current and predicted word within a sentence min_count=1, # Ignores all words with total frequency lower than this sg=1, # Training algorithm: 1 for skip-gram; 0 for CBOW hs=0, # If 0, and negative is non-zero, negative sampling will be used negative=5, # Number of \u0026#34;noise words\u0026#34; to be drawn for negative sampling workers=4, # Number of worker threads to train the model epochs=10, # Number of iterations over the corpus min_n=3, # Minimum length of character n-grams max_n=6 # Maximum length of character n-grams ) # Build the vocabulary from the provided text corpus model.build_vocab(common_texts) # Train the model on the corpus model.train( common_texts, total_examples=model.corpus_count, epochs=model.epochs ) # Calculate and print the cosine similarity between two words in the vocabulary print(f\u0026#34;Similarity between \u0026#39;computer\u0026#39; and \u0026#39;human\u0026#39;: {model.wv.similarity(\u0026#39;computer\u0026#39;, \u0026#39;human\u0026#39;)}\u0026#34;) # Demonstrate FastText\u0026#39;s ability to handle Out-Of-Vocabulary (OOV) words # Even if \u0026#39;computation\u0026#39; wasn\u0026#39;t in the training data, FastText constructs a vector using character n-grams oov_vector = model.wv[\u0026#39;computation\u0026#39;] # Check if a specific word exists in the model\u0026#39;s fixed vocabulary index word = \u0026#34;computer\u0026#34; is_in_vocab = word in model.wv.key_to_index print(f\u0026#34;\\nIs the word \u0026#39;{word}\u0026#39; in the model\u0026#39;s vocabulary? {is_in_vocab}\u0026#34;) ","permalink":"http://localhost:1313/posts/glove-and-fasttext/","summary":"How subword units and global co-occurrence matrices allow GloVe and FastText to capture nuances that Word2Vec missed.","title":"The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText"},{"content":" Before we had Large Language Models writing poetry, we had to teach computers that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; are related not just by spelling, but by meaning. This is the story of that breakthrough. It‚Äôs the moment we stopped counting words and started mapping their souls‚Äîturning raw text into a mathematical landscape where math can solve analogies. Welcome to the world of Word2Vec. üîÆ\nLanguage models require vector representations of words to capture semantic relationships. Before the 2010s, models used word count-based vector representations that captured only the frequency of words (e.g., One-Hot Encoding). The Problems: üöß\nThe Curse of Dimensionality Lack of meaning (Synonyms were treated as mathematically unrelated). In 2011, Mikolov et al. at Google introduced Word2Vec, which used a shallow neural network to learn vector representations of words. The best part? It could create much denser relationships between words than prior models, and it was unsupervised.\nThe training (prediction) is essentially \u0026ldquo;fake\u0026rdquo;‚Äîa pretext task. But the weight matrices we get on the side are a gold mine of fine-grained semantic relationships. ‚õèÔ∏è\nThere are two popular variants of Word2Vec:\nSkipGram Continuous Bag Of Words (CBOW) 1. Continuous Bag of Words (CBOW) üß© CBOW is a simple \u0026lsquo;fill in the blanks\u0026rsquo; machine. It takes the surrounding words as inputs and tries to predict the center word.\n\u0026ldquo;Continuous\u0026rdquo;: It operates in continuous vector space (unlike the discrete space of n-gram models).\n\u0026ldquo;Bag of words\u0026rdquo;: The order of the context does not matter. \u0026ldquo;The cat sat on the mat\u0026rdquo; and \u0026ldquo;The mat sat on the cat\u0026rdquo; are the same for CBOW. They produce exactly the same predictions.\nObjective: Maximize the probability of the target word $w_t$ given its context $w_{t-C}, \\dots, w_{t+C}$.\n2. Training Defining Dimensions:\n$V$: Vocabulary size (number of unique words in the corpus, e.g., 10,000) $N$: Dimension of the embedding space (e.g., 100) $C$: Number of context words (e.g., 2 words on left of target and 2 words on right) Step 1: Input Lookup \u0026amp; One-Hot Encoding\nMathematically, we represent each context word as a one-hot vector $x^{(c)}$. We have an Input Matrix $W_{in}$ of size $V \\times N$. For each context word (e.g., \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;), we select its corresponding row from $W_{in}$.\n$$ v_c = W_{in}^T x^{(c)} $$\nWhere $v_c$ is the $N$-dimensional vector for context word $c$.\nStep 2: Projection (The \u0026ldquo;Mixing\u0026rdquo;)\nWe combine the context vectors. In standard CBOW, we simply average them. This creates the hidden layer vector $h$.\n$$ h = \\frac{1}{2C} \\sum_{c=1}^{2C} v_c $$\n$h$ is a single vector of size $N \\times 1$. Note: This operation is linear and contains no activation function (like ReLU or Sigmoid).\nStep 3: Output Scoring (The \u0026ldquo;Dot Product\u0026rdquo;)\nWe have a second matrix, the Output Matrix $W_{out}$ of size $N \\times V$. We compute the raw score (logit) $u_j$ for every word $j$ in the vocabulary by taking the dot product of the hidden vector $h$ and the output vector $v\u0026rsquo;_j$.\n$$ u_j = {v\u0026rsquo;_j}^T h $$\nOr in matrix form:\n$$ u = W_{out}^T h $$\nResult $u$ is a vector of size $V \\times 1$.\nStep 4: Probability Conversion (Softmax)\nWe convert the raw scores $u$ into probabilities using the Softmax function. This tells us the probability that word $w_j$ is the center word.\n$$ y_j = P(w_j | \\text{context}) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 5: Loss Calculation (Cross-Entropy)\nWe compare the predicted distribution $y$ with the actual ground truth $t$ (a one-hot vector where the index of the true word is 1). The loss function $E$ is:\n$$ E = - \\sum_{j=1}^V t_j \\log(y_j) $$\nSince $t_j$ is 0 for all words except the target word (let\u0026rsquo;s call the target index $j^*$), this simplifies to:\n$$ E = - \\log(y_{j^*}) $$\n3. Backpropagation Details We need to update the weights to minimize $E$. We use the Chain Rule.\nA. Gradient w.r.t. Output Matrix ($W_{out}$)\nWe want to know how the error changes with respect to the raw score $u_j$.\n$$ \\frac{\\partial E}{\\partial u_j} = y_j - t_j $$\nLet\u0026rsquo;s call this error term $e_j$.\nIf $j$ is the target word: $e_j = y_j - 1$ (Result is negative; we boost the score). If $j$ is not target: $e_j = y_j - 0$ (Result is positive; we suppress the score). The update rule for the output vector $v\u0026rsquo;_j$ becomes:\n$$ v'_{j}(\\text{new}) = v'_{j}(\\text{old}) - \\eta \\cdot e_j \\cdot h $$ ($\\eta$ is the learning rate)\nB. Gradient w.r.t. Hidden Layer ($h$)\nWe backpropagate the error from the output layer to the hidden layer.\n$$ EH = \\sum_{j=1}^V e_j \\cdot v\u0026rsquo;_j $$\n$EH$ is an $N$-dimensional vector representing the aggregate error passed back to the projection layer.\nC. Gradient w.r.t. Input Matrix ($W_{in}$)\nSince $h$ was just an average of the input vectors, the error $EH$ is distributed to each context word\u0026rsquo;s input vector. For every word $w_c$ in the context:\n$$ v_{w_c}(\\text{new}) = v_{w_c}(\\text{old}) - \\eta \\cdot \\frac{1}{2C} \\cdot EH $$\nLet\u0026rsquo;s build a single pass through the network for a given context.\nimport torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # Define a simple Continuous Bag of Words (CBOW) style model class CBOWModel(nn.Module): def __init__(self, vocab_size, embedding_dim): super(CBOWModel, self).__init__() # The embedding layer stores the word vectors we want to learn self.embeddings = nn.Embedding(vocab_size, embedding_dim) # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word self.linear = nn.Linear(embedding_dim, vocab_size) def forward(self, inputs): # inputs: tensor of word indices for the context embeds = self.embeddings(inputs) # Aggregate context by calculating the mean of the word embeddings h = torch.mean(embeds, dim=1) # Produce logits (raw scores) for each word in the vocabulary logits = self.linear(h) return logits # Setup vocabulary and mappings word_to_ix = {\u0026#34;the\u0026#34;: 0, \u0026#34;quick\u0026#34;: 1, \u0026#34;brown\u0026#34;: 2, \u0026#34;fox\u0026#34;: 3, \u0026#34;jumps\u0026#34;: 4, \u0026#34;over\u0026#34;: 5, \u0026#34;lazy\u0026#34;: 6, \u0026#34;dog\u0026#34;: 7} ix_to_word = {v: k for k, v in word_to_ix.items()} # Configuration constants EMBEDDING_DIM = 5 VOCAB_SIZE = len(word_to_ix) LEARNING_RATE = 0.01 # Initialize the model, loss function, and optimizer model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM) loss_function = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE) # Prepare dummy training data: context words and the target word # Context: [\u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;over\u0026#34;] -\u0026gt; Target: \u0026#34;fox\u0026#34; context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long) target_idxs = torch.tensor([3], dtype=torch.long) # Perform a single optimization step model.zero_grad() # Forward pass: we reshape context to (1, -1) to simulate a batch of size 1 logits = model(context_idxs.view(1, -1)) # Calculate loss against the target word index loss = loss_function(logits, target_idxs) # Backpropagate and update weights loss.backward() optimizer.step() # Output results for the blog post print(\u0026#34;=== Model Training Snapshot ===\u0026#34;) print(f\u0026#34;Calculated Loss: {loss.item():.6f}\u0026#34;) print(f\u0026#34;\\n=== Learned Vector for \u0026#39;jumps\u0026#39; ===\u0026#34;) word_vec = model.embeddings(torch.tensor([word_to_ix[\u0026#39;jumps\u0026#39;]])) print(word_vec.detach().numpy()) print(\u0026#34;\\n=== Embedding Matrix (Weights) ===\u0026#34;) print(model.embeddings.weight.detach().numpy()) print(\u0026#34;\\n=== Linear Layer Weights ===\u0026#34;) print(model.linear.weight.detach().numpy()) Output shows single pass loss and learned vector for \u0026lsquo;jumps\u0026rsquo; word. This is a single training sample result but after multiple such passes both Embedding and linear weights should be very similar.\n=== Model Training Snapshot === Calculated Loss: 1.989656 === Learned Vector for \u0026#39;jumps\u0026#39; === [[-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775]] === Embedding Matrix (Weights) === [[ 1.66039 -0.11371879 0.6246518 0.35860053 -1.2504417 ] [-2.1847186 -0.77199775 -0.17050214 -0.38411248 -0.03913084] [ 0.11852697 0.90073633 0.8847807 0.7404524 0.900149 ] [ 0.07440972 -0.40259898 2.6246994 -0.08851447 0.02660969] [-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775] [-0.9245572 -0.5545908 0.9083091 -1.0755049 0.84047747] [-1.0237687 0.59466314 0.05621134 -0.6202532 1.3664424 ] [ 0.60998917 -1.0549186 1.6103884 0.8724912 -1.2486908 ]] === Linear Layer Weights === [[ 0.02165058 -0.28883642 0.14545658 -0.3442509 0.32704315] [-0.18731792 0.28583744 0.22635977 0.13245736 0.29019794] [ 0.3158916 -0.15826383 -0.03203773 0.16377363 -0.41457543] [-0.05080034 0.4180087 0.11228557 0.30218413 0.3025514 ] [-0.38419306 0.24475925 -0.39210224 -0.38660625 -0.2673145 ] [-0.32321444 0.12200444 -0.03569533 0.2891424 -0.07345333] [-0.33704326 0.2521956 0.31587374 0.22590035 0.29866052] [ 0.4117266 -0.44231793 0.24064957 -0.29684234 0.333821 ]] Interesting Facts About CBOW üß† The \u0026ldquo;Averaging Problem\u0026rdquo;: This is a common issue in CBOW models. The averaging and training is order-agnostic. All variants of word order in the context window will generate the same output. \u0026ldquo;Dog bit the man\u0026rdquo; and \u0026ldquo;Man bit the dog\u0026rdquo; are the same in CBOW\u0026rsquo;s eyes.\nSmoothing Effect: CBOW models are much faster to train than Skip-Gram but are generally smoother.\nRare Words: CBOW models struggle slightly with rare words because the presence of surrounding common words (like articles and prepositions) can skew the embeddings and dissolve their uniqueness.\nComputational Cost: Softmax requires summing over the entire vocabulary size, which can be computationally expensive if $V$ is very large. Solutions include Hierarchical Softmax and Negative Sampling (approximating the denominator by only checking the target word vs. 5 random noise words).\nVectors as Double Agents: Every word has a different representation in the embedding matrix and the output matrix. Most of the time $W_{out}$ is discarded, but some consider averaging both $W_{in}$ and $W_{out}$ for slightly better performance.\nLinear Relationships: The famous analogy King - Man + Woman = Queen emerges from the model because of the linear relationships between the words.\nInitialization: This is a no-brainer but worth mentioning. Initialization to zero vectors would mean no learning. We must initialize randomly.\n2. Skip-Gram (The \u0026ldquo;Burger Flip\u0026rdquo; of CBOW) üîÑ Skip-Gram flips the CBOW logic. While CBOW tries to predict the center word given the context, Skip-Gram tries to predict the scattered context given the center word.\nCore Concept: If a word like \u0026ldquo;monarch\u0026rdquo; can accurately predict words like \u0026ldquo;king\u0026rdquo;, \u0026ldquo;throne\u0026rdquo;, \u0026ldquo;crown\u0026rdquo;, \u0026ldquo;royalty\u0026rdquo;, and \u0026ldquo;power\u0026rdquo;, then it must effectively capture the concept of royalty.\nTraining Let\u0026rsquo;s define dimensions:\n$V$: Vocabulary size $D$: Embedding dimension $C$: Number of context words Step 1: Input Lookup\nUnlike CBOW, our input is just one word vector (the center word $w_t$). We grab the vector $v_c$ from the Input Matrix $W_{in}$.\n$$ h = v_{w_t} $$\nNote: There is no averaging here. The hidden layer $h$ is simply the raw vector of the center word.\nStep 2: Output Scoring (The Broadcast)\nThe model takes this single vector $h$ and compares it against the Output Matrix $W_{out}$ (the entire vocabulary). It computes a score $u_j$ for every word $j$ in the dictionary.\n$$ u = W_{out}^T h $$\nResult $u$: A vector of size $V \\times 1$ containing raw scores.\nStep 3: Probability Conversion (Softmax)\nWe apply Softmax to turn scores into probabilities.\n$$ P(w_j | w_t) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 4: The Multi-Target Loss\nHere is the major difference from CBOW. In CBOW, we had one target (the center). In Skip-Gram, we have $2C$ targets (all the surrounding words). For a sentence \u0026ldquo;The cat sat on mat\u0026rdquo; (Center: \u0026ldquo;sat\u0026rdquo;), the targets are \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;. We want to maximize the probability of all these true context words. The Loss function ($E$) sums the error for each context word:\n$$ E = - \\sum_{w_{ctx} \\in \\text{Window}} \\log P(w_{ctx} | w_t) $$\nStep 5: Backpropagation (Accumulating Gradients)\nSince one input word is responsible for predicting multiple output words, the error signals from all those context words add up.\nAt the Output Layer:\nIf \u0026ldquo;cat\u0026rdquo; was a target, the \u0026ldquo;cat\u0026rdquo; vector in $W_{out}$ gets a signal: \u0026ldquo;Move closer to \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; If \u0026ldquo;apple\u0026rdquo; was NOT a target, the \u0026ldquo;apple\u0026rdquo; vector gets a signal: \u0026ldquo;Move away from \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; At the Hidden Layer ($h$):\nThe center word \u0026ldquo;sat\u0026rdquo; receives feedback from all its neighbors simultaneously. Error signal = (Error from \u0026ldquo;The\u0026rdquo;) + (Error from \u0026ldquo;cat\u0026rdquo;) + (Error from \u0026ldquo;on\u0026rdquo;)\u0026hellip; The vector for \u0026ldquo;sat\u0026rdquo; in $W_{in}$ moves in the average direction of all its neighbors. 3. The \u0026ldquo;Computational Nightmare\u0026rdquo; \u0026amp; Negative Sampling The equations above describe Naive Softmax.\nThe Problem: If $V = 100,000$, computing the denominator $\\sum \\exp(u_k)$ takes 100,000 operations. Doing this for every word in a training set of billions of words is impossibly slow.\nThe Solution: Negative Sampling Instead of updating the entire vocabulary (1 correct word vs 99,999 wrong ones), we approximate the problem.\nPositive Pair: (sat, cat) $\\rightarrow$ Maximize probability (Label 1). Negative Pairs: We pick $K$ (e.g., 5) random words the model didn\u0026rsquo;t see, e.g., (sat, bulldozer), (sat, quantum). $\\rightarrow$ Minimize probability (Label 0). New Equation (Sigmoid instead of Softmax):\n$$ E = - \\log \\sigma({v'_{\\text{pos}}}^T v_{\\text{in}}) - \\sum_{k=1}^K \\log \\sigma(-{v'_{\\text{neg}_{k}}}^T v_{\\text{in}}) $$ Effect: We typically only update 6 vectors per step (1 pos + 5 negs) instead of 100,000.\nLet\u0026rsquo;s see this in code:\nimport torch import torch.nn as nn import torch.optim as optim class SkipGramNegativeSampling(nn.Module): \u0026#34;\u0026#34;\u0026#34; Skip-Gram with Negative Sampling (SGNS) implementation. SGNS approximates the softmax over the entire vocabulary by instead distinguishing between a real context word (positive) and K noise words (negative). \u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embedding_dim): super(SkipGramNegativeSampling, self).__init__() # Input embeddings: used when the word is the center word self.in_embed = nn.Embedding(vocab_size, embedding_dim) # Output embeddings: used when the word is a context or negative sample self.out_embed = nn.Embedding(vocab_size, embedding_dim) # Initialize weights with small values to prevent gradient saturation initrange = 0.5 / embedding_dim self.in_embed.weight.data.uniform_(-initrange, initrange) self.out_embed.weight.data.uniform_(-initrange, initrange) def forward(self, center_words, target_words, negative_words): \u0026#34;\u0026#34;\u0026#34; Computes the negative sampling loss. Args: center_words: (batch_size) target_words: (batch_size) negative_words: (batch_size, K) where K is number of negative samples \u0026#34;\u0026#34;\u0026#34; # Retrieve vectors v_c = self.in_embed(center_words) # (batch_size, embed_dim) u_o = self.out_embed(target_words) # (batch_size, embed_dim) u_n = self.out_embed(negative_words) # (batch_size, K, embed_dim) # 1. Positive Score: log(sigmoid(v_c ¬∑ u_o)) # Compute dot product: (batch, 1, dim) @ (batch, dim, 1) -\u0026gt; (batch, 1) pos_score = torch.bmm(u_o.unsqueeze(1), v_c.unsqueeze(2)).squeeze(2) pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-7) # 2. Negative Score: sum(log(sigmoid(-v_c ¬∑ u_n))) # Compute dot products for all K samples: (batch, K, dim) @ (batch, dim, 1) -\u0026gt; (batch, K) neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2) neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-7), dim=1, keepdim=True) # Total loss is the negative of the objective function loss = -(pos_loss + neg_loss) return torch.mean(loss) # --- Configuration \u0026amp; Mock Data --- VOCAB_SIZE = 100 EMBED_DIM = 10 word_to_ix = {\u0026#39;fox\u0026#39;: 0} # Example vocabulary mapping model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock inputs: 1 center word, 1 target word, 5 negative samples center_id = torch.tensor([0], dtype=torch.long) target_id = torch.tensor([1], dtype=torch.long) negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long) # Training Step model.zero_grad() loss = model(center_id, target_id, negative_ids) loss.backward() optimizer.step() # Output Results print(f\u0026#34;Loss after one step: {loss.item():.6f}\u0026#34;) word_vec = model.in_embed(torch.tensor([word_to_ix[\u0026#39;fox\u0026#39;]])) print(f\u0026#34;Vector for \u0026#39;fox\u0026#39;:\\n{word_vec.detach().numpy()}\u0026#34;) The outputs:\nLoss after one step: 4.158468 Vector for \u0026#39;fox\u0026#39;: [[ 0.04197385 0.02400733 -0.03800093 0.01672485 -0.03872231 -0.0061478 0.0121122 0.04057864 -0.036255 0.03861175]] Interesting Facts About Skip-Gram üí° Makes Rare Words Shine: Skip-Gram is more effective at capturing the meaning of rare words because it focuses on the context of the center word. In CBOW, a rare word\u0026rsquo;s vector is averaged with others, diluting its signal. In Skip-Gram, when the rare word appears as the center, it gets the full, undivided attention of the backpropagation update.\nSlow Training: This method creates more training samples. A window of size 2 creates 4 training pairs per center word, while CBOW creates only 1 training pair per center word.\nSemantic Focus: Skip-Gram puts more emphasis on the semantic relationship between words. It views the context as the primary signal, with the center word as the target. This makes it better at capturing the meaning of words in context. Skip-Gram tends to capture semantic relationships (King/Queen) slightly better, while CBOW captures syntactic ones (walk/walked) slightly better.\n","permalink":"http://localhost:1313/posts/word-embeddings/","summary":"Understanding the mathematics behind Word2Vec, CBOW, and Skip-Gram and how they map language to vector space.","title":"Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n5. Code Examples: We\u0026rsquo;ll use sentencepiece for tokenization and compare results for different models.\nimport sentencepiece as spm import os # Configuration for training the SentencePiece model # SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words. options = { # The source text file used for learning the vocabulary \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # The base name for the generated model (.model) and vocabulary (.vocab) files \u0026#39;model_prefix\u0026#39;: \u0026#39;bpe_model\u0026#39;, # Number of unique tokens in the final vocabulary \u0026#39;vocab_size\u0026#39;: 4000, # \u0026#39;bpe\u0026#39; (Byte Pair Encoding) merges frequent pairs of characters/sequences \u0026#39;model_type\u0026#39;: \u0026#39;bpe\u0026#39;, # Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets \u0026#39;character_coverage\u0026#39;: 0.9995, # When enabled, unknown characters are decomposed into UTF-8 bytes to avoid \u0026#39;unk\u0026#39; tokens \u0026#39;byte_fallback\u0026#39;: True, # Treats digits individually (0-9), preventing large numbers from being treated as single tokens \u0026#39;split_digits\u0026#39;: True, # Prevents adding a whitespace prefix to the first token; useful for fine-grained control \u0026#39;add_dummy_prefix\u0026#39;: False } try: print(\u0026#34;Starting the training process...\u0026#34;) # SentencePieceTrainer.train takes the dictionary of options to build the BPE model spm.SentencePieceTrainer.train(**options) print(\u0026#34;Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created.\u0026#34;) # Initialize the processor and load the newly trained model sp = spm.SentencePieceProcessor() sp.load(\u0026#39;bpe_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Model Metadata:\u0026#34;) # Retrieve the total number of tokens in the vocabulary print(f\u0026#34;Total Vocab Size: {sp.get_piece_size()}\u0026#34;) # Special tokens are used for sequence boundaries and handling unknown characters print(f\u0026#34;BOS (Beginning of Sentence) ID: {sp.bos_id()}\u0026#34;) print(f\u0026#34;EOS (End of Sentence) ID: {sp.eos_id()}\u0026#34;) print(f\u0026#34;UNK (Unknown) ID: {sp.unk_id()}\u0026#34;) print(f\u0026#34;PAD (Padding) ID: {sp.pad_id()}\u0026#34;) # Test the tokenizer on sample strings to see how it breaks down text test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: shows the actual subword units (tokens) print(f\u0026#34;Subword Tokens: {sp.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: shows the numerical mapping for each token print(f\u0026#34;Numerical IDs: {sp.encode_as_ids(text)}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred during training or processing: {e}\u0026#34;) You can see the following output:\nStarting training... Starting the training process... Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created. ------------------------------ Model Metadata: Total Vocab Size: 4000 BOS (Beginning of Sentence) ID: 1 EOS (End of Sentence) ID: 2 UNK (Unknown) ID: 0 PAD (Padding) ID: -1 --- Tokenization Test --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;el\u0026#39;, \u0026#39;lo\u0026#39;, \u0026#39;‚ñÅW\u0026#39;, \u0026#39;orld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 380, 309, 476, 619, 36, 3924, 3964, 3959, 3993, 3978, 3983, 57, 3975, 3976, 60, 3974] --- Tokenization Test --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅmo\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;inst\u0026#39;, \u0026#39;ag\u0026#39;, \u0026#39;ram\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅever\u0026#39;] Numerical IDs: [1894, 304, 3284, 376, 321, 313, 312, 1008, 3313, 1325, 428, 3937, 341, 304, 3284, 1495] The ! is fallen back to UTF bytes. and the word instagrammable is split into multiple subword tokens. Also, original text can be recreated right back by simply concatenating the tokens. And replacing underscore by whitespace. Now we can try Unigram model. and see the same sentences tokenized differently.\nfrom tokenizers import Tokenizer from tokenizers.models import WordPiece from tokenizers.trainers import WordPieceTrainer from tokenizers.pre_tokenizers import Whitespace try: # 1. Initialize the WordPiece Tokenizer # We specify the [UNK] token for handling words not found in the vocabulary. tokenizer = Tokenizer(WordPiece(unk_token=\u0026#34;[UNK]\u0026#34;)) # 2. Configure Pre-tokenization # Before the subword algorithm runs, we need to split the raw text into words. # Whitespace splitting is the standard first step for most English NLP tasks. tokenizer.pre_tokenizer = Whitespace() # 3. Initialize the Trainer # We define our target vocabulary size and the special tokens required for # downstream tasks (like BERT\u0026#39;s [CLS] for classification or [SEP] for separators). trainer = WordPieceTrainer( vocab_size=4000, special_tokens=[\u0026#34;[UNK]\u0026#34;, \u0026#34;[CLS]\u0026#34;, \u0026#34;[SEP]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[MASK]\u0026#34;] ) # 4. Train the Model # The tokenizer scans the training file to build a vocabulary of the most # frequent subword units. tokenizer.train(files=[\u0026#34;train_data.txt\u0026#34;], trainer=trainer) # 5. Persist the Model # Save the configuration and vocabulary to a JSON file for future inference. tokenizer.save(\u0026#34;wordpiece.json\u0026#34;) print(\u0026#34;Training complete. \u0026#39;wordpiece.json\u0026#39; created.\u0026#34;) # 6. Metadata Inspection print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;WordPiece Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {tokenizer.get_vocab_size()}\u0026#34;) # 7. Testing Subword Tokenization # WordPiece shines at handling rare words by breaking them into meaningful chunks. test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (WordPiece) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # Encode converts raw text into a Tokenizer object containing tokens and IDs output = tokenizer.encode(text) # \u0026#39;tokens\u0026#39; shows the subword breakdown (e.g., \u0026#39;un\u0026#39;, \u0026#39;##insta\u0026#39;, etc.) print(f\u0026#34;Subword Tokens: {output.tokens}\u0026#34;) # \u0026#39;ids\u0026#39; are the numerical indices mapped to the vocabulary print(f\u0026#34;Numerical IDs: {output.ids}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred with WordPiece model: {e}\u0026#34;) You can see the following output:\nWordPiece Model Metadata: Total Vocab Size: 2609 --- Tokenization Test (WordPiece) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;H\u0026#39;, \u0026#39;##el\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;##or\u0026#39;, \u0026#39;##ld\u0026#39;, \u0026#39;[UNK]\u0026#39;, \u0026#39;[UNK]\u0026#39;] Numerical IDs: [37, 180, 214, 52, 162, 418, 0, 0] --- Tokenization Test (WordPiece) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;##os\u0026#39;, \u0026#39;##t\u0026#39;, \u0026#39;un\u0026#39;, \u0026#39;##ins\u0026#39;, \u0026#39;##ta\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;##ra\u0026#39;, \u0026#39;##m\u0026#39;, \u0026#39;##ma\u0026#39;, \u0026#39;##ble\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;ever\u0026#39;] Numerical IDs: [691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240] We can see the special character ## in the output. We can also see the breakdowns not having any semantic meaning. They are likelihood based. We can try the Unigram model now.\n# \u0026#39;Unigram\u0026#39; is the default and usually recommended over BPE in SentencePiece. options_unigram = { \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # Path to the raw text file for training \u0026#39;model_prefix\u0026#39;: \u0026#39;unigram_model\u0026#39;, # Prefix for the output .model and .vocab files \u0026#39;vocab_size\u0026#39;: 1200, # Desired size of the final vocabulary \u0026#39;model_type\u0026#39;: \u0026#39;unigram\u0026#39;, # Specifies the Unigram language model algorithm \u0026#39;character_coverage\u0026#39;: 0.9995, # Percentage of characters covered by the model (0.9995 is standard for Latin scripts) \u0026#39;byte_fallback\u0026#39;: True, # Enables mapping unknown characters to UTF-8 bytes to avoid \u0026lt;unk\u0026gt; tokens \u0026#39;split_digits\u0026#39;: True, # Treats each digit as an individual token (useful for numerical data) \u0026#39;add_dummy_prefix\u0026#39;: False # Prevents adding a leading space (SentencePiece default is True) } try: # 1. Train the SentencePiece model using the defined options print(\u0026#34;Starting Unigram training...\u0026#34;) spm.SentencePieceTrainer.train(**options_unigram) print(\u0026#34;Training complete. \u0026#39;unigram_model.model\u0026#39; created.\u0026#34;) # 2. Load the trained model into a processor instance for inference sp_unigram = spm.SentencePieceProcessor() sp_unigram.load(\u0026#39;unigram_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Unigram Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {sp_unigram.get_piece_size()}\u0026#34;) # 3. Define test cases to evaluate how the model handles common and rare words test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] # 4. Iterate through test sentences to visualize subword segmentation for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (Unigram) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: Converts text into subword strings (visual representation) print(f\u0026#34;Subword Tokens: {sp_unigram.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: Converts text into numerical indices for model input print(f\u0026#34;Numerical IDs: {sp_unigram.encode_as_ids(text)}\u0026#34;) except Exception as e: # Handle potential errors during training or loading (e.g., missing input file) print(f\u0026#34;An error occurred with Unigram model: {e}\u0026#34;) The output is as follows:\nUnigram Model Metadata: Total Vocab Size: 1200 --- Tokenization Test (Unigram) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ll\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;ld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311] --- Tokenization Test (Unigram) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;T\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅm\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;sta\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;ra\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ver\u0026#39;] ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"A comprehensive guide to tokenization strategies: BPE, WordPiece, Unigram, and SentencePiece.","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"A brief introduction to Vectors \u0026amp; Verbs and formatting verification.","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"\nFeedforward network (FFN) blocks are the least talked about when in context to LLMs. But that is where more than 70% of the paramters come from. Something as simple and convinient as a couple of linear transforming matrices become a burden when models start scaling in Billions of parameters. So, getting the maximum juice of a lean structure is a challenge. Plus, mostly under represented, because the FFN sub-layer acts as the primary \u0026ldquo;knowledge\u0026rdquo; processing engine, often containing the majority of a model\u0026rsquo;s total parameters.\n","permalink":"http://localhost:1313/posts/feedforward-networks/","summary":"Exploring the hidden layers of trillion-parameter switchboards: Feedforward Neural Networks and Activation Functions.","title":"Anatomy of Trillion-Parameter Switchboards: Understanding Feedforward Blocks"},{"content":"\nWe are about to touch the holy grail of modern AI. The 2017 declaration was simple: Attention Is All You Need. But as we‚Äôve raced from GPT-1 to reasoning models, the definition of \u0026lsquo;Attention\u0026rsquo; has quietly transformed. Is the algorithm ruling the world today actually the same one we started with? Let‚Äôs look under the hood.\nüî• The Core Concept At its heart, the attention mechanism is a form of weighted aggregation. It is often described using a database retrieval analogy. Imagine you have a database of information (context).\nQuery ($Q$): What are you currently looking for? (The token being generated). Key ($K$): The indexing tag associated with each piece of information. Value ($V$): The actual information (The context token). The \u0026ldquo;Attention\u0026rdquo; is the process of matching your Query against all the Keys to determine how much \u0026ldquo;weight\u0026rdquo; to put on each Value. Attention scores are calculated in each transformer layer at every step of the sequence. This is a single unifying factor in all transformer models, from BERT to Gemini.\nüî• Standard Multi-Head Attention (MHA) The original mechanism introduced in the paper \u0026ldquo;Attention Is All You Need\u0026rdquo;.\nHere, instead of performing the Key-Query-Value (KQV) operation on the entire context only once, the attention is split into $N$ parallel heads. Each head learns a different aspect of the language (e.g., word meaning, syntax, semantic relationships, etc.). The equation is a simple culmination of dot products aided by a regularizer like Softmax.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nWhere:\n$QK^T$ computes the similarity scores between the Query and Key. $\\sqrt{d_k}$ is a scaling factor to prevent the dot product from growing too large, mitigating the vanishing/exploding gradient problem. The weighted sum is applied to the values of $V$. The Memory Bandwidth Bottleneck: \u0026gt; During inference, the model must store Key and Value matrices for all previous tokens. This is called the KV Cache. In MHA, every single head has its own unique set of Keys and Values. For large models with massive context windows, loading this massive KV cache from memory (HBM) to the computation chip (SRAM) becomes the primary bottleneck, slowing down generation significantly. Earlier models could often only generate ~8 tokens per second at inference due to this constraint.\nüî• Multi-Query Attention (MQA) This approach addresses the memory bottleneck by sharing the same set of Keys and Values across all heads while keeping separate Query heads.\nMHA: $H$ Query heads, $H$ Key heads, $H$ Value heads MQA: $H$ Query heads, 1 Key head, 1 Value head This drastically reduces the memory requirements of the KV cache by a factor of $H$ (often 8x or 16x). The GPU needs to load far less data, significantly speeding up inference. However, it does not reduce the computation (FLOPs)‚Äîso why is it faster?\nThis can be explained with the Kitchen Analogy: Imagine a kitchen (GPU) with 8 chefs (Heads).\nComputation (Math): The Chef chopping vegetables. Memory Access (Bandwidth): The Assistant running to the fridge to get ingredients. In MHA: Each chef demands unique vegetables from the fridge. The assistant runs back and forth 8 times, fetching 8 different crates. Most of the time is spent waiting for the assistant to return. This results in idle cores waiting for data to load from HBM.\nIn MQA: All chefs agree to use the same vegetables. The assistant runs back and forth only once, fetching one giant crate of potatoes and dumping it. This results in minimized memory transfer and no idle cores.\nThe Technical Reality: During text generation, the model is memory-bound, not compute-bound. The time it takes to move the KV Cache from HBM (High Bandwidth Memory) to the chip\u0026rsquo;s SRAM is the bottleneck. MQA reduces the data volume of that move by a factor of $H$.\nThe Capacity Gap: MQA is a trade-off between nuanced response and memory capacity. The model loses the ability to store different types of information, as a single $V$ matrix must now store all nuances of information flow in the language.\nThe Surgery Trick (Uptraining): Google Research figured out a way to take an existing model with MHA and convert it to MQA by averaging the weight matrices of $H$ heads. This is called Uptraining. You then train this \u0026ldquo;Frankenstein\u0026rdquo; model for ~5% of the original training steps for adjustments. This yields an MQA model that is nearly as good as the original MHA but vastly faster.\nüî• Grouped Query Attention (GQA) A \u0026ldquo;best of both worlds\u0026rdquo; solution used in modern models like Llama 2 and Llama 3.\nHow it works: GQA acts as an interpolation between MHA and MQA. Instead of having just 1 Key head (MQA) or $H$ Key heads (MHA), it divides the query heads into $G$ groups. Each group shares a single Key and Value head.\nEquation modification: If you have 8 query heads, you might create 4 groups (2 queries per group). This reduces the KV cache by 2x instead of 8x, preserving more quality than MQA while still speeding up inference.\nMHA: 8 people (heads) are working on a project. Each person has their own unique filing cabinet (KV cache). It takes forever to move 8 cabinets around. MQA: 8 people share one single filing cabinet. It\u0026rsquo;s very fast to move, but they fight over the organization and lose detail. GQA: The 8 people split into 4 teams of 2. Each team shares a cabinet. It\u0026rsquo;s a balance between speed and organizational detail. üî• Multi-Head Latent Attention (MLA) This is the most advanced mechanism for attention, used in models like DeepSeek-V3. It is a sophisticated design for massive efficiency that takes a radical approach: mathematically compressing the memory into a low-rank \u0026ldquo;latent\u0026rdquo; vector.\nKV Cache Explosion: As mentioned before, for a massive model like DeepSeek-V3 (671B parameters), storing the full Keys and Values for every token in a long conversation requires terabytes of high-speed memory (HBM), and chips spend more time waiting than churning.\nMHA (Standard) stores everything. MQA deletes redundancy but hurts quality. MLA keeps the quality of MHA but compresses the data 4x-6x smaller than even GQA.\nThe Zip File Approach: Low-Rank Joint Compression Let\u0026rsquo;s compare the math of MHA and MLA. In MHA, for every single token, we generate a unique Key ($k$) and Value ($v$) matrix for each attention head.\nThe Memory Problem: The $k$ is stored in the KV Cache. For 128 heads, for example, it calculates a massive vector of $128 \\times 128 = 16,384$ floats per token.\nFor MHA, the attention score looks like: $$\\text{Score} = q^T \\cdot k$$\nBut for MLA, instead of storing the full $k$, it projects the input down into a tiny latent vector $c_{KV}$, which is much smaller (e.g., 512 floats). $$c_{KV} = W_{DKV} \\cdot x$$\nNow here is the catch: Naive compression and decompression would mean constructing a small latent matrix for every token and deconstructing it every single time the information is required, which defeats the purpose of compression.\n$$k_{\\text{reconstructed}} = W_{UK} \\cdot c_{KV}$$ $$\\text{Score} = q^T \\cdot k_{\\text{reconstructed}}$$\nThe Matrix Absorption Trick (Optimization) Instead of reconstructing $k$ for every token, we can absorb the Up-Projection matrix ($W_{UK}$) into the Down-Projection matrix ($W_{DKV}$) and absorb the query $q$ into a new $Q$.\nFrom the original equation: $$\\text{Score} = q^T \\cdot (\\underbrace{W_{UK} \\cdot c_{KV}}_{\\text{This is } k})$$\nWe associate differently:\n$$\\text{Score} = (\\underbrace{q^T \\cdot W_{UK}}_{\\text{Absorbed Query}}) \\cdot c_{KV}$$ We change the order in which the matrix multiplication is performed.\nHow is this allowed? In linear algebra, matrix multiplication is associative:\n$$(A \\cdot B) \\cdot C = A \\cdot (B \\cdot C)$$ We can move the parentheses! Instead of grouping $(W_{UK} \\cdot c_{KV})$ to make the Key, we group $(q^T \\cdot W_{UK})$ to make a new Query.\nNote: The positional embedding information is left untouched. That rotational information from algorithms like RoPE is preserved outside the compression; it requires minimal space anyways.\n","permalink":"http://localhost:1313/posts/attention-mechanisms/","summary":"We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek\u0026rsquo;s MLA, how has the definition of \u0026lsquo;Attention\u0026rsquo; transformed?","title":"Attention Is All You Need, but exactly which one?: MHA, GQA and MLA"},{"content":"\nPositional Embeddings are the \u0026ldquo;voice\u0026rdquo; that tell Transformers where words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.\n\u0026ldquo;The dog chased the cat\u0026rdquo; and \u0026ldquo;cat chased the dog\u0026rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.\nüî• Sinusoidal Embeddings: The Odometer of Language Introduced in the seminal \u0026ldquo;Attention Is All You Need\u0026rdquo; paper, this was the industry standard for many years. This approach adds a vector to each word embedding based on its position. The vector is computed out of wavelike patterns that are periodic and smooth. Mathematically, for a position $pos$ and embedding dimension $i$:\n$$ PE_{(pos, i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $$\nThe model uses sine and cosine waves of different frequencies to capture both short and long-range dependencies. The frequency of the waves decreases as the embedding dimension increases, which helps capture long-range dependencies in the input sequence.\nThe creators wanted the model to calculate distance easily, which is why a linear function was used. The model simply rotates the current position vector using a rotation matrix to derive the next position.\n$$ \\begin{pmatrix} \\cos(k) \u0026 \\sin(k) \\\\ -\\sin(k) \u0026 \\cos(k) \\end{pmatrix} \\cdot PE(pos) \\approx PE(pos + k) $$ Low dimensions act like a fast-ticking clock (high frequency, changing rapidly) and high dimensions act like a slow clock (low frequency, changing slowly).\nThe number 10,000 is an arbitrarily chosen high number to reduce Aliasing. By choosing such a high number, the creators ensured that the wavelength of the slowest dimension is incredibly long. Consequently, the pattern practically never repeats, and each position remains unique.\nThis combination works like a mechanical odometer dial. Each subsequent dimension acts as a slower-moving gear. The model learns to read these \u0026ldquo;dials\u0026rdquo; to understand exactly where a word sits in the sequence.\nThis mitigated the challenge of exploding gradients that previous array-based positional embeddings suffered from (e.g., adding integers 1, 2, 3\u0026hellip;). This model keeps values between -1 and +1, adding stability while ensuring every position gets a distinct code.\nThe Problem: This approach pollutes the word embedding matrix (semantic information). While high-dimensional vectors are almost always orthogonal‚Äîmeaning the noise doesn\u0026rsquo;t fully destroy the word\u0026rsquo;s identity‚Äîit does not strictly guarantee purity.\nThis led to a more mature approach.\nüî• ALiBi: The Fading Streetlight Attention with Linear Biases (ALiBi) is a method where, instead of adding a vector to the embeddings at the start, the model injects positional information linearly, directly into the attention scores. If Sinusoidal Encoding asks, \u0026ldquo;Where am I on the map?\u0026rdquo;, ALiBi asks, \u0026ldquo;How much harder should I squint to see you?\u0026rdquo;\nThe biggest problem ALiBi solved is the \u0026ldquo;Invisible Wall\u0026rdquo;. If you train a Transformer on sequences of length 2048 and then feed it a book with 3000 words, it will crash or output garbage at word 2049. This is a failure of Extrapolation.\nALiBi throws away the idea of adding positional vectors entirely. Instead, it devises a \u0026ldquo;penalty\u0026rdquo; system for attention scores:\n$$ \\text{Score} = (q \\cdot k^\\top) - (m \\cdot \\text{distance}) $$\nDistance: If words A and B are neighbors, the distance is 0, so the penalty is 0. Although the distance is linear, it does not cause exploding gradients because the raw attention scores are normalized by a softmax function. This distance prioritizes nearby words (local context) over distant ones.\nSlope ($m$): If we always penalize distance equally, the model becomes myopic. ALiBi mitigates this by assigning a different slope value to each attention head:\nHead 1 (The Historian): Very low slope. The penalty grows slowly, allowing it to see far back. Head 2 (The Reader): Medium slope. Head 3 (The Myopic): Very high slope (e.g., 0.5); it effectively cannot see beyond 2-3 words. This mimics holding a lantern. Words close to you are bright, while words further away naturally fade into darkness.\nExtrapolation: If you walk 5 miles down the road (a longer sequence), your lantern works exactly the same way. The physics of light decay are constant, allowing the model to handle sequences longer than it was trained on.\nHowever, ALiBi forces a \u0026ldquo;sliding window\u0026rdquo; view where distant words eventually become invisible. This brings us to the modern standard.\nüî• RoPE: The Rotary Revolution Rotary Position Embeddings (RoPE) is the current industry standard (used in Llama 3, Mistral, PaLM) because it mathematically unifies \u0026ldquo;absolute position\u0026rdquo; with \u0026ldquo;relative position.\u0026rdquo; This method rotates the vectors in the Query and Key matrices based on their absolute position.\nIf a vector has 512 dimensions, RoPE breaks it into 256 pairs. Each pair is rotated by a different frequency:\n$$ \\theta_i = 10000^{-2i/d} $$\nThe First pair ($i = 0$) is rotated by the highest frequency. The Last pair ($i = 255$) is rotated by the lowest frequency. This spread ensures the model has \u0026ldquo;high precision\u0026rdquo; (fast rotation) to distinguish immediate neighbors, and \u0026ldquo;low precision\u0026rdquo; (slow rotation) to track global position without the pattern repeating.\nDoesn\u0026rsquo;t rotation change meaning? Yes and No. The vector for \u0026ldquo;Apple\u0026rdquo; at position 1 and \u0026ldquo;Apple\u0026rdquo; at position 256 will look completely different in terms of coordinates. However, rotation does not change the semantic strength (magnitude/norm) of the vector.\nThe Relativity Trick: Transformers compare words using dot products (angles). RoPE relies on the fact that the attention mechanism ignores the absolute rotation and only detects the difference in rotation (relative position). Thus, the \u0026ldquo;relative meaning\u0026rdquo; is encoded purely into the angle between words, leaving the semantic \u0026ldquo;magnitude\u0026rdquo; untouched.\nüî• LongRoPE: The Bifocal Lens While RoPE is powerful, it struggles when stretching context windows from 4k to 2 million tokens. The \u0026ldquo;fast\u0026rdquo; dimensions spin so rapidly over long distances that they become random noise. LongRoPE solves this using an evolutionary search algorithm to find a unique scaling factor ($\\lambda$) for each dimension.\nThe Equations Instead of rotating by the standard $\\theta_i$, LongRoPE rotates by a rescaled frequency:\n$$ \\theta\u0026rsquo;_i = \\lambda_i \\theta_i $$\nTo efficiently find these $\\lambda$ values without searching an infinite space, the algorithm enforces a monotonicity constraint based on NTK theory:\n$$ \\lambda_i \\le \\lambda_{i+1} $$\nThis ensures that low-frequency dimensions (global context) are stretched more than high-frequency dimensions (local context), creating a \u0026ldquo;bifocal\u0026rdquo; effect:\nHigh Frequencies (Local): Kept sharp ($\\lambda \\approx 1$) to maintain grammar. Low Frequencies (Global): Stretched ($\\lambda \u0026gt; 1$) to track massive distances without repeating. Not worth perusing more since the next one is much better at long context positional encoding. üî• HoPE: The Hyperbolic Slide Hyperbolic Rotary Positional Encoding (HoPE) moves from the geometry of a circle to the geometry of a hyperbola (inspired by Lorentz transformations). It was designed to fix a subtle flaw in RoPE: the \u0026ldquo;wobbly\u0026rdquo; or oscillatory nature of attention scores.\nThe Core Problem: RoPE\u0026rsquo;s \u0026ldquo;Wobble\u0026rdquo; In standard RoPE, we rotate vectors around a circle.\nThe Issue: Circles repeat. A dial at $361^\\circ$ looks identical to $1^\\circ$. Even with different frequencies, the dot product fluctuates up and down as distance increases. The Consequence: The model gets confused at long distances, potentially mistaking a word 1,000 tokens away for a neighbor just because the rotation cycles aligned, creating \u0026ldquo;noise.\u0026rdquo; The \u0026ldquo;Hyperbolic\u0026rdquo; Shift HoPE replaces trigonometric functions ($\\sin, \\cos$) with hyperbolic functions ($\\sinh, \\cosh$).\nCircular Rotation: Keeps distance from the center constant. Hyperbolic Rotation: Moves points along a hyperbola. Crucially, hyperbolic functions do not repeat; they grow or decay exponentially. By using hyperbolic rotation, HoPE guarantees that as words get further apart, the attention score decays monotonically (smoothly drops).\nThe Equations HoPE uses a hyperbolic matrix $B(\\theta, m)$ defined using the Lorentz Boost structure:\n$$ B(\\theta, m) = \\begin{pmatrix} \\cosh(m\\theta) \u0026 \\sinh(m\\theta) \\\\ \\sinh(m\\theta) \u0026 \\cosh(m\\theta) \\end{pmatrix} $$ Since hyperbolic functions grow to infinity, HoPE introduces a decay penalty ($e^{-m\\theta'}$). The final attention score simplifies beautifully to a function of relative distance: $$ \\text{Score} \\propto e^{-|m-n|(\\theta\u0026rsquo; - \\theta)} $$\nThis ensures the attention score exponentially decays as distance increases, eliminating the wobbles of standard RoPE. This the latest model that is taking up the space in the industry for extremely long context models.\n","permalink":"http://localhost:1313/posts/positional-embeddings/","summary":"From Sinusoidal to RoPE and HoPE: How Transformers learn to process word order and sequence length.","title":"The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE"},{"content":"Imagine trying to teach a computer the difference between \u0026ldquo;Apple\u0026rdquo; the fruit and \u0026ldquo;Apple\u0026rdquo; the company. To us, the distinction is intuitive. To a machine, it‚Äôs just a string of characters. How do we turn these strings into meaningful math? While early attempts like Word2Vec gave us a great start, they missed the forest for the trees‚Äîor in some cases, the twigs for the branches. Enter GloVe and FastText: two algorithms that revolutionized how machines understand the nuances of human language.\nPrevious static embedding models like Word2Vec successfully captured the local semantics of words, but they failed in addressing contexts between words that might not always appear in the same context window. Furthermore, they treated words as indivisible atomic units in continuous vector spaces, missing out on the internal structure of language.\nThe two simple but effective ideas behind GloVe and FastText are subword units and global co-occurrence. Let\u0026rsquo;s dive into them.\nGloVe: A Global Mapping Strategy Global Vectors for Word Representation (GloVe) was created at Stanford. The core idea was to train a model where word vectors are defined by how often they appear around other words across the entire corpus, rather than just locally.\nTraining Steps: Co-occurrence Matrix X: GloVe scans the entire corpus and creates a giant matrix (spreadsheet) recording the counts of how often words appear near each other. Factorization: It compresses this giant matrix into smaller, dense vectors. Objective: The function tries to equate the dot product of two word vectors with the logarithm of their probability of appearing together. The error between these values is then backpropagated to update the vectors. $$w_i \\cdot w_j + b_i + b_j = \\log(X_{ij})$$\n$w$: The word vector (what we want). $X_{ij}$: The count from our matrix. The Loss Function: We can\u0026rsquo;t satisfy the equation above perfectly for every word pair. So, we use a weighting function to minimize the squared error:\n$$J = \\sum_{i,j=1}^V f(X_{ij}) ({w_i}^T w_j + b_i + b_j - \\log(X_{ij}))^2$$\nIf we do not use the weighting function, the loss would be infinite for rare words (since $\\log(0) = \\infty$) and frequent words (like \u0026rsquo;the\u0026rsquo;) would overpower the model.\n$$ f(x) = \\begin{cases} (x/x_{max})^\\alpha \u0026amp; \\text{if } x \u0026lt; x_{max} \\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$\n(Typically $x_{max}=100$ and $\\alpha=0.75$)\nFastText: The Lego Bricks Strategy FastText was created by Facebook AI Research (FAIR). The idea was to treat words not as solid rocks, but as structures made of \u0026ldquo;Lego bricks\u0026rdquo; (subword units).\nWord2Vec struggled to create embeddings for words it hadn\u0026rsquo;t seen before (Out-Of-Vocabulary or OOV words). FastText solves this by breaking words down.\nGloVe treats a word like \u0026ldquo;Apple\u0026rdquo; as a single, atomic unit. FastText breaks \u0026ldquo;Apple\u0026rdquo; into character n-grams: \u0026lt;ap, app, ppl, ple, le\u0026gt; For a complex or new word like \u0026ldquo;Applesauce\u0026rdquo;, the model might not know the whole word, but it recognizes the Lego brick for \u0026ldquo;Apple\u0026rdquo; and the remaining bits from other subwords.\nFastText takes its inspiration from the SkipGram architecture. The vector for a word is simply the sum of the vectors of its n-gram subwords:\n$$v_w = \\sum_{g \\in G_w} z_g$$\nObjective: The objective is to predict context words given a target word.\n$$P(w_c | w_t) = \\frac{e^{s(v_{w_t} \\cdot v_{w_c})}}{\\sum_{w \\in V} e^{s(v_{w_t} \\cdot v_w)}}$$\nAnd the scoring function is defined as:\n$$s(w_t, w_c) = u_{w_t}^T (\\sum_{g \\in G_w} z_g)$$\n$u_{w_c}$: The vector for the context word (output vector). $z_g$: The vector for the n-gram $g$ (input vector). Modifications to Softmax: Calculating the full softmax for every word in the vocabulary is computationally expensive. FastText uses two methods to approximate this efficiently.\nOption A: Negative Sampling\nWe pick a few random \u0026ldquo;wrong\u0026rdquo; words every time to serve as \u0026ldquo;negative samples\u0026rdquo; and calculate the loss only for the correct word and these noise words.\n$$J = - \\log \\sigma(s(w_t, w_c)) - \\sum\\limits_{i=1}^{N} \\mathbb{E}_{n_i \\sim P_n(w)} [\\log \\sigma(-s(w_t, n_i))]$$\nThe sigmoid function $\\sigma$ squashes the score between 0 and 1. The first term pushes the probability of the correct word towards 1. The second term pushes the probability of the noise words towards 0. Option B: Hierarchical Softmax (Faster for infrequent words)\nInstead of a flat list of words, imagine the vocabulary arranged as a binary tree (Huffman Tree).\nRoot: Top of the tree. Leaves: The actual words. To calculate the probability of a word, we trace a path from the root to that leaf. At every branching node, we calculate the probability (sigmoid) of going left or right. The probability of a word $w$ is the product of the probabilities of the turns taken to reach it. Code Example from gensim.models import FastText from gensim.test.utils import common_texts # Initialize the FastText model with specific hyperparameters model = FastText( vector_size=100, # Dimensionality of the word vectors window=5, # Maximum distance between the current and predicted word within a sentence min_count=1, # Ignores all words with total frequency lower than this sg=1, # Training algorithm: 1 for skip-gram; 0 for CBOW hs=0, # If 0, and negative is non-zero, negative sampling will be used negative=5, # Number of \u0026#34;noise words\u0026#34; to be drawn for negative sampling workers=4, # Number of worker threads to train the model epochs=10, # Number of iterations over the corpus min_n=3, # Minimum length of character n-grams max_n=6 # Maximum length of character n-grams ) # Build the vocabulary from the provided text corpus model.build_vocab(common_texts) # Train the model on the corpus model.train( common_texts, total_examples=model.corpus_count, epochs=model.epochs ) # Calculate and print the cosine similarity between two words in the vocabulary print(f\u0026#34;Similarity between \u0026#39;computer\u0026#39; and \u0026#39;human\u0026#39;: {model.wv.similarity(\u0026#39;computer\u0026#39;, \u0026#39;human\u0026#39;)}\u0026#34;) # Demonstrate FastText\u0026#39;s ability to handle Out-Of-Vocabulary (OOV) words # Even if \u0026#39;computation\u0026#39; wasn\u0026#39;t in the training data, FastText constructs a vector using character n-grams oov_vector = model.wv[\u0026#39;computation\u0026#39;] # Check if a specific word exists in the model\u0026#39;s fixed vocabulary index word = \u0026#34;computer\u0026#34; is_in_vocab = word in model.wv.key_to_index print(f\u0026#34;\\nIs the word \u0026#39;{word}\u0026#39; in the model\u0026#39;s vocabulary? {is_in_vocab}\u0026#34;) ","permalink":"http://localhost:1313/posts/glove-and-fasttext/","summary":"How subword units and global co-occurrence matrices allow GloVe and FastText to capture nuances that Word2Vec missed.","title":"The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText"},{"content":" Before we had Large Language Models writing poetry, we had to teach computers that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; are related not just by spelling, but by meaning. This is the story of that breakthrough. It‚Äôs the moment we stopped counting words and started mapping their souls‚Äîturning raw text into a mathematical landscape where math can solve analogies. Welcome to the world of Word2Vec. üîÆ\nLanguage models require vector representations of words to capture semantic relationships. Before the 2010s, models used word count-based vector representations that captured only the frequency of words (e.g., One-Hot Encoding). The Problems: üöß\nThe Curse of Dimensionality Lack of meaning (Synonyms were treated as mathematically unrelated). In 2011, Mikolov et al. at Google introduced Word2Vec, which used a shallow neural network to learn vector representations of words. The best part? It could create much denser relationships between words than prior models, and it was unsupervised.\nThe training (prediction) is essentially \u0026ldquo;fake\u0026rdquo;‚Äîa pretext task. But the weight matrices we get on the side are a gold mine of fine-grained semantic relationships. ‚õèÔ∏è\nThere are two popular variants of Word2Vec:\nSkipGram Continuous Bag Of Words (CBOW) 1. Continuous Bag of Words (CBOW) üß© CBOW is a simple \u0026lsquo;fill in the blanks\u0026rsquo; machine. It takes the surrounding words as inputs and tries to predict the center word.\n\u0026ldquo;Continuous\u0026rdquo;: It operates in continuous vector space (unlike the discrete space of n-gram models).\n\u0026ldquo;Bag of words\u0026rdquo;: The order of the context does not matter. \u0026ldquo;The cat sat on the mat\u0026rdquo; and \u0026ldquo;The mat sat on the cat\u0026rdquo; are the same for CBOW. They produce exactly the same predictions.\nObjective: Maximize the probability of the target word $w_t$ given its context $w_{t-C}, \\dots, w_{t+C}$.\n2. Training Defining Dimensions:\n$V$: Vocabulary size (number of unique words in the corpus, e.g., 10,000) $N$: Dimension of the embedding space (e.g., 100) $C$: Number of context words (e.g., 2 words on left of target and 2 words on right) Step 1: Input Lookup \u0026amp; One-Hot Encoding\nMathematically, we represent each context word as a one-hot vector $x^{(c)}$. We have an Input Matrix $W_{in}$ of size $V \\times N$. For each context word (e.g., \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;), we select its corresponding row from $W_{in}$.\n$$ v_c = W_{in}^T x^{(c)} $$\nWhere $v_c$ is the $N$-dimensional vector for context word $c$.\nStep 2: Projection (The \u0026ldquo;Mixing\u0026rdquo;)\nWe combine the context vectors. In standard CBOW, we simply average them. This creates the hidden layer vector $h$.\n$$ h = \\frac{1}{2C} \\sum_{c=1}^{2C} v_c $$\n$h$ is a single vector of size $N \\times 1$. Note: This operation is linear and contains no activation function (like ReLU or Sigmoid).\nStep 3: Output Scoring (The \u0026ldquo;Dot Product\u0026rdquo;)\nWe have a second matrix, the Output Matrix $W_{out}$ of size $N \\times V$. We compute the raw score (logit) $u_j$ for every word $j$ in the vocabulary by taking the dot product of the hidden vector $h$ and the output vector $v\u0026rsquo;_j$.\n$$ u_j = {v\u0026rsquo;_j}^T h $$\nOr in matrix form:\n$$ u = W_{out}^T h $$\nResult $u$ is a vector of size $V \\times 1$.\nStep 4: Probability Conversion (Softmax)\nWe convert the raw scores $u$ into probabilities using the Softmax function. This tells us the probability that word $w_j$ is the center word.\n$$ y_j = P(w_j | \\text{context}) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 5: Loss Calculation (Cross-Entropy)\nWe compare the predicted distribution $y$ with the actual ground truth $t$ (a one-hot vector where the index of the true word is 1). The loss function $E$ is:\n$$ E = - \\sum_{j=1}^V t_j \\log(y_j) $$\nSince $t_j$ is 0 for all words except the target word (let\u0026rsquo;s call the target index $j^*$), this simplifies to:\n$$ E = - \\log(y_{j^*}) $$\n3. Backpropagation Details We need to update the weights to minimize $E$. We use the Chain Rule.\nA. Gradient w.r.t. Output Matrix ($W_{out}$)\nWe want to know how the error changes with respect to the raw score $u_j$.\n$$ \\frac{\\partial E}{\\partial u_j} = y_j - t_j $$\nLet\u0026rsquo;s call this error term $e_j$.\nIf $j$ is the target word: $e_j = y_j - 1$ (Result is negative; we boost the score). If $j$ is not target: $e_j = y_j - 0$ (Result is positive; we suppress the score). The update rule for the output vector $v\u0026rsquo;_j$ becomes:\n$$ v'_{j}(\\text{new}) = v'_{j}(\\text{old}) - \\eta \\cdot e_j \\cdot h $$ ($\\eta$ is the learning rate)\nB. Gradient w.r.t. Hidden Layer ($h$)\nWe backpropagate the error from the output layer to the hidden layer.\n$$ EH = \\sum_{j=1}^V e_j \\cdot v\u0026rsquo;_j $$\n$EH$ is an $N$-dimensional vector representing the aggregate error passed back to the projection layer.\nC. Gradient w.r.t. Input Matrix ($W_{in}$)\nSince $h$ was just an average of the input vectors, the error $EH$ is distributed to each context word\u0026rsquo;s input vector. For every word $w_c$ in the context:\n$$ v_{w_c}(\\text{new}) = v_{w_c}(\\text{old}) - \\eta \\cdot \\frac{1}{2C} \\cdot EH $$\nLet\u0026rsquo;s build a single pass through the network for a given context.\nimport torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # Define a simple Continuous Bag of Words (CBOW) style model class CBOWModel(nn.Module): def __init__(self, vocab_size, embedding_dim): super(CBOWModel, self).__init__() # The embedding layer stores the word vectors we want to learn self.embeddings = nn.Embedding(vocab_size, embedding_dim) # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word self.linear = nn.Linear(embedding_dim, vocab_size) def forward(self, inputs): # inputs: tensor of word indices for the context embeds = self.embeddings(inputs) # Aggregate context by calculating the mean of the word embeddings h = torch.mean(embeds, dim=1) # Produce logits (raw scores) for each word in the vocabulary logits = self.linear(h) return logits # Setup vocabulary and mappings word_to_ix = {\u0026#34;the\u0026#34;: 0, \u0026#34;quick\u0026#34;: 1, \u0026#34;brown\u0026#34;: 2, \u0026#34;fox\u0026#34;: 3, \u0026#34;jumps\u0026#34;: 4, \u0026#34;over\u0026#34;: 5, \u0026#34;lazy\u0026#34;: 6, \u0026#34;dog\u0026#34;: 7} ix_to_word = {v: k for k, v in word_to_ix.items()} # Configuration constants EMBEDDING_DIM = 5 VOCAB_SIZE = len(word_to_ix) LEARNING_RATE = 0.01 # Initialize the model, loss function, and optimizer model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM) loss_function = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE) # Prepare dummy training data: context words and the target word # Context: [\u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;over\u0026#34;] -\u0026gt; Target: \u0026#34;fox\u0026#34; context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long) target_idxs = torch.tensor([3], dtype=torch.long) # Perform a single optimization step model.zero_grad() # Forward pass: we reshape context to (1, -1) to simulate a batch of size 1 logits = model(context_idxs.view(1, -1)) # Calculate loss against the target word index loss = loss_function(logits, target_idxs) # Backpropagate and update weights loss.backward() optimizer.step() # Output results for the blog post print(\u0026#34;=== Model Training Snapshot ===\u0026#34;) print(f\u0026#34;Calculated Loss: {loss.item():.6f}\u0026#34;) print(f\u0026#34;\\n=== Learned Vector for \u0026#39;jumps\u0026#39; ===\u0026#34;) word_vec = model.embeddings(torch.tensor([word_to_ix[\u0026#39;jumps\u0026#39;]])) print(word_vec.detach().numpy()) print(\u0026#34;\\n=== Embedding Matrix (Weights) ===\u0026#34;) print(model.embeddings.weight.detach().numpy()) print(\u0026#34;\\n=== Linear Layer Weights ===\u0026#34;) print(model.linear.weight.detach().numpy()) Output shows single pass loss and learned vector for \u0026lsquo;jumps\u0026rsquo; word. This is a single training sample result but after multiple such passes both Embedding and linear weights should be very similar.\n=== Model Training Snapshot === Calculated Loss: 1.989656 === Learned Vector for \u0026#39;jumps\u0026#39; === [[-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775]] === Embedding Matrix (Weights) === [[ 1.66039 -0.11371879 0.6246518 0.35860053 -1.2504417 ] [-2.1847186 -0.77199775 -0.17050214 -0.38411248 -0.03913084] [ 0.11852697 0.90073633 0.8847807 0.7404524 0.900149 ] [ 0.07440972 -0.40259898 2.6246994 -0.08851447 0.02660969] [-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775] [-0.9245572 -0.5545908 0.9083091 -1.0755049 0.84047747] [-1.0237687 0.59466314 0.05621134 -0.6202532 1.3664424 ] [ 0.60998917 -1.0549186 1.6103884 0.8724912 -1.2486908 ]] === Linear Layer Weights === [[ 0.02165058 -0.28883642 0.14545658 -0.3442509 0.32704315] [-0.18731792 0.28583744 0.22635977 0.13245736 0.29019794] [ 0.3158916 -0.15826383 -0.03203773 0.16377363 -0.41457543] [-0.05080034 0.4180087 0.11228557 0.30218413 0.3025514 ] [-0.38419306 0.24475925 -0.39210224 -0.38660625 -0.2673145 ] [-0.32321444 0.12200444 -0.03569533 0.2891424 -0.07345333] [-0.33704326 0.2521956 0.31587374 0.22590035 0.29866052] [ 0.4117266 -0.44231793 0.24064957 -0.29684234 0.333821 ]] Interesting Facts About CBOW üß† The \u0026ldquo;Averaging Problem\u0026rdquo;: This is a common issue in CBOW models. The averaging and training is order-agnostic. All variants of word order in the context window will generate the same output. \u0026ldquo;Dog bit the man\u0026rdquo; and \u0026ldquo;Man bit the dog\u0026rdquo; are the same in CBOW\u0026rsquo;s eyes.\nSmoothing Effect: CBOW models are much faster to train than Skip-Gram but are generally smoother.\nRare Words: CBOW models struggle slightly with rare words because the presence of surrounding common words (like articles and prepositions) can skew the embeddings and dissolve their uniqueness.\nComputational Cost: Softmax requires summing over the entire vocabulary size, which can be computationally expensive if $V$ is very large. Solutions include Hierarchical Softmax and Negative Sampling (approximating the denominator by only checking the target word vs. 5 random noise words).\nVectors as Double Agents: Every word has a different representation in the embedding matrix and the output matrix. Most of the time $W_{out}$ is discarded, but some consider averaging both $W_{in}$ and $W_{out}$ for slightly better performance.\nLinear Relationships: The famous analogy King - Man + Woman = Queen emerges from the model because of the linear relationships between the words.\nInitialization: This is a no-brainer but worth mentioning. Initialization to zero vectors would mean no learning. We must initialize randomly.\n2. Skip-Gram (The \u0026ldquo;Burger Flip\u0026rdquo; of CBOW) üîÑ Skip-Gram flips the CBOW logic. While CBOW tries to predict the center word given the context, Skip-Gram tries to predict the scattered context given the center word.\nCore Concept: If a word like \u0026ldquo;monarch\u0026rdquo; can accurately predict words like \u0026ldquo;king\u0026rdquo;, \u0026ldquo;throne\u0026rdquo;, \u0026ldquo;crown\u0026rdquo;, \u0026ldquo;royalty\u0026rdquo;, and \u0026ldquo;power\u0026rdquo;, then it must effectively capture the concept of royalty.\nTraining Let\u0026rsquo;s define dimensions:\n$V$: Vocabulary size $D$: Embedding dimension $C$: Number of context words Step 1: Input Lookup\nUnlike CBOW, our input is just one word vector (the center word $w_t$). We grab the vector $v_c$ from the Input Matrix $W_{in}$.\n$$ h = v_{w_t} $$\nNote: There is no averaging here. The hidden layer $h$ is simply the raw vector of the center word.\nStep 2: Output Scoring (The Broadcast)\nThe model takes this single vector $h$ and compares it against the Output Matrix $W_{out}$ (the entire vocabulary). It computes a score $u_j$ for every word $j$ in the dictionary.\n$$ u = W_{out}^T h $$\nResult $u$: A vector of size $V \\times 1$ containing raw scores.\nStep 3: Probability Conversion (Softmax)\nWe apply Softmax to turn scores into probabilities.\n$$ P(w_j | w_t) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 4: The Multi-Target Loss\nHere is the major difference from CBOW. In CBOW, we had one target (the center). In Skip-Gram, we have $2C$ targets (all the surrounding words). For a sentence \u0026ldquo;The cat sat on mat\u0026rdquo; (Center: \u0026ldquo;sat\u0026rdquo;), the targets are \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;. We want to maximize the probability of all these true context words. The Loss function ($E$) sums the error for each context word:\n$$ E = - \\sum_{w_{ctx} \\in \\text{Window}} \\log P(w_{ctx} | w_t) $$\nStep 5: Backpropagation (Accumulating Gradients)\nSince one input word is responsible for predicting multiple output words, the error signals from all those context words add up.\nAt the Output Layer:\nIf \u0026ldquo;cat\u0026rdquo; was a target, the \u0026ldquo;cat\u0026rdquo; vector in $W_{out}$ gets a signal: \u0026ldquo;Move closer to \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; If \u0026ldquo;apple\u0026rdquo; was NOT a target, the \u0026ldquo;apple\u0026rdquo; vector gets a signal: \u0026ldquo;Move away from \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; At the Hidden Layer ($h$):\nThe center word \u0026ldquo;sat\u0026rdquo; receives feedback from all its neighbors simultaneously. Error signal = (Error from \u0026ldquo;The\u0026rdquo;) + (Error from \u0026ldquo;cat\u0026rdquo;) + (Error from \u0026ldquo;on\u0026rdquo;)\u0026hellip; The vector for \u0026ldquo;sat\u0026rdquo; in $W_{in}$ moves in the average direction of all its neighbors. 3. The \u0026ldquo;Computational Nightmare\u0026rdquo; \u0026amp; Negative Sampling The equations above describe Naive Softmax.\nThe Problem: If $V = 100,000$, computing the denominator $\\sum \\exp(u_k)$ takes 100,000 operations. Doing this for every word in a training set of billions of words is impossibly slow.\nThe Solution: Negative Sampling Instead of updating the entire vocabulary (1 correct word vs 99,999 wrong ones), we approximate the problem.\nPositive Pair: (sat, cat) $\\rightarrow$ Maximize probability (Label 1). Negative Pairs: We pick $K$ (e.g., 5) random words the model didn\u0026rsquo;t see, e.g., (sat, bulldozer), (sat, quantum). $\\rightarrow$ Minimize probability (Label 0). New Equation (Sigmoid instead of Softmax):\n$$ E = - \\log \\sigma({v'_{\\text{pos}}}^T v_{\\text{in}}) - \\sum_{k=1}^K \\log \\sigma(-{v'_{\\text{neg}_{k}}}^T v_{\\text{in}}) $$ Effect: We typically only update 6 vectors per step (1 pos + 5 negs) instead of 100,000.\nLet\u0026rsquo;s see this in code:\nimport torch import torch.nn as nn import torch.optim as optim class SkipGramNegativeSampling(nn.Module): \u0026#34;\u0026#34;\u0026#34; Skip-Gram with Negative Sampling (SGNS) implementation. SGNS approximates the softmax over the entire vocabulary by instead distinguishing between a real context word (positive) and K noise words (negative). \u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embedding_dim): super(SkipGramNegativeSampling, self).__init__() # Input embeddings: used when the word is the center word self.in_embed = nn.Embedding(vocab_size, embedding_dim) # Output embeddings: used when the word is a context or negative sample self.out_embed = nn.Embedding(vocab_size, embedding_dim) # Initialize weights with small values to prevent gradient saturation initrange = 0.5 / embedding_dim self.in_embed.weight.data.uniform_(-initrange, initrange) self.out_embed.weight.data.uniform_(-initrange, initrange) def forward(self, center_words, target_words, negative_words): \u0026#34;\u0026#34;\u0026#34; Computes the negative sampling loss. Args: center_words: (batch_size) target_words: (batch_size) negative_words: (batch_size, K) where K is number of negative samples \u0026#34;\u0026#34;\u0026#34; # Retrieve vectors v_c = self.in_embed(center_words) # (batch_size, embed_dim) u_o = self.out_embed(target_words) # (batch_size, embed_dim) u_n = self.out_embed(negative_words) # (batch_size, K, embed_dim) # 1. Positive Score: log(sigmoid(v_c ¬∑ u_o)) # Compute dot product: (batch, 1, dim) @ (batch, dim, 1) -\u0026gt; (batch, 1) pos_score = torch.bmm(u_o.unsqueeze(1), v_c.unsqueeze(2)).squeeze(2) pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-7) # 2. Negative Score: sum(log(sigmoid(-v_c ¬∑ u_n))) # Compute dot products for all K samples: (batch, K, dim) @ (batch, dim, 1) -\u0026gt; (batch, K) neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2) neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-7), dim=1, keepdim=True) # Total loss is the negative of the objective function loss = -(pos_loss + neg_loss) return torch.mean(loss) # --- Configuration \u0026amp; Mock Data --- VOCAB_SIZE = 100 EMBED_DIM = 10 word_to_ix = {\u0026#39;fox\u0026#39;: 0} # Example vocabulary mapping model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock inputs: 1 center word, 1 target word, 5 negative samples center_id = torch.tensor([0], dtype=torch.long) target_id = torch.tensor([1], dtype=torch.long) negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long) # Training Step model.zero_grad() loss = model(center_id, target_id, negative_ids) loss.backward() optimizer.step() # Output Results print(f\u0026#34;Loss after one step: {loss.item():.6f}\u0026#34;) word_vec = model.in_embed(torch.tensor([word_to_ix[\u0026#39;fox\u0026#39;]])) print(f\u0026#34;Vector for \u0026#39;fox\u0026#39;:\\n{word_vec.detach().numpy()}\u0026#34;) The outputs:\nLoss after one step: 4.158468 Vector for \u0026#39;fox\u0026#39;: [[ 0.04197385 0.02400733 -0.03800093 0.01672485 -0.03872231 -0.0061478 0.0121122 0.04057864 -0.036255 0.03861175]] Interesting Facts About Skip-Gram üí° Makes Rare Words Shine: Skip-Gram is more effective at capturing the meaning of rare words because it focuses on the context of the center word. In CBOW, a rare word\u0026rsquo;s vector is averaged with others, diluting its signal. In Skip-Gram, when the rare word appears as the center, it gets the full, undivided attention of the backpropagation update.\nSlow Training: This method creates more training samples. A window of size 2 creates 4 training pairs per center word, while CBOW creates only 1 training pair per center word.\nSemantic Focus: Skip-Gram puts more emphasis on the semantic relationship between words. It views the context as the primary signal, with the center word as the target. This makes it better at capturing the meaning of words in context. Skip-Gram tends to capture semantic relationships (King/Queen) slightly better, while CBOW captures syntactic ones (walk/walked) slightly better.\n","permalink":"http://localhost:1313/posts/word-embeddings/","summary":"Understanding the mathematics behind Word2Vec, CBOW, and Skip-Gram and how they map language to vector space.","title":"Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n5. Code Examples: We\u0026rsquo;ll use sentencepiece for tokenization and compare results for different models.\nimport sentencepiece as spm import os # Configuration for training the SentencePiece model # SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words. options = { # The source text file used for learning the vocabulary \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # The base name for the generated model (.model) and vocabulary (.vocab) files \u0026#39;model_prefix\u0026#39;: \u0026#39;bpe_model\u0026#39;, # Number of unique tokens in the final vocabulary \u0026#39;vocab_size\u0026#39;: 4000, # \u0026#39;bpe\u0026#39; (Byte Pair Encoding) merges frequent pairs of characters/sequences \u0026#39;model_type\u0026#39;: \u0026#39;bpe\u0026#39;, # Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets \u0026#39;character_coverage\u0026#39;: 0.9995, # When enabled, unknown characters are decomposed into UTF-8 bytes to avoid \u0026#39;unk\u0026#39; tokens \u0026#39;byte_fallback\u0026#39;: True, # Treats digits individually (0-9), preventing large numbers from being treated as single tokens \u0026#39;split_digits\u0026#39;: True, # Prevents adding a whitespace prefix to the first token; useful for fine-grained control \u0026#39;add_dummy_prefix\u0026#39;: False } try: print(\u0026#34;Starting the training process...\u0026#34;) # SentencePieceTrainer.train takes the dictionary of options to build the BPE model spm.SentencePieceTrainer.train(**options) print(\u0026#34;Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created.\u0026#34;) # Initialize the processor and load the newly trained model sp = spm.SentencePieceProcessor() sp.load(\u0026#39;bpe_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Model Metadata:\u0026#34;) # Retrieve the total number of tokens in the vocabulary print(f\u0026#34;Total Vocab Size: {sp.get_piece_size()}\u0026#34;) # Special tokens are used for sequence boundaries and handling unknown characters print(f\u0026#34;BOS (Beginning of Sentence) ID: {sp.bos_id()}\u0026#34;) print(f\u0026#34;EOS (End of Sentence) ID: {sp.eos_id()}\u0026#34;) print(f\u0026#34;UNK (Unknown) ID: {sp.unk_id()}\u0026#34;) print(f\u0026#34;PAD (Padding) ID: {sp.pad_id()}\u0026#34;) # Test the tokenizer on sample strings to see how it breaks down text test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: shows the actual subword units (tokens) print(f\u0026#34;Subword Tokens: {sp.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: shows the numerical mapping for each token print(f\u0026#34;Numerical IDs: {sp.encode_as_ids(text)}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred during training or processing: {e}\u0026#34;) You can see the following output:\nStarting training... Starting the training process... Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created. ------------------------------ Model Metadata: Total Vocab Size: 4000 BOS (Beginning of Sentence) ID: 1 EOS (End of Sentence) ID: 2 UNK (Unknown) ID: 0 PAD (Padding) ID: -1 --- Tokenization Test --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;el\u0026#39;, \u0026#39;lo\u0026#39;, \u0026#39;‚ñÅW\u0026#39;, \u0026#39;orld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 380, 309, 476, 619, 36, 3924, 3964, 3959, 3993, 3978, 3983, 57, 3975, 3976, 60, 3974] --- Tokenization Test --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅmo\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;inst\u0026#39;, \u0026#39;ag\u0026#39;, \u0026#39;ram\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅever\u0026#39;] Numerical IDs: [1894, 304, 3284, 376, 321, 313, 312, 1008, 3313, 1325, 428, 3937, 341, 304, 3284, 1495] The ! is fallen back to UTF bytes. and the word instagrammable is split into multiple subword tokens. Also, original text can be recreated right back by simply concatenating the tokens. And replacing underscore by whitespace. Now we can try Unigram model. and see the same sentences tokenized differently.\nfrom tokenizers import Tokenizer from tokenizers.models import WordPiece from tokenizers.trainers import WordPieceTrainer from tokenizers.pre_tokenizers import Whitespace try: # 1. Initialize the WordPiece Tokenizer # We specify the [UNK] token for handling words not found in the vocabulary. tokenizer = Tokenizer(WordPiece(unk_token=\u0026#34;[UNK]\u0026#34;)) # 2. Configure Pre-tokenization # Before the subword algorithm runs, we need to split the raw text into words. # Whitespace splitting is the standard first step for most English NLP tasks. tokenizer.pre_tokenizer = Whitespace() # 3. Initialize the Trainer # We define our target vocabulary size and the special tokens required for # downstream tasks (like BERT\u0026#39;s [CLS] for classification or [SEP] for separators). trainer = WordPieceTrainer( vocab_size=4000, special_tokens=[\u0026#34;[UNK]\u0026#34;, \u0026#34;[CLS]\u0026#34;, \u0026#34;[SEP]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[MASK]\u0026#34;] ) # 4. Train the Model # The tokenizer scans the training file to build a vocabulary of the most # frequent subword units. tokenizer.train(files=[\u0026#34;train_data.txt\u0026#34;], trainer=trainer) # 5. Persist the Model # Save the configuration and vocabulary to a JSON file for future inference. tokenizer.save(\u0026#34;wordpiece.json\u0026#34;) print(\u0026#34;Training complete. \u0026#39;wordpiece.json\u0026#39; created.\u0026#34;) # 6. Metadata Inspection print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;WordPiece Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {tokenizer.get_vocab_size()}\u0026#34;) # 7. Testing Subword Tokenization # WordPiece shines at handling rare words by breaking them into meaningful chunks. test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (WordPiece) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # Encode converts raw text into a Tokenizer object containing tokens and IDs output = tokenizer.encode(text) # \u0026#39;tokens\u0026#39; shows the subword breakdown (e.g., \u0026#39;un\u0026#39;, \u0026#39;##insta\u0026#39;, etc.) print(f\u0026#34;Subword Tokens: {output.tokens}\u0026#34;) # \u0026#39;ids\u0026#39; are the numerical indices mapped to the vocabulary print(f\u0026#34;Numerical IDs: {output.ids}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred with WordPiece model: {e}\u0026#34;) You can see the following output:\nWordPiece Model Metadata: Total Vocab Size: 2609 --- Tokenization Test (WordPiece) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;H\u0026#39;, \u0026#39;##el\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;##or\u0026#39;, \u0026#39;##ld\u0026#39;, \u0026#39;[UNK]\u0026#39;, \u0026#39;[UNK]\u0026#39;] Numerical IDs: [37, 180, 214, 52, 162, 418, 0, 0] --- Tokenization Test (WordPiece) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;##os\u0026#39;, \u0026#39;##t\u0026#39;, \u0026#39;un\u0026#39;, \u0026#39;##ins\u0026#39;, \u0026#39;##ta\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;##ra\u0026#39;, \u0026#39;##m\u0026#39;, \u0026#39;##ma\u0026#39;, \u0026#39;##ble\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;ever\u0026#39;] Numerical IDs: [691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240] We can see the special character ## in the output. We can also see the breakdowns not having any semantic meaning. They are likelihood based. We can try the Unigram model now.\n# \u0026#39;Unigram\u0026#39; is the default and usually recommended over BPE in SentencePiece. options_unigram = { \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # Path to the raw text file for training \u0026#39;model_prefix\u0026#39;: \u0026#39;unigram_model\u0026#39;, # Prefix for the output .model and .vocab files \u0026#39;vocab_size\u0026#39;: 1200, # Desired size of the final vocabulary \u0026#39;model_type\u0026#39;: \u0026#39;unigram\u0026#39;, # Specifies the Unigram language model algorithm \u0026#39;character_coverage\u0026#39;: 0.9995, # Percentage of characters covered by the model (0.9995 is standard for Latin scripts) \u0026#39;byte_fallback\u0026#39;: True, # Enables mapping unknown characters to UTF-8 bytes to avoid \u0026lt;unk\u0026gt; tokens \u0026#39;split_digits\u0026#39;: True, # Treats each digit as an individual token (useful for numerical data) \u0026#39;add_dummy_prefix\u0026#39;: False # Prevents adding a leading space (SentencePiece default is True) } try: # 1. Train the SentencePiece model using the defined options print(\u0026#34;Starting Unigram training...\u0026#34;) spm.SentencePieceTrainer.train(**options_unigram) print(\u0026#34;Training complete. \u0026#39;unigram_model.model\u0026#39; created.\u0026#34;) # 2. Load the trained model into a processor instance for inference sp_unigram = spm.SentencePieceProcessor() sp_unigram.load(\u0026#39;unigram_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Unigram Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {sp_unigram.get_piece_size()}\u0026#34;) # 3. Define test cases to evaluate how the model handles common and rare words test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] # 4. Iterate through test sentences to visualize subword segmentation for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (Unigram) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: Converts text into subword strings (visual representation) print(f\u0026#34;Subword Tokens: {sp_unigram.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: Converts text into numerical indices for model input print(f\u0026#34;Numerical IDs: {sp_unigram.encode_as_ids(text)}\u0026#34;) except Exception as e: # Handle potential errors during training or loading (e.g., missing input file) print(f\u0026#34;An error occurred with Unigram model: {e}\u0026#34;) The output is as follows:\nUnigram Model Metadata: Total Vocab Size: 1200 --- Tokenization Test (Unigram) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ll\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;ld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311] --- Tokenization Test (Unigram) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;T\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅm\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;sta\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;ra\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ver\u0026#39;] ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"A comprehensive guide to tokenization strategies: BPE, WordPiece, Unigram, and SentencePiece.","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"A brief introduction to Vectors \u0026amp; Verbs and formatting verification.","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"\nFeedforward network (FFN) blocks are the least talked about when in context to LLMs. But that is where more than 70% of the paramters come from. Something as simple and convinient as a couple of linear transforming matrices become a burden when models start scaling in Billions of parameters. So, getting the maximum juice of a lean structure is a challenge. Plus, mostly under represented, because the FFN sub-layer acts as the primary \u0026ldquo;knowledge\u0026rdquo; processing engine, often containing the majority of a model\u0026rsquo;s total parameters.\nStandard Position Wise Feedforward Network This is as simple as it can get, two-layered fully connected network of neurons with a single activation switch, most probably a ReLU variant (for LLMs). $$ FFN(x) = max(0, W_2(W_1(x)) + b_1)W_2 + b_2 $$\n","permalink":"http://localhost:1313/posts/feedforward-networks/","summary":"Exploring the hidden layers of trillion-parameter switchboards: Feedforward Neural Networks and Activation Functions.","title":"Anatomy of Trillion-Parameter Switchboards: Understanding Feedforward Blocks"},{"content":"\nWe are about to touch the holy grail of modern AI. The 2017 declaration was simple: Attention Is All You Need. But as we‚Äôve raced from GPT-1 to reasoning models, the definition of \u0026lsquo;Attention\u0026rsquo; has quietly transformed. Is the algorithm ruling the world today actually the same one we started with? Let‚Äôs look under the hood.\nüî• The Core Concept At its heart, the attention mechanism is a form of weighted aggregation. It is often described using a database retrieval analogy. Imagine you have a database of information (context).\nQuery ($Q$): What are you currently looking for? (The token being generated). Key ($K$): The indexing tag associated with each piece of information. Value ($V$): The actual information (The context token). The \u0026ldquo;Attention\u0026rdquo; is the process of matching your Query against all the Keys to determine how much \u0026ldquo;weight\u0026rdquo; to put on each Value. Attention scores are calculated in each transformer layer at every step of the sequence. This is a single unifying factor in all transformer models, from BERT to Gemini.\nüî• Standard Multi-Head Attention (MHA) The original mechanism introduced in the paper \u0026ldquo;Attention Is All You Need\u0026rdquo;.\nHere, instead of performing the Key-Query-Value (KQV) operation on the entire context only once, the attention is split into $N$ parallel heads. Each head learns a different aspect of the language (e.g., word meaning, syntax, semantic relationships, etc.). The equation is a simple culmination of dot products aided by a regularizer like Softmax.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nWhere:\n$QK^T$ computes the similarity scores between the Query and Key. $\\sqrt{d_k}$ is a scaling factor to prevent the dot product from growing too large, mitigating the vanishing/exploding gradient problem. The weighted sum is applied to the values of $V$. The Memory Bandwidth Bottleneck: \u0026gt; During inference, the model must store Key and Value matrices for all previous tokens. This is called the KV Cache. In MHA, every single head has its own unique set of Keys and Values. For large models with massive context windows, loading this massive KV cache from memory (HBM) to the computation chip (SRAM) becomes the primary bottleneck, slowing down generation significantly. Earlier models could often only generate ~8 tokens per second at inference due to this constraint.\nüî• Multi-Query Attention (MQA) This approach addresses the memory bottleneck by sharing the same set of Keys and Values across all heads while keeping separate Query heads.\nMHA: $H$ Query heads, $H$ Key heads, $H$ Value heads MQA: $H$ Query heads, 1 Key head, 1 Value head This drastically reduces the memory requirements of the KV cache by a factor of $H$ (often 8x or 16x). The GPU needs to load far less data, significantly speeding up inference. However, it does not reduce the computation (FLOPs)‚Äîso why is it faster?\nThis can be explained with the Kitchen Analogy: Imagine a kitchen (GPU) with 8 chefs (Heads).\nComputation (Math): The Chef chopping vegetables. Memory Access (Bandwidth): The Assistant running to the fridge to get ingredients. In MHA: Each chef demands unique vegetables from the fridge. The assistant runs back and forth 8 times, fetching 8 different crates. Most of the time is spent waiting for the assistant to return. This results in idle cores waiting for data to load from HBM.\nIn MQA: All chefs agree to use the same vegetables. The assistant runs back and forth only once, fetching one giant crate of potatoes and dumping it. This results in minimized memory transfer and no idle cores.\nThe Technical Reality: During text generation, the model is memory-bound, not compute-bound. The time it takes to move the KV Cache from HBM (High Bandwidth Memory) to the chip\u0026rsquo;s SRAM is the bottleneck. MQA reduces the data volume of that move by a factor of $H$.\nThe Capacity Gap: MQA is a trade-off between nuanced response and memory capacity. The model loses the ability to store different types of information, as a single $V$ matrix must now store all nuances of information flow in the language.\nThe Surgery Trick (Uptraining): Google Research figured out a way to take an existing model with MHA and convert it to MQA by averaging the weight matrices of $H$ heads. This is called Uptraining. You then train this \u0026ldquo;Frankenstein\u0026rdquo; model for ~5% of the original training steps for adjustments. This yields an MQA model that is nearly as good as the original MHA but vastly faster.\nüî• Grouped Query Attention (GQA) A \u0026ldquo;best of both worlds\u0026rdquo; solution used in modern models like Llama 2 and Llama 3.\nHow it works: GQA acts as an interpolation between MHA and MQA. Instead of having just 1 Key head (MQA) or $H$ Key heads (MHA), it divides the query heads into $G$ groups. Each group shares a single Key and Value head.\nEquation modification: If you have 8 query heads, you might create 4 groups (2 queries per group). This reduces the KV cache by 2x instead of 8x, preserving more quality than MQA while still speeding up inference.\nMHA: 8 people (heads) are working on a project. Each person has their own unique filing cabinet (KV cache). It takes forever to move 8 cabinets around. MQA: 8 people share one single filing cabinet. It\u0026rsquo;s very fast to move, but they fight over the organization and lose detail. GQA: The 8 people split into 4 teams of 2. Each team shares a cabinet. It\u0026rsquo;s a balance between speed and organizational detail. üî• Multi-Head Latent Attention (MLA) This is the most advanced mechanism for attention, used in models like DeepSeek-V3. It is a sophisticated design for massive efficiency that takes a radical approach: mathematically compressing the memory into a low-rank \u0026ldquo;latent\u0026rdquo; vector.\nKV Cache Explosion: As mentioned before, for a massive model like DeepSeek-V3 (671B parameters), storing the full Keys and Values for every token in a long conversation requires terabytes of high-speed memory (HBM), and chips spend more time waiting than churning.\nMHA (Standard) stores everything. MQA deletes redundancy but hurts quality. MLA keeps the quality of MHA but compresses the data 4x-6x smaller than even GQA.\nThe Zip File Approach: Low-Rank Joint Compression Let\u0026rsquo;s compare the math of MHA and MLA. In MHA, for every single token, we generate a unique Key ($k$) and Value ($v$) matrix for each attention head.\nThe Memory Problem: The $k$ is stored in the KV Cache. For 128 heads, for example, it calculates a massive vector of $128 \\times 128 = 16,384$ floats per token.\nFor MHA, the attention score looks like: $$\\text{Score} = q^T \\cdot k$$\nBut for MLA, instead of storing the full $k$, it projects the input down into a tiny latent vector $c_{KV}$, which is much smaller (e.g., 512 floats). $$c_{KV} = W_{DKV} \\cdot x$$\nNow here is the catch: Naive compression and decompression would mean constructing a small latent matrix for every token and deconstructing it every single time the information is required, which defeats the purpose of compression.\n$$k_{\\text{reconstructed}} = W_{UK} \\cdot c_{KV}$$ $$\\text{Score} = q^T \\cdot k_{\\text{reconstructed}}$$\nThe Matrix Absorption Trick (Optimization) Instead of reconstructing $k$ for every token, we can absorb the Up-Projection matrix ($W_{UK}$) into the Down-Projection matrix ($W_{DKV}$) and absorb the query $q$ into a new $Q$.\nFrom the original equation: $$\\text{Score} = q^T \\cdot (\\underbrace{W_{UK} \\cdot c_{KV}}_{\\text{This is } k})$$\nWe associate differently:\n$$\\text{Score} = (\\underbrace{q^T \\cdot W_{UK}}_{\\text{Absorbed Query}}) \\cdot c_{KV}$$ We change the order in which the matrix multiplication is performed.\nHow is this allowed? In linear algebra, matrix multiplication is associative:\n$$(A \\cdot B) \\cdot C = A \\cdot (B \\cdot C)$$ We can move the parentheses! Instead of grouping $(W_{UK} \\cdot c_{KV})$ to make the Key, we group $(q^T \\cdot W_{UK})$ to make a new Query.\nNote: The positional embedding information is left untouched. That rotational information from algorithms like RoPE is preserved outside the compression; it requires minimal space anyways.\n","permalink":"http://localhost:1313/posts/attention-mechanisms/","summary":"We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek\u0026rsquo;s MLA, how has the definition of \u0026lsquo;Attention\u0026rsquo; transformed?","title":"Attention Is All You Need, but exactly which one?: MHA, GQA and MLA"},{"content":"\nPositional Embeddings are the \u0026ldquo;voice\u0026rdquo; that tell Transformers where words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.\n\u0026ldquo;The dog chased the cat\u0026rdquo; and \u0026ldquo;cat chased the dog\u0026rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.\nüî• Sinusoidal Embeddings: The Odometer of Language Introduced in the seminal \u0026ldquo;Attention Is All You Need\u0026rdquo; paper, this was the industry standard for many years. This approach adds a vector to each word embedding based on its position. The vector is computed out of wavelike patterns that are periodic and smooth. Mathematically, for a position $pos$ and embedding dimension $i$:\n$$ PE_{(pos, i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $$\nThe model uses sine and cosine waves of different frequencies to capture both short and long-range dependencies. The frequency of the waves decreases as the embedding dimension increases, which helps capture long-range dependencies in the input sequence.\nThe creators wanted the model to calculate distance easily, which is why a linear function was used. The model simply rotates the current position vector using a rotation matrix to derive the next position.\n$$ \\begin{pmatrix} \\cos(k) \u0026 \\sin(k) \\\\ -\\sin(k) \u0026 \\cos(k) \\end{pmatrix} \\cdot PE(pos) \\approx PE(pos + k) $$ Low dimensions act like a fast-ticking clock (high frequency, changing rapidly) and high dimensions act like a slow clock (low frequency, changing slowly).\nThe number 10,000 is an arbitrarily chosen high number to reduce Aliasing. By choosing such a high number, the creators ensured that the wavelength of the slowest dimension is incredibly long. Consequently, the pattern practically never repeats, and each position remains unique.\nThis combination works like a mechanical odometer dial. Each subsequent dimension acts as a slower-moving gear. The model learns to read these \u0026ldquo;dials\u0026rdquo; to understand exactly where a word sits in the sequence.\nThis mitigated the challenge of exploding gradients that previous array-based positional embeddings suffered from (e.g., adding integers 1, 2, 3\u0026hellip;). This model keeps values between -1 and +1, adding stability while ensuring every position gets a distinct code.\nThe Problem: This approach pollutes the word embedding matrix (semantic information). While high-dimensional vectors are almost always orthogonal‚Äîmeaning the noise doesn\u0026rsquo;t fully destroy the word\u0026rsquo;s identity‚Äîit does not strictly guarantee purity.\nThis led to a more mature approach.\nüî• ALiBi: The Fading Streetlight Attention with Linear Biases (ALiBi) is a method where, instead of adding a vector to the embeddings at the start, the model injects positional information linearly, directly into the attention scores. If Sinusoidal Encoding asks, \u0026ldquo;Where am I on the map?\u0026rdquo;, ALiBi asks, \u0026ldquo;How much harder should I squint to see you?\u0026rdquo;\nThe biggest problem ALiBi solved is the \u0026ldquo;Invisible Wall\u0026rdquo;. If you train a Transformer on sequences of length 2048 and then feed it a book with 3000 words, it will crash or output garbage at word 2049. This is a failure of Extrapolation.\nALiBi throws away the idea of adding positional vectors entirely. Instead, it devises a \u0026ldquo;penalty\u0026rdquo; system for attention scores:\n$$ \\text{Score} = (q \\cdot k^\\top) - (m \\cdot \\text{distance}) $$\nDistance: If words A and B are neighbors, the distance is 0, so the penalty is 0. Although the distance is linear, it does not cause exploding gradients because the raw attention scores are normalized by a softmax function. This distance prioritizes nearby words (local context) over distant ones.\nSlope ($m$): If we always penalize distance equally, the model becomes myopic. ALiBi mitigates this by assigning a different slope value to each attention head:\nHead 1 (The Historian): Very low slope. The penalty grows slowly, allowing it to see far back. Head 2 (The Reader): Medium slope. Head 3 (The Myopic): Very high slope (e.g., 0.5); it effectively cannot see beyond 2-3 words. This mimics holding a lantern. Words close to you are bright, while words further away naturally fade into darkness.\nExtrapolation: If you walk 5 miles down the road (a longer sequence), your lantern works exactly the same way. The physics of light decay are constant, allowing the model to handle sequences longer than it was trained on.\nHowever, ALiBi forces a \u0026ldquo;sliding window\u0026rdquo; view where distant words eventually become invisible. This brings us to the modern standard.\nüî• RoPE: The Rotary Revolution Rotary Position Embeddings (RoPE) is the current industry standard (used in Llama 3, Mistral, PaLM) because it mathematically unifies \u0026ldquo;absolute position\u0026rdquo; with \u0026ldquo;relative position.\u0026rdquo; This method rotates the vectors in the Query and Key matrices based on their absolute position.\nIf a vector has 512 dimensions, RoPE breaks it into 256 pairs. Each pair is rotated by a different frequency:\n$$ \\theta_i = 10000^{-2i/d} $$\nThe First pair ($i = 0$) is rotated by the highest frequency. The Last pair ($i = 255$) is rotated by the lowest frequency. This spread ensures the model has \u0026ldquo;high precision\u0026rdquo; (fast rotation) to distinguish immediate neighbors, and \u0026ldquo;low precision\u0026rdquo; (slow rotation) to track global position without the pattern repeating.\nDoesn\u0026rsquo;t rotation change meaning? Yes and No. The vector for \u0026ldquo;Apple\u0026rdquo; at position 1 and \u0026ldquo;Apple\u0026rdquo; at position 256 will look completely different in terms of coordinates. However, rotation does not change the semantic strength (magnitude/norm) of the vector.\nThe Relativity Trick: Transformers compare words using dot products (angles). RoPE relies on the fact that the attention mechanism ignores the absolute rotation and only detects the difference in rotation (relative position). Thus, the \u0026ldquo;relative meaning\u0026rdquo; is encoded purely into the angle between words, leaving the semantic \u0026ldquo;magnitude\u0026rdquo; untouched.\nüî• LongRoPE: The Bifocal Lens While RoPE is powerful, it struggles when stretching context windows from 4k to 2 million tokens. The \u0026ldquo;fast\u0026rdquo; dimensions spin so rapidly over long distances that they become random noise. LongRoPE solves this using an evolutionary search algorithm to find a unique scaling factor ($\\lambda$) for each dimension.\nThe Equations Instead of rotating by the standard $\\theta_i$, LongRoPE rotates by a rescaled frequency:\n$$ \\theta\u0026rsquo;_i = \\lambda_i \\theta_i $$\nTo efficiently find these $\\lambda$ values without searching an infinite space, the algorithm enforces a monotonicity constraint based on NTK theory:\n$$ \\lambda_i \\le \\lambda_{i+1} $$\nThis ensures that low-frequency dimensions (global context) are stretched more than high-frequency dimensions (local context), creating a \u0026ldquo;bifocal\u0026rdquo; effect:\nHigh Frequencies (Local): Kept sharp ($\\lambda \\approx 1$) to maintain grammar. Low Frequencies (Global): Stretched ($\\lambda \u0026gt; 1$) to track massive distances without repeating. Not worth perusing more since the next one is much better at long context positional encoding. üî• HoPE: The Hyperbolic Slide Hyperbolic Rotary Positional Encoding (HoPE) moves from the geometry of a circle to the geometry of a hyperbola (inspired by Lorentz transformations). It was designed to fix a subtle flaw in RoPE: the \u0026ldquo;wobbly\u0026rdquo; or oscillatory nature of attention scores.\nThe Core Problem: RoPE\u0026rsquo;s \u0026ldquo;Wobble\u0026rdquo; In standard RoPE, we rotate vectors around a circle.\nThe Issue: Circles repeat. A dial at $361^\\circ$ looks identical to $1^\\circ$. Even with different frequencies, the dot product fluctuates up and down as distance increases. The Consequence: The model gets confused at long distances, potentially mistaking a word 1,000 tokens away for a neighbor just because the rotation cycles aligned, creating \u0026ldquo;noise.\u0026rdquo; The \u0026ldquo;Hyperbolic\u0026rdquo; Shift HoPE replaces trigonometric functions ($\\sin, \\cos$) with hyperbolic functions ($\\sinh, \\cosh$).\nCircular Rotation: Keeps distance from the center constant. Hyperbolic Rotation: Moves points along a hyperbola. Crucially, hyperbolic functions do not repeat; they grow or decay exponentially. By using hyperbolic rotation, HoPE guarantees that as words get further apart, the attention score decays monotonically (smoothly drops).\nThe Equations HoPE uses a hyperbolic matrix $B(\\theta, m)$ defined using the Lorentz Boost structure:\n$$ B(\\theta, m) = \\begin{pmatrix} \\cosh(m\\theta) \u0026 \\sinh(m\\theta) \\\\ \\sinh(m\\theta) \u0026 \\cosh(m\\theta) \\end{pmatrix} $$ Since hyperbolic functions grow to infinity, HoPE introduces a decay penalty ($e^{-m\\theta'}$). The final attention score simplifies beautifully to a function of relative distance: $$ \\text{Score} \\propto e^{-|m-n|(\\theta\u0026rsquo; - \\theta)} $$\nThis ensures the attention score exponentially decays as distance increases, eliminating the wobbles of standard RoPE. This the latest model that is taking up the space in the industry for extremely long context models.\n","permalink":"http://localhost:1313/posts/positional-embeddings/","summary":"From Sinusoidal to RoPE and HoPE: How Transformers learn to process word order and sequence length.","title":"The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE"},{"content":"Imagine trying to teach a computer the difference between \u0026ldquo;Apple\u0026rdquo; the fruit and \u0026ldquo;Apple\u0026rdquo; the company. To us, the distinction is intuitive. To a machine, it‚Äôs just a string of characters. How do we turn these strings into meaningful math? While early attempts like Word2Vec gave us a great start, they missed the forest for the trees‚Äîor in some cases, the twigs for the branches. Enter GloVe and FastText: two algorithms that revolutionized how machines understand the nuances of human language.\nPrevious static embedding models like Word2Vec successfully captured the local semantics of words, but they failed in addressing contexts between words that might not always appear in the same context window. Furthermore, they treated words as indivisible atomic units in continuous vector spaces, missing out on the internal structure of language.\nThe two simple but effective ideas behind GloVe and FastText are subword units and global co-occurrence. Let\u0026rsquo;s dive into them.\nGloVe: A Global Mapping Strategy Global Vectors for Word Representation (GloVe) was created at Stanford. The core idea was to train a model where word vectors are defined by how often they appear around other words across the entire corpus, rather than just locally.\nTraining Steps: Co-occurrence Matrix X: GloVe scans the entire corpus and creates a giant matrix (spreadsheet) recording the counts of how often words appear near each other. Factorization: It compresses this giant matrix into smaller, dense vectors. Objective: The function tries to equate the dot product of two word vectors with the logarithm of their probability of appearing together. The error between these values is then backpropagated to update the vectors. $$w_i \\cdot w_j + b_i + b_j = \\log(X_{ij})$$\n$w$: The word vector (what we want). $X_{ij}$: The count from our matrix. The Loss Function: We can\u0026rsquo;t satisfy the equation above perfectly for every word pair. So, we use a weighting function to minimize the squared error:\n$$J = \\sum_{i,j=1}^V f(X_{ij}) ({w_i}^T w_j + b_i + b_j - \\log(X_{ij}))^2$$\nIf we do not use the weighting function, the loss would be infinite for rare words (since $\\log(0) = \\infty$) and frequent words (like \u0026rsquo;the\u0026rsquo;) would overpower the model.\n$$ f(x) = \\begin{cases} (x/x_{max})^\\alpha \u0026amp; \\text{if } x \u0026lt; x_{max} \\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$\n(Typically $x_{max}=100$ and $\\alpha=0.75$)\nFastText: The Lego Bricks Strategy FastText was created by Facebook AI Research (FAIR). The idea was to treat words not as solid rocks, but as structures made of \u0026ldquo;Lego bricks\u0026rdquo; (subword units).\nWord2Vec struggled to create embeddings for words it hadn\u0026rsquo;t seen before (Out-Of-Vocabulary or OOV words). FastText solves this by breaking words down.\nGloVe treats a word like \u0026ldquo;Apple\u0026rdquo; as a single, atomic unit. FastText breaks \u0026ldquo;Apple\u0026rdquo; into character n-grams: \u0026lt;ap, app, ppl, ple, le\u0026gt; For a complex or new word like \u0026ldquo;Applesauce\u0026rdquo;, the model might not know the whole word, but it recognizes the Lego brick for \u0026ldquo;Apple\u0026rdquo; and the remaining bits from other subwords.\nFastText takes its inspiration from the SkipGram architecture. The vector for a word is simply the sum of the vectors of its n-gram subwords:\n$$v_w = \\sum_{g \\in G_w} z_g$$\nObjective: The objective is to predict context words given a target word.\n$$P(w_c | w_t) = \\frac{e^{s(v_{w_t} \\cdot v_{w_c})}}{\\sum_{w \\in V} e^{s(v_{w_t} \\cdot v_w)}}$$\nAnd the scoring function is defined as:\n$$s(w_t, w_c) = u_{w_t}^T (\\sum_{g \\in G_w} z_g)$$\n$u_{w_c}$: The vector for the context word (output vector). $z_g$: The vector for the n-gram $g$ (input vector). Modifications to Softmax: Calculating the full softmax for every word in the vocabulary is computationally expensive. FastText uses two methods to approximate this efficiently.\nOption A: Negative Sampling\nWe pick a few random \u0026ldquo;wrong\u0026rdquo; words every time to serve as \u0026ldquo;negative samples\u0026rdquo; and calculate the loss only for the correct word and these noise words.\n$$J = - \\log \\sigma(s(w_t, w_c)) - \\sum\\limits_{i=1}^{N} \\mathbb{E}_{n_i \\sim P_n(w)} [\\log \\sigma(-s(w_t, n_i))]$$\nThe sigmoid function $\\sigma$ squashes the score between 0 and 1. The first term pushes the probability of the correct word towards 1. The second term pushes the probability of the noise words towards 0. Option B: Hierarchical Softmax (Faster for infrequent words)\nInstead of a flat list of words, imagine the vocabulary arranged as a binary tree (Huffman Tree).\nRoot: Top of the tree. Leaves: The actual words. To calculate the probability of a word, we trace a path from the root to that leaf. At every branching node, we calculate the probability (sigmoid) of going left or right. The probability of a word $w$ is the product of the probabilities of the turns taken to reach it. Code Example from gensim.models import FastText from gensim.test.utils import common_texts # Initialize the FastText model with specific hyperparameters model = FastText( vector_size=100, # Dimensionality of the word vectors window=5, # Maximum distance between the current and predicted word within a sentence min_count=1, # Ignores all words with total frequency lower than this sg=1, # Training algorithm: 1 for skip-gram; 0 for CBOW hs=0, # If 0, and negative is non-zero, negative sampling will be used negative=5, # Number of \u0026#34;noise words\u0026#34; to be drawn for negative sampling workers=4, # Number of worker threads to train the model epochs=10, # Number of iterations over the corpus min_n=3, # Minimum length of character n-grams max_n=6 # Maximum length of character n-grams ) # Build the vocabulary from the provided text corpus model.build_vocab(common_texts) # Train the model on the corpus model.train( common_texts, total_examples=model.corpus_count, epochs=model.epochs ) # Calculate and print the cosine similarity between two words in the vocabulary print(f\u0026#34;Similarity between \u0026#39;computer\u0026#39; and \u0026#39;human\u0026#39;: {model.wv.similarity(\u0026#39;computer\u0026#39;, \u0026#39;human\u0026#39;)}\u0026#34;) # Demonstrate FastText\u0026#39;s ability to handle Out-Of-Vocabulary (OOV) words # Even if \u0026#39;computation\u0026#39; wasn\u0026#39;t in the training data, FastText constructs a vector using character n-grams oov_vector = model.wv[\u0026#39;computation\u0026#39;] # Check if a specific word exists in the model\u0026#39;s fixed vocabulary index word = \u0026#34;computer\u0026#34; is_in_vocab = word in model.wv.key_to_index print(f\u0026#34;\\nIs the word \u0026#39;{word}\u0026#39; in the model\u0026#39;s vocabulary? {is_in_vocab}\u0026#34;) ","permalink":"http://localhost:1313/posts/glove-and-fasttext/","summary":"How subword units and global co-occurrence matrices allow GloVe and FastText to capture nuances that Word2Vec missed.","title":"The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText"},{"content":" Before we had Large Language Models writing poetry, we had to teach computers that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; are related not just by spelling, but by meaning. This is the story of that breakthrough. It‚Äôs the moment we stopped counting words and started mapping their souls‚Äîturning raw text into a mathematical landscape where math can solve analogies. Welcome to the world of Word2Vec. üîÆ\nLanguage models require vector representations of words to capture semantic relationships. Before the 2010s, models used word count-based vector representations that captured only the frequency of words (e.g., One-Hot Encoding). The Problems: üöß\nThe Curse of Dimensionality Lack of meaning (Synonyms were treated as mathematically unrelated). In 2011, Mikolov et al. at Google introduced Word2Vec, which used a shallow neural network to learn vector representations of words. The best part? It could create much denser relationships between words than prior models, and it was unsupervised.\nThe training (prediction) is essentially \u0026ldquo;fake\u0026rdquo;‚Äîa pretext task. But the weight matrices we get on the side are a gold mine of fine-grained semantic relationships. ‚õèÔ∏è\nThere are two popular variants of Word2Vec:\nSkipGram Continuous Bag Of Words (CBOW) 1. Continuous Bag of Words (CBOW) üß© CBOW is a simple \u0026lsquo;fill in the blanks\u0026rsquo; machine. It takes the surrounding words as inputs and tries to predict the center word.\n\u0026ldquo;Continuous\u0026rdquo;: It operates in continuous vector space (unlike the discrete space of n-gram models).\n\u0026ldquo;Bag of words\u0026rdquo;: The order of the context does not matter. \u0026ldquo;The cat sat on the mat\u0026rdquo; and \u0026ldquo;The mat sat on the cat\u0026rdquo; are the same for CBOW. They produce exactly the same predictions.\nObjective: Maximize the probability of the target word $w_t$ given its context $w_{t-C}, \\dots, w_{t+C}$.\n2. Training Defining Dimensions:\n$V$: Vocabulary size (number of unique words in the corpus, e.g., 10,000) $N$: Dimension of the embedding space (e.g., 100) $C$: Number of context words (e.g., 2 words on left of target and 2 words on right) Step 1: Input Lookup \u0026amp; One-Hot Encoding\nMathematically, we represent each context word as a one-hot vector $x^{(c)}$. We have an Input Matrix $W_{in}$ of size $V \\times N$. For each context word (e.g., \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;), we select its corresponding row from $W_{in}$.\n$$ v_c = W_{in}^T x^{(c)} $$\nWhere $v_c$ is the $N$-dimensional vector for context word $c$.\nStep 2: Projection (The \u0026ldquo;Mixing\u0026rdquo;)\nWe combine the context vectors. In standard CBOW, we simply average them. This creates the hidden layer vector $h$.\n$$ h = \\frac{1}{2C} \\sum_{c=1}^{2C} v_c $$\n$h$ is a single vector of size $N \\times 1$. Note: This operation is linear and contains no activation function (like ReLU or Sigmoid).\nStep 3: Output Scoring (The \u0026ldquo;Dot Product\u0026rdquo;)\nWe have a second matrix, the Output Matrix $W_{out}$ of size $N \\times V$. We compute the raw score (logit) $u_j$ for every word $j$ in the vocabulary by taking the dot product of the hidden vector $h$ and the output vector $v\u0026rsquo;_j$.\n$$ u_j = {v\u0026rsquo;_j}^T h $$\nOr in matrix form:\n$$ u = W_{out}^T h $$\nResult $u$ is a vector of size $V \\times 1$.\nStep 4: Probability Conversion (Softmax)\nWe convert the raw scores $u$ into probabilities using the Softmax function. This tells us the probability that word $w_j$ is the center word.\n$$ y_j = P(w_j | \\text{context}) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 5: Loss Calculation (Cross-Entropy)\nWe compare the predicted distribution $y$ with the actual ground truth $t$ (a one-hot vector where the index of the true word is 1). The loss function $E$ is:\n$$ E = - \\sum_{j=1}^V t_j \\log(y_j) $$\nSince $t_j$ is 0 for all words except the target word (let\u0026rsquo;s call the target index $j^*$), this simplifies to:\n$$ E = - \\log(y_{j^*}) $$\n3. Backpropagation Details We need to update the weights to minimize $E$. We use the Chain Rule.\nA. Gradient w.r.t. Output Matrix ($W_{out}$)\nWe want to know how the error changes with respect to the raw score $u_j$.\n$$ \\frac{\\partial E}{\\partial u_j} = y_j - t_j $$\nLet\u0026rsquo;s call this error term $e_j$.\nIf $j$ is the target word: $e_j = y_j - 1$ (Result is negative; we boost the score). If $j$ is not target: $e_j = y_j - 0$ (Result is positive; we suppress the score). The update rule for the output vector $v\u0026rsquo;_j$ becomes:\n$$ v'_{j}(\\text{new}) = v'_{j}(\\text{old}) - \\eta \\cdot e_j \\cdot h $$ ($\\eta$ is the learning rate)\nB. Gradient w.r.t. Hidden Layer ($h$)\nWe backpropagate the error from the output layer to the hidden layer.\n$$ EH = \\sum_{j=1}^V e_j \\cdot v\u0026rsquo;_j $$\n$EH$ is an $N$-dimensional vector representing the aggregate error passed back to the projection layer.\nC. Gradient w.r.t. Input Matrix ($W_{in}$)\nSince $h$ was just an average of the input vectors, the error $EH$ is distributed to each context word\u0026rsquo;s input vector. For every word $w_c$ in the context:\n$$ v_{w_c}(\\text{new}) = v_{w_c}(\\text{old}) - \\eta \\cdot \\frac{1}{2C} \\cdot EH $$\nLet\u0026rsquo;s build a single pass through the network for a given context.\nimport torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # Define a simple Continuous Bag of Words (CBOW) style model class CBOWModel(nn.Module): def __init__(self, vocab_size, embedding_dim): super(CBOWModel, self).__init__() # The embedding layer stores the word vectors we want to learn self.embeddings = nn.Embedding(vocab_size, embedding_dim) # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word self.linear = nn.Linear(embedding_dim, vocab_size) def forward(self, inputs): # inputs: tensor of word indices for the context embeds = self.embeddings(inputs) # Aggregate context by calculating the mean of the word embeddings h = torch.mean(embeds, dim=1) # Produce logits (raw scores) for each word in the vocabulary logits = self.linear(h) return logits # Setup vocabulary and mappings word_to_ix = {\u0026#34;the\u0026#34;: 0, \u0026#34;quick\u0026#34;: 1, \u0026#34;brown\u0026#34;: 2, \u0026#34;fox\u0026#34;: 3, \u0026#34;jumps\u0026#34;: 4, \u0026#34;over\u0026#34;: 5, \u0026#34;lazy\u0026#34;: 6, \u0026#34;dog\u0026#34;: 7} ix_to_word = {v: k for k, v in word_to_ix.items()} # Configuration constants EMBEDDING_DIM = 5 VOCAB_SIZE = len(word_to_ix) LEARNING_RATE = 0.01 # Initialize the model, loss function, and optimizer model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM) loss_function = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE) # Prepare dummy training data: context words and the target word # Context: [\u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;over\u0026#34;] -\u0026gt; Target: \u0026#34;fox\u0026#34; context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long) target_idxs = torch.tensor([3], dtype=torch.long) # Perform a single optimization step model.zero_grad() # Forward pass: we reshape context to (1, -1) to simulate a batch of size 1 logits = model(context_idxs.view(1, -1)) # Calculate loss against the target word index loss = loss_function(logits, target_idxs) # Backpropagate and update weights loss.backward() optimizer.step() # Output results for the blog post print(\u0026#34;=== Model Training Snapshot ===\u0026#34;) print(f\u0026#34;Calculated Loss: {loss.item():.6f}\u0026#34;) print(f\u0026#34;\\n=== Learned Vector for \u0026#39;jumps\u0026#39; ===\u0026#34;) word_vec = model.embeddings(torch.tensor([word_to_ix[\u0026#39;jumps\u0026#39;]])) print(word_vec.detach().numpy()) print(\u0026#34;\\n=== Embedding Matrix (Weights) ===\u0026#34;) print(model.embeddings.weight.detach().numpy()) print(\u0026#34;\\n=== Linear Layer Weights ===\u0026#34;) print(model.linear.weight.detach().numpy()) Output shows single pass loss and learned vector for \u0026lsquo;jumps\u0026rsquo; word. This is a single training sample result but after multiple such passes both Embedding and linear weights should be very similar.\n=== Model Training Snapshot === Calculated Loss: 1.989656 === Learned Vector for \u0026#39;jumps\u0026#39; === [[-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775]] === Embedding Matrix (Weights) === [[ 1.66039 -0.11371879 0.6246518 0.35860053 -1.2504417 ] [-2.1847186 -0.77199775 -0.17050214 -0.38411248 -0.03913084] [ 0.11852697 0.90073633 0.8847807 0.7404524 0.900149 ] [ 0.07440972 -0.40259898 2.6246994 -0.08851447 0.02660969] [-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775] [-0.9245572 -0.5545908 0.9083091 -1.0755049 0.84047747] [-1.0237687 0.59466314 0.05621134 -0.6202532 1.3664424 ] [ 0.60998917 -1.0549186 1.6103884 0.8724912 -1.2486908 ]] === Linear Layer Weights === [[ 0.02165058 -0.28883642 0.14545658 -0.3442509 0.32704315] [-0.18731792 0.28583744 0.22635977 0.13245736 0.29019794] [ 0.3158916 -0.15826383 -0.03203773 0.16377363 -0.41457543] [-0.05080034 0.4180087 0.11228557 0.30218413 0.3025514 ] [-0.38419306 0.24475925 -0.39210224 -0.38660625 -0.2673145 ] [-0.32321444 0.12200444 -0.03569533 0.2891424 -0.07345333] [-0.33704326 0.2521956 0.31587374 0.22590035 0.29866052] [ 0.4117266 -0.44231793 0.24064957 -0.29684234 0.333821 ]] Interesting Facts About CBOW üß† The \u0026ldquo;Averaging Problem\u0026rdquo;: This is a common issue in CBOW models. The averaging and training is order-agnostic. All variants of word order in the context window will generate the same output. \u0026ldquo;Dog bit the man\u0026rdquo; and \u0026ldquo;Man bit the dog\u0026rdquo; are the same in CBOW\u0026rsquo;s eyes.\nSmoothing Effect: CBOW models are much faster to train than Skip-Gram but are generally smoother.\nRare Words: CBOW models struggle slightly with rare words because the presence of surrounding common words (like articles and prepositions) can skew the embeddings and dissolve their uniqueness.\nComputational Cost: Softmax requires summing over the entire vocabulary size, which can be computationally expensive if $V$ is very large. Solutions include Hierarchical Softmax and Negative Sampling (approximating the denominator by only checking the target word vs. 5 random noise words).\nVectors as Double Agents: Every word has a different representation in the embedding matrix and the output matrix. Most of the time $W_{out}$ is discarded, but some consider averaging both $W_{in}$ and $W_{out}$ for slightly better performance.\nLinear Relationships: The famous analogy King - Man + Woman = Queen emerges from the model because of the linear relationships between the words.\nInitialization: This is a no-brainer but worth mentioning. Initialization to zero vectors would mean no learning. We must initialize randomly.\n2. Skip-Gram (The \u0026ldquo;Burger Flip\u0026rdquo; of CBOW) üîÑ Skip-Gram flips the CBOW logic. While CBOW tries to predict the center word given the context, Skip-Gram tries to predict the scattered context given the center word.\nCore Concept: If a word like \u0026ldquo;monarch\u0026rdquo; can accurately predict words like \u0026ldquo;king\u0026rdquo;, \u0026ldquo;throne\u0026rdquo;, \u0026ldquo;crown\u0026rdquo;, \u0026ldquo;royalty\u0026rdquo;, and \u0026ldquo;power\u0026rdquo;, then it must effectively capture the concept of royalty.\nTraining Let\u0026rsquo;s define dimensions:\n$V$: Vocabulary size $D$: Embedding dimension $C$: Number of context words Step 1: Input Lookup\nUnlike CBOW, our input is just one word vector (the center word $w_t$). We grab the vector $v_c$ from the Input Matrix $W_{in}$.\n$$ h = v_{w_t} $$\nNote: There is no averaging here. The hidden layer $h$ is simply the raw vector of the center word.\nStep 2: Output Scoring (The Broadcast)\nThe model takes this single vector $h$ and compares it against the Output Matrix $W_{out}$ (the entire vocabulary). It computes a score $u_j$ for every word $j$ in the dictionary.\n$$ u = W_{out}^T h $$\nResult $u$: A vector of size $V \\times 1$ containing raw scores.\nStep 3: Probability Conversion (Softmax)\nWe apply Softmax to turn scores into probabilities.\n$$ P(w_j | w_t) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 4: The Multi-Target Loss\nHere is the major difference from CBOW. In CBOW, we had one target (the center). In Skip-Gram, we have $2C$ targets (all the surrounding words). For a sentence \u0026ldquo;The cat sat on mat\u0026rdquo; (Center: \u0026ldquo;sat\u0026rdquo;), the targets are \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;. We want to maximize the probability of all these true context words. The Loss function ($E$) sums the error for each context word:\n$$ E = - \\sum_{w_{ctx} \\in \\text{Window}} \\log P(w_{ctx} | w_t) $$\nStep 5: Backpropagation (Accumulating Gradients)\nSince one input word is responsible for predicting multiple output words, the error signals from all those context words add up.\nAt the Output Layer:\nIf \u0026ldquo;cat\u0026rdquo; was a target, the \u0026ldquo;cat\u0026rdquo; vector in $W_{out}$ gets a signal: \u0026ldquo;Move closer to \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; If \u0026ldquo;apple\u0026rdquo; was NOT a target, the \u0026ldquo;apple\u0026rdquo; vector gets a signal: \u0026ldquo;Move away from \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; At the Hidden Layer ($h$):\nThe center word \u0026ldquo;sat\u0026rdquo; receives feedback from all its neighbors simultaneously. Error signal = (Error from \u0026ldquo;The\u0026rdquo;) + (Error from \u0026ldquo;cat\u0026rdquo;) + (Error from \u0026ldquo;on\u0026rdquo;)\u0026hellip; The vector for \u0026ldquo;sat\u0026rdquo; in $W_{in}$ moves in the average direction of all its neighbors. 3. The \u0026ldquo;Computational Nightmare\u0026rdquo; \u0026amp; Negative Sampling The equations above describe Naive Softmax.\nThe Problem: If $V = 100,000$, computing the denominator $\\sum \\exp(u_k)$ takes 100,000 operations. Doing this for every word in a training set of billions of words is impossibly slow.\nThe Solution: Negative Sampling Instead of updating the entire vocabulary (1 correct word vs 99,999 wrong ones), we approximate the problem.\nPositive Pair: (sat, cat) $\\rightarrow$ Maximize probability (Label 1). Negative Pairs: We pick $K$ (e.g., 5) random words the model didn\u0026rsquo;t see, e.g., (sat, bulldozer), (sat, quantum). $\\rightarrow$ Minimize probability (Label 0). New Equation (Sigmoid instead of Softmax):\n$$ E = - \\log \\sigma({v'_{\\text{pos}}}^T v_{\\text{in}}) - \\sum_{k=1}^K \\log \\sigma(-{v'_{\\text{neg}_{k}}}^T v_{\\text{in}}) $$ Effect: We typically only update 6 vectors per step (1 pos + 5 negs) instead of 100,000.\nLet\u0026rsquo;s see this in code:\nimport torch import torch.nn as nn import torch.optim as optim class SkipGramNegativeSampling(nn.Module): \u0026#34;\u0026#34;\u0026#34; Skip-Gram with Negative Sampling (SGNS) implementation. SGNS approximates the softmax over the entire vocabulary by instead distinguishing between a real context word (positive) and K noise words (negative). \u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embedding_dim): super(SkipGramNegativeSampling, self).__init__() # Input embeddings: used when the word is the center word self.in_embed = nn.Embedding(vocab_size, embedding_dim) # Output embeddings: used when the word is a context or negative sample self.out_embed = nn.Embedding(vocab_size, embedding_dim) # Initialize weights with small values to prevent gradient saturation initrange = 0.5 / embedding_dim self.in_embed.weight.data.uniform_(-initrange, initrange) self.out_embed.weight.data.uniform_(-initrange, initrange) def forward(self, center_words, target_words, negative_words): \u0026#34;\u0026#34;\u0026#34; Computes the negative sampling loss. Args: center_words: (batch_size) target_words: (batch_size) negative_words: (batch_size, K) where K is number of negative samples \u0026#34;\u0026#34;\u0026#34; # Retrieve vectors v_c = self.in_embed(center_words) # (batch_size, embed_dim) u_o = self.out_embed(target_words) # (batch_size, embed_dim) u_n = self.out_embed(negative_words) # (batch_size, K, embed_dim) # 1. Positive Score: log(sigmoid(v_c ¬∑ u_o)) # Compute dot product: (batch, 1, dim) @ (batch, dim, 1) -\u0026gt; (batch, 1) pos_score = torch.bmm(u_o.unsqueeze(1), v_c.unsqueeze(2)).squeeze(2) pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-7) # 2. Negative Score: sum(log(sigmoid(-v_c ¬∑ u_n))) # Compute dot products for all K samples: (batch, K, dim) @ (batch, dim, 1) -\u0026gt; (batch, K) neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2) neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-7), dim=1, keepdim=True) # Total loss is the negative of the objective function loss = -(pos_loss + neg_loss) return torch.mean(loss) # --- Configuration \u0026amp; Mock Data --- VOCAB_SIZE = 100 EMBED_DIM = 10 word_to_ix = {\u0026#39;fox\u0026#39;: 0} # Example vocabulary mapping model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock inputs: 1 center word, 1 target word, 5 negative samples center_id = torch.tensor([0], dtype=torch.long) target_id = torch.tensor([1], dtype=torch.long) negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long) # Training Step model.zero_grad() loss = model(center_id, target_id, negative_ids) loss.backward() optimizer.step() # Output Results print(f\u0026#34;Loss after one step: {loss.item():.6f}\u0026#34;) word_vec = model.in_embed(torch.tensor([word_to_ix[\u0026#39;fox\u0026#39;]])) print(f\u0026#34;Vector for \u0026#39;fox\u0026#39;:\\n{word_vec.detach().numpy()}\u0026#34;) The outputs:\nLoss after one step: 4.158468 Vector for \u0026#39;fox\u0026#39;: [[ 0.04197385 0.02400733 -0.03800093 0.01672485 -0.03872231 -0.0061478 0.0121122 0.04057864 -0.036255 0.03861175]] Interesting Facts About Skip-Gram üí° Makes Rare Words Shine: Skip-Gram is more effective at capturing the meaning of rare words because it focuses on the context of the center word. In CBOW, a rare word\u0026rsquo;s vector is averaged with others, diluting its signal. In Skip-Gram, when the rare word appears as the center, it gets the full, undivided attention of the backpropagation update.\nSlow Training: This method creates more training samples. A window of size 2 creates 4 training pairs per center word, while CBOW creates only 1 training pair per center word.\nSemantic Focus: Skip-Gram puts more emphasis on the semantic relationship between words. It views the context as the primary signal, with the center word as the target. This makes it better at capturing the meaning of words in context. Skip-Gram tends to capture semantic relationships (King/Queen) slightly better, while CBOW captures syntactic ones (walk/walked) slightly better.\n","permalink":"http://localhost:1313/posts/word-embeddings/","summary":"Understanding the mathematics behind Word2Vec, CBOW, and Skip-Gram and how they map language to vector space.","title":"Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n5. Code Examples: We\u0026rsquo;ll use sentencepiece for tokenization and compare results for different models.\nimport sentencepiece as spm import os # Configuration for training the SentencePiece model # SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words. options = { # The source text file used for learning the vocabulary \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # The base name for the generated model (.model) and vocabulary (.vocab) files \u0026#39;model_prefix\u0026#39;: \u0026#39;bpe_model\u0026#39;, # Number of unique tokens in the final vocabulary \u0026#39;vocab_size\u0026#39;: 4000, # \u0026#39;bpe\u0026#39; (Byte Pair Encoding) merges frequent pairs of characters/sequences \u0026#39;model_type\u0026#39;: \u0026#39;bpe\u0026#39;, # Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets \u0026#39;character_coverage\u0026#39;: 0.9995, # When enabled, unknown characters are decomposed into UTF-8 bytes to avoid \u0026#39;unk\u0026#39; tokens \u0026#39;byte_fallback\u0026#39;: True, # Treats digits individually (0-9), preventing large numbers from being treated as single tokens \u0026#39;split_digits\u0026#39;: True, # Prevents adding a whitespace prefix to the first token; useful for fine-grained control \u0026#39;add_dummy_prefix\u0026#39;: False } try: print(\u0026#34;Starting the training process...\u0026#34;) # SentencePieceTrainer.train takes the dictionary of options to build the BPE model spm.SentencePieceTrainer.train(**options) print(\u0026#34;Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created.\u0026#34;) # Initialize the processor and load the newly trained model sp = spm.SentencePieceProcessor() sp.load(\u0026#39;bpe_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Model Metadata:\u0026#34;) # Retrieve the total number of tokens in the vocabulary print(f\u0026#34;Total Vocab Size: {sp.get_piece_size()}\u0026#34;) # Special tokens are used for sequence boundaries and handling unknown characters print(f\u0026#34;BOS (Beginning of Sentence) ID: {sp.bos_id()}\u0026#34;) print(f\u0026#34;EOS (End of Sentence) ID: {sp.eos_id()}\u0026#34;) print(f\u0026#34;UNK (Unknown) ID: {sp.unk_id()}\u0026#34;) print(f\u0026#34;PAD (Padding) ID: {sp.pad_id()}\u0026#34;) # Test the tokenizer on sample strings to see how it breaks down text test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: shows the actual subword units (tokens) print(f\u0026#34;Subword Tokens: {sp.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: shows the numerical mapping for each token print(f\u0026#34;Numerical IDs: {sp.encode_as_ids(text)}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred during training or processing: {e}\u0026#34;) You can see the following output:\nStarting training... Starting the training process... Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created. ------------------------------ Model Metadata: Total Vocab Size: 4000 BOS (Beginning of Sentence) ID: 1 EOS (End of Sentence) ID: 2 UNK (Unknown) ID: 0 PAD (Padding) ID: -1 --- Tokenization Test --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;el\u0026#39;, \u0026#39;lo\u0026#39;, \u0026#39;‚ñÅW\u0026#39;, \u0026#39;orld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 380, 309, 476, 619, 36, 3924, 3964, 3959, 3993, 3978, 3983, 57, 3975, 3976, 60, 3974] --- Tokenization Test --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅmo\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;inst\u0026#39;, \u0026#39;ag\u0026#39;, \u0026#39;ram\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅever\u0026#39;] Numerical IDs: [1894, 304, 3284, 376, 321, 313, 312, 1008, 3313, 1325, 428, 3937, 341, 304, 3284, 1495] The ! is fallen back to UTF bytes. and the word instagrammable is split into multiple subword tokens. Also, original text can be recreated right back by simply concatenating the tokens. And replacing underscore by whitespace. Now we can try Unigram model. and see the same sentences tokenized differently.\nfrom tokenizers import Tokenizer from tokenizers.models import WordPiece from tokenizers.trainers import WordPieceTrainer from tokenizers.pre_tokenizers import Whitespace try: # 1. Initialize the WordPiece Tokenizer # We specify the [UNK] token for handling words not found in the vocabulary. tokenizer = Tokenizer(WordPiece(unk_token=\u0026#34;[UNK]\u0026#34;)) # 2. Configure Pre-tokenization # Before the subword algorithm runs, we need to split the raw text into words. # Whitespace splitting is the standard first step for most English NLP tasks. tokenizer.pre_tokenizer = Whitespace() # 3. Initialize the Trainer # We define our target vocabulary size and the special tokens required for # downstream tasks (like BERT\u0026#39;s [CLS] for classification or [SEP] for separators). trainer = WordPieceTrainer( vocab_size=4000, special_tokens=[\u0026#34;[UNK]\u0026#34;, \u0026#34;[CLS]\u0026#34;, \u0026#34;[SEP]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[MASK]\u0026#34;] ) # 4. Train the Model # The tokenizer scans the training file to build a vocabulary of the most # frequent subword units. tokenizer.train(files=[\u0026#34;train_data.txt\u0026#34;], trainer=trainer) # 5. Persist the Model # Save the configuration and vocabulary to a JSON file for future inference. tokenizer.save(\u0026#34;wordpiece.json\u0026#34;) print(\u0026#34;Training complete. \u0026#39;wordpiece.json\u0026#39; created.\u0026#34;) # 6. Metadata Inspection print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;WordPiece Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {tokenizer.get_vocab_size()}\u0026#34;) # 7. Testing Subword Tokenization # WordPiece shines at handling rare words by breaking them into meaningful chunks. test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (WordPiece) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # Encode converts raw text into a Tokenizer object containing tokens and IDs output = tokenizer.encode(text) # \u0026#39;tokens\u0026#39; shows the subword breakdown (e.g., \u0026#39;un\u0026#39;, \u0026#39;##insta\u0026#39;, etc.) print(f\u0026#34;Subword Tokens: {output.tokens}\u0026#34;) # \u0026#39;ids\u0026#39; are the numerical indices mapped to the vocabulary print(f\u0026#34;Numerical IDs: {output.ids}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred with WordPiece model: {e}\u0026#34;) You can see the following output:\nWordPiece Model Metadata: Total Vocab Size: 2609 --- Tokenization Test (WordPiece) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;H\u0026#39;, \u0026#39;##el\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;##or\u0026#39;, \u0026#39;##ld\u0026#39;, \u0026#39;[UNK]\u0026#39;, \u0026#39;[UNK]\u0026#39;] Numerical IDs: [37, 180, 214, 52, 162, 418, 0, 0] --- Tokenization Test (WordPiece) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;##os\u0026#39;, \u0026#39;##t\u0026#39;, \u0026#39;un\u0026#39;, \u0026#39;##ins\u0026#39;, \u0026#39;##ta\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;##ra\u0026#39;, \u0026#39;##m\u0026#39;, \u0026#39;##ma\u0026#39;, \u0026#39;##ble\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;ever\u0026#39;] Numerical IDs: [691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240] We can see the special character ## in the output. We can also see the breakdowns not having any semantic meaning. They are likelihood based. We can try the Unigram model now.\n# \u0026#39;Unigram\u0026#39; is the default and usually recommended over BPE in SentencePiece. options_unigram = { \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # Path to the raw text file for training \u0026#39;model_prefix\u0026#39;: \u0026#39;unigram_model\u0026#39;, # Prefix for the output .model and .vocab files \u0026#39;vocab_size\u0026#39;: 1200, # Desired size of the final vocabulary \u0026#39;model_type\u0026#39;: \u0026#39;unigram\u0026#39;, # Specifies the Unigram language model algorithm \u0026#39;character_coverage\u0026#39;: 0.9995, # Percentage of characters covered by the model (0.9995 is standard for Latin scripts) \u0026#39;byte_fallback\u0026#39;: True, # Enables mapping unknown characters to UTF-8 bytes to avoid \u0026lt;unk\u0026gt; tokens \u0026#39;split_digits\u0026#39;: True, # Treats each digit as an individual token (useful for numerical data) \u0026#39;add_dummy_prefix\u0026#39;: False # Prevents adding a leading space (SentencePiece default is True) } try: # 1. Train the SentencePiece model using the defined options print(\u0026#34;Starting Unigram training...\u0026#34;) spm.SentencePieceTrainer.train(**options_unigram) print(\u0026#34;Training complete. \u0026#39;unigram_model.model\u0026#39; created.\u0026#34;) # 2. Load the trained model into a processor instance for inference sp_unigram = spm.SentencePieceProcessor() sp_unigram.load(\u0026#39;unigram_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Unigram Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {sp_unigram.get_piece_size()}\u0026#34;) # 3. Define test cases to evaluate how the model handles common and rare words test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] # 4. Iterate through test sentences to visualize subword segmentation for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (Unigram) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: Converts text into subword strings (visual representation) print(f\u0026#34;Subword Tokens: {sp_unigram.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: Converts text into numerical indices for model input print(f\u0026#34;Numerical IDs: {sp_unigram.encode_as_ids(text)}\u0026#34;) except Exception as e: # Handle potential errors during training or loading (e.g., missing input file) print(f\u0026#34;An error occurred with Unigram model: {e}\u0026#34;) The output is as follows:\nUnigram Model Metadata: Total Vocab Size: 1200 --- Tokenization Test (Unigram) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ll\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;ld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311] --- Tokenization Test (Unigram) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;T\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅm\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;sta\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;ra\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ver\u0026#39;] ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"A comprehensive guide to tokenization strategies: BPE, WordPiece, Unigram, and SentencePiece.","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"A brief introduction to Vectors \u0026amp; Verbs and formatting verification.","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"\nFeedforward network (FFN) blocks are the least talked about when in context to LLMs. But that is where more than 70% of the paramters come from. Something as simple and convinient as a couple of linear transforming matrices become a burden when models start scaling in Billions of parameters. So, getting the maximum juice of a lean structure is a challenge. Plus, mostly under represented, because the FFN sub-layer acts as the primary \u0026ldquo;knowledge\u0026rdquo; processing engine, often containing the majority of a model\u0026rsquo;s total parameters.\nStandard Position Wise Feedforward Network This is as simple as it can get, two-layered fully connected network of neurons with a single activation switch, most probably a ReLU variant (for LLMs). $$ FFN(x) = max(0, W_2(W_1(x)) + b_1)W_2 + b_2 $$ Same transformation is applied for each token in parallel, making it efficient for GPU computation. For Example, in BERT model the 512 dimensions of each token are expanded to 2048 to allow for \u0026ldquo;intricate interactions\u0026rdquo; before projecting back to 512 dimensions. This part is important, in every transformer stacking of layers is possible only because the output of every matches it\u0026rsquo;s input dimensions.\n","permalink":"http://localhost:1313/posts/feedforward-networks/","summary":"Exploring the hidden layers of trillion-parameter switchboards: Feedforward Neural Networks and Activation Functions.","title":"Anatomy of Trillion-Parameter Switchboards: Understanding Feedforward Blocks"},{"content":"\nWe are about to touch the holy grail of modern AI. The 2017 declaration was simple: Attention Is All You Need. But as we‚Äôve raced from GPT-1 to reasoning models, the definition of \u0026lsquo;Attention\u0026rsquo; has quietly transformed. Is the algorithm ruling the world today actually the same one we started with? Let‚Äôs look under the hood.\nüî• The Core Concept At its heart, the attention mechanism is a form of weighted aggregation. It is often described using a database retrieval analogy. Imagine you have a database of information (context).\nQuery ($Q$): What are you currently looking for? (The token being generated). Key ($K$): The indexing tag associated with each piece of information. Value ($V$): The actual information (The context token). The \u0026ldquo;Attention\u0026rdquo; is the process of matching your Query against all the Keys to determine how much \u0026ldquo;weight\u0026rdquo; to put on each Value. Attention scores are calculated in each transformer layer at every step of the sequence. This is a single unifying factor in all transformer models, from BERT to Gemini.\nüî• Standard Multi-Head Attention (MHA) The original mechanism introduced in the paper \u0026ldquo;Attention Is All You Need\u0026rdquo;.\nHere, instead of performing the Key-Query-Value (KQV) operation on the entire context only once, the attention is split into $N$ parallel heads. Each head learns a different aspect of the language (e.g., word meaning, syntax, semantic relationships, etc.). The equation is a simple culmination of dot products aided by a regularizer like Softmax.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nWhere:\n$QK^T$ computes the similarity scores between the Query and Key. $\\sqrt{d_k}$ is a scaling factor to prevent the dot product from growing too large, mitigating the vanishing/exploding gradient problem. The weighted sum is applied to the values of $V$. The Memory Bandwidth Bottleneck: \u0026gt; During inference, the model must store Key and Value matrices for all previous tokens. This is called the KV Cache. In MHA, every single head has its own unique set of Keys and Values. For large models with massive context windows, loading this massive KV cache from memory (HBM) to the computation chip (SRAM) becomes the primary bottleneck, slowing down generation significantly. Earlier models could often only generate ~8 tokens per second at inference due to this constraint.\nüî• Multi-Query Attention (MQA) This approach addresses the memory bottleneck by sharing the same set of Keys and Values across all heads while keeping separate Query heads.\nMHA: $H$ Query heads, $H$ Key heads, $H$ Value heads MQA: $H$ Query heads, 1 Key head, 1 Value head This drastically reduces the memory requirements of the KV cache by a factor of $H$ (often 8x or 16x). The GPU needs to load far less data, significantly speeding up inference. However, it does not reduce the computation (FLOPs)‚Äîso why is it faster?\nThis can be explained with the Kitchen Analogy: Imagine a kitchen (GPU) with 8 chefs (Heads).\nComputation (Math): The Chef chopping vegetables. Memory Access (Bandwidth): The Assistant running to the fridge to get ingredients. In MHA: Each chef demands unique vegetables from the fridge. The assistant runs back and forth 8 times, fetching 8 different crates. Most of the time is spent waiting for the assistant to return. This results in idle cores waiting for data to load from HBM.\nIn MQA: All chefs agree to use the same vegetables. The assistant runs back and forth only once, fetching one giant crate of potatoes and dumping it. This results in minimized memory transfer and no idle cores.\nThe Technical Reality: During text generation, the model is memory-bound, not compute-bound. The time it takes to move the KV Cache from HBM (High Bandwidth Memory) to the chip\u0026rsquo;s SRAM is the bottleneck. MQA reduces the data volume of that move by a factor of $H$.\nThe Capacity Gap: MQA is a trade-off between nuanced response and memory capacity. The model loses the ability to store different types of information, as a single $V$ matrix must now store all nuances of information flow in the language.\nThe Surgery Trick (Uptraining): Google Research figured out a way to take an existing model with MHA and convert it to MQA by averaging the weight matrices of $H$ heads. This is called Uptraining. You then train this \u0026ldquo;Frankenstein\u0026rdquo; model for ~5% of the original training steps for adjustments. This yields an MQA model that is nearly as good as the original MHA but vastly faster.\nüî• Grouped Query Attention (GQA) A \u0026ldquo;best of both worlds\u0026rdquo; solution used in modern models like Llama 2 and Llama 3.\nHow it works: GQA acts as an interpolation between MHA and MQA. Instead of having just 1 Key head (MQA) or $H$ Key heads (MHA), it divides the query heads into $G$ groups. Each group shares a single Key and Value head.\nEquation modification: If you have 8 query heads, you might create 4 groups (2 queries per group). This reduces the KV cache by 2x instead of 8x, preserving more quality than MQA while still speeding up inference.\nMHA: 8 people (heads) are working on a project. Each person has their own unique filing cabinet (KV cache). It takes forever to move 8 cabinets around. MQA: 8 people share one single filing cabinet. It\u0026rsquo;s very fast to move, but they fight over the organization and lose detail. GQA: The 8 people split into 4 teams of 2. Each team shares a cabinet. It\u0026rsquo;s a balance between speed and organizational detail. üî• Multi-Head Latent Attention (MLA) This is the most advanced mechanism for attention, used in models like DeepSeek-V3. It is a sophisticated design for massive efficiency that takes a radical approach: mathematically compressing the memory into a low-rank \u0026ldquo;latent\u0026rdquo; vector.\nKV Cache Explosion: As mentioned before, for a massive model like DeepSeek-V3 (671B parameters), storing the full Keys and Values for every token in a long conversation requires terabytes of high-speed memory (HBM), and chips spend more time waiting than churning.\nMHA (Standard) stores everything. MQA deletes redundancy but hurts quality. MLA keeps the quality of MHA but compresses the data 4x-6x smaller than even GQA.\nThe Zip File Approach: Low-Rank Joint Compression Let\u0026rsquo;s compare the math of MHA and MLA. In MHA, for every single token, we generate a unique Key ($k$) and Value ($v$) matrix for each attention head.\nThe Memory Problem: The $k$ is stored in the KV Cache. For 128 heads, for example, it calculates a massive vector of $128 \\times 128 = 16,384$ floats per token.\nFor MHA, the attention score looks like: $$\\text{Score} = q^T \\cdot k$$\nBut for MLA, instead of storing the full $k$, it projects the input down into a tiny latent vector $c_{KV}$, which is much smaller (e.g., 512 floats). $$c_{KV} = W_{DKV} \\cdot x$$\nNow here is the catch: Naive compression and decompression would mean constructing a small latent matrix for every token and deconstructing it every single time the information is required, which defeats the purpose of compression.\n$$k_{\\text{reconstructed}} = W_{UK} \\cdot c_{KV}$$ $$\\text{Score} = q^T \\cdot k_{\\text{reconstructed}}$$\nThe Matrix Absorption Trick (Optimization) Instead of reconstructing $k$ for every token, we can absorb the Up-Projection matrix ($W_{UK}$) into the Down-Projection matrix ($W_{DKV}$) and absorb the query $q$ into a new $Q$.\nFrom the original equation: $$\\text{Score} = q^T \\cdot (\\underbrace{W_{UK} \\cdot c_{KV}}_{\\text{This is } k})$$\nWe associate differently:\n$$\\text{Score} = (\\underbrace{q^T \\cdot W_{UK}}_{\\text{Absorbed Query}}) \\cdot c_{KV}$$ We change the order in which the matrix multiplication is performed.\nHow is this allowed? In linear algebra, matrix multiplication is associative:\n$$(A \\cdot B) \\cdot C = A \\cdot (B \\cdot C)$$ We can move the parentheses! Instead of grouping $(W_{UK} \\cdot c_{KV})$ to make the Key, we group $(q^T \\cdot W_{UK})$ to make a new Query.\nNote: The positional embedding information is left untouched. That rotational information from algorithms like RoPE is preserved outside the compression; it requires minimal space anyways.\n","permalink":"http://localhost:1313/posts/attention-mechanisms/","summary":"We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek\u0026rsquo;s MLA, how has the definition of \u0026lsquo;Attention\u0026rsquo; transformed?","title":"Attention Is All You Need, but exactly which one?: MHA, GQA and MLA"},{"content":"\nPositional Embeddings are the \u0026ldquo;voice\u0026rdquo; that tell Transformers where words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.\n\u0026ldquo;The dog chased the cat\u0026rdquo; and \u0026ldquo;cat chased the dog\u0026rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.\nüî• Sinusoidal Embeddings: The Odometer of Language Introduced in the seminal \u0026ldquo;Attention Is All You Need\u0026rdquo; paper, this was the industry standard for many years. This approach adds a vector to each word embedding based on its position. The vector is computed out of wavelike patterns that are periodic and smooth. Mathematically, for a position $pos$ and embedding dimension $i$:\n$$ PE_{(pos, i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $$\nThe model uses sine and cosine waves of different frequencies to capture both short and long-range dependencies. The frequency of the waves decreases as the embedding dimension increases, which helps capture long-range dependencies in the input sequence.\nThe creators wanted the model to calculate distance easily, which is why a linear function was used. The model simply rotates the current position vector using a rotation matrix to derive the next position.\n$$ \\begin{pmatrix} \\cos(k) \u0026 \\sin(k) \\\\ -\\sin(k) \u0026 \\cos(k) \\end{pmatrix} \\cdot PE(pos) \\approx PE(pos + k) $$ Low dimensions act like a fast-ticking clock (high frequency, changing rapidly) and high dimensions act like a slow clock (low frequency, changing slowly).\nThe number 10,000 is an arbitrarily chosen high number to reduce Aliasing. By choosing such a high number, the creators ensured that the wavelength of the slowest dimension is incredibly long. Consequently, the pattern practically never repeats, and each position remains unique.\nThis combination works like a mechanical odometer dial. Each subsequent dimension acts as a slower-moving gear. The model learns to read these \u0026ldquo;dials\u0026rdquo; to understand exactly where a word sits in the sequence.\nThis mitigated the challenge of exploding gradients that previous array-based positional embeddings suffered from (e.g., adding integers 1, 2, 3\u0026hellip;). This model keeps values between -1 and +1, adding stability while ensuring every position gets a distinct code.\nThe Problem: This approach pollutes the word embedding matrix (semantic information). While high-dimensional vectors are almost always orthogonal‚Äîmeaning the noise doesn\u0026rsquo;t fully destroy the word\u0026rsquo;s identity‚Äîit does not strictly guarantee purity.\nThis led to a more mature approach.\nüî• ALiBi: The Fading Streetlight Attention with Linear Biases (ALiBi) is a method where, instead of adding a vector to the embeddings at the start, the model injects positional information linearly, directly into the attention scores. If Sinusoidal Encoding asks, \u0026ldquo;Where am I on the map?\u0026rdquo;, ALiBi asks, \u0026ldquo;How much harder should I squint to see you?\u0026rdquo;\nThe biggest problem ALiBi solved is the \u0026ldquo;Invisible Wall\u0026rdquo;. If you train a Transformer on sequences of length 2048 and then feed it a book with 3000 words, it will crash or output garbage at word 2049. This is a failure of Extrapolation.\nALiBi throws away the idea of adding positional vectors entirely. Instead, it devises a \u0026ldquo;penalty\u0026rdquo; system for attention scores:\n$$ \\text{Score} = (q \\cdot k^\\top) - (m \\cdot \\text{distance}) $$\nDistance: If words A and B are neighbors, the distance is 0, so the penalty is 0. Although the distance is linear, it does not cause exploding gradients because the raw attention scores are normalized by a softmax function. This distance prioritizes nearby words (local context) over distant ones.\nSlope ($m$): If we always penalize distance equally, the model becomes myopic. ALiBi mitigates this by assigning a different slope value to each attention head:\nHead 1 (The Historian): Very low slope. The penalty grows slowly, allowing it to see far back. Head 2 (The Reader): Medium slope. Head 3 (The Myopic): Very high slope (e.g., 0.5); it effectively cannot see beyond 2-3 words. This mimics holding a lantern. Words close to you are bright, while words further away naturally fade into darkness.\nExtrapolation: If you walk 5 miles down the road (a longer sequence), your lantern works exactly the same way. The physics of light decay are constant, allowing the model to handle sequences longer than it was trained on.\nHowever, ALiBi forces a \u0026ldquo;sliding window\u0026rdquo; view where distant words eventually become invisible. This brings us to the modern standard.\nüî• RoPE: The Rotary Revolution Rotary Position Embeddings (RoPE) is the current industry standard (used in Llama 3, Mistral, PaLM) because it mathematically unifies \u0026ldquo;absolute position\u0026rdquo; with \u0026ldquo;relative position.\u0026rdquo; This method rotates the vectors in the Query and Key matrices based on their absolute position.\nIf a vector has 512 dimensions, RoPE breaks it into 256 pairs. Each pair is rotated by a different frequency:\n$$ \\theta_i = 10000^{-2i/d} $$\nThe First pair ($i = 0$) is rotated by the highest frequency. The Last pair ($i = 255$) is rotated by the lowest frequency. This spread ensures the model has \u0026ldquo;high precision\u0026rdquo; (fast rotation) to distinguish immediate neighbors, and \u0026ldquo;low precision\u0026rdquo; (slow rotation) to track global position without the pattern repeating.\nDoesn\u0026rsquo;t rotation change meaning? Yes and No. The vector for \u0026ldquo;Apple\u0026rdquo; at position 1 and \u0026ldquo;Apple\u0026rdquo; at position 256 will look completely different in terms of coordinates. However, rotation does not change the semantic strength (magnitude/norm) of the vector.\nThe Relativity Trick: Transformers compare words using dot products (angles). RoPE relies on the fact that the attention mechanism ignores the absolute rotation and only detects the difference in rotation (relative position). Thus, the \u0026ldquo;relative meaning\u0026rdquo; is encoded purely into the angle between words, leaving the semantic \u0026ldquo;magnitude\u0026rdquo; untouched.\nüî• LongRoPE: The Bifocal Lens While RoPE is powerful, it struggles when stretching context windows from 4k to 2 million tokens. The \u0026ldquo;fast\u0026rdquo; dimensions spin so rapidly over long distances that they become random noise. LongRoPE solves this using an evolutionary search algorithm to find a unique scaling factor ($\\lambda$) for each dimension.\nThe Equations Instead of rotating by the standard $\\theta_i$, LongRoPE rotates by a rescaled frequency:\n$$ \\theta\u0026rsquo;_i = \\lambda_i \\theta_i $$\nTo efficiently find these $\\lambda$ values without searching an infinite space, the algorithm enforces a monotonicity constraint based on NTK theory:\n$$ \\lambda_i \\le \\lambda_{i+1} $$\nThis ensures that low-frequency dimensions (global context) are stretched more than high-frequency dimensions (local context), creating a \u0026ldquo;bifocal\u0026rdquo; effect:\nHigh Frequencies (Local): Kept sharp ($\\lambda \\approx 1$) to maintain grammar. Low Frequencies (Global): Stretched ($\\lambda \u0026gt; 1$) to track massive distances without repeating. Not worth perusing more since the next one is much better at long context positional encoding. üî• HoPE: The Hyperbolic Slide Hyperbolic Rotary Positional Encoding (HoPE) moves from the geometry of a circle to the geometry of a hyperbola (inspired by Lorentz transformations). It was designed to fix a subtle flaw in RoPE: the \u0026ldquo;wobbly\u0026rdquo; or oscillatory nature of attention scores.\nThe Core Problem: RoPE\u0026rsquo;s \u0026ldquo;Wobble\u0026rdquo; In standard RoPE, we rotate vectors around a circle.\nThe Issue: Circles repeat. A dial at $361^\\circ$ looks identical to $1^\\circ$. Even with different frequencies, the dot product fluctuates up and down as distance increases. The Consequence: The model gets confused at long distances, potentially mistaking a word 1,000 tokens away for a neighbor just because the rotation cycles aligned, creating \u0026ldquo;noise.\u0026rdquo; The \u0026ldquo;Hyperbolic\u0026rdquo; Shift HoPE replaces trigonometric functions ($\\sin, \\cos$) with hyperbolic functions ($\\sinh, \\cosh$).\nCircular Rotation: Keeps distance from the center constant. Hyperbolic Rotation: Moves points along a hyperbola. Crucially, hyperbolic functions do not repeat; they grow or decay exponentially. By using hyperbolic rotation, HoPE guarantees that as words get further apart, the attention score decays monotonically (smoothly drops).\nThe Equations HoPE uses a hyperbolic matrix $B(\\theta, m)$ defined using the Lorentz Boost structure:\n$$ B(\\theta, m) = \\begin{pmatrix} \\cosh(m\\theta) \u0026 \\sinh(m\\theta) \\\\ \\sinh(m\\theta) \u0026 \\cosh(m\\theta) \\end{pmatrix} $$ Since hyperbolic functions grow to infinity, HoPE introduces a decay penalty ($e^{-m\\theta'}$). The final attention score simplifies beautifully to a function of relative distance: $$ \\text{Score} \\propto e^{-|m-n|(\\theta\u0026rsquo; - \\theta)} $$\nThis ensures the attention score exponentially decays as distance increases, eliminating the wobbles of standard RoPE. This the latest model that is taking up the space in the industry for extremely long context models.\n","permalink":"http://localhost:1313/posts/positional-embeddings/","summary":"From Sinusoidal to RoPE and HoPE: How Transformers learn to process word order and sequence length.","title":"The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE"},{"content":"Imagine trying to teach a computer the difference between \u0026ldquo;Apple\u0026rdquo; the fruit and \u0026ldquo;Apple\u0026rdquo; the company. To us, the distinction is intuitive. To a machine, it‚Äôs just a string of characters. How do we turn these strings into meaningful math? While early attempts like Word2Vec gave us a great start, they missed the forest for the trees‚Äîor in some cases, the twigs for the branches. Enter GloVe and FastText: two algorithms that revolutionized how machines understand the nuances of human language.\nPrevious static embedding models like Word2Vec successfully captured the local semantics of words, but they failed in addressing contexts between words that might not always appear in the same context window. Furthermore, they treated words as indivisible atomic units in continuous vector spaces, missing out on the internal structure of language.\nThe two simple but effective ideas behind GloVe and FastText are subword units and global co-occurrence. Let\u0026rsquo;s dive into them.\nGloVe: A Global Mapping Strategy Global Vectors for Word Representation (GloVe) was created at Stanford. The core idea was to train a model where word vectors are defined by how often they appear around other words across the entire corpus, rather than just locally.\nTraining Steps: Co-occurrence Matrix X: GloVe scans the entire corpus and creates a giant matrix (spreadsheet) recording the counts of how often words appear near each other. Factorization: It compresses this giant matrix into smaller, dense vectors. Objective: The function tries to equate the dot product of two word vectors with the logarithm of their probability of appearing together. The error between these values is then backpropagated to update the vectors. $$w_i \\cdot w_j + b_i + b_j = \\log(X_{ij})$$\n$w$: The word vector (what we want). $X_{ij}$: The count from our matrix. The Loss Function: We can\u0026rsquo;t satisfy the equation above perfectly for every word pair. So, we use a weighting function to minimize the squared error:\n$$J = \\sum_{i,j=1}^V f(X_{ij}) ({w_i}^T w_j + b_i + b_j - \\log(X_{ij}))^2$$\nIf we do not use the weighting function, the loss would be infinite for rare words (since $\\log(0) = \\infty$) and frequent words (like \u0026rsquo;the\u0026rsquo;) would overpower the model.\n$$ f(x) = \\begin{cases} (x/x_{max})^\\alpha \u0026amp; \\text{if } x \u0026lt; x_{max} \\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$\n(Typically $x_{max}=100$ and $\\alpha=0.75$)\nFastText: The Lego Bricks Strategy FastText was created by Facebook AI Research (FAIR). The idea was to treat words not as solid rocks, but as structures made of \u0026ldquo;Lego bricks\u0026rdquo; (subword units).\nWord2Vec struggled to create embeddings for words it hadn\u0026rsquo;t seen before (Out-Of-Vocabulary or OOV words). FastText solves this by breaking words down.\nGloVe treats a word like \u0026ldquo;Apple\u0026rdquo; as a single, atomic unit. FastText breaks \u0026ldquo;Apple\u0026rdquo; into character n-grams: \u0026lt;ap, app, ppl, ple, le\u0026gt; For a complex or new word like \u0026ldquo;Applesauce\u0026rdquo;, the model might not know the whole word, but it recognizes the Lego brick for \u0026ldquo;Apple\u0026rdquo; and the remaining bits from other subwords.\nFastText takes its inspiration from the SkipGram architecture. The vector for a word is simply the sum of the vectors of its n-gram subwords:\n$$v_w = \\sum_{g \\in G_w} z_g$$\nObjective: The objective is to predict context words given a target word.\n$$P(w_c | w_t) = \\frac{e^{s(v_{w_t} \\cdot v_{w_c})}}{\\sum_{w \\in V} e^{s(v_{w_t} \\cdot v_w)}}$$\nAnd the scoring function is defined as:\n$$s(w_t, w_c) = u_{w_t}^T (\\sum_{g \\in G_w} z_g)$$\n$u_{w_c}$: The vector for the context word (output vector). $z_g$: The vector for the n-gram $g$ (input vector). Modifications to Softmax: Calculating the full softmax for every word in the vocabulary is computationally expensive. FastText uses two methods to approximate this efficiently.\nOption A: Negative Sampling\nWe pick a few random \u0026ldquo;wrong\u0026rdquo; words every time to serve as \u0026ldquo;negative samples\u0026rdquo; and calculate the loss only for the correct word and these noise words.\n$$J = - \\log \\sigma(s(w_t, w_c)) - \\sum\\limits_{i=1}^{N} \\mathbb{E}_{n_i \\sim P_n(w)} [\\log \\sigma(-s(w_t, n_i))]$$\nThe sigmoid function $\\sigma$ squashes the score between 0 and 1. The first term pushes the probability of the correct word towards 1. The second term pushes the probability of the noise words towards 0. Option B: Hierarchical Softmax (Faster for infrequent words)\nInstead of a flat list of words, imagine the vocabulary arranged as a binary tree (Huffman Tree).\nRoot: Top of the tree. Leaves: The actual words. To calculate the probability of a word, we trace a path from the root to that leaf. At every branching node, we calculate the probability (sigmoid) of going left or right. The probability of a word $w$ is the product of the probabilities of the turns taken to reach it. Code Example from gensim.models import FastText from gensim.test.utils import common_texts # Initialize the FastText model with specific hyperparameters model = FastText( vector_size=100, # Dimensionality of the word vectors window=5, # Maximum distance between the current and predicted word within a sentence min_count=1, # Ignores all words with total frequency lower than this sg=1, # Training algorithm: 1 for skip-gram; 0 for CBOW hs=0, # If 0, and negative is non-zero, negative sampling will be used negative=5, # Number of \u0026#34;noise words\u0026#34; to be drawn for negative sampling workers=4, # Number of worker threads to train the model epochs=10, # Number of iterations over the corpus min_n=3, # Minimum length of character n-grams max_n=6 # Maximum length of character n-grams ) # Build the vocabulary from the provided text corpus model.build_vocab(common_texts) # Train the model on the corpus model.train( common_texts, total_examples=model.corpus_count, epochs=model.epochs ) # Calculate and print the cosine similarity between two words in the vocabulary print(f\u0026#34;Similarity between \u0026#39;computer\u0026#39; and \u0026#39;human\u0026#39;: {model.wv.similarity(\u0026#39;computer\u0026#39;, \u0026#39;human\u0026#39;)}\u0026#34;) # Demonstrate FastText\u0026#39;s ability to handle Out-Of-Vocabulary (OOV) words # Even if \u0026#39;computation\u0026#39; wasn\u0026#39;t in the training data, FastText constructs a vector using character n-grams oov_vector = model.wv[\u0026#39;computation\u0026#39;] # Check if a specific word exists in the model\u0026#39;s fixed vocabulary index word = \u0026#34;computer\u0026#34; is_in_vocab = word in model.wv.key_to_index print(f\u0026#34;\\nIs the word \u0026#39;{word}\u0026#39; in the model\u0026#39;s vocabulary? {is_in_vocab}\u0026#34;) ","permalink":"http://localhost:1313/posts/glove-and-fasttext/","summary":"How subword units and global co-occurrence matrices allow GloVe and FastText to capture nuances that Word2Vec missed.","title":"The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText"},{"content":" Before we had Large Language Models writing poetry, we had to teach computers that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; are related not just by spelling, but by meaning. This is the story of that breakthrough. It‚Äôs the moment we stopped counting words and started mapping their souls‚Äîturning raw text into a mathematical landscape where math can solve analogies. Welcome to the world of Word2Vec. üîÆ\nLanguage models require vector representations of words to capture semantic relationships. Before the 2010s, models used word count-based vector representations that captured only the frequency of words (e.g., One-Hot Encoding). The Problems: üöß\nThe Curse of Dimensionality Lack of meaning (Synonyms were treated as mathematically unrelated). In 2011, Mikolov et al. at Google introduced Word2Vec, which used a shallow neural network to learn vector representations of words. The best part? It could create much denser relationships between words than prior models, and it was unsupervised.\nThe training (prediction) is essentially \u0026ldquo;fake\u0026rdquo;‚Äîa pretext task. But the weight matrices we get on the side are a gold mine of fine-grained semantic relationships. ‚õèÔ∏è\nThere are two popular variants of Word2Vec:\nSkipGram Continuous Bag Of Words (CBOW) 1. Continuous Bag of Words (CBOW) üß© CBOW is a simple \u0026lsquo;fill in the blanks\u0026rsquo; machine. It takes the surrounding words as inputs and tries to predict the center word.\n\u0026ldquo;Continuous\u0026rdquo;: It operates in continuous vector space (unlike the discrete space of n-gram models).\n\u0026ldquo;Bag of words\u0026rdquo;: The order of the context does not matter. \u0026ldquo;The cat sat on the mat\u0026rdquo; and \u0026ldquo;The mat sat on the cat\u0026rdquo; are the same for CBOW. They produce exactly the same predictions.\nObjective: Maximize the probability of the target word $w_t$ given its context $w_{t-C}, \\dots, w_{t+C}$.\n2. Training Defining Dimensions:\n$V$: Vocabulary size (number of unique words in the corpus, e.g., 10,000) $N$: Dimension of the embedding space (e.g., 100) $C$: Number of context words (e.g., 2 words on left of target and 2 words on right) Step 1: Input Lookup \u0026amp; One-Hot Encoding\nMathematically, we represent each context word as a one-hot vector $x^{(c)}$. We have an Input Matrix $W_{in}$ of size $V \\times N$. For each context word (e.g., \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;), we select its corresponding row from $W_{in}$.\n$$ v_c = W_{in}^T x^{(c)} $$\nWhere $v_c$ is the $N$-dimensional vector for context word $c$.\nStep 2: Projection (The \u0026ldquo;Mixing\u0026rdquo;)\nWe combine the context vectors. In standard CBOW, we simply average them. This creates the hidden layer vector $h$.\n$$ h = \\frac{1}{2C} \\sum_{c=1}^{2C} v_c $$\n$h$ is a single vector of size $N \\times 1$. Note: This operation is linear and contains no activation function (like ReLU or Sigmoid).\nStep 3: Output Scoring (The \u0026ldquo;Dot Product\u0026rdquo;)\nWe have a second matrix, the Output Matrix $W_{out}$ of size $N \\times V$. We compute the raw score (logit) $u_j$ for every word $j$ in the vocabulary by taking the dot product of the hidden vector $h$ and the output vector $v\u0026rsquo;_j$.\n$$ u_j = {v\u0026rsquo;_j}^T h $$\nOr in matrix form:\n$$ u = W_{out}^T h $$\nResult $u$ is a vector of size $V \\times 1$.\nStep 4: Probability Conversion (Softmax)\nWe convert the raw scores $u$ into probabilities using the Softmax function. This tells us the probability that word $w_j$ is the center word.\n$$ y_j = P(w_j | \\text{context}) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 5: Loss Calculation (Cross-Entropy)\nWe compare the predicted distribution $y$ with the actual ground truth $t$ (a one-hot vector where the index of the true word is 1). The loss function $E$ is:\n$$ E = - \\sum_{j=1}^V t_j \\log(y_j) $$\nSince $t_j$ is 0 for all words except the target word (let\u0026rsquo;s call the target index $j^*$), this simplifies to:\n$$ E = - \\log(y_{j^*}) $$\n3. Backpropagation Details We need to update the weights to minimize $E$. We use the Chain Rule.\nA. Gradient w.r.t. Output Matrix ($W_{out}$)\nWe want to know how the error changes with respect to the raw score $u_j$.\n$$ \\frac{\\partial E}{\\partial u_j} = y_j - t_j $$\nLet\u0026rsquo;s call this error term $e_j$.\nIf $j$ is the target word: $e_j = y_j - 1$ (Result is negative; we boost the score). If $j$ is not target: $e_j = y_j - 0$ (Result is positive; we suppress the score). The update rule for the output vector $v\u0026rsquo;_j$ becomes:\n$$ v'_{j}(\\text{new}) = v'_{j}(\\text{old}) - \\eta \\cdot e_j \\cdot h $$ ($\\eta$ is the learning rate)\nB. Gradient w.r.t. Hidden Layer ($h$)\nWe backpropagate the error from the output layer to the hidden layer.\n$$ EH = \\sum_{j=1}^V e_j \\cdot v\u0026rsquo;_j $$\n$EH$ is an $N$-dimensional vector representing the aggregate error passed back to the projection layer.\nC. Gradient w.r.t. Input Matrix ($W_{in}$)\nSince $h$ was just an average of the input vectors, the error $EH$ is distributed to each context word\u0026rsquo;s input vector. For every word $w_c$ in the context:\n$$ v_{w_c}(\\text{new}) = v_{w_c}(\\text{old}) - \\eta \\cdot \\frac{1}{2C} \\cdot EH $$\nLet\u0026rsquo;s build a single pass through the network for a given context.\nimport torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # Define a simple Continuous Bag of Words (CBOW) style model class CBOWModel(nn.Module): def __init__(self, vocab_size, embedding_dim): super(CBOWModel, self).__init__() # The embedding layer stores the word vectors we want to learn self.embeddings = nn.Embedding(vocab_size, embedding_dim) # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word self.linear = nn.Linear(embedding_dim, vocab_size) def forward(self, inputs): # inputs: tensor of word indices for the context embeds = self.embeddings(inputs) # Aggregate context by calculating the mean of the word embeddings h = torch.mean(embeds, dim=1) # Produce logits (raw scores) for each word in the vocabulary logits = self.linear(h) return logits # Setup vocabulary and mappings word_to_ix = {\u0026#34;the\u0026#34;: 0, \u0026#34;quick\u0026#34;: 1, \u0026#34;brown\u0026#34;: 2, \u0026#34;fox\u0026#34;: 3, \u0026#34;jumps\u0026#34;: 4, \u0026#34;over\u0026#34;: 5, \u0026#34;lazy\u0026#34;: 6, \u0026#34;dog\u0026#34;: 7} ix_to_word = {v: k for k, v in word_to_ix.items()} # Configuration constants EMBEDDING_DIM = 5 VOCAB_SIZE = len(word_to_ix) LEARNING_RATE = 0.01 # Initialize the model, loss function, and optimizer model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM) loss_function = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE) # Prepare dummy training data: context words and the target word # Context: [\u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;over\u0026#34;] -\u0026gt; Target: \u0026#34;fox\u0026#34; context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long) target_idxs = torch.tensor([3], dtype=torch.long) # Perform a single optimization step model.zero_grad() # Forward pass: we reshape context to (1, -1) to simulate a batch of size 1 logits = model(context_idxs.view(1, -1)) # Calculate loss against the target word index loss = loss_function(logits, target_idxs) # Backpropagate and update weights loss.backward() optimizer.step() # Output results for the blog post print(\u0026#34;=== Model Training Snapshot ===\u0026#34;) print(f\u0026#34;Calculated Loss: {loss.item():.6f}\u0026#34;) print(f\u0026#34;\\n=== Learned Vector for \u0026#39;jumps\u0026#39; ===\u0026#34;) word_vec = model.embeddings(torch.tensor([word_to_ix[\u0026#39;jumps\u0026#39;]])) print(word_vec.detach().numpy()) print(\u0026#34;\\n=== Embedding Matrix (Weights) ===\u0026#34;) print(model.embeddings.weight.detach().numpy()) print(\u0026#34;\\n=== Linear Layer Weights ===\u0026#34;) print(model.linear.weight.detach().numpy()) Output shows single pass loss and learned vector for \u0026lsquo;jumps\u0026rsquo; word. This is a single training sample result but after multiple such passes both Embedding and linear weights should be very similar.\n=== Model Training Snapshot === Calculated Loss: 1.989656 === Learned Vector for \u0026#39;jumps\u0026#39; === [[-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775]] === Embedding Matrix (Weights) === [[ 1.66039 -0.11371879 0.6246518 0.35860053 -1.2504417 ] [-2.1847186 -0.77199775 -0.17050214 -0.38411248 -0.03913084] [ 0.11852697 0.90073633 0.8847807 0.7404524 0.900149 ] [ 0.07440972 -0.40259898 2.6246994 -0.08851447 0.02660969] [-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775] [-0.9245572 -0.5545908 0.9083091 -1.0755049 0.84047747] [-1.0237687 0.59466314 0.05621134 -0.6202532 1.3664424 ] [ 0.60998917 -1.0549186 1.6103884 0.8724912 -1.2486908 ]] === Linear Layer Weights === [[ 0.02165058 -0.28883642 0.14545658 -0.3442509 0.32704315] [-0.18731792 0.28583744 0.22635977 0.13245736 0.29019794] [ 0.3158916 -0.15826383 -0.03203773 0.16377363 -0.41457543] [-0.05080034 0.4180087 0.11228557 0.30218413 0.3025514 ] [-0.38419306 0.24475925 -0.39210224 -0.38660625 -0.2673145 ] [-0.32321444 0.12200444 -0.03569533 0.2891424 -0.07345333] [-0.33704326 0.2521956 0.31587374 0.22590035 0.29866052] [ 0.4117266 -0.44231793 0.24064957 -0.29684234 0.333821 ]] Interesting Facts About CBOW üß† The \u0026ldquo;Averaging Problem\u0026rdquo;: This is a common issue in CBOW models. The averaging and training is order-agnostic. All variants of word order in the context window will generate the same output. \u0026ldquo;Dog bit the man\u0026rdquo; and \u0026ldquo;Man bit the dog\u0026rdquo; are the same in CBOW\u0026rsquo;s eyes.\nSmoothing Effect: CBOW models are much faster to train than Skip-Gram but are generally smoother.\nRare Words: CBOW models struggle slightly with rare words because the presence of surrounding common words (like articles and prepositions) can skew the embeddings and dissolve their uniqueness.\nComputational Cost: Softmax requires summing over the entire vocabulary size, which can be computationally expensive if $V$ is very large. Solutions include Hierarchical Softmax and Negative Sampling (approximating the denominator by only checking the target word vs. 5 random noise words).\nVectors as Double Agents: Every word has a different representation in the embedding matrix and the output matrix. Most of the time $W_{out}$ is discarded, but some consider averaging both $W_{in}$ and $W_{out}$ for slightly better performance.\nLinear Relationships: The famous analogy King - Man + Woman = Queen emerges from the model because of the linear relationships between the words.\nInitialization: This is a no-brainer but worth mentioning. Initialization to zero vectors would mean no learning. We must initialize randomly.\n2. Skip-Gram (The \u0026ldquo;Burger Flip\u0026rdquo; of CBOW) üîÑ Skip-Gram flips the CBOW logic. While CBOW tries to predict the center word given the context, Skip-Gram tries to predict the scattered context given the center word.\nCore Concept: If a word like \u0026ldquo;monarch\u0026rdquo; can accurately predict words like \u0026ldquo;king\u0026rdquo;, \u0026ldquo;throne\u0026rdquo;, \u0026ldquo;crown\u0026rdquo;, \u0026ldquo;royalty\u0026rdquo;, and \u0026ldquo;power\u0026rdquo;, then it must effectively capture the concept of royalty.\nTraining Let\u0026rsquo;s define dimensions:\n$V$: Vocabulary size $D$: Embedding dimension $C$: Number of context words Step 1: Input Lookup\nUnlike CBOW, our input is just one word vector (the center word $w_t$). We grab the vector $v_c$ from the Input Matrix $W_{in}$.\n$$ h = v_{w_t} $$\nNote: There is no averaging here. The hidden layer $h$ is simply the raw vector of the center word.\nStep 2: Output Scoring (The Broadcast)\nThe model takes this single vector $h$ and compares it against the Output Matrix $W_{out}$ (the entire vocabulary). It computes a score $u_j$ for every word $j$ in the dictionary.\n$$ u = W_{out}^T h $$\nResult $u$: A vector of size $V \\times 1$ containing raw scores.\nStep 3: Probability Conversion (Softmax)\nWe apply Softmax to turn scores into probabilities.\n$$ P(w_j | w_t) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 4: The Multi-Target Loss\nHere is the major difference from CBOW. In CBOW, we had one target (the center). In Skip-Gram, we have $2C$ targets (all the surrounding words). For a sentence \u0026ldquo;The cat sat on mat\u0026rdquo; (Center: \u0026ldquo;sat\u0026rdquo;), the targets are \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;. We want to maximize the probability of all these true context words. The Loss function ($E$) sums the error for each context word:\n$$ E = - \\sum_{w_{ctx} \\in \\text{Window}} \\log P(w_{ctx} | w_t) $$\nStep 5: Backpropagation (Accumulating Gradients)\nSince one input word is responsible for predicting multiple output words, the error signals from all those context words add up.\nAt the Output Layer:\nIf \u0026ldquo;cat\u0026rdquo; was a target, the \u0026ldquo;cat\u0026rdquo; vector in $W_{out}$ gets a signal: \u0026ldquo;Move closer to \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; If \u0026ldquo;apple\u0026rdquo; was NOT a target, the \u0026ldquo;apple\u0026rdquo; vector gets a signal: \u0026ldquo;Move away from \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; At the Hidden Layer ($h$):\nThe center word \u0026ldquo;sat\u0026rdquo; receives feedback from all its neighbors simultaneously. Error signal = (Error from \u0026ldquo;The\u0026rdquo;) + (Error from \u0026ldquo;cat\u0026rdquo;) + (Error from \u0026ldquo;on\u0026rdquo;)\u0026hellip; The vector for \u0026ldquo;sat\u0026rdquo; in $W_{in}$ moves in the average direction of all its neighbors. 3. The \u0026ldquo;Computational Nightmare\u0026rdquo; \u0026amp; Negative Sampling The equations above describe Naive Softmax.\nThe Problem: If $V = 100,000$, computing the denominator $\\sum \\exp(u_k)$ takes 100,000 operations. Doing this for every word in a training set of billions of words is impossibly slow.\nThe Solution: Negative Sampling Instead of updating the entire vocabulary (1 correct word vs 99,999 wrong ones), we approximate the problem.\nPositive Pair: (sat, cat) $\\rightarrow$ Maximize probability (Label 1). Negative Pairs: We pick $K$ (e.g., 5) random words the model didn\u0026rsquo;t see, e.g., (sat, bulldozer), (sat, quantum). $\\rightarrow$ Minimize probability (Label 0). New Equation (Sigmoid instead of Softmax):\n$$ E = - \\log \\sigma({v'_{\\text{pos}}}^T v_{\\text{in}}) - \\sum_{k=1}^K \\log \\sigma(-{v'_{\\text{neg}_{k}}}^T v_{\\text{in}}) $$ Effect: We typically only update 6 vectors per step (1 pos + 5 negs) instead of 100,000.\nLet\u0026rsquo;s see this in code:\nimport torch import torch.nn as nn import torch.optim as optim class SkipGramNegativeSampling(nn.Module): \u0026#34;\u0026#34;\u0026#34; Skip-Gram with Negative Sampling (SGNS) implementation. SGNS approximates the softmax over the entire vocabulary by instead distinguishing between a real context word (positive) and K noise words (negative). \u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embedding_dim): super(SkipGramNegativeSampling, self).__init__() # Input embeddings: used when the word is the center word self.in_embed = nn.Embedding(vocab_size, embedding_dim) # Output embeddings: used when the word is a context or negative sample self.out_embed = nn.Embedding(vocab_size, embedding_dim) # Initialize weights with small values to prevent gradient saturation initrange = 0.5 / embedding_dim self.in_embed.weight.data.uniform_(-initrange, initrange) self.out_embed.weight.data.uniform_(-initrange, initrange) def forward(self, center_words, target_words, negative_words): \u0026#34;\u0026#34;\u0026#34; Computes the negative sampling loss. Args: center_words: (batch_size) target_words: (batch_size) negative_words: (batch_size, K) where K is number of negative samples \u0026#34;\u0026#34;\u0026#34; # Retrieve vectors v_c = self.in_embed(center_words) # (batch_size, embed_dim) u_o = self.out_embed(target_words) # (batch_size, embed_dim) u_n = self.out_embed(negative_words) # (batch_size, K, embed_dim) # 1. Positive Score: log(sigmoid(v_c ¬∑ u_o)) # Compute dot product: (batch, 1, dim) @ (batch, dim, 1) -\u0026gt; (batch, 1) pos_score = torch.bmm(u_o.unsqueeze(1), v_c.unsqueeze(2)).squeeze(2) pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-7) # 2. Negative Score: sum(log(sigmoid(-v_c ¬∑ u_n))) # Compute dot products for all K samples: (batch, K, dim) @ (batch, dim, 1) -\u0026gt; (batch, K) neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2) neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-7), dim=1, keepdim=True) # Total loss is the negative of the objective function loss = -(pos_loss + neg_loss) return torch.mean(loss) # --- Configuration \u0026amp; Mock Data --- VOCAB_SIZE = 100 EMBED_DIM = 10 word_to_ix = {\u0026#39;fox\u0026#39;: 0} # Example vocabulary mapping model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock inputs: 1 center word, 1 target word, 5 negative samples center_id = torch.tensor([0], dtype=torch.long) target_id = torch.tensor([1], dtype=torch.long) negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long) # Training Step model.zero_grad() loss = model(center_id, target_id, negative_ids) loss.backward() optimizer.step() # Output Results print(f\u0026#34;Loss after one step: {loss.item():.6f}\u0026#34;) word_vec = model.in_embed(torch.tensor([word_to_ix[\u0026#39;fox\u0026#39;]])) print(f\u0026#34;Vector for \u0026#39;fox\u0026#39;:\\n{word_vec.detach().numpy()}\u0026#34;) The outputs:\nLoss after one step: 4.158468 Vector for \u0026#39;fox\u0026#39;: [[ 0.04197385 0.02400733 -0.03800093 0.01672485 -0.03872231 -0.0061478 0.0121122 0.04057864 -0.036255 0.03861175]] Interesting Facts About Skip-Gram üí° Makes Rare Words Shine: Skip-Gram is more effective at capturing the meaning of rare words because it focuses on the context of the center word. In CBOW, a rare word\u0026rsquo;s vector is averaged with others, diluting its signal. In Skip-Gram, when the rare word appears as the center, it gets the full, undivided attention of the backpropagation update.\nSlow Training: This method creates more training samples. A window of size 2 creates 4 training pairs per center word, while CBOW creates only 1 training pair per center word.\nSemantic Focus: Skip-Gram puts more emphasis on the semantic relationship between words. It views the context as the primary signal, with the center word as the target. This makes it better at capturing the meaning of words in context. Skip-Gram tends to capture semantic relationships (King/Queen) slightly better, while CBOW captures syntactic ones (walk/walked) slightly better.\n","permalink":"http://localhost:1313/posts/word-embeddings/","summary":"Understanding the mathematics behind Word2Vec, CBOW, and Skip-Gram and how they map language to vector space.","title":"Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n5. Code Examples: We\u0026rsquo;ll use sentencepiece for tokenization and compare results for different models.\nimport sentencepiece as spm import os # Configuration for training the SentencePiece model # SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words. options = { # The source text file used for learning the vocabulary \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # The base name for the generated model (.model) and vocabulary (.vocab) files \u0026#39;model_prefix\u0026#39;: \u0026#39;bpe_model\u0026#39;, # Number of unique tokens in the final vocabulary \u0026#39;vocab_size\u0026#39;: 4000, # \u0026#39;bpe\u0026#39; (Byte Pair Encoding) merges frequent pairs of characters/sequences \u0026#39;model_type\u0026#39;: \u0026#39;bpe\u0026#39;, # Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets \u0026#39;character_coverage\u0026#39;: 0.9995, # When enabled, unknown characters are decomposed into UTF-8 bytes to avoid \u0026#39;unk\u0026#39; tokens \u0026#39;byte_fallback\u0026#39;: True, # Treats digits individually (0-9), preventing large numbers from being treated as single tokens \u0026#39;split_digits\u0026#39;: True, # Prevents adding a whitespace prefix to the first token; useful for fine-grained control \u0026#39;add_dummy_prefix\u0026#39;: False } try: print(\u0026#34;Starting the training process...\u0026#34;) # SentencePieceTrainer.train takes the dictionary of options to build the BPE model spm.SentencePieceTrainer.train(**options) print(\u0026#34;Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created.\u0026#34;) # Initialize the processor and load the newly trained model sp = spm.SentencePieceProcessor() sp.load(\u0026#39;bpe_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Model Metadata:\u0026#34;) # Retrieve the total number of tokens in the vocabulary print(f\u0026#34;Total Vocab Size: {sp.get_piece_size()}\u0026#34;) # Special tokens are used for sequence boundaries and handling unknown characters print(f\u0026#34;BOS (Beginning of Sentence) ID: {sp.bos_id()}\u0026#34;) print(f\u0026#34;EOS (End of Sentence) ID: {sp.eos_id()}\u0026#34;) print(f\u0026#34;UNK (Unknown) ID: {sp.unk_id()}\u0026#34;) print(f\u0026#34;PAD (Padding) ID: {sp.pad_id()}\u0026#34;) # Test the tokenizer on sample strings to see how it breaks down text test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: shows the actual subword units (tokens) print(f\u0026#34;Subword Tokens: {sp.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: shows the numerical mapping for each token print(f\u0026#34;Numerical IDs: {sp.encode_as_ids(text)}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred during training or processing: {e}\u0026#34;) You can see the following output:\nStarting training... Starting the training process... Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created. ------------------------------ Model Metadata: Total Vocab Size: 4000 BOS (Beginning of Sentence) ID: 1 EOS (End of Sentence) ID: 2 UNK (Unknown) ID: 0 PAD (Padding) ID: -1 --- Tokenization Test --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;el\u0026#39;, \u0026#39;lo\u0026#39;, \u0026#39;‚ñÅW\u0026#39;, \u0026#39;orld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 380, 309, 476, 619, 36, 3924, 3964, 3959, 3993, 3978, 3983, 57, 3975, 3976, 60, 3974] --- Tokenization Test --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅmo\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;inst\u0026#39;, \u0026#39;ag\u0026#39;, \u0026#39;ram\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅever\u0026#39;] Numerical IDs: [1894, 304, 3284, 376, 321, 313, 312, 1008, 3313, 1325, 428, 3937, 341, 304, 3284, 1495] The ! is fallen back to UTF bytes. and the word instagrammable is split into multiple subword tokens. Also, original text can be recreated right back by simply concatenating the tokens. And replacing underscore by whitespace. Now we can try Unigram model. and see the same sentences tokenized differently.\nfrom tokenizers import Tokenizer from tokenizers.models import WordPiece from tokenizers.trainers import WordPieceTrainer from tokenizers.pre_tokenizers import Whitespace try: # 1. Initialize the WordPiece Tokenizer # We specify the [UNK] token for handling words not found in the vocabulary. tokenizer = Tokenizer(WordPiece(unk_token=\u0026#34;[UNK]\u0026#34;)) # 2. Configure Pre-tokenization # Before the subword algorithm runs, we need to split the raw text into words. # Whitespace splitting is the standard first step for most English NLP tasks. tokenizer.pre_tokenizer = Whitespace() # 3. Initialize the Trainer # We define our target vocabulary size and the special tokens required for # downstream tasks (like BERT\u0026#39;s [CLS] for classification or [SEP] for separators). trainer = WordPieceTrainer( vocab_size=4000, special_tokens=[\u0026#34;[UNK]\u0026#34;, \u0026#34;[CLS]\u0026#34;, \u0026#34;[SEP]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[MASK]\u0026#34;] ) # 4. Train the Model # The tokenizer scans the training file to build a vocabulary of the most # frequent subword units. tokenizer.train(files=[\u0026#34;train_data.txt\u0026#34;], trainer=trainer) # 5. Persist the Model # Save the configuration and vocabulary to a JSON file for future inference. tokenizer.save(\u0026#34;wordpiece.json\u0026#34;) print(\u0026#34;Training complete. \u0026#39;wordpiece.json\u0026#39; created.\u0026#34;) # 6. Metadata Inspection print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;WordPiece Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {tokenizer.get_vocab_size()}\u0026#34;) # 7. Testing Subword Tokenization # WordPiece shines at handling rare words by breaking them into meaningful chunks. test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (WordPiece) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # Encode converts raw text into a Tokenizer object containing tokens and IDs output = tokenizer.encode(text) # \u0026#39;tokens\u0026#39; shows the subword breakdown (e.g., \u0026#39;un\u0026#39;, \u0026#39;##insta\u0026#39;, etc.) print(f\u0026#34;Subword Tokens: {output.tokens}\u0026#34;) # \u0026#39;ids\u0026#39; are the numerical indices mapped to the vocabulary print(f\u0026#34;Numerical IDs: {output.ids}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred with WordPiece model: {e}\u0026#34;) You can see the following output:\nWordPiece Model Metadata: Total Vocab Size: 2609 --- Tokenization Test (WordPiece) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;H\u0026#39;, \u0026#39;##el\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;##or\u0026#39;, \u0026#39;##ld\u0026#39;, \u0026#39;[UNK]\u0026#39;, \u0026#39;[UNK]\u0026#39;] Numerical IDs: [37, 180, 214, 52, 162, 418, 0, 0] --- Tokenization Test (WordPiece) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;##os\u0026#39;, \u0026#39;##t\u0026#39;, \u0026#39;un\u0026#39;, \u0026#39;##ins\u0026#39;, \u0026#39;##ta\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;##ra\u0026#39;, \u0026#39;##m\u0026#39;, \u0026#39;##ma\u0026#39;, \u0026#39;##ble\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;ever\u0026#39;] Numerical IDs: [691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240] We can see the special character ## in the output. We can also see the breakdowns not having any semantic meaning. They are likelihood based. We can try the Unigram model now.\n# \u0026#39;Unigram\u0026#39; is the default and usually recommended over BPE in SentencePiece. options_unigram = { \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # Path to the raw text file for training \u0026#39;model_prefix\u0026#39;: \u0026#39;unigram_model\u0026#39;, # Prefix for the output .model and .vocab files \u0026#39;vocab_size\u0026#39;: 1200, # Desired size of the final vocabulary \u0026#39;model_type\u0026#39;: \u0026#39;unigram\u0026#39;, # Specifies the Unigram language model algorithm \u0026#39;character_coverage\u0026#39;: 0.9995, # Percentage of characters covered by the model (0.9995 is standard for Latin scripts) \u0026#39;byte_fallback\u0026#39;: True, # Enables mapping unknown characters to UTF-8 bytes to avoid \u0026lt;unk\u0026gt; tokens \u0026#39;split_digits\u0026#39;: True, # Treats each digit as an individual token (useful for numerical data) \u0026#39;add_dummy_prefix\u0026#39;: False # Prevents adding a leading space (SentencePiece default is True) } try: # 1. Train the SentencePiece model using the defined options print(\u0026#34;Starting Unigram training...\u0026#34;) spm.SentencePieceTrainer.train(**options_unigram) print(\u0026#34;Training complete. \u0026#39;unigram_model.model\u0026#39; created.\u0026#34;) # 2. Load the trained model into a processor instance for inference sp_unigram = spm.SentencePieceProcessor() sp_unigram.load(\u0026#39;unigram_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Unigram Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {sp_unigram.get_piece_size()}\u0026#34;) # 3. Define test cases to evaluate how the model handles common and rare words test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] # 4. Iterate through test sentences to visualize subword segmentation for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (Unigram) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: Converts text into subword strings (visual representation) print(f\u0026#34;Subword Tokens: {sp_unigram.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: Converts text into numerical indices for model input print(f\u0026#34;Numerical IDs: {sp_unigram.encode_as_ids(text)}\u0026#34;) except Exception as e: # Handle potential errors during training or loading (e.g., missing input file) print(f\u0026#34;An error occurred with Unigram model: {e}\u0026#34;) The output is as follows:\nUnigram Model Metadata: Total Vocab Size: 1200 --- Tokenization Test (Unigram) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ll\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;ld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311] --- Tokenization Test (Unigram) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;T\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅm\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;sta\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;ra\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ver\u0026#39;] ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"A comprehensive guide to tokenization strategies: BPE, WordPiece, Unigram, and SentencePiece.","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"A brief introduction to Vectors \u0026amp; Verbs and formatting verification.","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"\nFeedforward network (FFN) blocks are the least talked about when in context to LLMs. But that is where more than 70% of the paramters come from. Something as simple and convinient as a couple of linear transforming matrices become a burden when models start scaling in Billions of parameters. So, getting the maximum juice of a lean structure is a challenge. Plus, mostly under represented, because the FFN sub-layer acts as the primary \u0026ldquo;knowledge\u0026rdquo; processing engine, often containing the majority of a model\u0026rsquo;s total parameters.\nStandard Position Wise Feedforward Network This is as simple as it can get, two-layered fully connected network of neurons with a single activation switch, most probably a ReLU variant (for LLMs). $$ FFN(x) = max(0, W_2(W_1(x)) + b_1)W_2 + b_2 $$ Same transformation is applied for each token in parallel, making it efficient for GPU computation. For Example, in BERT model the 512 dimensions of each token are expanded to 2048 to allow for \u0026ldquo;intricate interactions\u0026rdquo; before projecting back to 512 dimensions. This part is important, in every transformer stacking of layers is possible only because the output of every matches it\u0026rsquo;s input dimensions.\nSWiGLU Sigmoid Weighted Gated Linear Unit (SWiGLU) has become the Gold Standard for feed forward networks in LLMs like Llama 3 and PaLM. The primary reason for this choice is the Swish function (as shown in the first image) instead of ReLU or GELU are as following: Mathematical Linearity: Swish (SILU) is defined as $x\\cdot sigmoid(\\beta x)$,\n","permalink":"http://localhost:1313/posts/feedforward-networks/","summary":"Exploring the hidden layers of trillion-parameter switchboards: Feedforward Neural Networks and Activation Functions.","title":"Anatomy of Trillion-Parameter Switchboards: Understanding Feedforward Blocks"},{"content":"\nWe are about to touch the holy grail of modern AI. The 2017 declaration was simple: Attention Is All You Need. But as we‚Äôve raced from GPT-1 to reasoning models, the definition of \u0026lsquo;Attention\u0026rsquo; has quietly transformed. Is the algorithm ruling the world today actually the same one we started with? Let‚Äôs look under the hood.\nüî• The Core Concept At its heart, the attention mechanism is a form of weighted aggregation. It is often described using a database retrieval analogy. Imagine you have a database of information (context).\nQuery ($Q$): What are you currently looking for? (The token being generated). Key ($K$): The indexing tag associated with each piece of information. Value ($V$): The actual information (The context token). The \u0026ldquo;Attention\u0026rdquo; is the process of matching your Query against all the Keys to determine how much \u0026ldquo;weight\u0026rdquo; to put on each Value. Attention scores are calculated in each transformer layer at every step of the sequence. This is a single unifying factor in all transformer models, from BERT to Gemini.\nüî• Standard Multi-Head Attention (MHA) The original mechanism introduced in the paper \u0026ldquo;Attention Is All You Need\u0026rdquo;.\nHere, instead of performing the Key-Query-Value (KQV) operation on the entire context only once, the attention is split into $N$ parallel heads. Each head learns a different aspect of the language (e.g., word meaning, syntax, semantic relationships, etc.). The equation is a simple culmination of dot products aided by a regularizer like Softmax.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nWhere:\n$QK^T$ computes the similarity scores between the Query and Key. $\\sqrt{d_k}$ is a scaling factor to prevent the dot product from growing too large, mitigating the vanishing/exploding gradient problem. The weighted sum is applied to the values of $V$. The Memory Bandwidth Bottleneck: \u0026gt; During inference, the model must store Key and Value matrices for all previous tokens. This is called the KV Cache. In MHA, every single head has its own unique set of Keys and Values. For large models with massive context windows, loading this massive KV cache from memory (HBM) to the computation chip (SRAM) becomes the primary bottleneck, slowing down generation significantly. Earlier models could often only generate ~8 tokens per second at inference due to this constraint.\nüî• Multi-Query Attention (MQA) This approach addresses the memory bottleneck by sharing the same set of Keys and Values across all heads while keeping separate Query heads.\nMHA: $H$ Query heads, $H$ Key heads, $H$ Value heads MQA: $H$ Query heads, 1 Key head, 1 Value head This drastically reduces the memory requirements of the KV cache by a factor of $H$ (often 8x or 16x). The GPU needs to load far less data, significantly speeding up inference. However, it does not reduce the computation (FLOPs)‚Äîso why is it faster?\nThis can be explained with the Kitchen Analogy: Imagine a kitchen (GPU) with 8 chefs (Heads).\nComputation (Math): The Chef chopping vegetables. Memory Access (Bandwidth): The Assistant running to the fridge to get ingredients. In MHA: Each chef demands unique vegetables from the fridge. The assistant runs back and forth 8 times, fetching 8 different crates. Most of the time is spent waiting for the assistant to return. This results in idle cores waiting for data to load from HBM.\nIn MQA: All chefs agree to use the same vegetables. The assistant runs back and forth only once, fetching one giant crate of potatoes and dumping it. This results in minimized memory transfer and no idle cores.\nThe Technical Reality: During text generation, the model is memory-bound, not compute-bound. The time it takes to move the KV Cache from HBM (High Bandwidth Memory) to the chip\u0026rsquo;s SRAM is the bottleneck. MQA reduces the data volume of that move by a factor of $H$.\nThe Capacity Gap: MQA is a trade-off between nuanced response and memory capacity. The model loses the ability to store different types of information, as a single $V$ matrix must now store all nuances of information flow in the language.\nThe Surgery Trick (Uptraining): Google Research figured out a way to take an existing model with MHA and convert it to MQA by averaging the weight matrices of $H$ heads. This is called Uptraining. You then train this \u0026ldquo;Frankenstein\u0026rdquo; model for ~5% of the original training steps for adjustments. This yields an MQA model that is nearly as good as the original MHA but vastly faster.\nüî• Grouped Query Attention (GQA) A \u0026ldquo;best of both worlds\u0026rdquo; solution used in modern models like Llama 2 and Llama 3.\nHow it works: GQA acts as an interpolation between MHA and MQA. Instead of having just 1 Key head (MQA) or $H$ Key heads (MHA), it divides the query heads into $G$ groups. Each group shares a single Key and Value head.\nEquation modification: If you have 8 query heads, you might create 4 groups (2 queries per group). This reduces the KV cache by 2x instead of 8x, preserving more quality than MQA while still speeding up inference.\nMHA: 8 people (heads) are working on a project. Each person has their own unique filing cabinet (KV cache). It takes forever to move 8 cabinets around. MQA: 8 people share one single filing cabinet. It\u0026rsquo;s very fast to move, but they fight over the organization and lose detail. GQA: The 8 people split into 4 teams of 2. Each team shares a cabinet. It\u0026rsquo;s a balance between speed and organizational detail. üî• Multi-Head Latent Attention (MLA) This is the most advanced mechanism for attention, used in models like DeepSeek-V3. It is a sophisticated design for massive efficiency that takes a radical approach: mathematically compressing the memory into a low-rank \u0026ldquo;latent\u0026rdquo; vector.\nKV Cache Explosion: As mentioned before, for a massive model like DeepSeek-V3 (671B parameters), storing the full Keys and Values for every token in a long conversation requires terabytes of high-speed memory (HBM), and chips spend more time waiting than churning.\nMHA (Standard) stores everything. MQA deletes redundancy but hurts quality. MLA keeps the quality of MHA but compresses the data 4x-6x smaller than even GQA.\nThe Zip File Approach: Low-Rank Joint Compression Let\u0026rsquo;s compare the math of MHA and MLA. In MHA, for every single token, we generate a unique Key ($k$) and Value ($v$) matrix for each attention head.\nThe Memory Problem: The $k$ is stored in the KV Cache. For 128 heads, for example, it calculates a massive vector of $128 \\times 128 = 16,384$ floats per token.\nFor MHA, the attention score looks like: $$\\text{Score} = q^T \\cdot k$$\nBut for MLA, instead of storing the full $k$, it projects the input down into a tiny latent vector $c_{KV}$, which is much smaller (e.g., 512 floats). $$c_{KV} = W_{DKV} \\cdot x$$\nNow here is the catch: Naive compression and decompression would mean constructing a small latent matrix for every token and deconstructing it every single time the information is required, which defeats the purpose of compression.\n$$k_{\\text{reconstructed}} = W_{UK} \\cdot c_{KV}$$ $$\\text{Score} = q^T \\cdot k_{\\text{reconstructed}}$$\nThe Matrix Absorption Trick (Optimization) Instead of reconstructing $k$ for every token, we can absorb the Up-Projection matrix ($W_{UK}$) into the Down-Projection matrix ($W_{DKV}$) and absorb the query $q$ into a new $Q$.\nFrom the original equation: $$\\text{Score} = q^T \\cdot (\\underbrace{W_{UK} \\cdot c_{KV}}_{\\text{This is } k})$$\nWe associate differently:\n$$\\text{Score} = (\\underbrace{q^T \\cdot W_{UK}}_{\\text{Absorbed Query}}) \\cdot c_{KV}$$ We change the order in which the matrix multiplication is performed.\nHow is this allowed? In linear algebra, matrix multiplication is associative:\n$$(A \\cdot B) \\cdot C = A \\cdot (B \\cdot C)$$ We can move the parentheses! Instead of grouping $(W_{UK} \\cdot c_{KV})$ to make the Key, we group $(q^T \\cdot W_{UK})$ to make a new Query.\nNote: The positional embedding information is left untouched. That rotational information from algorithms like RoPE is preserved outside the compression; it requires minimal space anyways.\n","permalink":"http://localhost:1313/posts/attention-mechanisms/","summary":"We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek\u0026rsquo;s MLA, how has the definition of \u0026lsquo;Attention\u0026rsquo; transformed?","title":"Attention Is All You Need, but exactly which one?: MHA, GQA and MLA"},{"content":"\nPositional Embeddings are the \u0026ldquo;voice\u0026rdquo; that tell Transformers where words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.\n\u0026ldquo;The dog chased the cat\u0026rdquo; and \u0026ldquo;cat chased the dog\u0026rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.\nüî• Sinusoidal Embeddings: The Odometer of Language Introduced in the seminal \u0026ldquo;Attention Is All You Need\u0026rdquo; paper, this was the industry standard for many years. This approach adds a vector to each word embedding based on its position. The vector is computed out of wavelike patterns that are periodic and smooth. Mathematically, for a position $pos$ and embedding dimension $i$:\n$$ PE_{(pos, i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $$\nThe model uses sine and cosine waves of different frequencies to capture both short and long-range dependencies. The frequency of the waves decreases as the embedding dimension increases, which helps capture long-range dependencies in the input sequence.\nThe creators wanted the model to calculate distance easily, which is why a linear function was used. The model simply rotates the current position vector using a rotation matrix to derive the next position.\n$$ \\begin{pmatrix} \\cos(k) \u0026 \\sin(k) \\\\ -\\sin(k) \u0026 \\cos(k) \\end{pmatrix} \\cdot PE(pos) \\approx PE(pos + k) $$ Low dimensions act like a fast-ticking clock (high frequency, changing rapidly) and high dimensions act like a slow clock (low frequency, changing slowly).\nThe number 10,000 is an arbitrarily chosen high number to reduce Aliasing. By choosing such a high number, the creators ensured that the wavelength of the slowest dimension is incredibly long. Consequently, the pattern practically never repeats, and each position remains unique.\nThis combination works like a mechanical odometer dial. Each subsequent dimension acts as a slower-moving gear. The model learns to read these \u0026ldquo;dials\u0026rdquo; to understand exactly where a word sits in the sequence.\nThis mitigated the challenge of exploding gradients that previous array-based positional embeddings suffered from (e.g., adding integers 1, 2, 3\u0026hellip;). This model keeps values between -1 and +1, adding stability while ensuring every position gets a distinct code.\nThe Problem: This approach pollutes the word embedding matrix (semantic information). While high-dimensional vectors are almost always orthogonal‚Äîmeaning the noise doesn\u0026rsquo;t fully destroy the word\u0026rsquo;s identity‚Äîit does not strictly guarantee purity.\nThis led to a more mature approach.\nüî• ALiBi: The Fading Streetlight Attention with Linear Biases (ALiBi) is a method where, instead of adding a vector to the embeddings at the start, the model injects positional information linearly, directly into the attention scores. If Sinusoidal Encoding asks, \u0026ldquo;Where am I on the map?\u0026rdquo;, ALiBi asks, \u0026ldquo;How much harder should I squint to see you?\u0026rdquo;\nThe biggest problem ALiBi solved is the \u0026ldquo;Invisible Wall\u0026rdquo;. If you train a Transformer on sequences of length 2048 and then feed it a book with 3000 words, it will crash or output garbage at word 2049. This is a failure of Extrapolation.\nALiBi throws away the idea of adding positional vectors entirely. Instead, it devises a \u0026ldquo;penalty\u0026rdquo; system for attention scores:\n$$ \\text{Score} = (q \\cdot k^\\top) - (m \\cdot \\text{distance}) $$\nDistance: If words A and B are neighbors, the distance is 0, so the penalty is 0. Although the distance is linear, it does not cause exploding gradients because the raw attention scores are normalized by a softmax function. This distance prioritizes nearby words (local context) over distant ones.\nSlope ($m$): If we always penalize distance equally, the model becomes myopic. ALiBi mitigates this by assigning a different slope value to each attention head:\nHead 1 (The Historian): Very low slope. The penalty grows slowly, allowing it to see far back. Head 2 (The Reader): Medium slope. Head 3 (The Myopic): Very high slope (e.g., 0.5); it effectively cannot see beyond 2-3 words. This mimics holding a lantern. Words close to you are bright, while words further away naturally fade into darkness.\nExtrapolation: If you walk 5 miles down the road (a longer sequence), your lantern works exactly the same way. The physics of light decay are constant, allowing the model to handle sequences longer than it was trained on.\nHowever, ALiBi forces a \u0026ldquo;sliding window\u0026rdquo; view where distant words eventually become invisible. This brings us to the modern standard.\nüî• RoPE: The Rotary Revolution Rotary Position Embeddings (RoPE) is the current industry standard (used in Llama 3, Mistral, PaLM) because it mathematically unifies \u0026ldquo;absolute position\u0026rdquo; with \u0026ldquo;relative position.\u0026rdquo; This method rotates the vectors in the Query and Key matrices based on their absolute position.\nIf a vector has 512 dimensions, RoPE breaks it into 256 pairs. Each pair is rotated by a different frequency:\n$$ \\theta_i = 10000^{-2i/d} $$\nThe First pair ($i = 0$) is rotated by the highest frequency. The Last pair ($i = 255$) is rotated by the lowest frequency. This spread ensures the model has \u0026ldquo;high precision\u0026rdquo; (fast rotation) to distinguish immediate neighbors, and \u0026ldquo;low precision\u0026rdquo; (slow rotation) to track global position without the pattern repeating.\nDoesn\u0026rsquo;t rotation change meaning? Yes and No. The vector for \u0026ldquo;Apple\u0026rdquo; at position 1 and \u0026ldquo;Apple\u0026rdquo; at position 256 will look completely different in terms of coordinates. However, rotation does not change the semantic strength (magnitude/norm) of the vector.\nThe Relativity Trick: Transformers compare words using dot products (angles). RoPE relies on the fact that the attention mechanism ignores the absolute rotation and only detects the difference in rotation (relative position). Thus, the \u0026ldquo;relative meaning\u0026rdquo; is encoded purely into the angle between words, leaving the semantic \u0026ldquo;magnitude\u0026rdquo; untouched.\nüî• LongRoPE: The Bifocal Lens While RoPE is powerful, it struggles when stretching context windows from 4k to 2 million tokens. The \u0026ldquo;fast\u0026rdquo; dimensions spin so rapidly over long distances that they become random noise. LongRoPE solves this using an evolutionary search algorithm to find a unique scaling factor ($\\lambda$) for each dimension.\nThe Equations Instead of rotating by the standard $\\theta_i$, LongRoPE rotates by a rescaled frequency:\n$$ \\theta\u0026rsquo;_i = \\lambda_i \\theta_i $$\nTo efficiently find these $\\lambda$ values without searching an infinite space, the algorithm enforces a monotonicity constraint based on NTK theory:\n$$ \\lambda_i \\le \\lambda_{i+1} $$\nThis ensures that low-frequency dimensions (global context) are stretched more than high-frequency dimensions (local context), creating a \u0026ldquo;bifocal\u0026rdquo; effect:\nHigh Frequencies (Local): Kept sharp ($\\lambda \\approx 1$) to maintain grammar. Low Frequencies (Global): Stretched ($\\lambda \u0026gt; 1$) to track massive distances without repeating. Not worth perusing more since the next one is much better at long context positional encoding. üî• HoPE: The Hyperbolic Slide Hyperbolic Rotary Positional Encoding (HoPE) moves from the geometry of a circle to the geometry of a hyperbola (inspired by Lorentz transformations). It was designed to fix a subtle flaw in RoPE: the \u0026ldquo;wobbly\u0026rdquo; or oscillatory nature of attention scores.\nThe Core Problem: RoPE\u0026rsquo;s \u0026ldquo;Wobble\u0026rdquo; In standard RoPE, we rotate vectors around a circle.\nThe Issue: Circles repeat. A dial at $361^\\circ$ looks identical to $1^\\circ$. Even with different frequencies, the dot product fluctuates up and down as distance increases. The Consequence: The model gets confused at long distances, potentially mistaking a word 1,000 tokens away for a neighbor just because the rotation cycles aligned, creating \u0026ldquo;noise.\u0026rdquo; The \u0026ldquo;Hyperbolic\u0026rdquo; Shift HoPE replaces trigonometric functions ($\\sin, \\cos$) with hyperbolic functions ($\\sinh, \\cosh$).\nCircular Rotation: Keeps distance from the center constant. Hyperbolic Rotation: Moves points along a hyperbola. Crucially, hyperbolic functions do not repeat; they grow or decay exponentially. By using hyperbolic rotation, HoPE guarantees that as words get further apart, the attention score decays monotonically (smoothly drops).\nThe Equations HoPE uses a hyperbolic matrix $B(\\theta, m)$ defined using the Lorentz Boost structure:\n$$ B(\\theta, m) = \\begin{pmatrix} \\cosh(m\\theta) \u0026 \\sinh(m\\theta) \\\\ \\sinh(m\\theta) \u0026 \\cosh(m\\theta) \\end{pmatrix} $$ Since hyperbolic functions grow to infinity, HoPE introduces a decay penalty ($e^{-m\\theta'}$). The final attention score simplifies beautifully to a function of relative distance: $$ \\text{Score} \\propto e^{-|m-n|(\\theta\u0026rsquo; - \\theta)} $$\nThis ensures the attention score exponentially decays as distance increases, eliminating the wobbles of standard RoPE. This the latest model that is taking up the space in the industry for extremely long context models.\n","permalink":"http://localhost:1313/posts/positional-embeddings/","summary":"From Sinusoidal to RoPE and HoPE: How Transformers learn to process word order and sequence length.","title":"The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE"},{"content":"Imagine trying to teach a computer the difference between \u0026ldquo;Apple\u0026rdquo; the fruit and \u0026ldquo;Apple\u0026rdquo; the company. To us, the distinction is intuitive. To a machine, it‚Äôs just a string of characters. How do we turn these strings into meaningful math? While early attempts like Word2Vec gave us a great start, they missed the forest for the trees‚Äîor in some cases, the twigs for the branches. Enter GloVe and FastText: two algorithms that revolutionized how machines understand the nuances of human language.\nPrevious static embedding models like Word2Vec successfully captured the local semantics of words, but they failed in addressing contexts between words that might not always appear in the same context window. Furthermore, they treated words as indivisible atomic units in continuous vector spaces, missing out on the internal structure of language.\nThe two simple but effective ideas behind GloVe and FastText are subword units and global co-occurrence. Let\u0026rsquo;s dive into them.\nGloVe: A Global Mapping Strategy Global Vectors for Word Representation (GloVe) was created at Stanford. The core idea was to train a model where word vectors are defined by how often they appear around other words across the entire corpus, rather than just locally.\nTraining Steps: Co-occurrence Matrix X: GloVe scans the entire corpus and creates a giant matrix (spreadsheet) recording the counts of how often words appear near each other. Factorization: It compresses this giant matrix into smaller, dense vectors. Objective: The function tries to equate the dot product of two word vectors with the logarithm of their probability of appearing together. The error between these values is then backpropagated to update the vectors. $$w_i \\cdot w_j + b_i + b_j = \\log(X_{ij})$$\n$w$: The word vector (what we want). $X_{ij}$: The count from our matrix. The Loss Function: We can\u0026rsquo;t satisfy the equation above perfectly for every word pair. So, we use a weighting function to minimize the squared error:\n$$J = \\sum_{i,j=1}^V f(X_{ij}) ({w_i}^T w_j + b_i + b_j - \\log(X_{ij}))^2$$\nIf we do not use the weighting function, the loss would be infinite for rare words (since $\\log(0) = \\infty$) and frequent words (like \u0026rsquo;the\u0026rsquo;) would overpower the model.\n$$ f(x) = \\begin{cases} (x/x_{max})^\\alpha \u0026amp; \\text{if } x \u0026lt; x_{max} \\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$\n(Typically $x_{max}=100$ and $\\alpha=0.75$)\nFastText: The Lego Bricks Strategy FastText was created by Facebook AI Research (FAIR). The idea was to treat words not as solid rocks, but as structures made of \u0026ldquo;Lego bricks\u0026rdquo; (subword units).\nWord2Vec struggled to create embeddings for words it hadn\u0026rsquo;t seen before (Out-Of-Vocabulary or OOV words). FastText solves this by breaking words down.\nGloVe treats a word like \u0026ldquo;Apple\u0026rdquo; as a single, atomic unit. FastText breaks \u0026ldquo;Apple\u0026rdquo; into character n-grams: \u0026lt;ap, app, ppl, ple, le\u0026gt; For a complex or new word like \u0026ldquo;Applesauce\u0026rdquo;, the model might not know the whole word, but it recognizes the Lego brick for \u0026ldquo;Apple\u0026rdquo; and the remaining bits from other subwords.\nFastText takes its inspiration from the SkipGram architecture. The vector for a word is simply the sum of the vectors of its n-gram subwords:\n$$v_w = \\sum_{g \\in G_w} z_g$$\nObjective: The objective is to predict context words given a target word.\n$$P(w_c | w_t) = \\frac{e^{s(v_{w_t} \\cdot v_{w_c})}}{\\sum_{w \\in V} e^{s(v_{w_t} \\cdot v_w)}}$$\nAnd the scoring function is defined as:\n$$s(w_t, w_c) = u_{w_t}^T (\\sum_{g \\in G_w} z_g)$$\n$u_{w_c}$: The vector for the context word (output vector). $z_g$: The vector for the n-gram $g$ (input vector). Modifications to Softmax: Calculating the full softmax for every word in the vocabulary is computationally expensive. FastText uses two methods to approximate this efficiently.\nOption A: Negative Sampling\nWe pick a few random \u0026ldquo;wrong\u0026rdquo; words every time to serve as \u0026ldquo;negative samples\u0026rdquo; and calculate the loss only for the correct word and these noise words.\n$$J = - \\log \\sigma(s(w_t, w_c)) - \\sum\\limits_{i=1}^{N} \\mathbb{E}_{n_i \\sim P_n(w)} [\\log \\sigma(-s(w_t, n_i))]$$\nThe sigmoid function $\\sigma$ squashes the score between 0 and 1. The first term pushes the probability of the correct word towards 1. The second term pushes the probability of the noise words towards 0. Option B: Hierarchical Softmax (Faster for infrequent words)\nInstead of a flat list of words, imagine the vocabulary arranged as a binary tree (Huffman Tree).\nRoot: Top of the tree. Leaves: The actual words. To calculate the probability of a word, we trace a path from the root to that leaf. At every branching node, we calculate the probability (sigmoid) of going left or right. The probability of a word $w$ is the product of the probabilities of the turns taken to reach it. Code Example from gensim.models import FastText from gensim.test.utils import common_texts # Initialize the FastText model with specific hyperparameters model = FastText( vector_size=100, # Dimensionality of the word vectors window=5, # Maximum distance between the current and predicted word within a sentence min_count=1, # Ignores all words with total frequency lower than this sg=1, # Training algorithm: 1 for skip-gram; 0 for CBOW hs=0, # If 0, and negative is non-zero, negative sampling will be used negative=5, # Number of \u0026#34;noise words\u0026#34; to be drawn for negative sampling workers=4, # Number of worker threads to train the model epochs=10, # Number of iterations over the corpus min_n=3, # Minimum length of character n-grams max_n=6 # Maximum length of character n-grams ) # Build the vocabulary from the provided text corpus model.build_vocab(common_texts) # Train the model on the corpus model.train( common_texts, total_examples=model.corpus_count, epochs=model.epochs ) # Calculate and print the cosine similarity between two words in the vocabulary print(f\u0026#34;Similarity between \u0026#39;computer\u0026#39; and \u0026#39;human\u0026#39;: {model.wv.similarity(\u0026#39;computer\u0026#39;, \u0026#39;human\u0026#39;)}\u0026#34;) # Demonstrate FastText\u0026#39;s ability to handle Out-Of-Vocabulary (OOV) words # Even if \u0026#39;computation\u0026#39; wasn\u0026#39;t in the training data, FastText constructs a vector using character n-grams oov_vector = model.wv[\u0026#39;computation\u0026#39;] # Check if a specific word exists in the model\u0026#39;s fixed vocabulary index word = \u0026#34;computer\u0026#34; is_in_vocab = word in model.wv.key_to_index print(f\u0026#34;\\nIs the word \u0026#39;{word}\u0026#39; in the model\u0026#39;s vocabulary? {is_in_vocab}\u0026#34;) ","permalink":"http://localhost:1313/posts/glove-and-fasttext/","summary":"How subword units and global co-occurrence matrices allow GloVe and FastText to capture nuances that Word2Vec missed.","title":"The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText"},{"content":" Before we had Large Language Models writing poetry, we had to teach computers that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; are related not just by spelling, but by meaning. This is the story of that breakthrough. It‚Äôs the moment we stopped counting words and started mapping their souls‚Äîturning raw text into a mathematical landscape where math can solve analogies. Welcome to the world of Word2Vec. üîÆ\nLanguage models require vector representations of words to capture semantic relationships. Before the 2010s, models used word count-based vector representations that captured only the frequency of words (e.g., One-Hot Encoding). The Problems: üöß\nThe Curse of Dimensionality Lack of meaning (Synonyms were treated as mathematically unrelated). In 2011, Mikolov et al. at Google introduced Word2Vec, which used a shallow neural network to learn vector representations of words. The best part? It could create much denser relationships between words than prior models, and it was unsupervised.\nThe training (prediction) is essentially \u0026ldquo;fake\u0026rdquo;‚Äîa pretext task. But the weight matrices we get on the side are a gold mine of fine-grained semantic relationships. ‚õèÔ∏è\nThere are two popular variants of Word2Vec:\nSkipGram Continuous Bag Of Words (CBOW) 1. Continuous Bag of Words (CBOW) üß© CBOW is a simple \u0026lsquo;fill in the blanks\u0026rsquo; machine. It takes the surrounding words as inputs and tries to predict the center word.\n\u0026ldquo;Continuous\u0026rdquo;: It operates in continuous vector space (unlike the discrete space of n-gram models).\n\u0026ldquo;Bag of words\u0026rdquo;: The order of the context does not matter. \u0026ldquo;The cat sat on the mat\u0026rdquo; and \u0026ldquo;The mat sat on the cat\u0026rdquo; are the same for CBOW. They produce exactly the same predictions.\nObjective: Maximize the probability of the target word $w_t$ given its context $w_{t-C}, \\dots, w_{t+C}$.\n2. Training Defining Dimensions:\n$V$: Vocabulary size (number of unique words in the corpus, e.g., 10,000) $N$: Dimension of the embedding space (e.g., 100) $C$: Number of context words (e.g., 2 words on left of target and 2 words on right) Step 1: Input Lookup \u0026amp; One-Hot Encoding\nMathematically, we represent each context word as a one-hot vector $x^{(c)}$. We have an Input Matrix $W_{in}$ of size $V \\times N$. For each context word (e.g., \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;), we select its corresponding row from $W_{in}$.\n$$ v_c = W_{in}^T x^{(c)} $$\nWhere $v_c$ is the $N$-dimensional vector for context word $c$.\nStep 2: Projection (The \u0026ldquo;Mixing\u0026rdquo;)\nWe combine the context vectors. In standard CBOW, we simply average them. This creates the hidden layer vector $h$.\n$$ h = \\frac{1}{2C} \\sum_{c=1}^{2C} v_c $$\n$h$ is a single vector of size $N \\times 1$. Note: This operation is linear and contains no activation function (like ReLU or Sigmoid).\nStep 3: Output Scoring (The \u0026ldquo;Dot Product\u0026rdquo;)\nWe have a second matrix, the Output Matrix $W_{out}$ of size $N \\times V$. We compute the raw score (logit) $u_j$ for every word $j$ in the vocabulary by taking the dot product of the hidden vector $h$ and the output vector $v\u0026rsquo;_j$.\n$$ u_j = {v\u0026rsquo;_j}^T h $$\nOr in matrix form:\n$$ u = W_{out}^T h $$\nResult $u$ is a vector of size $V \\times 1$.\nStep 4: Probability Conversion (Softmax)\nWe convert the raw scores $u$ into probabilities using the Softmax function. This tells us the probability that word $w_j$ is the center word.\n$$ y_j = P(w_j | \\text{context}) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 5: Loss Calculation (Cross-Entropy)\nWe compare the predicted distribution $y$ with the actual ground truth $t$ (a one-hot vector where the index of the true word is 1). The loss function $E$ is:\n$$ E = - \\sum_{j=1}^V t_j \\log(y_j) $$\nSince $t_j$ is 0 for all words except the target word (let\u0026rsquo;s call the target index $j^*$), this simplifies to:\n$$ E = - \\log(y_{j^*}) $$\n3. Backpropagation Details We need to update the weights to minimize $E$. We use the Chain Rule.\nA. Gradient w.r.t. Output Matrix ($W_{out}$)\nWe want to know how the error changes with respect to the raw score $u_j$.\n$$ \\frac{\\partial E}{\\partial u_j} = y_j - t_j $$\nLet\u0026rsquo;s call this error term $e_j$.\nIf $j$ is the target word: $e_j = y_j - 1$ (Result is negative; we boost the score). If $j$ is not target: $e_j = y_j - 0$ (Result is positive; we suppress the score). The update rule for the output vector $v\u0026rsquo;_j$ becomes:\n$$ v'_{j}(\\text{new}) = v'_{j}(\\text{old}) - \\eta \\cdot e_j \\cdot h $$ ($\\eta$ is the learning rate)\nB. Gradient w.r.t. Hidden Layer ($h$)\nWe backpropagate the error from the output layer to the hidden layer.\n$$ EH = \\sum_{j=1}^V e_j \\cdot v\u0026rsquo;_j $$\n$EH$ is an $N$-dimensional vector representing the aggregate error passed back to the projection layer.\nC. Gradient w.r.t. Input Matrix ($W_{in}$)\nSince $h$ was just an average of the input vectors, the error $EH$ is distributed to each context word\u0026rsquo;s input vector. For every word $w_c$ in the context:\n$$ v_{w_c}(\\text{new}) = v_{w_c}(\\text{old}) - \\eta \\cdot \\frac{1}{2C} \\cdot EH $$\nLet\u0026rsquo;s build a single pass through the network for a given context.\nimport torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # Define a simple Continuous Bag of Words (CBOW) style model class CBOWModel(nn.Module): def __init__(self, vocab_size, embedding_dim): super(CBOWModel, self).__init__() # The embedding layer stores the word vectors we want to learn self.embeddings = nn.Embedding(vocab_size, embedding_dim) # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word self.linear = nn.Linear(embedding_dim, vocab_size) def forward(self, inputs): # inputs: tensor of word indices for the context embeds = self.embeddings(inputs) # Aggregate context by calculating the mean of the word embeddings h = torch.mean(embeds, dim=1) # Produce logits (raw scores) for each word in the vocabulary logits = self.linear(h) return logits # Setup vocabulary and mappings word_to_ix = {\u0026#34;the\u0026#34;: 0, \u0026#34;quick\u0026#34;: 1, \u0026#34;brown\u0026#34;: 2, \u0026#34;fox\u0026#34;: 3, \u0026#34;jumps\u0026#34;: 4, \u0026#34;over\u0026#34;: 5, \u0026#34;lazy\u0026#34;: 6, \u0026#34;dog\u0026#34;: 7} ix_to_word = {v: k for k, v in word_to_ix.items()} # Configuration constants EMBEDDING_DIM = 5 VOCAB_SIZE = len(word_to_ix) LEARNING_RATE = 0.01 # Initialize the model, loss function, and optimizer model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM) loss_function = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE) # Prepare dummy training data: context words and the target word # Context: [\u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;over\u0026#34;] -\u0026gt; Target: \u0026#34;fox\u0026#34; context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long) target_idxs = torch.tensor([3], dtype=torch.long) # Perform a single optimization step model.zero_grad() # Forward pass: we reshape context to (1, -1) to simulate a batch of size 1 logits = model(context_idxs.view(1, -1)) # Calculate loss against the target word index loss = loss_function(logits, target_idxs) # Backpropagate and update weights loss.backward() optimizer.step() # Output results for the blog post print(\u0026#34;=== Model Training Snapshot ===\u0026#34;) print(f\u0026#34;Calculated Loss: {loss.item():.6f}\u0026#34;) print(f\u0026#34;\\n=== Learned Vector for \u0026#39;jumps\u0026#39; ===\u0026#34;) word_vec = model.embeddings(torch.tensor([word_to_ix[\u0026#39;jumps\u0026#39;]])) print(word_vec.detach().numpy()) print(\u0026#34;\\n=== Embedding Matrix (Weights) ===\u0026#34;) print(model.embeddings.weight.detach().numpy()) print(\u0026#34;\\n=== Linear Layer Weights ===\u0026#34;) print(model.linear.weight.detach().numpy()) Output shows single pass loss and learned vector for \u0026lsquo;jumps\u0026rsquo; word. This is a single training sample result but after multiple such passes both Embedding and linear weights should be very similar.\n=== Model Training Snapshot === Calculated Loss: 1.989656 === Learned Vector for \u0026#39;jumps\u0026#39; === [[-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775]] === Embedding Matrix (Weights) === [[ 1.66039 -0.11371879 0.6246518 0.35860053 -1.2504417 ] [-2.1847186 -0.77199775 -0.17050214 -0.38411248 -0.03913084] [ 0.11852697 0.90073633 0.8847807 0.7404524 0.900149 ] [ 0.07440972 -0.40259898 2.6246994 -0.08851447 0.02660969] [-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775] [-0.9245572 -0.5545908 0.9083091 -1.0755049 0.84047747] [-1.0237687 0.59466314 0.05621134 -0.6202532 1.3664424 ] [ 0.60998917 -1.0549186 1.6103884 0.8724912 -1.2486908 ]] === Linear Layer Weights === [[ 0.02165058 -0.28883642 0.14545658 -0.3442509 0.32704315] [-0.18731792 0.28583744 0.22635977 0.13245736 0.29019794] [ 0.3158916 -0.15826383 -0.03203773 0.16377363 -0.41457543] [-0.05080034 0.4180087 0.11228557 0.30218413 0.3025514 ] [-0.38419306 0.24475925 -0.39210224 -0.38660625 -0.2673145 ] [-0.32321444 0.12200444 -0.03569533 0.2891424 -0.07345333] [-0.33704326 0.2521956 0.31587374 0.22590035 0.29866052] [ 0.4117266 -0.44231793 0.24064957 -0.29684234 0.333821 ]] Interesting Facts About CBOW üß† The \u0026ldquo;Averaging Problem\u0026rdquo;: This is a common issue in CBOW models. The averaging and training is order-agnostic. All variants of word order in the context window will generate the same output. \u0026ldquo;Dog bit the man\u0026rdquo; and \u0026ldquo;Man bit the dog\u0026rdquo; are the same in CBOW\u0026rsquo;s eyes.\nSmoothing Effect: CBOW models are much faster to train than Skip-Gram but are generally smoother.\nRare Words: CBOW models struggle slightly with rare words because the presence of surrounding common words (like articles and prepositions) can skew the embeddings and dissolve their uniqueness.\nComputational Cost: Softmax requires summing over the entire vocabulary size, which can be computationally expensive if $V$ is very large. Solutions include Hierarchical Softmax and Negative Sampling (approximating the denominator by only checking the target word vs. 5 random noise words).\nVectors as Double Agents: Every word has a different representation in the embedding matrix and the output matrix. Most of the time $W_{out}$ is discarded, but some consider averaging both $W_{in}$ and $W_{out}$ for slightly better performance.\nLinear Relationships: The famous analogy King - Man + Woman = Queen emerges from the model because of the linear relationships between the words.\nInitialization: This is a no-brainer but worth mentioning. Initialization to zero vectors would mean no learning. We must initialize randomly.\n2. Skip-Gram (The \u0026ldquo;Burger Flip\u0026rdquo; of CBOW) üîÑ Skip-Gram flips the CBOW logic. While CBOW tries to predict the center word given the context, Skip-Gram tries to predict the scattered context given the center word.\nCore Concept: If a word like \u0026ldquo;monarch\u0026rdquo; can accurately predict words like \u0026ldquo;king\u0026rdquo;, \u0026ldquo;throne\u0026rdquo;, \u0026ldquo;crown\u0026rdquo;, \u0026ldquo;royalty\u0026rdquo;, and \u0026ldquo;power\u0026rdquo;, then it must effectively capture the concept of royalty.\nTraining Let\u0026rsquo;s define dimensions:\n$V$: Vocabulary size $D$: Embedding dimension $C$: Number of context words Step 1: Input Lookup\nUnlike CBOW, our input is just one word vector (the center word $w_t$). We grab the vector $v_c$ from the Input Matrix $W_{in}$.\n$$ h = v_{w_t} $$\nNote: There is no averaging here. The hidden layer $h$ is simply the raw vector of the center word.\nStep 2: Output Scoring (The Broadcast)\nThe model takes this single vector $h$ and compares it against the Output Matrix $W_{out}$ (the entire vocabulary). It computes a score $u_j$ for every word $j$ in the dictionary.\n$$ u = W_{out}^T h $$\nResult $u$: A vector of size $V \\times 1$ containing raw scores.\nStep 3: Probability Conversion (Softmax)\nWe apply Softmax to turn scores into probabilities.\n$$ P(w_j | w_t) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 4: The Multi-Target Loss\nHere is the major difference from CBOW. In CBOW, we had one target (the center). In Skip-Gram, we have $2C$ targets (all the surrounding words). For a sentence \u0026ldquo;The cat sat on mat\u0026rdquo; (Center: \u0026ldquo;sat\u0026rdquo;), the targets are \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;. We want to maximize the probability of all these true context words. The Loss function ($E$) sums the error for each context word:\n$$ E = - \\sum_{w_{ctx} \\in \\text{Window}} \\log P(w_{ctx} | w_t) $$\nStep 5: Backpropagation (Accumulating Gradients)\nSince one input word is responsible for predicting multiple output words, the error signals from all those context words add up.\nAt the Output Layer:\nIf \u0026ldquo;cat\u0026rdquo; was a target, the \u0026ldquo;cat\u0026rdquo; vector in $W_{out}$ gets a signal: \u0026ldquo;Move closer to \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; If \u0026ldquo;apple\u0026rdquo; was NOT a target, the \u0026ldquo;apple\u0026rdquo; vector gets a signal: \u0026ldquo;Move away from \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; At the Hidden Layer ($h$):\nThe center word \u0026ldquo;sat\u0026rdquo; receives feedback from all its neighbors simultaneously. Error signal = (Error from \u0026ldquo;The\u0026rdquo;) + (Error from \u0026ldquo;cat\u0026rdquo;) + (Error from \u0026ldquo;on\u0026rdquo;)\u0026hellip; The vector for \u0026ldquo;sat\u0026rdquo; in $W_{in}$ moves in the average direction of all its neighbors. 3. The \u0026ldquo;Computational Nightmare\u0026rdquo; \u0026amp; Negative Sampling The equations above describe Naive Softmax.\nThe Problem: If $V = 100,000$, computing the denominator $\\sum \\exp(u_k)$ takes 100,000 operations. Doing this for every word in a training set of billions of words is impossibly slow.\nThe Solution: Negative Sampling Instead of updating the entire vocabulary (1 correct word vs 99,999 wrong ones), we approximate the problem.\nPositive Pair: (sat, cat) $\\rightarrow$ Maximize probability (Label 1). Negative Pairs: We pick $K$ (e.g., 5) random words the model didn\u0026rsquo;t see, e.g., (sat, bulldozer), (sat, quantum). $\\rightarrow$ Minimize probability (Label 0). New Equation (Sigmoid instead of Softmax):\n$$ E = - \\log \\sigma({v'_{\\text{pos}}}^T v_{\\text{in}}) - \\sum_{k=1}^K \\log \\sigma(-{v'_{\\text{neg}_{k}}}^T v_{\\text{in}}) $$ Effect: We typically only update 6 vectors per step (1 pos + 5 negs) instead of 100,000.\nLet\u0026rsquo;s see this in code:\nimport torch import torch.nn as nn import torch.optim as optim class SkipGramNegativeSampling(nn.Module): \u0026#34;\u0026#34;\u0026#34; Skip-Gram with Negative Sampling (SGNS) implementation. SGNS approximates the softmax over the entire vocabulary by instead distinguishing between a real context word (positive) and K noise words (negative). \u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embedding_dim): super(SkipGramNegativeSampling, self).__init__() # Input embeddings: used when the word is the center word self.in_embed = nn.Embedding(vocab_size, embedding_dim) # Output embeddings: used when the word is a context or negative sample self.out_embed = nn.Embedding(vocab_size, embedding_dim) # Initialize weights with small values to prevent gradient saturation initrange = 0.5 / embedding_dim self.in_embed.weight.data.uniform_(-initrange, initrange) self.out_embed.weight.data.uniform_(-initrange, initrange) def forward(self, center_words, target_words, negative_words): \u0026#34;\u0026#34;\u0026#34; Computes the negative sampling loss. Args: center_words: (batch_size) target_words: (batch_size) negative_words: (batch_size, K) where K is number of negative samples \u0026#34;\u0026#34;\u0026#34; # Retrieve vectors v_c = self.in_embed(center_words) # (batch_size, embed_dim) u_o = self.out_embed(target_words) # (batch_size, embed_dim) u_n = self.out_embed(negative_words) # (batch_size, K, embed_dim) # 1. Positive Score: log(sigmoid(v_c ¬∑ u_o)) # Compute dot product: (batch, 1, dim) @ (batch, dim, 1) -\u0026gt; (batch, 1) pos_score = torch.bmm(u_o.unsqueeze(1), v_c.unsqueeze(2)).squeeze(2) pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-7) # 2. Negative Score: sum(log(sigmoid(-v_c ¬∑ u_n))) # Compute dot products for all K samples: (batch, K, dim) @ (batch, dim, 1) -\u0026gt; (batch, K) neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2) neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-7), dim=1, keepdim=True) # Total loss is the negative of the objective function loss = -(pos_loss + neg_loss) return torch.mean(loss) # --- Configuration \u0026amp; Mock Data --- VOCAB_SIZE = 100 EMBED_DIM = 10 word_to_ix = {\u0026#39;fox\u0026#39;: 0} # Example vocabulary mapping model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock inputs: 1 center word, 1 target word, 5 negative samples center_id = torch.tensor([0], dtype=torch.long) target_id = torch.tensor([1], dtype=torch.long) negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long) # Training Step model.zero_grad() loss = model(center_id, target_id, negative_ids) loss.backward() optimizer.step() # Output Results print(f\u0026#34;Loss after one step: {loss.item():.6f}\u0026#34;) word_vec = model.in_embed(torch.tensor([word_to_ix[\u0026#39;fox\u0026#39;]])) print(f\u0026#34;Vector for \u0026#39;fox\u0026#39;:\\n{word_vec.detach().numpy()}\u0026#34;) The outputs:\nLoss after one step: 4.158468 Vector for \u0026#39;fox\u0026#39;: [[ 0.04197385 0.02400733 -0.03800093 0.01672485 -0.03872231 -0.0061478 0.0121122 0.04057864 -0.036255 0.03861175]] Interesting Facts About Skip-Gram üí° Makes Rare Words Shine: Skip-Gram is more effective at capturing the meaning of rare words because it focuses on the context of the center word. In CBOW, a rare word\u0026rsquo;s vector is averaged with others, diluting its signal. In Skip-Gram, when the rare word appears as the center, it gets the full, undivided attention of the backpropagation update.\nSlow Training: This method creates more training samples. A window of size 2 creates 4 training pairs per center word, while CBOW creates only 1 training pair per center word.\nSemantic Focus: Skip-Gram puts more emphasis on the semantic relationship between words. It views the context as the primary signal, with the center word as the target. This makes it better at capturing the meaning of words in context. Skip-Gram tends to capture semantic relationships (King/Queen) slightly better, while CBOW captures syntactic ones (walk/walked) slightly better.\n","permalink":"http://localhost:1313/posts/word-embeddings/","summary":"Understanding the mathematics behind Word2Vec, CBOW, and Skip-Gram and how they map language to vector space.","title":"Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n5. Code Examples: We\u0026rsquo;ll use sentencepiece for tokenization and compare results for different models.\nimport sentencepiece as spm import os # Configuration for training the SentencePiece model # SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words. options = { # The source text file used for learning the vocabulary \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # The base name for the generated model (.model) and vocabulary (.vocab) files \u0026#39;model_prefix\u0026#39;: \u0026#39;bpe_model\u0026#39;, # Number of unique tokens in the final vocabulary \u0026#39;vocab_size\u0026#39;: 4000, # \u0026#39;bpe\u0026#39; (Byte Pair Encoding) merges frequent pairs of characters/sequences \u0026#39;model_type\u0026#39;: \u0026#39;bpe\u0026#39;, # Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets \u0026#39;character_coverage\u0026#39;: 0.9995, # When enabled, unknown characters are decomposed into UTF-8 bytes to avoid \u0026#39;unk\u0026#39; tokens \u0026#39;byte_fallback\u0026#39;: True, # Treats digits individually (0-9), preventing large numbers from being treated as single tokens \u0026#39;split_digits\u0026#39;: True, # Prevents adding a whitespace prefix to the first token; useful for fine-grained control \u0026#39;add_dummy_prefix\u0026#39;: False } try: print(\u0026#34;Starting the training process...\u0026#34;) # SentencePieceTrainer.train takes the dictionary of options to build the BPE model spm.SentencePieceTrainer.train(**options) print(\u0026#34;Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created.\u0026#34;) # Initialize the processor and load the newly trained model sp = spm.SentencePieceProcessor() sp.load(\u0026#39;bpe_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Model Metadata:\u0026#34;) # Retrieve the total number of tokens in the vocabulary print(f\u0026#34;Total Vocab Size: {sp.get_piece_size()}\u0026#34;) # Special tokens are used for sequence boundaries and handling unknown characters print(f\u0026#34;BOS (Beginning of Sentence) ID: {sp.bos_id()}\u0026#34;) print(f\u0026#34;EOS (End of Sentence) ID: {sp.eos_id()}\u0026#34;) print(f\u0026#34;UNK (Unknown) ID: {sp.unk_id()}\u0026#34;) print(f\u0026#34;PAD (Padding) ID: {sp.pad_id()}\u0026#34;) # Test the tokenizer on sample strings to see how it breaks down text test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: shows the actual subword units (tokens) print(f\u0026#34;Subword Tokens: {sp.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: shows the numerical mapping for each token print(f\u0026#34;Numerical IDs: {sp.encode_as_ids(text)}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred during training or processing: {e}\u0026#34;) You can see the following output:\nStarting training... Starting the training process... Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created. ------------------------------ Model Metadata: Total Vocab Size: 4000 BOS (Beginning of Sentence) ID: 1 EOS (End of Sentence) ID: 2 UNK (Unknown) ID: 0 PAD (Padding) ID: -1 --- Tokenization Test --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;el\u0026#39;, \u0026#39;lo\u0026#39;, \u0026#39;‚ñÅW\u0026#39;, \u0026#39;orld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 380, 309, 476, 619, 36, 3924, 3964, 3959, 3993, 3978, 3983, 57, 3975, 3976, 60, 3974] --- Tokenization Test --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅmo\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;inst\u0026#39;, \u0026#39;ag\u0026#39;, \u0026#39;ram\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅever\u0026#39;] Numerical IDs: [1894, 304, 3284, 376, 321, 313, 312, 1008, 3313, 1325, 428, 3937, 341, 304, 3284, 1495] The ! is fallen back to UTF bytes. and the word instagrammable is split into multiple subword tokens. Also, original text can be recreated right back by simply concatenating the tokens. And replacing underscore by whitespace. Now we can try Unigram model. and see the same sentences tokenized differently.\nfrom tokenizers import Tokenizer from tokenizers.models import WordPiece from tokenizers.trainers import WordPieceTrainer from tokenizers.pre_tokenizers import Whitespace try: # 1. Initialize the WordPiece Tokenizer # We specify the [UNK] token for handling words not found in the vocabulary. tokenizer = Tokenizer(WordPiece(unk_token=\u0026#34;[UNK]\u0026#34;)) # 2. Configure Pre-tokenization # Before the subword algorithm runs, we need to split the raw text into words. # Whitespace splitting is the standard first step for most English NLP tasks. tokenizer.pre_tokenizer = Whitespace() # 3. Initialize the Trainer # We define our target vocabulary size and the special tokens required for # downstream tasks (like BERT\u0026#39;s [CLS] for classification or [SEP] for separators). trainer = WordPieceTrainer( vocab_size=4000, special_tokens=[\u0026#34;[UNK]\u0026#34;, \u0026#34;[CLS]\u0026#34;, \u0026#34;[SEP]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[MASK]\u0026#34;] ) # 4. Train the Model # The tokenizer scans the training file to build a vocabulary of the most # frequent subword units. tokenizer.train(files=[\u0026#34;train_data.txt\u0026#34;], trainer=trainer) # 5. Persist the Model # Save the configuration and vocabulary to a JSON file for future inference. tokenizer.save(\u0026#34;wordpiece.json\u0026#34;) print(\u0026#34;Training complete. \u0026#39;wordpiece.json\u0026#39; created.\u0026#34;) # 6. Metadata Inspection print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;WordPiece Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {tokenizer.get_vocab_size()}\u0026#34;) # 7. Testing Subword Tokenization # WordPiece shines at handling rare words by breaking them into meaningful chunks. test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (WordPiece) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # Encode converts raw text into a Tokenizer object containing tokens and IDs output = tokenizer.encode(text) # \u0026#39;tokens\u0026#39; shows the subword breakdown (e.g., \u0026#39;un\u0026#39;, \u0026#39;##insta\u0026#39;, etc.) print(f\u0026#34;Subword Tokens: {output.tokens}\u0026#34;) # \u0026#39;ids\u0026#39; are the numerical indices mapped to the vocabulary print(f\u0026#34;Numerical IDs: {output.ids}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred with WordPiece model: {e}\u0026#34;) You can see the following output:\nWordPiece Model Metadata: Total Vocab Size: 2609 --- Tokenization Test (WordPiece) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;H\u0026#39;, \u0026#39;##el\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;##or\u0026#39;, \u0026#39;##ld\u0026#39;, \u0026#39;[UNK]\u0026#39;, \u0026#39;[UNK]\u0026#39;] Numerical IDs: [37, 180, 214, 52, 162, 418, 0, 0] --- Tokenization Test (WordPiece) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;##os\u0026#39;, \u0026#39;##t\u0026#39;, \u0026#39;un\u0026#39;, \u0026#39;##ins\u0026#39;, \u0026#39;##ta\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;##ra\u0026#39;, \u0026#39;##m\u0026#39;, \u0026#39;##ma\u0026#39;, \u0026#39;##ble\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;ever\u0026#39;] Numerical IDs: [691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240] We can see the special character ## in the output. We can also see the breakdowns not having any semantic meaning. They are likelihood based. We can try the Unigram model now.\n# \u0026#39;Unigram\u0026#39; is the default and usually recommended over BPE in SentencePiece. options_unigram = { \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # Path to the raw text file for training \u0026#39;model_prefix\u0026#39;: \u0026#39;unigram_model\u0026#39;, # Prefix for the output .model and .vocab files \u0026#39;vocab_size\u0026#39;: 1200, # Desired size of the final vocabulary \u0026#39;model_type\u0026#39;: \u0026#39;unigram\u0026#39;, # Specifies the Unigram language model algorithm \u0026#39;character_coverage\u0026#39;: 0.9995, # Percentage of characters covered by the model (0.9995 is standard for Latin scripts) \u0026#39;byte_fallback\u0026#39;: True, # Enables mapping unknown characters to UTF-8 bytes to avoid \u0026lt;unk\u0026gt; tokens \u0026#39;split_digits\u0026#39;: True, # Treats each digit as an individual token (useful for numerical data) \u0026#39;add_dummy_prefix\u0026#39;: False # Prevents adding a leading space (SentencePiece default is True) } try: # 1. Train the SentencePiece model using the defined options print(\u0026#34;Starting Unigram training...\u0026#34;) spm.SentencePieceTrainer.train(**options_unigram) print(\u0026#34;Training complete. \u0026#39;unigram_model.model\u0026#39; created.\u0026#34;) # 2. Load the trained model into a processor instance for inference sp_unigram = spm.SentencePieceProcessor() sp_unigram.load(\u0026#39;unigram_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Unigram Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {sp_unigram.get_piece_size()}\u0026#34;) # 3. Define test cases to evaluate how the model handles common and rare words test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] # 4. Iterate through test sentences to visualize subword segmentation for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (Unigram) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: Converts text into subword strings (visual representation) print(f\u0026#34;Subword Tokens: {sp_unigram.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: Converts text into numerical indices for model input print(f\u0026#34;Numerical IDs: {sp_unigram.encode_as_ids(text)}\u0026#34;) except Exception as e: # Handle potential errors during training or loading (e.g., missing input file) print(f\u0026#34;An error occurred with Unigram model: {e}\u0026#34;) The output is as follows:\nUnigram Model Metadata: Total Vocab Size: 1200 --- Tokenization Test (Unigram) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ll\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;ld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311] --- Tokenization Test (Unigram) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;T\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅm\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;sta\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;ra\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ver\u0026#39;] ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"A comprehensive guide to tokenization strategies: BPE, WordPiece, Unigram, and SentencePiece.","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"A brief introduction to Vectors \u0026amp; Verbs and formatting verification.","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"\nFeedforward network (FFN) blocks are the least talked about when in context to LLMs. But that is where more than 70% of the paramters come from. Something as simple and convinient as a couple of linear transforming matrices become a burden when models start scaling in Billions of parameters. So, getting the maximum juice of a lean structure is a challenge. Plus, mostly under represented, because the FFN sub-layer acts as the primary \u0026ldquo;knowledge\u0026rdquo; processing engine, often containing the majority of a model\u0026rsquo;s total parameters.\nStandard Position Wise Feedforward Network This is as simple as it can get, two-layered fully connected network of neurons with a single activation switch, most probably a ReLU variant (for LLMs). $$ FFN(x) = max(0, W_2(W_1(x)) + b_1)W_2 + b_2 $$ Same transformation is applied for each token in parallel, making it efficient for GPU computation. For Example, in BERT model the 512 dimensions of each token are expanded to 2048 to allow for \u0026ldquo;intricate interactions\u0026rdquo; before projecting back to 512 dimensions. This part is important, in every transformer stacking of layers is possible only because the output of every matches it\u0026rsquo;s input dimensions.\nSWiGLU Sigmoid Weighted Gated Linear Unit (SWiGLU) has become the Gold Standard for feed forward networks in LLMs like Llama 3 and PaLM. The primary reason for this choice is the Swish function (as shown in the first image) instead of ReLU or GELU are as following: Mathematical Linearity: Swish (SILU) is defined as $x\\cdot sigmoid(\\beta x)$, $\\beta$ is a learnable parameter. It is a small monotonic function that allowes small negetive values to pass through, making it mathematically linear. While GELU ($x \\cdot \\phi(x)$)\n","permalink":"http://localhost:1313/posts/feedforward-networks/","summary":"Exploring the hidden layers of trillion-parameter switchboards: Feedforward Neural Networks and Activation Functions.","title":"Anatomy of Trillion-Parameter Switchboards: Understanding Feedforward Blocks"},{"content":"\nWe are about to touch the holy grail of modern AI. The 2017 declaration was simple: Attention Is All You Need. But as we‚Äôve raced from GPT-1 to reasoning models, the definition of \u0026lsquo;Attention\u0026rsquo; has quietly transformed. Is the algorithm ruling the world today actually the same one we started with? Let‚Äôs look under the hood.\nüî• The Core Concept At its heart, the attention mechanism is a form of weighted aggregation. It is often described using a database retrieval analogy. Imagine you have a database of information (context).\nQuery ($Q$): What are you currently looking for? (The token being generated). Key ($K$): The indexing tag associated with each piece of information. Value ($V$): The actual information (The context token). The \u0026ldquo;Attention\u0026rdquo; is the process of matching your Query against all the Keys to determine how much \u0026ldquo;weight\u0026rdquo; to put on each Value. Attention scores are calculated in each transformer layer at every step of the sequence. This is a single unifying factor in all transformer models, from BERT to Gemini.\nüî• Standard Multi-Head Attention (MHA) The original mechanism introduced in the paper \u0026ldquo;Attention Is All You Need\u0026rdquo;.\nHere, instead of performing the Key-Query-Value (KQV) operation on the entire context only once, the attention is split into $N$ parallel heads. Each head learns a different aspect of the language (e.g., word meaning, syntax, semantic relationships, etc.). The equation is a simple culmination of dot products aided by a regularizer like Softmax.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nWhere:\n$QK^T$ computes the similarity scores between the Query and Key. $\\sqrt{d_k}$ is a scaling factor to prevent the dot product from growing too large, mitigating the vanishing/exploding gradient problem. The weighted sum is applied to the values of $V$. The Memory Bandwidth Bottleneck: \u0026gt; During inference, the model must store Key and Value matrices for all previous tokens. This is called the KV Cache. In MHA, every single head has its own unique set of Keys and Values. For large models with massive context windows, loading this massive KV cache from memory (HBM) to the computation chip (SRAM) becomes the primary bottleneck, slowing down generation significantly. Earlier models could often only generate ~8 tokens per second at inference due to this constraint.\nüî• Multi-Query Attention (MQA) This approach addresses the memory bottleneck by sharing the same set of Keys and Values across all heads while keeping separate Query heads.\nMHA: $H$ Query heads, $H$ Key heads, $H$ Value heads MQA: $H$ Query heads, 1 Key head, 1 Value head This drastically reduces the memory requirements of the KV cache by a factor of $H$ (often 8x or 16x). The GPU needs to load far less data, significantly speeding up inference. However, it does not reduce the computation (FLOPs)‚Äîso why is it faster?\nThis can be explained with the Kitchen Analogy: Imagine a kitchen (GPU) with 8 chefs (Heads).\nComputation (Math): The Chef chopping vegetables. Memory Access (Bandwidth): The Assistant running to the fridge to get ingredients. In MHA: Each chef demands unique vegetables from the fridge. The assistant runs back and forth 8 times, fetching 8 different crates. Most of the time is spent waiting for the assistant to return. This results in idle cores waiting for data to load from HBM.\nIn MQA: All chefs agree to use the same vegetables. The assistant runs back and forth only once, fetching one giant crate of potatoes and dumping it. This results in minimized memory transfer and no idle cores.\nThe Technical Reality: During text generation, the model is memory-bound, not compute-bound. The time it takes to move the KV Cache from HBM (High Bandwidth Memory) to the chip\u0026rsquo;s SRAM is the bottleneck. MQA reduces the data volume of that move by a factor of $H$.\nThe Capacity Gap: MQA is a trade-off between nuanced response and memory capacity. The model loses the ability to store different types of information, as a single $V$ matrix must now store all nuances of information flow in the language.\nThe Surgery Trick (Uptraining): Google Research figured out a way to take an existing model with MHA and convert it to MQA by averaging the weight matrices of $H$ heads. This is called Uptraining. You then train this \u0026ldquo;Frankenstein\u0026rdquo; model for ~5% of the original training steps for adjustments. This yields an MQA model that is nearly as good as the original MHA but vastly faster.\nüî• Grouped Query Attention (GQA) A \u0026ldquo;best of both worlds\u0026rdquo; solution used in modern models like Llama 2 and Llama 3.\nHow it works: GQA acts as an interpolation between MHA and MQA. Instead of having just 1 Key head (MQA) or $H$ Key heads (MHA), it divides the query heads into $G$ groups. Each group shares a single Key and Value head.\nEquation modification: If you have 8 query heads, you might create 4 groups (2 queries per group). This reduces the KV cache by 2x instead of 8x, preserving more quality than MQA while still speeding up inference.\nMHA: 8 people (heads) are working on a project. Each person has their own unique filing cabinet (KV cache). It takes forever to move 8 cabinets around. MQA: 8 people share one single filing cabinet. It\u0026rsquo;s very fast to move, but they fight over the organization and lose detail. GQA: The 8 people split into 4 teams of 2. Each team shares a cabinet. It\u0026rsquo;s a balance between speed and organizational detail. üî• Multi-Head Latent Attention (MLA) This is the most advanced mechanism for attention, used in models like DeepSeek-V3. It is a sophisticated design for massive efficiency that takes a radical approach: mathematically compressing the memory into a low-rank \u0026ldquo;latent\u0026rdquo; vector.\nKV Cache Explosion: As mentioned before, for a massive model like DeepSeek-V3 (671B parameters), storing the full Keys and Values for every token in a long conversation requires terabytes of high-speed memory (HBM), and chips spend more time waiting than churning.\nMHA (Standard) stores everything. MQA deletes redundancy but hurts quality. MLA keeps the quality of MHA but compresses the data 4x-6x smaller than even GQA.\nThe Zip File Approach: Low-Rank Joint Compression Let\u0026rsquo;s compare the math of MHA and MLA. In MHA, for every single token, we generate a unique Key ($k$) and Value ($v$) matrix for each attention head.\nThe Memory Problem: The $k$ is stored in the KV Cache. For 128 heads, for example, it calculates a massive vector of $128 \\times 128 = 16,384$ floats per token.\nFor MHA, the attention score looks like: $$\\text{Score} = q^T \\cdot k$$\nBut for MLA, instead of storing the full $k$, it projects the input down into a tiny latent vector $c_{KV}$, which is much smaller (e.g., 512 floats). $$c_{KV} = W_{DKV} \\cdot x$$\nNow here is the catch: Naive compression and decompression would mean constructing a small latent matrix for every token and deconstructing it every single time the information is required, which defeats the purpose of compression.\n$$k_{\\text{reconstructed}} = W_{UK} \\cdot c_{KV}$$ $$\\text{Score} = q^T \\cdot k_{\\text{reconstructed}}$$\nThe Matrix Absorption Trick (Optimization) Instead of reconstructing $k$ for every token, we can absorb the Up-Projection matrix ($W_{UK}$) into the Down-Projection matrix ($W_{DKV}$) and absorb the query $q$ into a new $Q$.\nFrom the original equation: $$\\text{Score} = q^T \\cdot (\\underbrace{W_{UK} \\cdot c_{KV}}_{\\text{This is } k})$$\nWe associate differently:\n$$\\text{Score} = (\\underbrace{q^T \\cdot W_{UK}}_{\\text{Absorbed Query}}) \\cdot c_{KV}$$ We change the order in which the matrix multiplication is performed.\nHow is this allowed? In linear algebra, matrix multiplication is associative:\n$$(A \\cdot B) \\cdot C = A \\cdot (B \\cdot C)$$ We can move the parentheses! Instead of grouping $(W_{UK} \\cdot c_{KV})$ to make the Key, we group $(q^T \\cdot W_{UK})$ to make a new Query.\nNote: The positional embedding information is left untouched. That rotational information from algorithms like RoPE is preserved outside the compression; it requires minimal space anyways.\n","permalink":"http://localhost:1313/posts/attention-mechanisms/","summary":"We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek\u0026rsquo;s MLA, how has the definition of \u0026lsquo;Attention\u0026rsquo; transformed?","title":"Attention Is All You Need, but exactly which one?: MHA, GQA and MLA"},{"content":"\nPositional Embeddings are the \u0026ldquo;voice\u0026rdquo; that tell Transformers where words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.\n\u0026ldquo;The dog chased the cat\u0026rdquo; and \u0026ldquo;cat chased the dog\u0026rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.\nüî• Sinusoidal Embeddings: The Odometer of Language Introduced in the seminal \u0026ldquo;Attention Is All You Need\u0026rdquo; paper, this was the industry standard for many years. This approach adds a vector to each word embedding based on its position. The vector is computed out of wavelike patterns that are periodic and smooth. Mathematically, for a position $pos$ and embedding dimension $i$:\n$$ PE_{(pos, i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $$\nThe model uses sine and cosine waves of different frequencies to capture both short and long-range dependencies. The frequency of the waves decreases as the embedding dimension increases, which helps capture long-range dependencies in the input sequence.\nThe creators wanted the model to calculate distance easily, which is why a linear function was used. The model simply rotates the current position vector using a rotation matrix to derive the next position.\n$$ \\begin{pmatrix} \\cos(k) \u0026 \\sin(k) \\\\ -\\sin(k) \u0026 \\cos(k) \\end{pmatrix} \\cdot PE(pos) \\approx PE(pos + k) $$ Low dimensions act like a fast-ticking clock (high frequency, changing rapidly) and high dimensions act like a slow clock (low frequency, changing slowly).\nThe number 10,000 is an arbitrarily chosen high number to reduce Aliasing. By choosing such a high number, the creators ensured that the wavelength of the slowest dimension is incredibly long. Consequently, the pattern practically never repeats, and each position remains unique.\nThis combination works like a mechanical odometer dial. Each subsequent dimension acts as a slower-moving gear. The model learns to read these \u0026ldquo;dials\u0026rdquo; to understand exactly where a word sits in the sequence.\nThis mitigated the challenge of exploding gradients that previous array-based positional embeddings suffered from (e.g., adding integers 1, 2, 3\u0026hellip;). This model keeps values between -1 and +1, adding stability while ensuring every position gets a distinct code.\nThe Problem: This approach pollutes the word embedding matrix (semantic information). While high-dimensional vectors are almost always orthogonal‚Äîmeaning the noise doesn\u0026rsquo;t fully destroy the word\u0026rsquo;s identity‚Äîit does not strictly guarantee purity.\nThis led to a more mature approach.\nüî• ALiBi: The Fading Streetlight Attention with Linear Biases (ALiBi) is a method where, instead of adding a vector to the embeddings at the start, the model injects positional information linearly, directly into the attention scores. If Sinusoidal Encoding asks, \u0026ldquo;Where am I on the map?\u0026rdquo;, ALiBi asks, \u0026ldquo;How much harder should I squint to see you?\u0026rdquo;\nThe biggest problem ALiBi solved is the \u0026ldquo;Invisible Wall\u0026rdquo;. If you train a Transformer on sequences of length 2048 and then feed it a book with 3000 words, it will crash or output garbage at word 2049. This is a failure of Extrapolation.\nALiBi throws away the idea of adding positional vectors entirely. Instead, it devises a \u0026ldquo;penalty\u0026rdquo; system for attention scores:\n$$ \\text{Score} = (q \\cdot k^\\top) - (m \\cdot \\text{distance}) $$\nDistance: If words A and B are neighbors, the distance is 0, so the penalty is 0. Although the distance is linear, it does not cause exploding gradients because the raw attention scores are normalized by a softmax function. This distance prioritizes nearby words (local context) over distant ones.\nSlope ($m$): If we always penalize distance equally, the model becomes myopic. ALiBi mitigates this by assigning a different slope value to each attention head:\nHead 1 (The Historian): Very low slope. The penalty grows slowly, allowing it to see far back. Head 2 (The Reader): Medium slope. Head 3 (The Myopic): Very high slope (e.g., 0.5); it effectively cannot see beyond 2-3 words. This mimics holding a lantern. Words close to you are bright, while words further away naturally fade into darkness.\nExtrapolation: If you walk 5 miles down the road (a longer sequence), your lantern works exactly the same way. The physics of light decay are constant, allowing the model to handle sequences longer than it was trained on.\nHowever, ALiBi forces a \u0026ldquo;sliding window\u0026rdquo; view where distant words eventually become invisible. This brings us to the modern standard.\nüî• RoPE: The Rotary Revolution Rotary Position Embeddings (RoPE) is the current industry standard (used in Llama 3, Mistral, PaLM) because it mathematically unifies \u0026ldquo;absolute position\u0026rdquo; with \u0026ldquo;relative position.\u0026rdquo; This method rotates the vectors in the Query and Key matrices based on their absolute position.\nIf a vector has 512 dimensions, RoPE breaks it into 256 pairs. Each pair is rotated by a different frequency:\n$$ \\theta_i = 10000^{-2i/d} $$\nThe First pair ($i = 0$) is rotated by the highest frequency. The Last pair ($i = 255$) is rotated by the lowest frequency. This spread ensures the model has \u0026ldquo;high precision\u0026rdquo; (fast rotation) to distinguish immediate neighbors, and \u0026ldquo;low precision\u0026rdquo; (slow rotation) to track global position without the pattern repeating.\nDoesn\u0026rsquo;t rotation change meaning? Yes and No. The vector for \u0026ldquo;Apple\u0026rdquo; at position 1 and \u0026ldquo;Apple\u0026rdquo; at position 256 will look completely different in terms of coordinates. However, rotation does not change the semantic strength (magnitude/norm) of the vector.\nThe Relativity Trick: Transformers compare words using dot products (angles). RoPE relies on the fact that the attention mechanism ignores the absolute rotation and only detects the difference in rotation (relative position). Thus, the \u0026ldquo;relative meaning\u0026rdquo; is encoded purely into the angle between words, leaving the semantic \u0026ldquo;magnitude\u0026rdquo; untouched.\nüî• LongRoPE: The Bifocal Lens While RoPE is powerful, it struggles when stretching context windows from 4k to 2 million tokens. The \u0026ldquo;fast\u0026rdquo; dimensions spin so rapidly over long distances that they become random noise. LongRoPE solves this using an evolutionary search algorithm to find a unique scaling factor ($\\lambda$) for each dimension.\nThe Equations Instead of rotating by the standard $\\theta_i$, LongRoPE rotates by a rescaled frequency:\n$$ \\theta\u0026rsquo;_i = \\lambda_i \\theta_i $$\nTo efficiently find these $\\lambda$ values without searching an infinite space, the algorithm enforces a monotonicity constraint based on NTK theory:\n$$ \\lambda_i \\le \\lambda_{i+1} $$\nThis ensures that low-frequency dimensions (global context) are stretched more than high-frequency dimensions (local context), creating a \u0026ldquo;bifocal\u0026rdquo; effect:\nHigh Frequencies (Local): Kept sharp ($\\lambda \\approx 1$) to maintain grammar. Low Frequencies (Global): Stretched ($\\lambda \u0026gt; 1$) to track massive distances without repeating. Not worth perusing more since the next one is much better at long context positional encoding. üî• HoPE: The Hyperbolic Slide Hyperbolic Rotary Positional Encoding (HoPE) moves from the geometry of a circle to the geometry of a hyperbola (inspired by Lorentz transformations). It was designed to fix a subtle flaw in RoPE: the \u0026ldquo;wobbly\u0026rdquo; or oscillatory nature of attention scores.\nThe Core Problem: RoPE\u0026rsquo;s \u0026ldquo;Wobble\u0026rdquo; In standard RoPE, we rotate vectors around a circle.\nThe Issue: Circles repeat. A dial at $361^\\circ$ looks identical to $1^\\circ$. Even with different frequencies, the dot product fluctuates up and down as distance increases. The Consequence: The model gets confused at long distances, potentially mistaking a word 1,000 tokens away for a neighbor just because the rotation cycles aligned, creating \u0026ldquo;noise.\u0026rdquo; The \u0026ldquo;Hyperbolic\u0026rdquo; Shift HoPE replaces trigonometric functions ($\\sin, \\cos$) with hyperbolic functions ($\\sinh, \\cosh$).\nCircular Rotation: Keeps distance from the center constant. Hyperbolic Rotation: Moves points along a hyperbola. Crucially, hyperbolic functions do not repeat; they grow or decay exponentially. By using hyperbolic rotation, HoPE guarantees that as words get further apart, the attention score decays monotonically (smoothly drops).\nThe Equations HoPE uses a hyperbolic matrix $B(\\theta, m)$ defined using the Lorentz Boost structure:\n$$ B(\\theta, m) = \\begin{pmatrix} \\cosh(m\\theta) \u0026 \\sinh(m\\theta) \\\\ \\sinh(m\\theta) \u0026 \\cosh(m\\theta) \\end{pmatrix} $$ Since hyperbolic functions grow to infinity, HoPE introduces a decay penalty ($e^{-m\\theta'}$). The final attention score simplifies beautifully to a function of relative distance: $$ \\text{Score} \\propto e^{-|m-n|(\\theta\u0026rsquo; - \\theta)} $$\nThis ensures the attention score exponentially decays as distance increases, eliminating the wobbles of standard RoPE. This the latest model that is taking up the space in the industry for extremely long context models.\n","permalink":"http://localhost:1313/posts/positional-embeddings/","summary":"From Sinusoidal to RoPE and HoPE: How Transformers learn to process word order and sequence length.","title":"The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE"},{"content":"Imagine trying to teach a computer the difference between \u0026ldquo;Apple\u0026rdquo; the fruit and \u0026ldquo;Apple\u0026rdquo; the company. To us, the distinction is intuitive. To a machine, it‚Äôs just a string of characters. How do we turn these strings into meaningful math? While early attempts like Word2Vec gave us a great start, they missed the forest for the trees‚Äîor in some cases, the twigs for the branches. Enter GloVe and FastText: two algorithms that revolutionized how machines understand the nuances of human language.\nPrevious static embedding models like Word2Vec successfully captured the local semantics of words, but they failed in addressing contexts between words that might not always appear in the same context window. Furthermore, they treated words as indivisible atomic units in continuous vector spaces, missing out on the internal structure of language.\nThe two simple but effective ideas behind GloVe and FastText are subword units and global co-occurrence. Let\u0026rsquo;s dive into them.\nGloVe: A Global Mapping Strategy Global Vectors for Word Representation (GloVe) was created at Stanford. The core idea was to train a model where word vectors are defined by how often they appear around other words across the entire corpus, rather than just locally.\nTraining Steps: Co-occurrence Matrix X: GloVe scans the entire corpus and creates a giant matrix (spreadsheet) recording the counts of how often words appear near each other. Factorization: It compresses this giant matrix into smaller, dense vectors. Objective: The function tries to equate the dot product of two word vectors with the logarithm of their probability of appearing together. The error between these values is then backpropagated to update the vectors. $$w_i \\cdot w_j + b_i + b_j = \\log(X_{ij})$$\n$w$: The word vector (what we want). $X_{ij}$: The count from our matrix. The Loss Function: We can\u0026rsquo;t satisfy the equation above perfectly for every word pair. So, we use a weighting function to minimize the squared error:\n$$J = \\sum_{i,j=1}^V f(X_{ij}) ({w_i}^T w_j + b_i + b_j - \\log(X_{ij}))^2$$\nIf we do not use the weighting function, the loss would be infinite for rare words (since $\\log(0) = \\infty$) and frequent words (like \u0026rsquo;the\u0026rsquo;) would overpower the model.\n$$ f(x) = \\begin{cases} (x/x_{max})^\\alpha \u0026amp; \\text{if } x \u0026lt; x_{max} \\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$\n(Typically $x_{max}=100$ and $\\alpha=0.75$)\nFastText: The Lego Bricks Strategy FastText was created by Facebook AI Research (FAIR). The idea was to treat words not as solid rocks, but as structures made of \u0026ldquo;Lego bricks\u0026rdquo; (subword units).\nWord2Vec struggled to create embeddings for words it hadn\u0026rsquo;t seen before (Out-Of-Vocabulary or OOV words). FastText solves this by breaking words down.\nGloVe treats a word like \u0026ldquo;Apple\u0026rdquo; as a single, atomic unit. FastText breaks \u0026ldquo;Apple\u0026rdquo; into character n-grams: \u0026lt;ap, app, ppl, ple, le\u0026gt; For a complex or new word like \u0026ldquo;Applesauce\u0026rdquo;, the model might not know the whole word, but it recognizes the Lego brick for \u0026ldquo;Apple\u0026rdquo; and the remaining bits from other subwords.\nFastText takes its inspiration from the SkipGram architecture. The vector for a word is simply the sum of the vectors of its n-gram subwords:\n$$v_w = \\sum_{g \\in G_w} z_g$$\nObjective: The objective is to predict context words given a target word.\n$$P(w_c | w_t) = \\frac{e^{s(v_{w_t} \\cdot v_{w_c})}}{\\sum_{w \\in V} e^{s(v_{w_t} \\cdot v_w)}}$$\nAnd the scoring function is defined as:\n$$s(w_t, w_c) = u_{w_t}^T (\\sum_{g \\in G_w} z_g)$$\n$u_{w_c}$: The vector for the context word (output vector). $z_g$: The vector for the n-gram $g$ (input vector). Modifications to Softmax: Calculating the full softmax for every word in the vocabulary is computationally expensive. FastText uses two methods to approximate this efficiently.\nOption A: Negative Sampling\nWe pick a few random \u0026ldquo;wrong\u0026rdquo; words every time to serve as \u0026ldquo;negative samples\u0026rdquo; and calculate the loss only for the correct word and these noise words.\n$$J = - \\log \\sigma(s(w_t, w_c)) - \\sum\\limits_{i=1}^{N} \\mathbb{E}_{n_i \\sim P_n(w)} [\\log \\sigma(-s(w_t, n_i))]$$\nThe sigmoid function $\\sigma$ squashes the score between 0 and 1. The first term pushes the probability of the correct word towards 1. The second term pushes the probability of the noise words towards 0. Option B: Hierarchical Softmax (Faster for infrequent words)\nInstead of a flat list of words, imagine the vocabulary arranged as a binary tree (Huffman Tree).\nRoot: Top of the tree. Leaves: The actual words. To calculate the probability of a word, we trace a path from the root to that leaf. At every branching node, we calculate the probability (sigmoid) of going left or right. The probability of a word $w$ is the product of the probabilities of the turns taken to reach it. Code Example from gensim.models import FastText from gensim.test.utils import common_texts # Initialize the FastText model with specific hyperparameters model = FastText( vector_size=100, # Dimensionality of the word vectors window=5, # Maximum distance between the current and predicted word within a sentence min_count=1, # Ignores all words with total frequency lower than this sg=1, # Training algorithm: 1 for skip-gram; 0 for CBOW hs=0, # If 0, and negative is non-zero, negative sampling will be used negative=5, # Number of \u0026#34;noise words\u0026#34; to be drawn for negative sampling workers=4, # Number of worker threads to train the model epochs=10, # Number of iterations over the corpus min_n=3, # Minimum length of character n-grams max_n=6 # Maximum length of character n-grams ) # Build the vocabulary from the provided text corpus model.build_vocab(common_texts) # Train the model on the corpus model.train( common_texts, total_examples=model.corpus_count, epochs=model.epochs ) # Calculate and print the cosine similarity between two words in the vocabulary print(f\u0026#34;Similarity between \u0026#39;computer\u0026#39; and \u0026#39;human\u0026#39;: {model.wv.similarity(\u0026#39;computer\u0026#39;, \u0026#39;human\u0026#39;)}\u0026#34;) # Demonstrate FastText\u0026#39;s ability to handle Out-Of-Vocabulary (OOV) words # Even if \u0026#39;computation\u0026#39; wasn\u0026#39;t in the training data, FastText constructs a vector using character n-grams oov_vector = model.wv[\u0026#39;computation\u0026#39;] # Check if a specific word exists in the model\u0026#39;s fixed vocabulary index word = \u0026#34;computer\u0026#34; is_in_vocab = word in model.wv.key_to_index print(f\u0026#34;\\nIs the word \u0026#39;{word}\u0026#39; in the model\u0026#39;s vocabulary? {is_in_vocab}\u0026#34;) ","permalink":"http://localhost:1313/posts/glove-and-fasttext/","summary":"How subword units and global co-occurrence matrices allow GloVe and FastText to capture nuances that Word2Vec missed.","title":"The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText"},{"content":" Before we had Large Language Models writing poetry, we had to teach computers that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; are related not just by spelling, but by meaning. This is the story of that breakthrough. It‚Äôs the moment we stopped counting words and started mapping their souls‚Äîturning raw text into a mathematical landscape where math can solve analogies. Welcome to the world of Word2Vec. üîÆ\nLanguage models require vector representations of words to capture semantic relationships. Before the 2010s, models used word count-based vector representations that captured only the frequency of words (e.g., One-Hot Encoding). The Problems: üöß\nThe Curse of Dimensionality Lack of meaning (Synonyms were treated as mathematically unrelated). In 2011, Mikolov et al. at Google introduced Word2Vec, which used a shallow neural network to learn vector representations of words. The best part? It could create much denser relationships between words than prior models, and it was unsupervised.\nThe training (prediction) is essentially \u0026ldquo;fake\u0026rdquo;‚Äîa pretext task. But the weight matrices we get on the side are a gold mine of fine-grained semantic relationships. ‚õèÔ∏è\nThere are two popular variants of Word2Vec:\nSkipGram Continuous Bag Of Words (CBOW) 1. Continuous Bag of Words (CBOW) üß© CBOW is a simple \u0026lsquo;fill in the blanks\u0026rsquo; machine. It takes the surrounding words as inputs and tries to predict the center word.\n\u0026ldquo;Continuous\u0026rdquo;: It operates in continuous vector space (unlike the discrete space of n-gram models).\n\u0026ldquo;Bag of words\u0026rdquo;: The order of the context does not matter. \u0026ldquo;The cat sat on the mat\u0026rdquo; and \u0026ldquo;The mat sat on the cat\u0026rdquo; are the same for CBOW. They produce exactly the same predictions.\nObjective: Maximize the probability of the target word $w_t$ given its context $w_{t-C}, \\dots, w_{t+C}$.\n2. Training Defining Dimensions:\n$V$: Vocabulary size (number of unique words in the corpus, e.g., 10,000) $N$: Dimension of the embedding space (e.g., 100) $C$: Number of context words (e.g., 2 words on left of target and 2 words on right) Step 1: Input Lookup \u0026amp; One-Hot Encoding\nMathematically, we represent each context word as a one-hot vector $x^{(c)}$. We have an Input Matrix $W_{in}$ of size $V \\times N$. For each context word (e.g., \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;), we select its corresponding row from $W_{in}$.\n$$ v_c = W_{in}^T x^{(c)} $$\nWhere $v_c$ is the $N$-dimensional vector for context word $c$.\nStep 2: Projection (The \u0026ldquo;Mixing\u0026rdquo;)\nWe combine the context vectors. In standard CBOW, we simply average them. This creates the hidden layer vector $h$.\n$$ h = \\frac{1}{2C} \\sum_{c=1}^{2C} v_c $$\n$h$ is a single vector of size $N \\times 1$. Note: This operation is linear and contains no activation function (like ReLU or Sigmoid).\nStep 3: Output Scoring (The \u0026ldquo;Dot Product\u0026rdquo;)\nWe have a second matrix, the Output Matrix $W_{out}$ of size $N \\times V$. We compute the raw score (logit) $u_j$ for every word $j$ in the vocabulary by taking the dot product of the hidden vector $h$ and the output vector $v\u0026rsquo;_j$.\n$$ u_j = {v\u0026rsquo;_j}^T h $$\nOr in matrix form:\n$$ u = W_{out}^T h $$\nResult $u$ is a vector of size $V \\times 1$.\nStep 4: Probability Conversion (Softmax)\nWe convert the raw scores $u$ into probabilities using the Softmax function. This tells us the probability that word $w_j$ is the center word.\n$$ y_j = P(w_j | \\text{context}) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 5: Loss Calculation (Cross-Entropy)\nWe compare the predicted distribution $y$ with the actual ground truth $t$ (a one-hot vector where the index of the true word is 1). The loss function $E$ is:\n$$ E = - \\sum_{j=1}^V t_j \\log(y_j) $$\nSince $t_j$ is 0 for all words except the target word (let\u0026rsquo;s call the target index $j^*$), this simplifies to:\n$$ E = - \\log(y_{j^*}) $$\n3. Backpropagation Details We need to update the weights to minimize $E$. We use the Chain Rule.\nA. Gradient w.r.t. Output Matrix ($W_{out}$)\nWe want to know how the error changes with respect to the raw score $u_j$.\n$$ \\frac{\\partial E}{\\partial u_j} = y_j - t_j $$\nLet\u0026rsquo;s call this error term $e_j$.\nIf $j$ is the target word: $e_j = y_j - 1$ (Result is negative; we boost the score). If $j$ is not target: $e_j = y_j - 0$ (Result is positive; we suppress the score). The update rule for the output vector $v\u0026rsquo;_j$ becomes:\n$$ v'_{j}(\\text{new}) = v'_{j}(\\text{old}) - \\eta \\cdot e_j \\cdot h $$ ($\\eta$ is the learning rate)\nB. Gradient w.r.t. Hidden Layer ($h$)\nWe backpropagate the error from the output layer to the hidden layer.\n$$ EH = \\sum_{j=1}^V e_j \\cdot v\u0026rsquo;_j $$\n$EH$ is an $N$-dimensional vector representing the aggregate error passed back to the projection layer.\nC. Gradient w.r.t. Input Matrix ($W_{in}$)\nSince $h$ was just an average of the input vectors, the error $EH$ is distributed to each context word\u0026rsquo;s input vector. For every word $w_c$ in the context:\n$$ v_{w_c}(\\text{new}) = v_{w_c}(\\text{old}) - \\eta \\cdot \\frac{1}{2C} \\cdot EH $$\nLet\u0026rsquo;s build a single pass through the network for a given context.\nimport torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # Define a simple Continuous Bag of Words (CBOW) style model class CBOWModel(nn.Module): def __init__(self, vocab_size, embedding_dim): super(CBOWModel, self).__init__() # The embedding layer stores the word vectors we want to learn self.embeddings = nn.Embedding(vocab_size, embedding_dim) # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word self.linear = nn.Linear(embedding_dim, vocab_size) def forward(self, inputs): # inputs: tensor of word indices for the context embeds = self.embeddings(inputs) # Aggregate context by calculating the mean of the word embeddings h = torch.mean(embeds, dim=1) # Produce logits (raw scores) for each word in the vocabulary logits = self.linear(h) return logits # Setup vocabulary and mappings word_to_ix = {\u0026#34;the\u0026#34;: 0, \u0026#34;quick\u0026#34;: 1, \u0026#34;brown\u0026#34;: 2, \u0026#34;fox\u0026#34;: 3, \u0026#34;jumps\u0026#34;: 4, \u0026#34;over\u0026#34;: 5, \u0026#34;lazy\u0026#34;: 6, \u0026#34;dog\u0026#34;: 7} ix_to_word = {v: k for k, v in word_to_ix.items()} # Configuration constants EMBEDDING_DIM = 5 VOCAB_SIZE = len(word_to_ix) LEARNING_RATE = 0.01 # Initialize the model, loss function, and optimizer model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM) loss_function = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE) # Prepare dummy training data: context words and the target word # Context: [\u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;over\u0026#34;] -\u0026gt; Target: \u0026#34;fox\u0026#34; context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long) target_idxs = torch.tensor([3], dtype=torch.long) # Perform a single optimization step model.zero_grad() # Forward pass: we reshape context to (1, -1) to simulate a batch of size 1 logits = model(context_idxs.view(1, -1)) # Calculate loss against the target word index loss = loss_function(logits, target_idxs) # Backpropagate and update weights loss.backward() optimizer.step() # Output results for the blog post print(\u0026#34;=== Model Training Snapshot ===\u0026#34;) print(f\u0026#34;Calculated Loss: {loss.item():.6f}\u0026#34;) print(f\u0026#34;\\n=== Learned Vector for \u0026#39;jumps\u0026#39; ===\u0026#34;) word_vec = model.embeddings(torch.tensor([word_to_ix[\u0026#39;jumps\u0026#39;]])) print(word_vec.detach().numpy()) print(\u0026#34;\\n=== Embedding Matrix (Weights) ===\u0026#34;) print(model.embeddings.weight.detach().numpy()) print(\u0026#34;\\n=== Linear Layer Weights ===\u0026#34;) print(model.linear.weight.detach().numpy()) Output shows single pass loss and learned vector for \u0026lsquo;jumps\u0026rsquo; word. This is a single training sample result but after multiple such passes both Embedding and linear weights should be very similar.\n=== Model Training Snapshot === Calculated Loss: 1.989656 === Learned Vector for \u0026#39;jumps\u0026#39; === [[-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775]] === Embedding Matrix (Weights) === [[ 1.66039 -0.11371879 0.6246518 0.35860053 -1.2504417 ] [-2.1847186 -0.77199775 -0.17050214 -0.38411248 -0.03913084] [ 0.11852697 0.90073633 0.8847807 0.7404524 0.900149 ] [ 0.07440972 -0.40259898 2.6246994 -0.08851447 0.02660969] [-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775] [-0.9245572 -0.5545908 0.9083091 -1.0755049 0.84047747] [-1.0237687 0.59466314 0.05621134 -0.6202532 1.3664424 ] [ 0.60998917 -1.0549186 1.6103884 0.8724912 -1.2486908 ]] === Linear Layer Weights === [[ 0.02165058 -0.28883642 0.14545658 -0.3442509 0.32704315] [-0.18731792 0.28583744 0.22635977 0.13245736 0.29019794] [ 0.3158916 -0.15826383 -0.03203773 0.16377363 -0.41457543] [-0.05080034 0.4180087 0.11228557 0.30218413 0.3025514 ] [-0.38419306 0.24475925 -0.39210224 -0.38660625 -0.2673145 ] [-0.32321444 0.12200444 -0.03569533 0.2891424 -0.07345333] [-0.33704326 0.2521956 0.31587374 0.22590035 0.29866052] [ 0.4117266 -0.44231793 0.24064957 -0.29684234 0.333821 ]] Interesting Facts About CBOW üß† The \u0026ldquo;Averaging Problem\u0026rdquo;: This is a common issue in CBOW models. The averaging and training is order-agnostic. All variants of word order in the context window will generate the same output. \u0026ldquo;Dog bit the man\u0026rdquo; and \u0026ldquo;Man bit the dog\u0026rdquo; are the same in CBOW\u0026rsquo;s eyes.\nSmoothing Effect: CBOW models are much faster to train than Skip-Gram but are generally smoother.\nRare Words: CBOW models struggle slightly with rare words because the presence of surrounding common words (like articles and prepositions) can skew the embeddings and dissolve their uniqueness.\nComputational Cost: Softmax requires summing over the entire vocabulary size, which can be computationally expensive if $V$ is very large. Solutions include Hierarchical Softmax and Negative Sampling (approximating the denominator by only checking the target word vs. 5 random noise words).\nVectors as Double Agents: Every word has a different representation in the embedding matrix and the output matrix. Most of the time $W_{out}$ is discarded, but some consider averaging both $W_{in}$ and $W_{out}$ for slightly better performance.\nLinear Relationships: The famous analogy King - Man + Woman = Queen emerges from the model because of the linear relationships between the words.\nInitialization: This is a no-brainer but worth mentioning. Initialization to zero vectors would mean no learning. We must initialize randomly.\n2. Skip-Gram (The \u0026ldquo;Burger Flip\u0026rdquo; of CBOW) üîÑ Skip-Gram flips the CBOW logic. While CBOW tries to predict the center word given the context, Skip-Gram tries to predict the scattered context given the center word.\nCore Concept: If a word like \u0026ldquo;monarch\u0026rdquo; can accurately predict words like \u0026ldquo;king\u0026rdquo;, \u0026ldquo;throne\u0026rdquo;, \u0026ldquo;crown\u0026rdquo;, \u0026ldquo;royalty\u0026rdquo;, and \u0026ldquo;power\u0026rdquo;, then it must effectively capture the concept of royalty.\nTraining Let\u0026rsquo;s define dimensions:\n$V$: Vocabulary size $D$: Embedding dimension $C$: Number of context words Step 1: Input Lookup\nUnlike CBOW, our input is just one word vector (the center word $w_t$). We grab the vector $v_c$ from the Input Matrix $W_{in}$.\n$$ h = v_{w_t} $$\nNote: There is no averaging here. The hidden layer $h$ is simply the raw vector of the center word.\nStep 2: Output Scoring (The Broadcast)\nThe model takes this single vector $h$ and compares it against the Output Matrix $W_{out}$ (the entire vocabulary). It computes a score $u_j$ for every word $j$ in the dictionary.\n$$ u = W_{out}^T h $$\nResult $u$: A vector of size $V \\times 1$ containing raw scores.\nStep 3: Probability Conversion (Softmax)\nWe apply Softmax to turn scores into probabilities.\n$$ P(w_j | w_t) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 4: The Multi-Target Loss\nHere is the major difference from CBOW. In CBOW, we had one target (the center). In Skip-Gram, we have $2C$ targets (all the surrounding words). For a sentence \u0026ldquo;The cat sat on mat\u0026rdquo; (Center: \u0026ldquo;sat\u0026rdquo;), the targets are \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;. We want to maximize the probability of all these true context words. The Loss function ($E$) sums the error for each context word:\n$$ E = - \\sum_{w_{ctx} \\in \\text{Window}} \\log P(w_{ctx} | w_t) $$\nStep 5: Backpropagation (Accumulating Gradients)\nSince one input word is responsible for predicting multiple output words, the error signals from all those context words add up.\nAt the Output Layer:\nIf \u0026ldquo;cat\u0026rdquo; was a target, the \u0026ldquo;cat\u0026rdquo; vector in $W_{out}$ gets a signal: \u0026ldquo;Move closer to \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; If \u0026ldquo;apple\u0026rdquo; was NOT a target, the \u0026ldquo;apple\u0026rdquo; vector gets a signal: \u0026ldquo;Move away from \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; At the Hidden Layer ($h$):\nThe center word \u0026ldquo;sat\u0026rdquo; receives feedback from all its neighbors simultaneously. Error signal = (Error from \u0026ldquo;The\u0026rdquo;) + (Error from \u0026ldquo;cat\u0026rdquo;) + (Error from \u0026ldquo;on\u0026rdquo;)\u0026hellip; The vector for \u0026ldquo;sat\u0026rdquo; in $W_{in}$ moves in the average direction of all its neighbors. 3. The \u0026ldquo;Computational Nightmare\u0026rdquo; \u0026amp; Negative Sampling The equations above describe Naive Softmax.\nThe Problem: If $V = 100,000$, computing the denominator $\\sum \\exp(u_k)$ takes 100,000 operations. Doing this for every word in a training set of billions of words is impossibly slow.\nThe Solution: Negative Sampling Instead of updating the entire vocabulary (1 correct word vs 99,999 wrong ones), we approximate the problem.\nPositive Pair: (sat, cat) $\\rightarrow$ Maximize probability (Label 1). Negative Pairs: We pick $K$ (e.g., 5) random words the model didn\u0026rsquo;t see, e.g., (sat, bulldozer), (sat, quantum). $\\rightarrow$ Minimize probability (Label 0). New Equation (Sigmoid instead of Softmax):\n$$ E = - \\log \\sigma({v'_{\\text{pos}}}^T v_{\\text{in}}) - \\sum_{k=1}^K \\log \\sigma(-{v'_{\\text{neg}_{k}}}^T v_{\\text{in}}) $$ Effect: We typically only update 6 vectors per step (1 pos + 5 negs) instead of 100,000.\nLet\u0026rsquo;s see this in code:\nimport torch import torch.nn as nn import torch.optim as optim class SkipGramNegativeSampling(nn.Module): \u0026#34;\u0026#34;\u0026#34; Skip-Gram with Negative Sampling (SGNS) implementation. SGNS approximates the softmax over the entire vocabulary by instead distinguishing between a real context word (positive) and K noise words (negative). \u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embedding_dim): super(SkipGramNegativeSampling, self).__init__() # Input embeddings: used when the word is the center word self.in_embed = nn.Embedding(vocab_size, embedding_dim) # Output embeddings: used when the word is a context or negative sample self.out_embed = nn.Embedding(vocab_size, embedding_dim) # Initialize weights with small values to prevent gradient saturation initrange = 0.5 / embedding_dim self.in_embed.weight.data.uniform_(-initrange, initrange) self.out_embed.weight.data.uniform_(-initrange, initrange) def forward(self, center_words, target_words, negative_words): \u0026#34;\u0026#34;\u0026#34; Computes the negative sampling loss. Args: center_words: (batch_size) target_words: (batch_size) negative_words: (batch_size, K) where K is number of negative samples \u0026#34;\u0026#34;\u0026#34; # Retrieve vectors v_c = self.in_embed(center_words) # (batch_size, embed_dim) u_o = self.out_embed(target_words) # (batch_size, embed_dim) u_n = self.out_embed(negative_words) # (batch_size, K, embed_dim) # 1. Positive Score: log(sigmoid(v_c ¬∑ u_o)) # Compute dot product: (batch, 1, dim) @ (batch, dim, 1) -\u0026gt; (batch, 1) pos_score = torch.bmm(u_o.unsqueeze(1), v_c.unsqueeze(2)).squeeze(2) pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-7) # 2. Negative Score: sum(log(sigmoid(-v_c ¬∑ u_n))) # Compute dot products for all K samples: (batch, K, dim) @ (batch, dim, 1) -\u0026gt; (batch, K) neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2) neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-7), dim=1, keepdim=True) # Total loss is the negative of the objective function loss = -(pos_loss + neg_loss) return torch.mean(loss) # --- Configuration \u0026amp; Mock Data --- VOCAB_SIZE = 100 EMBED_DIM = 10 word_to_ix = {\u0026#39;fox\u0026#39;: 0} # Example vocabulary mapping model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock inputs: 1 center word, 1 target word, 5 negative samples center_id = torch.tensor([0], dtype=torch.long) target_id = torch.tensor([1], dtype=torch.long) negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long) # Training Step model.zero_grad() loss = model(center_id, target_id, negative_ids) loss.backward() optimizer.step() # Output Results print(f\u0026#34;Loss after one step: {loss.item():.6f}\u0026#34;) word_vec = model.in_embed(torch.tensor([word_to_ix[\u0026#39;fox\u0026#39;]])) print(f\u0026#34;Vector for \u0026#39;fox\u0026#39;:\\n{word_vec.detach().numpy()}\u0026#34;) The outputs:\nLoss after one step: 4.158468 Vector for \u0026#39;fox\u0026#39;: [[ 0.04197385 0.02400733 -0.03800093 0.01672485 -0.03872231 -0.0061478 0.0121122 0.04057864 -0.036255 0.03861175]] Interesting Facts About Skip-Gram üí° Makes Rare Words Shine: Skip-Gram is more effective at capturing the meaning of rare words because it focuses on the context of the center word. In CBOW, a rare word\u0026rsquo;s vector is averaged with others, diluting its signal. In Skip-Gram, when the rare word appears as the center, it gets the full, undivided attention of the backpropagation update.\nSlow Training: This method creates more training samples. A window of size 2 creates 4 training pairs per center word, while CBOW creates only 1 training pair per center word.\nSemantic Focus: Skip-Gram puts more emphasis on the semantic relationship between words. It views the context as the primary signal, with the center word as the target. This makes it better at capturing the meaning of words in context. Skip-Gram tends to capture semantic relationships (King/Queen) slightly better, while CBOW captures syntactic ones (walk/walked) slightly better.\n","permalink":"http://localhost:1313/posts/word-embeddings/","summary":"Understanding the mathematics behind Word2Vec, CBOW, and Skip-Gram and how they map language to vector space.","title":"Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n5. Code Examples: We\u0026rsquo;ll use sentencepiece for tokenization and compare results for different models.\nimport sentencepiece as spm import os # Configuration for training the SentencePiece model # SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words. options = { # The source text file used for learning the vocabulary \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # The base name for the generated model (.model) and vocabulary (.vocab) files \u0026#39;model_prefix\u0026#39;: \u0026#39;bpe_model\u0026#39;, # Number of unique tokens in the final vocabulary \u0026#39;vocab_size\u0026#39;: 4000, # \u0026#39;bpe\u0026#39; (Byte Pair Encoding) merges frequent pairs of characters/sequences \u0026#39;model_type\u0026#39;: \u0026#39;bpe\u0026#39;, # Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets \u0026#39;character_coverage\u0026#39;: 0.9995, # When enabled, unknown characters are decomposed into UTF-8 bytes to avoid \u0026#39;unk\u0026#39; tokens \u0026#39;byte_fallback\u0026#39;: True, # Treats digits individually (0-9), preventing large numbers from being treated as single tokens \u0026#39;split_digits\u0026#39;: True, # Prevents adding a whitespace prefix to the first token; useful for fine-grained control \u0026#39;add_dummy_prefix\u0026#39;: False } try: print(\u0026#34;Starting the training process...\u0026#34;) # SentencePieceTrainer.train takes the dictionary of options to build the BPE model spm.SentencePieceTrainer.train(**options) print(\u0026#34;Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created.\u0026#34;) # Initialize the processor and load the newly trained model sp = spm.SentencePieceProcessor() sp.load(\u0026#39;bpe_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Model Metadata:\u0026#34;) # Retrieve the total number of tokens in the vocabulary print(f\u0026#34;Total Vocab Size: {sp.get_piece_size()}\u0026#34;) # Special tokens are used for sequence boundaries and handling unknown characters print(f\u0026#34;BOS (Beginning of Sentence) ID: {sp.bos_id()}\u0026#34;) print(f\u0026#34;EOS (End of Sentence) ID: {sp.eos_id()}\u0026#34;) print(f\u0026#34;UNK (Unknown) ID: {sp.unk_id()}\u0026#34;) print(f\u0026#34;PAD (Padding) ID: {sp.pad_id()}\u0026#34;) # Test the tokenizer on sample strings to see how it breaks down text test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: shows the actual subword units (tokens) print(f\u0026#34;Subword Tokens: {sp.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: shows the numerical mapping for each token print(f\u0026#34;Numerical IDs: {sp.encode_as_ids(text)}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred during training or processing: {e}\u0026#34;) You can see the following output:\nStarting training... Starting the training process... Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created. ------------------------------ Model Metadata: Total Vocab Size: 4000 BOS (Beginning of Sentence) ID: 1 EOS (End of Sentence) ID: 2 UNK (Unknown) ID: 0 PAD (Padding) ID: -1 --- Tokenization Test --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;el\u0026#39;, \u0026#39;lo\u0026#39;, \u0026#39;‚ñÅW\u0026#39;, \u0026#39;orld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 380, 309, 476, 619, 36, 3924, 3964, 3959, 3993, 3978, 3983, 57, 3975, 3976, 60, 3974] --- Tokenization Test --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅmo\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;inst\u0026#39;, \u0026#39;ag\u0026#39;, \u0026#39;ram\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅever\u0026#39;] Numerical IDs: [1894, 304, 3284, 376, 321, 313, 312, 1008, 3313, 1325, 428, 3937, 341, 304, 3284, 1495] The ! is fallen back to UTF bytes. and the word instagrammable is split into multiple subword tokens. Also, original text can be recreated right back by simply concatenating the tokens. And replacing underscore by whitespace. Now we can try Unigram model. and see the same sentences tokenized differently.\nfrom tokenizers import Tokenizer from tokenizers.models import WordPiece from tokenizers.trainers import WordPieceTrainer from tokenizers.pre_tokenizers import Whitespace try: # 1. Initialize the WordPiece Tokenizer # We specify the [UNK] token for handling words not found in the vocabulary. tokenizer = Tokenizer(WordPiece(unk_token=\u0026#34;[UNK]\u0026#34;)) # 2. Configure Pre-tokenization # Before the subword algorithm runs, we need to split the raw text into words. # Whitespace splitting is the standard first step for most English NLP tasks. tokenizer.pre_tokenizer = Whitespace() # 3. Initialize the Trainer # We define our target vocabulary size and the special tokens required for # downstream tasks (like BERT\u0026#39;s [CLS] for classification or [SEP] for separators). trainer = WordPieceTrainer( vocab_size=4000, special_tokens=[\u0026#34;[UNK]\u0026#34;, \u0026#34;[CLS]\u0026#34;, \u0026#34;[SEP]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[MASK]\u0026#34;] ) # 4. Train the Model # The tokenizer scans the training file to build a vocabulary of the most # frequent subword units. tokenizer.train(files=[\u0026#34;train_data.txt\u0026#34;], trainer=trainer) # 5. Persist the Model # Save the configuration and vocabulary to a JSON file for future inference. tokenizer.save(\u0026#34;wordpiece.json\u0026#34;) print(\u0026#34;Training complete. \u0026#39;wordpiece.json\u0026#39; created.\u0026#34;) # 6. Metadata Inspection print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;WordPiece Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {tokenizer.get_vocab_size()}\u0026#34;) # 7. Testing Subword Tokenization # WordPiece shines at handling rare words by breaking them into meaningful chunks. test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (WordPiece) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # Encode converts raw text into a Tokenizer object containing tokens and IDs output = tokenizer.encode(text) # \u0026#39;tokens\u0026#39; shows the subword breakdown (e.g., \u0026#39;un\u0026#39;, \u0026#39;##insta\u0026#39;, etc.) print(f\u0026#34;Subword Tokens: {output.tokens}\u0026#34;) # \u0026#39;ids\u0026#39; are the numerical indices mapped to the vocabulary print(f\u0026#34;Numerical IDs: {output.ids}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred with WordPiece model: {e}\u0026#34;) You can see the following output:\nWordPiece Model Metadata: Total Vocab Size: 2609 --- Tokenization Test (WordPiece) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;H\u0026#39;, \u0026#39;##el\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;##or\u0026#39;, \u0026#39;##ld\u0026#39;, \u0026#39;[UNK]\u0026#39;, \u0026#39;[UNK]\u0026#39;] Numerical IDs: [37, 180, 214, 52, 162, 418, 0, 0] --- Tokenization Test (WordPiece) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;##os\u0026#39;, \u0026#39;##t\u0026#39;, \u0026#39;un\u0026#39;, \u0026#39;##ins\u0026#39;, \u0026#39;##ta\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;##ra\u0026#39;, \u0026#39;##m\u0026#39;, \u0026#39;##ma\u0026#39;, \u0026#39;##ble\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;ever\u0026#39;] Numerical IDs: [691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240] We can see the special character ## in the output. We can also see the breakdowns not having any semantic meaning. They are likelihood based. We can try the Unigram model now.\n# \u0026#39;Unigram\u0026#39; is the default and usually recommended over BPE in SentencePiece. options_unigram = { \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # Path to the raw text file for training \u0026#39;model_prefix\u0026#39;: \u0026#39;unigram_model\u0026#39;, # Prefix for the output .model and .vocab files \u0026#39;vocab_size\u0026#39;: 1200, # Desired size of the final vocabulary \u0026#39;model_type\u0026#39;: \u0026#39;unigram\u0026#39;, # Specifies the Unigram language model algorithm \u0026#39;character_coverage\u0026#39;: 0.9995, # Percentage of characters covered by the model (0.9995 is standard for Latin scripts) \u0026#39;byte_fallback\u0026#39;: True, # Enables mapping unknown characters to UTF-8 bytes to avoid \u0026lt;unk\u0026gt; tokens \u0026#39;split_digits\u0026#39;: True, # Treats each digit as an individual token (useful for numerical data) \u0026#39;add_dummy_prefix\u0026#39;: False # Prevents adding a leading space (SentencePiece default is True) } try: # 1. Train the SentencePiece model using the defined options print(\u0026#34;Starting Unigram training...\u0026#34;) spm.SentencePieceTrainer.train(**options_unigram) print(\u0026#34;Training complete. \u0026#39;unigram_model.model\u0026#39; created.\u0026#34;) # 2. Load the trained model into a processor instance for inference sp_unigram = spm.SentencePieceProcessor() sp_unigram.load(\u0026#39;unigram_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Unigram Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {sp_unigram.get_piece_size()}\u0026#34;) # 3. Define test cases to evaluate how the model handles common and rare words test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] # 4. Iterate through test sentences to visualize subword segmentation for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (Unigram) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: Converts text into subword strings (visual representation) print(f\u0026#34;Subword Tokens: {sp_unigram.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: Converts text into numerical indices for model input print(f\u0026#34;Numerical IDs: {sp_unigram.encode_as_ids(text)}\u0026#34;) except Exception as e: # Handle potential errors during training or loading (e.g., missing input file) print(f\u0026#34;An error occurred with Unigram model: {e}\u0026#34;) The output is as follows:\nUnigram Model Metadata: Total Vocab Size: 1200 --- Tokenization Test (Unigram) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ll\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;ld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311] --- Tokenization Test (Unigram) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;T\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅm\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;sta\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;ra\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ver\u0026#39;] ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"A comprehensive guide to tokenization strategies: BPE, WordPiece, Unigram, and SentencePiece.","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"A brief introduction to Vectors \u0026amp; Verbs and formatting verification.","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"\nFeedforward network (FFN) blocks are the least talked about when in context to LLMs. But that is where more than 70% of the paramters come from. Something as simple and convinient as a couple of linear transforming matrices become a burden when models start scaling in Billions of parameters. So, getting the maximum juice of a lean structure is a challenge. Plus, mostly under represented, because the FFN sub-layer acts as the primary \u0026ldquo;knowledge\u0026rdquo; processing engine, often containing the majority of a model\u0026rsquo;s total parameters.\nStandard Position Wise Feedforward Network This is as simple as it can get, two-layered fully connected network of neurons with a single activation switch, most probably a ReLU variant (for LLMs). $$ FFN(x) = max(0, W_2(W_1(x)) + b_1)W_2 + b_2 $$ Same transformation is applied for each token in parallel, making it efficient for GPU computation. For Example, in BERT model the 512 dimensions of each token are expanded to 2048 to allow for \u0026ldquo;intricate interactions\u0026rdquo; before projecting back to 512 dimensions. This part is important, in every transformer stacking of layers is possible only because the output of every matches it\u0026rsquo;s input dimensions.\nSWiGLU Sigmoid Weighted Gated Linear Unit (SWiGLU) has become the Gold Standard for feed forward networks in LLMs like Llama 3 and PaLM. The primary reason for this choice is the Swish function (as shown in the first image) instead of ReLU or GELU are as following: Mathematical Linearity: Swish (SILU) is defined as $x\\cdot sigmoid(\\beta x)$, $\\beta$ is a learnable parameter. It is a small monotonic function that allowes small negetive values to pass through, making it mathematically linear. While GELU ($x \\cdot \\phi(x)$) is also smooth and nonmonotonic, Swish\u0026rsquo;s curve and equations have emperically improved the model\u0026rsquo;s performance. Computational Efficiency: SILU is simpler to compute. $x/(1+exp(-x))$ compared to the standard GELU, which is an approximation of the tanh function. In the massive scale of modern LLMs these shortcuts save significant training time. Performance: Research in the PaLM and Llama papers found that Swish-based Gated Linear Units consistently outperformed GELU-based versions in terms of perplexity and downstream task accuracy. $$SwiGLU(x, W, V, b, c) = (Swish(xW + b) \\otimes (xV + c))W_2$$\n","permalink":"http://localhost:1313/posts/feedforward-networks/","summary":"Exploring the hidden layers of trillion-parameter switchboards: Feedforward Neural Networks and Activation Functions.","title":"Anatomy of Trillion-Parameter Switchboards: Understanding Feedforward Blocks"},{"content":"\nWe are about to touch the holy grail of modern AI. The 2017 declaration was simple: Attention Is All You Need. But as we‚Äôve raced from GPT-1 to reasoning models, the definition of \u0026lsquo;Attention\u0026rsquo; has quietly transformed. Is the algorithm ruling the world today actually the same one we started with? Let‚Äôs look under the hood.\nüî• The Core Concept At its heart, the attention mechanism is a form of weighted aggregation. It is often described using a database retrieval analogy. Imagine you have a database of information (context).\nQuery ($Q$): What are you currently looking for? (The token being generated). Key ($K$): The indexing tag associated with each piece of information. Value ($V$): The actual information (The context token). The \u0026ldquo;Attention\u0026rdquo; is the process of matching your Query against all the Keys to determine how much \u0026ldquo;weight\u0026rdquo; to put on each Value. Attention scores are calculated in each transformer layer at every step of the sequence. This is a single unifying factor in all transformer models, from BERT to Gemini.\nüî• Standard Multi-Head Attention (MHA) The original mechanism introduced in the paper \u0026ldquo;Attention Is All You Need\u0026rdquo;.\nHere, instead of performing the Key-Query-Value (KQV) operation on the entire context only once, the attention is split into $N$ parallel heads. Each head learns a different aspect of the language (e.g., word meaning, syntax, semantic relationships, etc.). The equation is a simple culmination of dot products aided by a regularizer like Softmax.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nWhere:\n$QK^T$ computes the similarity scores between the Query and Key. $\\sqrt{d_k}$ is a scaling factor to prevent the dot product from growing too large, mitigating the vanishing/exploding gradient problem. The weighted sum is applied to the values of $V$. The Memory Bandwidth Bottleneck: \u0026gt; During inference, the model must store Key and Value matrices for all previous tokens. This is called the KV Cache. In MHA, every single head has its own unique set of Keys and Values. For large models with massive context windows, loading this massive KV cache from memory (HBM) to the computation chip (SRAM) becomes the primary bottleneck, slowing down generation significantly. Earlier models could often only generate ~8 tokens per second at inference due to this constraint.\nüî• Multi-Query Attention (MQA) This approach addresses the memory bottleneck by sharing the same set of Keys and Values across all heads while keeping separate Query heads.\nMHA: $H$ Query heads, $H$ Key heads, $H$ Value heads MQA: $H$ Query heads, 1 Key head, 1 Value head This drastically reduces the memory requirements of the KV cache by a factor of $H$ (often 8x or 16x). The GPU needs to load far less data, significantly speeding up inference. However, it does not reduce the computation (FLOPs)‚Äîso why is it faster?\nThis can be explained with the Kitchen Analogy: Imagine a kitchen (GPU) with 8 chefs (Heads).\nComputation (Math): The Chef chopping vegetables. Memory Access (Bandwidth): The Assistant running to the fridge to get ingredients. In MHA: Each chef demands unique vegetables from the fridge. The assistant runs back and forth 8 times, fetching 8 different crates. Most of the time is spent waiting for the assistant to return. This results in idle cores waiting for data to load from HBM.\nIn MQA: All chefs agree to use the same vegetables. The assistant runs back and forth only once, fetching one giant crate of potatoes and dumping it. This results in minimized memory transfer and no idle cores.\nThe Technical Reality: During text generation, the model is memory-bound, not compute-bound. The time it takes to move the KV Cache from HBM (High Bandwidth Memory) to the chip\u0026rsquo;s SRAM is the bottleneck. MQA reduces the data volume of that move by a factor of $H$.\nThe Capacity Gap: MQA is a trade-off between nuanced response and memory capacity. The model loses the ability to store different types of information, as a single $V$ matrix must now store all nuances of information flow in the language.\nThe Surgery Trick (Uptraining): Google Research figured out a way to take an existing model with MHA and convert it to MQA by averaging the weight matrices of $H$ heads. This is called Uptraining. You then train this \u0026ldquo;Frankenstein\u0026rdquo; model for ~5% of the original training steps for adjustments. This yields an MQA model that is nearly as good as the original MHA but vastly faster.\nüî• Grouped Query Attention (GQA) A \u0026ldquo;best of both worlds\u0026rdquo; solution used in modern models like Llama 2 and Llama 3.\nHow it works: GQA acts as an interpolation between MHA and MQA. Instead of having just 1 Key head (MQA) or $H$ Key heads (MHA), it divides the query heads into $G$ groups. Each group shares a single Key and Value head.\nEquation modification: If you have 8 query heads, you might create 4 groups (2 queries per group). This reduces the KV cache by 2x instead of 8x, preserving more quality than MQA while still speeding up inference.\nMHA: 8 people (heads) are working on a project. Each person has their own unique filing cabinet (KV cache). It takes forever to move 8 cabinets around. MQA: 8 people share one single filing cabinet. It\u0026rsquo;s very fast to move, but they fight over the organization and lose detail. GQA: The 8 people split into 4 teams of 2. Each team shares a cabinet. It\u0026rsquo;s a balance between speed and organizational detail. üî• Multi-Head Latent Attention (MLA) This is the most advanced mechanism for attention, used in models like DeepSeek-V3. It is a sophisticated design for massive efficiency that takes a radical approach: mathematically compressing the memory into a low-rank \u0026ldquo;latent\u0026rdquo; vector.\nKV Cache Explosion: As mentioned before, for a massive model like DeepSeek-V3 (671B parameters), storing the full Keys and Values for every token in a long conversation requires terabytes of high-speed memory (HBM), and chips spend more time waiting than churning.\nMHA (Standard) stores everything. MQA deletes redundancy but hurts quality. MLA keeps the quality of MHA but compresses the data 4x-6x smaller than even GQA.\nThe Zip File Approach: Low-Rank Joint Compression Let\u0026rsquo;s compare the math of MHA and MLA. In MHA, for every single token, we generate a unique Key ($k$) and Value ($v$) matrix for each attention head.\nThe Memory Problem: The $k$ is stored in the KV Cache. For 128 heads, for example, it calculates a massive vector of $128 \\times 128 = 16,384$ floats per token.\nFor MHA, the attention score looks like: $$\\text{Score} = q^T \\cdot k$$\nBut for MLA, instead of storing the full $k$, it projects the input down into a tiny latent vector $c_{KV}$, which is much smaller (e.g., 512 floats). $$c_{KV} = W_{DKV} \\cdot x$$\nNow here is the catch: Naive compression and decompression would mean constructing a small latent matrix for every token and deconstructing it every single time the information is required, which defeats the purpose of compression.\n$$k_{\\text{reconstructed}} = W_{UK} \\cdot c_{KV}$$ $$\\text{Score} = q^T \\cdot k_{\\text{reconstructed}}$$\nThe Matrix Absorption Trick (Optimization) Instead of reconstructing $k$ for every token, we can absorb the Up-Projection matrix ($W_{UK}$) into the Down-Projection matrix ($W_{DKV}$) and absorb the query $q$ into a new $Q$.\nFrom the original equation: $$\\text{Score} = q^T \\cdot (\\underbrace{W_{UK} \\cdot c_{KV}}_{\\text{This is } k})$$\nWe associate differently:\n$$\\text{Score} = (\\underbrace{q^T \\cdot W_{UK}}_{\\text{Absorbed Query}}) \\cdot c_{KV}$$ We change the order in which the matrix multiplication is performed.\nHow is this allowed? In linear algebra, matrix multiplication is associative:\n$$(A \\cdot B) \\cdot C = A \\cdot (B \\cdot C)$$ We can move the parentheses! Instead of grouping $(W_{UK} \\cdot c_{KV})$ to make the Key, we group $(q^T \\cdot W_{UK})$ to make a new Query.\nNote: The positional embedding information is left untouched. That rotational information from algorithms like RoPE is preserved outside the compression; it requires minimal space anyways.\n","permalink":"http://localhost:1313/posts/attention-mechanisms/","summary":"We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek\u0026rsquo;s MLA, how has the definition of \u0026lsquo;Attention\u0026rsquo; transformed?","title":"Attention Is All You Need, but exactly which one?: MHA, GQA and MLA"},{"content":"\nPositional Embeddings are the \u0026ldquo;voice\u0026rdquo; that tell Transformers where words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.\n\u0026ldquo;The dog chased the cat\u0026rdquo; and \u0026ldquo;cat chased the dog\u0026rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.\nüî• Sinusoidal Embeddings: The Odometer of Language Introduced in the seminal \u0026ldquo;Attention Is All You Need\u0026rdquo; paper, this was the industry standard for many years. This approach adds a vector to each word embedding based on its position. The vector is computed out of wavelike patterns that are periodic and smooth. Mathematically, for a position $pos$ and embedding dimension $i$:\n$$ PE_{(pos, i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $$\nThe model uses sine and cosine waves of different frequencies to capture both short and long-range dependencies. The frequency of the waves decreases as the embedding dimension increases, which helps capture long-range dependencies in the input sequence.\nThe creators wanted the model to calculate distance easily, which is why a linear function was used. The model simply rotates the current position vector using a rotation matrix to derive the next position.\n$$ \\begin{pmatrix} \\cos(k) \u0026 \\sin(k) \\\\ -\\sin(k) \u0026 \\cos(k) \\end{pmatrix} \\cdot PE(pos) \\approx PE(pos + k) $$ Low dimensions act like a fast-ticking clock (high frequency, changing rapidly) and high dimensions act like a slow clock (low frequency, changing slowly).\nThe number 10,000 is an arbitrarily chosen high number to reduce Aliasing. By choosing such a high number, the creators ensured that the wavelength of the slowest dimension is incredibly long. Consequently, the pattern practically never repeats, and each position remains unique.\nThis combination works like a mechanical odometer dial. Each subsequent dimension acts as a slower-moving gear. The model learns to read these \u0026ldquo;dials\u0026rdquo; to understand exactly where a word sits in the sequence.\nThis mitigated the challenge of exploding gradients that previous array-based positional embeddings suffered from (e.g., adding integers 1, 2, 3\u0026hellip;). This model keeps values between -1 and +1, adding stability while ensuring every position gets a distinct code.\nThe Problem: This approach pollutes the word embedding matrix (semantic information). While high-dimensional vectors are almost always orthogonal‚Äîmeaning the noise doesn\u0026rsquo;t fully destroy the word\u0026rsquo;s identity‚Äîit does not strictly guarantee purity.\nThis led to a more mature approach.\nüî• ALiBi: The Fading Streetlight Attention with Linear Biases (ALiBi) is a method where, instead of adding a vector to the embeddings at the start, the model injects positional information linearly, directly into the attention scores. If Sinusoidal Encoding asks, \u0026ldquo;Where am I on the map?\u0026rdquo;, ALiBi asks, \u0026ldquo;How much harder should I squint to see you?\u0026rdquo;\nThe biggest problem ALiBi solved is the \u0026ldquo;Invisible Wall\u0026rdquo;. If you train a Transformer on sequences of length 2048 and then feed it a book with 3000 words, it will crash or output garbage at word 2049. This is a failure of Extrapolation.\nALiBi throws away the idea of adding positional vectors entirely. Instead, it devises a \u0026ldquo;penalty\u0026rdquo; system for attention scores:\n$$ \\text{Score} = (q \\cdot k^\\top) - (m \\cdot \\text{distance}) $$\nDistance: If words A and B are neighbors, the distance is 0, so the penalty is 0. Although the distance is linear, it does not cause exploding gradients because the raw attention scores are normalized by a softmax function. This distance prioritizes nearby words (local context) over distant ones.\nSlope ($m$): If we always penalize distance equally, the model becomes myopic. ALiBi mitigates this by assigning a different slope value to each attention head:\nHead 1 (The Historian): Very low slope. The penalty grows slowly, allowing it to see far back. Head 2 (The Reader): Medium slope. Head 3 (The Myopic): Very high slope (e.g., 0.5); it effectively cannot see beyond 2-3 words. This mimics holding a lantern. Words close to you are bright, while words further away naturally fade into darkness.\nExtrapolation: If you walk 5 miles down the road (a longer sequence), your lantern works exactly the same way. The physics of light decay are constant, allowing the model to handle sequences longer than it was trained on.\nHowever, ALiBi forces a \u0026ldquo;sliding window\u0026rdquo; view where distant words eventually become invisible. This brings us to the modern standard.\nüî• RoPE: The Rotary Revolution Rotary Position Embeddings (RoPE) is the current industry standard (used in Llama 3, Mistral, PaLM) because it mathematically unifies \u0026ldquo;absolute position\u0026rdquo; with \u0026ldquo;relative position.\u0026rdquo; This method rotates the vectors in the Query and Key matrices based on their absolute position.\nIf a vector has 512 dimensions, RoPE breaks it into 256 pairs. Each pair is rotated by a different frequency:\n$$ \\theta_i = 10000^{-2i/d} $$\nThe First pair ($i = 0$) is rotated by the highest frequency. The Last pair ($i = 255$) is rotated by the lowest frequency. This spread ensures the model has \u0026ldquo;high precision\u0026rdquo; (fast rotation) to distinguish immediate neighbors, and \u0026ldquo;low precision\u0026rdquo; (slow rotation) to track global position without the pattern repeating.\nDoesn\u0026rsquo;t rotation change meaning? Yes and No. The vector for \u0026ldquo;Apple\u0026rdquo; at position 1 and \u0026ldquo;Apple\u0026rdquo; at position 256 will look completely different in terms of coordinates. However, rotation does not change the semantic strength (magnitude/norm) of the vector.\nThe Relativity Trick: Transformers compare words using dot products (angles). RoPE relies on the fact that the attention mechanism ignores the absolute rotation and only detects the difference in rotation (relative position). Thus, the \u0026ldquo;relative meaning\u0026rdquo; is encoded purely into the angle between words, leaving the semantic \u0026ldquo;magnitude\u0026rdquo; untouched.\nüî• LongRoPE: The Bifocal Lens While RoPE is powerful, it struggles when stretching context windows from 4k to 2 million tokens. The \u0026ldquo;fast\u0026rdquo; dimensions spin so rapidly over long distances that they become random noise. LongRoPE solves this using an evolutionary search algorithm to find a unique scaling factor ($\\lambda$) for each dimension.\nThe Equations Instead of rotating by the standard $\\theta_i$, LongRoPE rotates by a rescaled frequency:\n$$ \\theta\u0026rsquo;_i = \\lambda_i \\theta_i $$\nTo efficiently find these $\\lambda$ values without searching an infinite space, the algorithm enforces a monotonicity constraint based on NTK theory:\n$$ \\lambda_i \\le \\lambda_{i+1} $$\nThis ensures that low-frequency dimensions (global context) are stretched more than high-frequency dimensions (local context), creating a \u0026ldquo;bifocal\u0026rdquo; effect:\nHigh Frequencies (Local): Kept sharp ($\\lambda \\approx 1$) to maintain grammar. Low Frequencies (Global): Stretched ($\\lambda \u0026gt; 1$) to track massive distances without repeating. Not worth perusing more since the next one is much better at long context positional encoding. üî• HoPE: The Hyperbolic Slide Hyperbolic Rotary Positional Encoding (HoPE) moves from the geometry of a circle to the geometry of a hyperbola (inspired by Lorentz transformations). It was designed to fix a subtle flaw in RoPE: the \u0026ldquo;wobbly\u0026rdquo; or oscillatory nature of attention scores.\nThe Core Problem: RoPE\u0026rsquo;s \u0026ldquo;Wobble\u0026rdquo; In standard RoPE, we rotate vectors around a circle.\nThe Issue: Circles repeat. A dial at $361^\\circ$ looks identical to $1^\\circ$. Even with different frequencies, the dot product fluctuates up and down as distance increases. The Consequence: The model gets confused at long distances, potentially mistaking a word 1,000 tokens away for a neighbor just because the rotation cycles aligned, creating \u0026ldquo;noise.\u0026rdquo; The \u0026ldquo;Hyperbolic\u0026rdquo; Shift HoPE replaces trigonometric functions ($\\sin, \\cos$) with hyperbolic functions ($\\sinh, \\cosh$).\nCircular Rotation: Keeps distance from the center constant. Hyperbolic Rotation: Moves points along a hyperbola. Crucially, hyperbolic functions do not repeat; they grow or decay exponentially. By using hyperbolic rotation, HoPE guarantees that as words get further apart, the attention score decays monotonically (smoothly drops).\nThe Equations HoPE uses a hyperbolic matrix $B(\\theta, m)$ defined using the Lorentz Boost structure:\n$$ B(\\theta, m) = \\begin{pmatrix} \\cosh(m\\theta) \u0026 \\sinh(m\\theta) \\\\ \\sinh(m\\theta) \u0026 \\cosh(m\\theta) \\end{pmatrix} $$ Since hyperbolic functions grow to infinity, HoPE introduces a decay penalty ($e^{-m\\theta'}$). The final attention score simplifies beautifully to a function of relative distance: $$ \\text{Score} \\propto e^{-|m-n|(\\theta\u0026rsquo; - \\theta)} $$\nThis ensures the attention score exponentially decays as distance increases, eliminating the wobbles of standard RoPE. This the latest model that is taking up the space in the industry for extremely long context models.\n","permalink":"http://localhost:1313/posts/positional-embeddings/","summary":"From Sinusoidal to RoPE and HoPE: How Transformers learn to process word order and sequence length.","title":"The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE"},{"content":"Imagine trying to teach a computer the difference between \u0026ldquo;Apple\u0026rdquo; the fruit and \u0026ldquo;Apple\u0026rdquo; the company. To us, the distinction is intuitive. To a machine, it‚Äôs just a string of characters. How do we turn these strings into meaningful math? While early attempts like Word2Vec gave us a great start, they missed the forest for the trees‚Äîor in some cases, the twigs for the branches. Enter GloVe and FastText: two algorithms that revolutionized how machines understand the nuances of human language.\nPrevious static embedding models like Word2Vec successfully captured the local semantics of words, but they failed in addressing contexts between words that might not always appear in the same context window. Furthermore, they treated words as indivisible atomic units in continuous vector spaces, missing out on the internal structure of language.\nThe two simple but effective ideas behind GloVe and FastText are subword units and global co-occurrence. Let\u0026rsquo;s dive into them.\nGloVe: A Global Mapping Strategy Global Vectors for Word Representation (GloVe) was created at Stanford. The core idea was to train a model where word vectors are defined by how often they appear around other words across the entire corpus, rather than just locally.\nTraining Steps: Co-occurrence Matrix X: GloVe scans the entire corpus and creates a giant matrix (spreadsheet) recording the counts of how often words appear near each other. Factorization: It compresses this giant matrix into smaller, dense vectors. Objective: The function tries to equate the dot product of two word vectors with the logarithm of their probability of appearing together. The error between these values is then backpropagated to update the vectors. $$w_i \\cdot w_j + b_i + b_j = \\log(X_{ij})$$\n$w$: The word vector (what we want). $X_{ij}$: The count from our matrix. The Loss Function: We can\u0026rsquo;t satisfy the equation above perfectly for every word pair. So, we use a weighting function to minimize the squared error:\n$$J = \\sum_{i,j=1}^V f(X_{ij}) ({w_i}^T w_j + b_i + b_j - \\log(X_{ij}))^2$$\nIf we do not use the weighting function, the loss would be infinite for rare words (since $\\log(0) = \\infty$) and frequent words (like \u0026rsquo;the\u0026rsquo;) would overpower the model.\n$$ f(x) = \\begin{cases} (x/x_{max})^\\alpha \u0026amp; \\text{if } x \u0026lt; x_{max} \\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$\n(Typically $x_{max}=100$ and $\\alpha=0.75$)\nFastText: The Lego Bricks Strategy FastText was created by Facebook AI Research (FAIR). The idea was to treat words not as solid rocks, but as structures made of \u0026ldquo;Lego bricks\u0026rdquo; (subword units).\nWord2Vec struggled to create embeddings for words it hadn\u0026rsquo;t seen before (Out-Of-Vocabulary or OOV words). FastText solves this by breaking words down.\nGloVe treats a word like \u0026ldquo;Apple\u0026rdquo; as a single, atomic unit. FastText breaks \u0026ldquo;Apple\u0026rdquo; into character n-grams: \u0026lt;ap, app, ppl, ple, le\u0026gt; For a complex or new word like \u0026ldquo;Applesauce\u0026rdquo;, the model might not know the whole word, but it recognizes the Lego brick for \u0026ldquo;Apple\u0026rdquo; and the remaining bits from other subwords.\nFastText takes its inspiration from the SkipGram architecture. The vector for a word is simply the sum of the vectors of its n-gram subwords:\n$$v_w = \\sum_{g \\in G_w} z_g$$\nObjective: The objective is to predict context words given a target word.\n$$P(w_c | w_t) = \\frac{e^{s(v_{w_t} \\cdot v_{w_c})}}{\\sum_{w \\in V} e^{s(v_{w_t} \\cdot v_w)}}$$\nAnd the scoring function is defined as:\n$$s(w_t, w_c) = u_{w_t}^T (\\sum_{g \\in G_w} z_g)$$\n$u_{w_c}$: The vector for the context word (output vector). $z_g$: The vector for the n-gram $g$ (input vector). Modifications to Softmax: Calculating the full softmax for every word in the vocabulary is computationally expensive. FastText uses two methods to approximate this efficiently.\nOption A: Negative Sampling\nWe pick a few random \u0026ldquo;wrong\u0026rdquo; words every time to serve as \u0026ldquo;negative samples\u0026rdquo; and calculate the loss only for the correct word and these noise words.\n$$J = - \\log \\sigma(s(w_t, w_c)) - \\sum\\limits_{i=1}^{N} \\mathbb{E}_{n_i \\sim P_n(w)} [\\log \\sigma(-s(w_t, n_i))]$$\nThe sigmoid function $\\sigma$ squashes the score between 0 and 1. The first term pushes the probability of the correct word towards 1. The second term pushes the probability of the noise words towards 0. Option B: Hierarchical Softmax (Faster for infrequent words)\nInstead of a flat list of words, imagine the vocabulary arranged as a binary tree (Huffman Tree).\nRoot: Top of the tree. Leaves: The actual words. To calculate the probability of a word, we trace a path from the root to that leaf. At every branching node, we calculate the probability (sigmoid) of going left or right. The probability of a word $w$ is the product of the probabilities of the turns taken to reach it. Code Example from gensim.models import FastText from gensim.test.utils import common_texts # Initialize the FastText model with specific hyperparameters model = FastText( vector_size=100, # Dimensionality of the word vectors window=5, # Maximum distance between the current and predicted word within a sentence min_count=1, # Ignores all words with total frequency lower than this sg=1, # Training algorithm: 1 for skip-gram; 0 for CBOW hs=0, # If 0, and negative is non-zero, negative sampling will be used negative=5, # Number of \u0026#34;noise words\u0026#34; to be drawn for negative sampling workers=4, # Number of worker threads to train the model epochs=10, # Number of iterations over the corpus min_n=3, # Minimum length of character n-grams max_n=6 # Maximum length of character n-grams ) # Build the vocabulary from the provided text corpus model.build_vocab(common_texts) # Train the model on the corpus model.train( common_texts, total_examples=model.corpus_count, epochs=model.epochs ) # Calculate and print the cosine similarity between two words in the vocabulary print(f\u0026#34;Similarity between \u0026#39;computer\u0026#39; and \u0026#39;human\u0026#39;: {model.wv.similarity(\u0026#39;computer\u0026#39;, \u0026#39;human\u0026#39;)}\u0026#34;) # Demonstrate FastText\u0026#39;s ability to handle Out-Of-Vocabulary (OOV) words # Even if \u0026#39;computation\u0026#39; wasn\u0026#39;t in the training data, FastText constructs a vector using character n-grams oov_vector = model.wv[\u0026#39;computation\u0026#39;] # Check if a specific word exists in the model\u0026#39;s fixed vocabulary index word = \u0026#34;computer\u0026#34; is_in_vocab = word in model.wv.key_to_index print(f\u0026#34;\\nIs the word \u0026#39;{word}\u0026#39; in the model\u0026#39;s vocabulary? {is_in_vocab}\u0026#34;) ","permalink":"http://localhost:1313/posts/glove-and-fasttext/","summary":"How subword units and global co-occurrence matrices allow GloVe and FastText to capture nuances that Word2Vec missed.","title":"The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText"},{"content":" Before we had Large Language Models writing poetry, we had to teach computers that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; are related not just by spelling, but by meaning. This is the story of that breakthrough. It‚Äôs the moment we stopped counting words and started mapping their souls‚Äîturning raw text into a mathematical landscape where math can solve analogies. Welcome to the world of Word2Vec. üîÆ\nLanguage models require vector representations of words to capture semantic relationships. Before the 2010s, models used word count-based vector representations that captured only the frequency of words (e.g., One-Hot Encoding). The Problems: üöß\nThe Curse of Dimensionality Lack of meaning (Synonyms were treated as mathematically unrelated). In 2011, Mikolov et al. at Google introduced Word2Vec, which used a shallow neural network to learn vector representations of words. The best part? It could create much denser relationships between words than prior models, and it was unsupervised.\nThe training (prediction) is essentially \u0026ldquo;fake\u0026rdquo;‚Äîa pretext task. But the weight matrices we get on the side are a gold mine of fine-grained semantic relationships. ‚õèÔ∏è\nThere are two popular variants of Word2Vec:\nSkipGram Continuous Bag Of Words (CBOW) 1. Continuous Bag of Words (CBOW) üß© CBOW is a simple \u0026lsquo;fill in the blanks\u0026rsquo; machine. It takes the surrounding words as inputs and tries to predict the center word.\n\u0026ldquo;Continuous\u0026rdquo;: It operates in continuous vector space (unlike the discrete space of n-gram models).\n\u0026ldquo;Bag of words\u0026rdquo;: The order of the context does not matter. \u0026ldquo;The cat sat on the mat\u0026rdquo; and \u0026ldquo;The mat sat on the cat\u0026rdquo; are the same for CBOW. They produce exactly the same predictions.\nObjective: Maximize the probability of the target word $w_t$ given its context $w_{t-C}, \\dots, w_{t+C}$.\n2. Training Defining Dimensions:\n$V$: Vocabulary size (number of unique words in the corpus, e.g., 10,000) $N$: Dimension of the embedding space (e.g., 100) $C$: Number of context words (e.g., 2 words on left of target and 2 words on right) Step 1: Input Lookup \u0026amp; One-Hot Encoding\nMathematically, we represent each context word as a one-hot vector $x^{(c)}$. We have an Input Matrix $W_{in}$ of size $V \\times N$. For each context word (e.g., \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;), we select its corresponding row from $W_{in}$.\n$$ v_c = W_{in}^T x^{(c)} $$\nWhere $v_c$ is the $N$-dimensional vector for context word $c$.\nStep 2: Projection (The \u0026ldquo;Mixing\u0026rdquo;)\nWe combine the context vectors. In standard CBOW, we simply average them. This creates the hidden layer vector $h$.\n$$ h = \\frac{1}{2C} \\sum_{c=1}^{2C} v_c $$\n$h$ is a single vector of size $N \\times 1$. Note: This operation is linear and contains no activation function (like ReLU or Sigmoid).\nStep 3: Output Scoring (The \u0026ldquo;Dot Product\u0026rdquo;)\nWe have a second matrix, the Output Matrix $W_{out}$ of size $N \\times V$. We compute the raw score (logit) $u_j$ for every word $j$ in the vocabulary by taking the dot product of the hidden vector $h$ and the output vector $v\u0026rsquo;_j$.\n$$ u_j = {v\u0026rsquo;_j}^T h $$\nOr in matrix form:\n$$ u = W_{out}^T h $$\nResult $u$ is a vector of size $V \\times 1$.\nStep 4: Probability Conversion (Softmax)\nWe convert the raw scores $u$ into probabilities using the Softmax function. This tells us the probability that word $w_j$ is the center word.\n$$ y_j = P(w_j | \\text{context}) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 5: Loss Calculation (Cross-Entropy)\nWe compare the predicted distribution $y$ with the actual ground truth $t$ (a one-hot vector where the index of the true word is 1). The loss function $E$ is:\n$$ E = - \\sum_{j=1}^V t_j \\log(y_j) $$\nSince $t_j$ is 0 for all words except the target word (let\u0026rsquo;s call the target index $j^*$), this simplifies to:\n$$ E = - \\log(y_{j^*}) $$\n3. Backpropagation Details We need to update the weights to minimize $E$. We use the Chain Rule.\nA. Gradient w.r.t. Output Matrix ($W_{out}$)\nWe want to know how the error changes with respect to the raw score $u_j$.\n$$ \\frac{\\partial E}{\\partial u_j} = y_j - t_j $$\nLet\u0026rsquo;s call this error term $e_j$.\nIf $j$ is the target word: $e_j = y_j - 1$ (Result is negative; we boost the score). If $j$ is not target: $e_j = y_j - 0$ (Result is positive; we suppress the score). The update rule for the output vector $v\u0026rsquo;_j$ becomes:\n$$ v'_{j}(\\text{new}) = v'_{j}(\\text{old}) - \\eta \\cdot e_j \\cdot h $$ ($\\eta$ is the learning rate)\nB. Gradient w.r.t. Hidden Layer ($h$)\nWe backpropagate the error from the output layer to the hidden layer.\n$$ EH = \\sum_{j=1}^V e_j \\cdot v\u0026rsquo;_j $$\n$EH$ is an $N$-dimensional vector representing the aggregate error passed back to the projection layer.\nC. Gradient w.r.t. Input Matrix ($W_{in}$)\nSince $h$ was just an average of the input vectors, the error $EH$ is distributed to each context word\u0026rsquo;s input vector. For every word $w_c$ in the context:\n$$ v_{w_c}(\\text{new}) = v_{w_c}(\\text{old}) - \\eta \\cdot \\frac{1}{2C} \\cdot EH $$\nLet\u0026rsquo;s build a single pass through the network for a given context.\nimport torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # Define a simple Continuous Bag of Words (CBOW) style model class CBOWModel(nn.Module): def __init__(self, vocab_size, embedding_dim): super(CBOWModel, self).__init__() # The embedding layer stores the word vectors we want to learn self.embeddings = nn.Embedding(vocab_size, embedding_dim) # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word self.linear = nn.Linear(embedding_dim, vocab_size) def forward(self, inputs): # inputs: tensor of word indices for the context embeds = self.embeddings(inputs) # Aggregate context by calculating the mean of the word embeddings h = torch.mean(embeds, dim=1) # Produce logits (raw scores) for each word in the vocabulary logits = self.linear(h) return logits # Setup vocabulary and mappings word_to_ix = {\u0026#34;the\u0026#34;: 0, \u0026#34;quick\u0026#34;: 1, \u0026#34;brown\u0026#34;: 2, \u0026#34;fox\u0026#34;: 3, \u0026#34;jumps\u0026#34;: 4, \u0026#34;over\u0026#34;: 5, \u0026#34;lazy\u0026#34;: 6, \u0026#34;dog\u0026#34;: 7} ix_to_word = {v: k for k, v in word_to_ix.items()} # Configuration constants EMBEDDING_DIM = 5 VOCAB_SIZE = len(word_to_ix) LEARNING_RATE = 0.01 # Initialize the model, loss function, and optimizer model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM) loss_function = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE) # Prepare dummy training data: context words and the target word # Context: [\u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;over\u0026#34;] -\u0026gt; Target: \u0026#34;fox\u0026#34; context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long) target_idxs = torch.tensor([3], dtype=torch.long) # Perform a single optimization step model.zero_grad() # Forward pass: we reshape context to (1, -1) to simulate a batch of size 1 logits = model(context_idxs.view(1, -1)) # Calculate loss against the target word index loss = loss_function(logits, target_idxs) # Backpropagate and update weights loss.backward() optimizer.step() # Output results for the blog post print(\u0026#34;=== Model Training Snapshot ===\u0026#34;) print(f\u0026#34;Calculated Loss: {loss.item():.6f}\u0026#34;) print(f\u0026#34;\\n=== Learned Vector for \u0026#39;jumps\u0026#39; ===\u0026#34;) word_vec = model.embeddings(torch.tensor([word_to_ix[\u0026#39;jumps\u0026#39;]])) print(word_vec.detach().numpy()) print(\u0026#34;\\n=== Embedding Matrix (Weights) ===\u0026#34;) print(model.embeddings.weight.detach().numpy()) print(\u0026#34;\\n=== Linear Layer Weights ===\u0026#34;) print(model.linear.weight.detach().numpy()) Output shows single pass loss and learned vector for \u0026lsquo;jumps\u0026rsquo; word. This is a single training sample result but after multiple such passes both Embedding and linear weights should be very similar.\n=== Model Training Snapshot === Calculated Loss: 1.989656 === Learned Vector for \u0026#39;jumps\u0026#39; === [[-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775]] === Embedding Matrix (Weights) === [[ 1.66039 -0.11371879 0.6246518 0.35860053 -1.2504417 ] [-2.1847186 -0.77199775 -0.17050214 -0.38411248 -0.03913084] [ 0.11852697 0.90073633 0.8847807 0.7404524 0.900149 ] [ 0.07440972 -0.40259898 2.6246994 -0.08851447 0.02660969] [-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775] [-0.9245572 -0.5545908 0.9083091 -1.0755049 0.84047747] [-1.0237687 0.59466314 0.05621134 -0.6202532 1.3664424 ] [ 0.60998917 -1.0549186 1.6103884 0.8724912 -1.2486908 ]] === Linear Layer Weights === [[ 0.02165058 -0.28883642 0.14545658 -0.3442509 0.32704315] [-0.18731792 0.28583744 0.22635977 0.13245736 0.29019794] [ 0.3158916 -0.15826383 -0.03203773 0.16377363 -0.41457543] [-0.05080034 0.4180087 0.11228557 0.30218413 0.3025514 ] [-0.38419306 0.24475925 -0.39210224 -0.38660625 -0.2673145 ] [-0.32321444 0.12200444 -0.03569533 0.2891424 -0.07345333] [-0.33704326 0.2521956 0.31587374 0.22590035 0.29866052] [ 0.4117266 -0.44231793 0.24064957 -0.29684234 0.333821 ]] Interesting Facts About CBOW üß† The \u0026ldquo;Averaging Problem\u0026rdquo;: This is a common issue in CBOW models. The averaging and training is order-agnostic. All variants of word order in the context window will generate the same output. \u0026ldquo;Dog bit the man\u0026rdquo; and \u0026ldquo;Man bit the dog\u0026rdquo; are the same in CBOW\u0026rsquo;s eyes.\nSmoothing Effect: CBOW models are much faster to train than Skip-Gram but are generally smoother.\nRare Words: CBOW models struggle slightly with rare words because the presence of surrounding common words (like articles and prepositions) can skew the embeddings and dissolve their uniqueness.\nComputational Cost: Softmax requires summing over the entire vocabulary size, which can be computationally expensive if $V$ is very large. Solutions include Hierarchical Softmax and Negative Sampling (approximating the denominator by only checking the target word vs. 5 random noise words).\nVectors as Double Agents: Every word has a different representation in the embedding matrix and the output matrix. Most of the time $W_{out}$ is discarded, but some consider averaging both $W_{in}$ and $W_{out}$ for slightly better performance.\nLinear Relationships: The famous analogy King - Man + Woman = Queen emerges from the model because of the linear relationships between the words.\nInitialization: This is a no-brainer but worth mentioning. Initialization to zero vectors would mean no learning. We must initialize randomly.\n2. Skip-Gram (The \u0026ldquo;Burger Flip\u0026rdquo; of CBOW) üîÑ Skip-Gram flips the CBOW logic. While CBOW tries to predict the center word given the context, Skip-Gram tries to predict the scattered context given the center word.\nCore Concept: If a word like \u0026ldquo;monarch\u0026rdquo; can accurately predict words like \u0026ldquo;king\u0026rdquo;, \u0026ldquo;throne\u0026rdquo;, \u0026ldquo;crown\u0026rdquo;, \u0026ldquo;royalty\u0026rdquo;, and \u0026ldquo;power\u0026rdquo;, then it must effectively capture the concept of royalty.\nTraining Let\u0026rsquo;s define dimensions:\n$V$: Vocabulary size $D$: Embedding dimension $C$: Number of context words Step 1: Input Lookup\nUnlike CBOW, our input is just one word vector (the center word $w_t$). We grab the vector $v_c$ from the Input Matrix $W_{in}$.\n$$ h = v_{w_t} $$\nNote: There is no averaging here. The hidden layer $h$ is simply the raw vector of the center word.\nStep 2: Output Scoring (The Broadcast)\nThe model takes this single vector $h$ and compares it against the Output Matrix $W_{out}$ (the entire vocabulary). It computes a score $u_j$ for every word $j$ in the dictionary.\n$$ u = W_{out}^T h $$\nResult $u$: A vector of size $V \\times 1$ containing raw scores.\nStep 3: Probability Conversion (Softmax)\nWe apply Softmax to turn scores into probabilities.\n$$ P(w_j | w_t) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 4: The Multi-Target Loss\nHere is the major difference from CBOW. In CBOW, we had one target (the center). In Skip-Gram, we have $2C$ targets (all the surrounding words). For a sentence \u0026ldquo;The cat sat on mat\u0026rdquo; (Center: \u0026ldquo;sat\u0026rdquo;), the targets are \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;. We want to maximize the probability of all these true context words. The Loss function ($E$) sums the error for each context word:\n$$ E = - \\sum_{w_{ctx} \\in \\text{Window}} \\log P(w_{ctx} | w_t) $$\nStep 5: Backpropagation (Accumulating Gradients)\nSince one input word is responsible for predicting multiple output words, the error signals from all those context words add up.\nAt the Output Layer:\nIf \u0026ldquo;cat\u0026rdquo; was a target, the \u0026ldquo;cat\u0026rdquo; vector in $W_{out}$ gets a signal: \u0026ldquo;Move closer to \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; If \u0026ldquo;apple\u0026rdquo; was NOT a target, the \u0026ldquo;apple\u0026rdquo; vector gets a signal: \u0026ldquo;Move away from \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; At the Hidden Layer ($h$):\nThe center word \u0026ldquo;sat\u0026rdquo; receives feedback from all its neighbors simultaneously. Error signal = (Error from \u0026ldquo;The\u0026rdquo;) + (Error from \u0026ldquo;cat\u0026rdquo;) + (Error from \u0026ldquo;on\u0026rdquo;)\u0026hellip; The vector for \u0026ldquo;sat\u0026rdquo; in $W_{in}$ moves in the average direction of all its neighbors. 3. The \u0026ldquo;Computational Nightmare\u0026rdquo; \u0026amp; Negative Sampling The equations above describe Naive Softmax.\nThe Problem: If $V = 100,000$, computing the denominator $\\sum \\exp(u_k)$ takes 100,000 operations. Doing this for every word in a training set of billions of words is impossibly slow.\nThe Solution: Negative Sampling Instead of updating the entire vocabulary (1 correct word vs 99,999 wrong ones), we approximate the problem.\nPositive Pair: (sat, cat) $\\rightarrow$ Maximize probability (Label 1). Negative Pairs: We pick $K$ (e.g., 5) random words the model didn\u0026rsquo;t see, e.g., (sat, bulldozer), (sat, quantum). $\\rightarrow$ Minimize probability (Label 0). New Equation (Sigmoid instead of Softmax):\n$$ E = - \\log \\sigma({v'_{\\text{pos}}}^T v_{\\text{in}}) - \\sum_{k=1}^K \\log \\sigma(-{v'_{\\text{neg}_{k}}}^T v_{\\text{in}}) $$ Effect: We typically only update 6 vectors per step (1 pos + 5 negs) instead of 100,000.\nLet\u0026rsquo;s see this in code:\nimport torch import torch.nn as nn import torch.optim as optim class SkipGramNegativeSampling(nn.Module): \u0026#34;\u0026#34;\u0026#34; Skip-Gram with Negative Sampling (SGNS) implementation. SGNS approximates the softmax over the entire vocabulary by instead distinguishing between a real context word (positive) and K noise words (negative). \u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embedding_dim): super(SkipGramNegativeSampling, self).__init__() # Input embeddings: used when the word is the center word self.in_embed = nn.Embedding(vocab_size, embedding_dim) # Output embeddings: used when the word is a context or negative sample self.out_embed = nn.Embedding(vocab_size, embedding_dim) # Initialize weights with small values to prevent gradient saturation initrange = 0.5 / embedding_dim self.in_embed.weight.data.uniform_(-initrange, initrange) self.out_embed.weight.data.uniform_(-initrange, initrange) def forward(self, center_words, target_words, negative_words): \u0026#34;\u0026#34;\u0026#34; Computes the negative sampling loss. Args: center_words: (batch_size) target_words: (batch_size) negative_words: (batch_size, K) where K is number of negative samples \u0026#34;\u0026#34;\u0026#34; # Retrieve vectors v_c = self.in_embed(center_words) # (batch_size, embed_dim) u_o = self.out_embed(target_words) # (batch_size, embed_dim) u_n = self.out_embed(negative_words) # (batch_size, K, embed_dim) # 1. Positive Score: log(sigmoid(v_c ¬∑ u_o)) # Compute dot product: (batch, 1, dim) @ (batch, dim, 1) -\u0026gt; (batch, 1) pos_score = torch.bmm(u_o.unsqueeze(1), v_c.unsqueeze(2)).squeeze(2) pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-7) # 2. Negative Score: sum(log(sigmoid(-v_c ¬∑ u_n))) # Compute dot products for all K samples: (batch, K, dim) @ (batch, dim, 1) -\u0026gt; (batch, K) neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2) neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-7), dim=1, keepdim=True) # Total loss is the negative of the objective function loss = -(pos_loss + neg_loss) return torch.mean(loss) # --- Configuration \u0026amp; Mock Data --- VOCAB_SIZE = 100 EMBED_DIM = 10 word_to_ix = {\u0026#39;fox\u0026#39;: 0} # Example vocabulary mapping model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock inputs: 1 center word, 1 target word, 5 negative samples center_id = torch.tensor([0], dtype=torch.long) target_id = torch.tensor([1], dtype=torch.long) negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long) # Training Step model.zero_grad() loss = model(center_id, target_id, negative_ids) loss.backward() optimizer.step() # Output Results print(f\u0026#34;Loss after one step: {loss.item():.6f}\u0026#34;) word_vec = model.in_embed(torch.tensor([word_to_ix[\u0026#39;fox\u0026#39;]])) print(f\u0026#34;Vector for \u0026#39;fox\u0026#39;:\\n{word_vec.detach().numpy()}\u0026#34;) The outputs:\nLoss after one step: 4.158468 Vector for \u0026#39;fox\u0026#39;: [[ 0.04197385 0.02400733 -0.03800093 0.01672485 -0.03872231 -0.0061478 0.0121122 0.04057864 -0.036255 0.03861175]] Interesting Facts About Skip-Gram üí° Makes Rare Words Shine: Skip-Gram is more effective at capturing the meaning of rare words because it focuses on the context of the center word. In CBOW, a rare word\u0026rsquo;s vector is averaged with others, diluting its signal. In Skip-Gram, when the rare word appears as the center, it gets the full, undivided attention of the backpropagation update.\nSlow Training: This method creates more training samples. A window of size 2 creates 4 training pairs per center word, while CBOW creates only 1 training pair per center word.\nSemantic Focus: Skip-Gram puts more emphasis on the semantic relationship between words. It views the context as the primary signal, with the center word as the target. This makes it better at capturing the meaning of words in context. Skip-Gram tends to capture semantic relationships (King/Queen) slightly better, while CBOW captures syntactic ones (walk/walked) slightly better.\n","permalink":"http://localhost:1313/posts/word-embeddings/","summary":"Understanding the mathematics behind Word2Vec, CBOW, and Skip-Gram and how they map language to vector space.","title":"Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n5. Code Examples: We\u0026rsquo;ll use sentencepiece for tokenization and compare results for different models.\nimport sentencepiece as spm import os # Configuration for training the SentencePiece model # SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words. options = { # The source text file used for learning the vocabulary \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # The base name for the generated model (.model) and vocabulary (.vocab) files \u0026#39;model_prefix\u0026#39;: \u0026#39;bpe_model\u0026#39;, # Number of unique tokens in the final vocabulary \u0026#39;vocab_size\u0026#39;: 4000, # \u0026#39;bpe\u0026#39; (Byte Pair Encoding) merges frequent pairs of characters/sequences \u0026#39;model_type\u0026#39;: \u0026#39;bpe\u0026#39;, # Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets \u0026#39;character_coverage\u0026#39;: 0.9995, # When enabled, unknown characters are decomposed into UTF-8 bytes to avoid \u0026#39;unk\u0026#39; tokens \u0026#39;byte_fallback\u0026#39;: True, # Treats digits individually (0-9), preventing large numbers from being treated as single tokens \u0026#39;split_digits\u0026#39;: True, # Prevents adding a whitespace prefix to the first token; useful for fine-grained control \u0026#39;add_dummy_prefix\u0026#39;: False } try: print(\u0026#34;Starting the training process...\u0026#34;) # SentencePieceTrainer.train takes the dictionary of options to build the BPE model spm.SentencePieceTrainer.train(**options) print(\u0026#34;Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created.\u0026#34;) # Initialize the processor and load the newly trained model sp = spm.SentencePieceProcessor() sp.load(\u0026#39;bpe_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Model Metadata:\u0026#34;) # Retrieve the total number of tokens in the vocabulary print(f\u0026#34;Total Vocab Size: {sp.get_piece_size()}\u0026#34;) # Special tokens are used for sequence boundaries and handling unknown characters print(f\u0026#34;BOS (Beginning of Sentence) ID: {sp.bos_id()}\u0026#34;) print(f\u0026#34;EOS (End of Sentence) ID: {sp.eos_id()}\u0026#34;) print(f\u0026#34;UNK (Unknown) ID: {sp.unk_id()}\u0026#34;) print(f\u0026#34;PAD (Padding) ID: {sp.pad_id()}\u0026#34;) # Test the tokenizer on sample strings to see how it breaks down text test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: shows the actual subword units (tokens) print(f\u0026#34;Subword Tokens: {sp.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: shows the numerical mapping for each token print(f\u0026#34;Numerical IDs: {sp.encode_as_ids(text)}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred during training or processing: {e}\u0026#34;) You can see the following output:\nStarting training... Starting the training process... Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created. ------------------------------ Model Metadata: Total Vocab Size: 4000 BOS (Beginning of Sentence) ID: 1 EOS (End of Sentence) ID: 2 UNK (Unknown) ID: 0 PAD (Padding) ID: -1 --- Tokenization Test --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;el\u0026#39;, \u0026#39;lo\u0026#39;, \u0026#39;‚ñÅW\u0026#39;, \u0026#39;orld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 380, 309, 476, 619, 36, 3924, 3964, 3959, 3993, 3978, 3983, 57, 3975, 3976, 60, 3974] --- Tokenization Test --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅmo\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;inst\u0026#39;, \u0026#39;ag\u0026#39;, \u0026#39;ram\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅever\u0026#39;] Numerical IDs: [1894, 304, 3284, 376, 321, 313, 312, 1008, 3313, 1325, 428, 3937, 341, 304, 3284, 1495] The ! is fallen back to UTF bytes. and the word instagrammable is split into multiple subword tokens. Also, original text can be recreated right back by simply concatenating the tokens. And replacing underscore by whitespace. Now we can try Unigram model. and see the same sentences tokenized differently.\nfrom tokenizers import Tokenizer from tokenizers.models import WordPiece from tokenizers.trainers import WordPieceTrainer from tokenizers.pre_tokenizers import Whitespace try: # 1. Initialize the WordPiece Tokenizer # We specify the [UNK] token for handling words not found in the vocabulary. tokenizer = Tokenizer(WordPiece(unk_token=\u0026#34;[UNK]\u0026#34;)) # 2. Configure Pre-tokenization # Before the subword algorithm runs, we need to split the raw text into words. # Whitespace splitting is the standard first step for most English NLP tasks. tokenizer.pre_tokenizer = Whitespace() # 3. Initialize the Trainer # We define our target vocabulary size and the special tokens required for # downstream tasks (like BERT\u0026#39;s [CLS] for classification or [SEP] for separators). trainer = WordPieceTrainer( vocab_size=4000, special_tokens=[\u0026#34;[UNK]\u0026#34;, \u0026#34;[CLS]\u0026#34;, \u0026#34;[SEP]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[MASK]\u0026#34;] ) # 4. Train the Model # The tokenizer scans the training file to build a vocabulary of the most # frequent subword units. tokenizer.train(files=[\u0026#34;train_data.txt\u0026#34;], trainer=trainer) # 5. Persist the Model # Save the configuration and vocabulary to a JSON file for future inference. tokenizer.save(\u0026#34;wordpiece.json\u0026#34;) print(\u0026#34;Training complete. \u0026#39;wordpiece.json\u0026#39; created.\u0026#34;) # 6. Metadata Inspection print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;WordPiece Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {tokenizer.get_vocab_size()}\u0026#34;) # 7. Testing Subword Tokenization # WordPiece shines at handling rare words by breaking them into meaningful chunks. test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (WordPiece) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # Encode converts raw text into a Tokenizer object containing tokens and IDs output = tokenizer.encode(text) # \u0026#39;tokens\u0026#39; shows the subword breakdown (e.g., \u0026#39;un\u0026#39;, \u0026#39;##insta\u0026#39;, etc.) print(f\u0026#34;Subword Tokens: {output.tokens}\u0026#34;) # \u0026#39;ids\u0026#39; are the numerical indices mapped to the vocabulary print(f\u0026#34;Numerical IDs: {output.ids}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred with WordPiece model: {e}\u0026#34;) You can see the following output:\nWordPiece Model Metadata: Total Vocab Size: 2609 --- Tokenization Test (WordPiece) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;H\u0026#39;, \u0026#39;##el\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;##or\u0026#39;, \u0026#39;##ld\u0026#39;, \u0026#39;[UNK]\u0026#39;, \u0026#39;[UNK]\u0026#39;] Numerical IDs: [37, 180, 214, 52, 162, 418, 0, 0] --- Tokenization Test (WordPiece) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;##os\u0026#39;, \u0026#39;##t\u0026#39;, \u0026#39;un\u0026#39;, \u0026#39;##ins\u0026#39;, \u0026#39;##ta\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;##ra\u0026#39;, \u0026#39;##m\u0026#39;, \u0026#39;##ma\u0026#39;, \u0026#39;##ble\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;ever\u0026#39;] Numerical IDs: [691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240] We can see the special character ## in the output. We can also see the breakdowns not having any semantic meaning. They are likelihood based. We can try the Unigram model now.\n# \u0026#39;Unigram\u0026#39; is the default and usually recommended over BPE in SentencePiece. options_unigram = { \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # Path to the raw text file for training \u0026#39;model_prefix\u0026#39;: \u0026#39;unigram_model\u0026#39;, # Prefix for the output .model and .vocab files \u0026#39;vocab_size\u0026#39;: 1200, # Desired size of the final vocabulary \u0026#39;model_type\u0026#39;: \u0026#39;unigram\u0026#39;, # Specifies the Unigram language model algorithm \u0026#39;character_coverage\u0026#39;: 0.9995, # Percentage of characters covered by the model (0.9995 is standard for Latin scripts) \u0026#39;byte_fallback\u0026#39;: True, # Enables mapping unknown characters to UTF-8 bytes to avoid \u0026lt;unk\u0026gt; tokens \u0026#39;split_digits\u0026#39;: True, # Treats each digit as an individual token (useful for numerical data) \u0026#39;add_dummy_prefix\u0026#39;: False # Prevents adding a leading space (SentencePiece default is True) } try: # 1. Train the SentencePiece model using the defined options print(\u0026#34;Starting Unigram training...\u0026#34;) spm.SentencePieceTrainer.train(**options_unigram) print(\u0026#34;Training complete. \u0026#39;unigram_model.model\u0026#39; created.\u0026#34;) # 2. Load the trained model into a processor instance for inference sp_unigram = spm.SentencePieceProcessor() sp_unigram.load(\u0026#39;unigram_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Unigram Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {sp_unigram.get_piece_size()}\u0026#34;) # 3. Define test cases to evaluate how the model handles common and rare words test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] # 4. Iterate through test sentences to visualize subword segmentation for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (Unigram) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: Converts text into subword strings (visual representation) print(f\u0026#34;Subword Tokens: {sp_unigram.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: Converts text into numerical indices for model input print(f\u0026#34;Numerical IDs: {sp_unigram.encode_as_ids(text)}\u0026#34;) except Exception as e: # Handle potential errors during training or loading (e.g., missing input file) print(f\u0026#34;An error occurred with Unigram model: {e}\u0026#34;) The output is as follows:\nUnigram Model Metadata: Total Vocab Size: 1200 --- Tokenization Test (Unigram) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ll\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;ld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311] --- Tokenization Test (Unigram) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;T\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅm\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;sta\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;ra\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ver\u0026#39;] ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"A comprehensive guide to tokenization strategies: BPE, WordPiece, Unigram, and SentencePiece.","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"A brief introduction to Vectors \u0026amp; Verbs and formatting verification.","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"\nFeedforward network (FFN) blocks are the least talked about when in context to LLMs. But that is where more than 70% of the paramters come from. Something as simple and convinient as a couple of linear transforming matrices become a burden when models start scaling in Billions of parameters. So, getting the maximum juice of a lean structure is a challenge. Plus, mostly under represented, because the FFN sub-layer acts as the primary \u0026ldquo;knowledge\u0026rdquo; processing engine, often containing the majority of a model\u0026rsquo;s total parameters.\nStandard Position Wise Feedforward Network This is as simple as it can get, two-layered fully connected network of neurons with a single activation switch, most probably a ReLU variant (for LLMs). $$ FFN(x) = max(0, W_2(W_1(x)) + b_1)W_2 + b_2 $$ Same transformation is applied for each token in parallel, making it efficient for GPU computation. For Example, in BERT model the 512 dimensions of each token are expanded to 2048 to allow for \u0026ldquo;intricate interactions\u0026rdquo; before projecting back to 512 dimensions. This part is important, in every transformer stacking of layers is possible only because the output of every matches it\u0026rsquo;s input dimensions.\nSWiGLU Sigmoid Weighted Gated Linear Unit (SWiGLU) has become the Gold Standard for feed forward networks in LLMs like Llama 3 and PaLM. The primary reason for this choice is the Swish function (as shown in the first image) instead of ReLU or GELU are as following: Mathematical Linearity: Swish (SILU) is defined as $x\\cdot sigmoid(\\beta x)$, $\\beta$ is a learnable parameter. It is a small monotonic function that allowes small negetive values to pass through, making it mathematically linear. While GELU ($x \\cdot \\phi(x)$) is also smooth and nonmonotonic, Swish\u0026rsquo;s curve and equations have emperically improved the model\u0026rsquo;s performance. Computational Efficiency: SILU is simpler to compute. $x/(1+exp(-x))$ compared to the standard GELU, which is an approximation of the tanh function. In the massive scale of modern LLMs these shortcuts save significant training time. Performance: Research in the PaLM and Llama papers found that Swish-based Gated Linear Units consistently outperformed GELU-based versions in terms of perplexity and downstream task accuracy. $$SwiGLU(x, W, V, b, c) = (Swish(xW + b) \\otimes (xV + c))W_2$$ Where $W$ and $V$ are weight matrices for the gate and the data path, and $\\otimes$ is element-wise multiplication\n","permalink":"http://localhost:1313/posts/feedforward-networks/","summary":"Exploring the hidden layers of trillion-parameter switchboards: Feedforward Neural Networks and Activation Functions.","title":"Anatomy of Trillion-Parameter Switchboards: Understanding Feedforward Blocks"},{"content":"\nWe are about to touch the holy grail of modern AI. The 2017 declaration was simple: Attention Is All You Need. But as we‚Äôve raced from GPT-1 to reasoning models, the definition of \u0026lsquo;Attention\u0026rsquo; has quietly transformed. Is the algorithm ruling the world today actually the same one we started with? Let‚Äôs look under the hood.\nüî• The Core Concept At its heart, the attention mechanism is a form of weighted aggregation. It is often described using a database retrieval analogy. Imagine you have a database of information (context).\nQuery ($Q$): What are you currently looking for? (The token being generated). Key ($K$): The indexing tag associated with each piece of information. Value ($V$): The actual information (The context token). The \u0026ldquo;Attention\u0026rdquo; is the process of matching your Query against all the Keys to determine how much \u0026ldquo;weight\u0026rdquo; to put on each Value. Attention scores are calculated in each transformer layer at every step of the sequence. This is a single unifying factor in all transformer models, from BERT to Gemini.\nüî• Standard Multi-Head Attention (MHA) The original mechanism introduced in the paper \u0026ldquo;Attention Is All You Need\u0026rdquo;.\nHere, instead of performing the Key-Query-Value (KQV) operation on the entire context only once, the attention is split into $N$ parallel heads. Each head learns a different aspect of the language (e.g., word meaning, syntax, semantic relationships, etc.). The equation is a simple culmination of dot products aided by a regularizer like Softmax.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nWhere:\n$QK^T$ computes the similarity scores between the Query and Key. $\\sqrt{d_k}$ is a scaling factor to prevent the dot product from growing too large, mitigating the vanishing/exploding gradient problem. The weighted sum is applied to the values of $V$. The Memory Bandwidth Bottleneck: \u0026gt; During inference, the model must store Key and Value matrices for all previous tokens. This is called the KV Cache. In MHA, every single head has its own unique set of Keys and Values. For large models with massive context windows, loading this massive KV cache from memory (HBM) to the computation chip (SRAM) becomes the primary bottleneck, slowing down generation significantly. Earlier models could often only generate ~8 tokens per second at inference due to this constraint.\nüî• Multi-Query Attention (MQA) This approach addresses the memory bottleneck by sharing the same set of Keys and Values across all heads while keeping separate Query heads.\nMHA: $H$ Query heads, $H$ Key heads, $H$ Value heads MQA: $H$ Query heads, 1 Key head, 1 Value head This drastically reduces the memory requirements of the KV cache by a factor of $H$ (often 8x or 16x). The GPU needs to load far less data, significantly speeding up inference. However, it does not reduce the computation (FLOPs)‚Äîso why is it faster?\nThis can be explained with the Kitchen Analogy: Imagine a kitchen (GPU) with 8 chefs (Heads).\nComputation (Math): The Chef chopping vegetables. Memory Access (Bandwidth): The Assistant running to the fridge to get ingredients. In MHA: Each chef demands unique vegetables from the fridge. The assistant runs back and forth 8 times, fetching 8 different crates. Most of the time is spent waiting for the assistant to return. This results in idle cores waiting for data to load from HBM.\nIn MQA: All chefs agree to use the same vegetables. The assistant runs back and forth only once, fetching one giant crate of potatoes and dumping it. This results in minimized memory transfer and no idle cores.\nThe Technical Reality: During text generation, the model is memory-bound, not compute-bound. The time it takes to move the KV Cache from HBM (High Bandwidth Memory) to the chip\u0026rsquo;s SRAM is the bottleneck. MQA reduces the data volume of that move by a factor of $H$.\nThe Capacity Gap: MQA is a trade-off between nuanced response and memory capacity. The model loses the ability to store different types of information, as a single $V$ matrix must now store all nuances of information flow in the language.\nThe Surgery Trick (Uptraining): Google Research figured out a way to take an existing model with MHA and convert it to MQA by averaging the weight matrices of $H$ heads. This is called Uptraining. You then train this \u0026ldquo;Frankenstein\u0026rdquo; model for ~5% of the original training steps for adjustments. This yields an MQA model that is nearly as good as the original MHA but vastly faster.\nüî• Grouped Query Attention (GQA) A \u0026ldquo;best of both worlds\u0026rdquo; solution used in modern models like Llama 2 and Llama 3.\nHow it works: GQA acts as an interpolation between MHA and MQA. Instead of having just 1 Key head (MQA) or $H$ Key heads (MHA), it divides the query heads into $G$ groups. Each group shares a single Key and Value head.\nEquation modification: If you have 8 query heads, you might create 4 groups (2 queries per group). This reduces the KV cache by 2x instead of 8x, preserving more quality than MQA while still speeding up inference.\nMHA: 8 people (heads) are working on a project. Each person has their own unique filing cabinet (KV cache). It takes forever to move 8 cabinets around. MQA: 8 people share one single filing cabinet. It\u0026rsquo;s very fast to move, but they fight over the organization and lose detail. GQA: The 8 people split into 4 teams of 2. Each team shares a cabinet. It\u0026rsquo;s a balance between speed and organizational detail. üî• Multi-Head Latent Attention (MLA) This is the most advanced mechanism for attention, used in models like DeepSeek-V3. It is a sophisticated design for massive efficiency that takes a radical approach: mathematically compressing the memory into a low-rank \u0026ldquo;latent\u0026rdquo; vector.\nKV Cache Explosion: As mentioned before, for a massive model like DeepSeek-V3 (671B parameters), storing the full Keys and Values for every token in a long conversation requires terabytes of high-speed memory (HBM), and chips spend more time waiting than churning.\nMHA (Standard) stores everything. MQA deletes redundancy but hurts quality. MLA keeps the quality of MHA but compresses the data 4x-6x smaller than even GQA.\nThe Zip File Approach: Low-Rank Joint Compression Let\u0026rsquo;s compare the math of MHA and MLA. In MHA, for every single token, we generate a unique Key ($k$) and Value ($v$) matrix for each attention head.\nThe Memory Problem: The $k$ is stored in the KV Cache. For 128 heads, for example, it calculates a massive vector of $128 \\times 128 = 16,384$ floats per token.\nFor MHA, the attention score looks like: $$\\text{Score} = q^T \\cdot k$$\nBut for MLA, instead of storing the full $k$, it projects the input down into a tiny latent vector $c_{KV}$, which is much smaller (e.g., 512 floats). $$c_{KV} = W_{DKV} \\cdot x$$\nNow here is the catch: Naive compression and decompression would mean constructing a small latent matrix for every token and deconstructing it every single time the information is required, which defeats the purpose of compression.\n$$k_{\\text{reconstructed}} = W_{UK} \\cdot c_{KV}$$ $$\\text{Score} = q^T \\cdot k_{\\text{reconstructed}}$$\nThe Matrix Absorption Trick (Optimization) Instead of reconstructing $k$ for every token, we can absorb the Up-Projection matrix ($W_{UK}$) into the Down-Projection matrix ($W_{DKV}$) and absorb the query $q$ into a new $Q$.\nFrom the original equation: $$\\text{Score} = q^T \\cdot (\\underbrace{W_{UK} \\cdot c_{KV}}_{\\text{This is } k})$$\nWe associate differently:\n$$\\text{Score} = (\\underbrace{q^T \\cdot W_{UK}}_{\\text{Absorbed Query}}) \\cdot c_{KV}$$ We change the order in which the matrix multiplication is performed.\nHow is this allowed? In linear algebra, matrix multiplication is associative:\n$$(A \\cdot B) \\cdot C = A \\cdot (B \\cdot C)$$ We can move the parentheses! Instead of grouping $(W_{UK} \\cdot c_{KV})$ to make the Key, we group $(q^T \\cdot W_{UK})$ to make a new Query.\nNote: The positional embedding information is left untouched. That rotational information from algorithms like RoPE is preserved outside the compression; it requires minimal space anyways.\n","permalink":"http://localhost:1313/posts/attention-mechanisms/","summary":"We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek\u0026rsquo;s MLA, how has the definition of \u0026lsquo;Attention\u0026rsquo; transformed?","title":"Attention Is All You Need, but exactly which one?: MHA, GQA and MLA"},{"content":"\nPositional Embeddings are the \u0026ldquo;voice\u0026rdquo; that tell Transformers where words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.\n\u0026ldquo;The dog chased the cat\u0026rdquo; and \u0026ldquo;cat chased the dog\u0026rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.\nüî• Sinusoidal Embeddings: The Odometer of Language Introduced in the seminal \u0026ldquo;Attention Is All You Need\u0026rdquo; paper, this was the industry standard for many years. This approach adds a vector to each word embedding based on its position. The vector is computed out of wavelike patterns that are periodic and smooth. Mathematically, for a position $pos$ and embedding dimension $i$:\n$$ PE_{(pos, i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $$\nThe model uses sine and cosine waves of different frequencies to capture both short and long-range dependencies. The frequency of the waves decreases as the embedding dimension increases, which helps capture long-range dependencies in the input sequence.\nThe creators wanted the model to calculate distance easily, which is why a linear function was used. The model simply rotates the current position vector using a rotation matrix to derive the next position.\n$$ \\begin{pmatrix} \\cos(k) \u0026 \\sin(k) \\\\ -\\sin(k) \u0026 \\cos(k) \\end{pmatrix} \\cdot PE(pos) \\approx PE(pos + k) $$ Low dimensions act like a fast-ticking clock (high frequency, changing rapidly) and high dimensions act like a slow clock (low frequency, changing slowly).\nThe number 10,000 is an arbitrarily chosen high number to reduce Aliasing. By choosing such a high number, the creators ensured that the wavelength of the slowest dimension is incredibly long. Consequently, the pattern practically never repeats, and each position remains unique.\nThis combination works like a mechanical odometer dial. Each subsequent dimension acts as a slower-moving gear. The model learns to read these \u0026ldquo;dials\u0026rdquo; to understand exactly where a word sits in the sequence.\nThis mitigated the challenge of exploding gradients that previous array-based positional embeddings suffered from (e.g., adding integers 1, 2, 3\u0026hellip;). This model keeps values between -1 and +1, adding stability while ensuring every position gets a distinct code.\nThe Problem: This approach pollutes the word embedding matrix (semantic information). While high-dimensional vectors are almost always orthogonal‚Äîmeaning the noise doesn\u0026rsquo;t fully destroy the word\u0026rsquo;s identity‚Äîit does not strictly guarantee purity.\nThis led to a more mature approach.\nüî• ALiBi: The Fading Streetlight Attention with Linear Biases (ALiBi) is a method where, instead of adding a vector to the embeddings at the start, the model injects positional information linearly, directly into the attention scores. If Sinusoidal Encoding asks, \u0026ldquo;Where am I on the map?\u0026rdquo;, ALiBi asks, \u0026ldquo;How much harder should I squint to see you?\u0026rdquo;\nThe biggest problem ALiBi solved is the \u0026ldquo;Invisible Wall\u0026rdquo;. If you train a Transformer on sequences of length 2048 and then feed it a book with 3000 words, it will crash or output garbage at word 2049. This is a failure of Extrapolation.\nALiBi throws away the idea of adding positional vectors entirely. Instead, it devises a \u0026ldquo;penalty\u0026rdquo; system for attention scores:\n$$ \\text{Score} = (q \\cdot k^\\top) - (m \\cdot \\text{distance}) $$\nDistance: If words A and B are neighbors, the distance is 0, so the penalty is 0. Although the distance is linear, it does not cause exploding gradients because the raw attention scores are normalized by a softmax function. This distance prioritizes nearby words (local context) over distant ones.\nSlope ($m$): If we always penalize distance equally, the model becomes myopic. ALiBi mitigates this by assigning a different slope value to each attention head:\nHead 1 (The Historian): Very low slope. The penalty grows slowly, allowing it to see far back. Head 2 (The Reader): Medium slope. Head 3 (The Myopic): Very high slope (e.g., 0.5); it effectively cannot see beyond 2-3 words. This mimics holding a lantern. Words close to you are bright, while words further away naturally fade into darkness.\nExtrapolation: If you walk 5 miles down the road (a longer sequence), your lantern works exactly the same way. The physics of light decay are constant, allowing the model to handle sequences longer than it was trained on.\nHowever, ALiBi forces a \u0026ldquo;sliding window\u0026rdquo; view where distant words eventually become invisible. This brings us to the modern standard.\nüî• RoPE: The Rotary Revolution Rotary Position Embeddings (RoPE) is the current industry standard (used in Llama 3, Mistral, PaLM) because it mathematically unifies \u0026ldquo;absolute position\u0026rdquo; with \u0026ldquo;relative position.\u0026rdquo; This method rotates the vectors in the Query and Key matrices based on their absolute position.\nIf a vector has 512 dimensions, RoPE breaks it into 256 pairs. Each pair is rotated by a different frequency:\n$$ \\theta_i = 10000^{-2i/d} $$\nThe First pair ($i = 0$) is rotated by the highest frequency. The Last pair ($i = 255$) is rotated by the lowest frequency. This spread ensures the model has \u0026ldquo;high precision\u0026rdquo; (fast rotation) to distinguish immediate neighbors, and \u0026ldquo;low precision\u0026rdquo; (slow rotation) to track global position without the pattern repeating.\nDoesn\u0026rsquo;t rotation change meaning? Yes and No. The vector for \u0026ldquo;Apple\u0026rdquo; at position 1 and \u0026ldquo;Apple\u0026rdquo; at position 256 will look completely different in terms of coordinates. However, rotation does not change the semantic strength (magnitude/norm) of the vector.\nThe Relativity Trick: Transformers compare words using dot products (angles). RoPE relies on the fact that the attention mechanism ignores the absolute rotation and only detects the difference in rotation (relative position). Thus, the \u0026ldquo;relative meaning\u0026rdquo; is encoded purely into the angle between words, leaving the semantic \u0026ldquo;magnitude\u0026rdquo; untouched.\nüî• LongRoPE: The Bifocal Lens While RoPE is powerful, it struggles when stretching context windows from 4k to 2 million tokens. The \u0026ldquo;fast\u0026rdquo; dimensions spin so rapidly over long distances that they become random noise. LongRoPE solves this using an evolutionary search algorithm to find a unique scaling factor ($\\lambda$) for each dimension.\nThe Equations Instead of rotating by the standard $\\theta_i$, LongRoPE rotates by a rescaled frequency:\n$$ \\theta\u0026rsquo;_i = \\lambda_i \\theta_i $$\nTo efficiently find these $\\lambda$ values without searching an infinite space, the algorithm enforces a monotonicity constraint based on NTK theory:\n$$ \\lambda_i \\le \\lambda_{i+1} $$\nThis ensures that low-frequency dimensions (global context) are stretched more than high-frequency dimensions (local context), creating a \u0026ldquo;bifocal\u0026rdquo; effect:\nHigh Frequencies (Local): Kept sharp ($\\lambda \\approx 1$) to maintain grammar. Low Frequencies (Global): Stretched ($\\lambda \u0026gt; 1$) to track massive distances without repeating. Not worth perusing more since the next one is much better at long context positional encoding. üî• HoPE: The Hyperbolic Slide Hyperbolic Rotary Positional Encoding (HoPE) moves from the geometry of a circle to the geometry of a hyperbola (inspired by Lorentz transformations). It was designed to fix a subtle flaw in RoPE: the \u0026ldquo;wobbly\u0026rdquo; or oscillatory nature of attention scores.\nThe Core Problem: RoPE\u0026rsquo;s \u0026ldquo;Wobble\u0026rdquo; In standard RoPE, we rotate vectors around a circle.\nThe Issue: Circles repeat. A dial at $361^\\circ$ looks identical to $1^\\circ$. Even with different frequencies, the dot product fluctuates up and down as distance increases. The Consequence: The model gets confused at long distances, potentially mistaking a word 1,000 tokens away for a neighbor just because the rotation cycles aligned, creating \u0026ldquo;noise.\u0026rdquo; The \u0026ldquo;Hyperbolic\u0026rdquo; Shift HoPE replaces trigonometric functions ($\\sin, \\cos$) with hyperbolic functions ($\\sinh, \\cosh$).\nCircular Rotation: Keeps distance from the center constant. Hyperbolic Rotation: Moves points along a hyperbola. Crucially, hyperbolic functions do not repeat; they grow or decay exponentially. By using hyperbolic rotation, HoPE guarantees that as words get further apart, the attention score decays monotonically (smoothly drops).\nThe Equations HoPE uses a hyperbolic matrix $B(\\theta, m)$ defined using the Lorentz Boost structure:\n$$ B(\\theta, m) = \\begin{pmatrix} \\cosh(m\\theta) \u0026 \\sinh(m\\theta) \\\\ \\sinh(m\\theta) \u0026 \\cosh(m\\theta) \\end{pmatrix} $$ Since hyperbolic functions grow to infinity, HoPE introduces a decay penalty ($e^{-m\\theta'}$). The final attention score simplifies beautifully to a function of relative distance: $$ \\text{Score} \\propto e^{-|m-n|(\\theta\u0026rsquo; - \\theta)} $$\nThis ensures the attention score exponentially decays as distance increases, eliminating the wobbles of standard RoPE. This the latest model that is taking up the space in the industry for extremely long context models.\n","permalink":"http://localhost:1313/posts/positional-embeddings/","summary":"From Sinusoidal to RoPE and HoPE: How Transformers learn to process word order and sequence length.","title":"The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE"},{"content":"Imagine trying to teach a computer the difference between \u0026ldquo;Apple\u0026rdquo; the fruit and \u0026ldquo;Apple\u0026rdquo; the company. To us, the distinction is intuitive. To a machine, it‚Äôs just a string of characters. How do we turn these strings into meaningful math? While early attempts like Word2Vec gave us a great start, they missed the forest for the trees‚Äîor in some cases, the twigs for the branches. Enter GloVe and FastText: two algorithms that revolutionized how machines understand the nuances of human language.\nPrevious static embedding models like Word2Vec successfully captured the local semantics of words, but they failed in addressing contexts between words that might not always appear in the same context window. Furthermore, they treated words as indivisible atomic units in continuous vector spaces, missing out on the internal structure of language.\nThe two simple but effective ideas behind GloVe and FastText are subword units and global co-occurrence. Let\u0026rsquo;s dive into them.\nGloVe: A Global Mapping Strategy Global Vectors for Word Representation (GloVe) was created at Stanford. The core idea was to train a model where word vectors are defined by how often they appear around other words across the entire corpus, rather than just locally.\nTraining Steps: Co-occurrence Matrix X: GloVe scans the entire corpus and creates a giant matrix (spreadsheet) recording the counts of how often words appear near each other. Factorization: It compresses this giant matrix into smaller, dense vectors. Objective: The function tries to equate the dot product of two word vectors with the logarithm of their probability of appearing together. The error between these values is then backpropagated to update the vectors. $$w_i \\cdot w_j + b_i + b_j = \\log(X_{ij})$$\n$w$: The word vector (what we want). $X_{ij}$: The count from our matrix. The Loss Function: We can\u0026rsquo;t satisfy the equation above perfectly for every word pair. So, we use a weighting function to minimize the squared error:\n$$J = \\sum_{i,j=1}^V f(X_{ij}) ({w_i}^T w_j + b_i + b_j - \\log(X_{ij}))^2$$\nIf we do not use the weighting function, the loss would be infinite for rare words (since $\\log(0) = \\infty$) and frequent words (like \u0026rsquo;the\u0026rsquo;) would overpower the model.\n$$ f(x) = \\begin{cases} (x/x_{max})^\\alpha \u0026amp; \\text{if } x \u0026lt; x_{max} \\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$\n(Typically $x_{max}=100$ and $\\alpha=0.75$)\nFastText: The Lego Bricks Strategy FastText was created by Facebook AI Research (FAIR). The idea was to treat words not as solid rocks, but as structures made of \u0026ldquo;Lego bricks\u0026rdquo; (subword units).\nWord2Vec struggled to create embeddings for words it hadn\u0026rsquo;t seen before (Out-Of-Vocabulary or OOV words). FastText solves this by breaking words down.\nGloVe treats a word like \u0026ldquo;Apple\u0026rdquo; as a single, atomic unit. FastText breaks \u0026ldquo;Apple\u0026rdquo; into character n-grams: \u0026lt;ap, app, ppl, ple, le\u0026gt; For a complex or new word like \u0026ldquo;Applesauce\u0026rdquo;, the model might not know the whole word, but it recognizes the Lego brick for \u0026ldquo;Apple\u0026rdquo; and the remaining bits from other subwords.\nFastText takes its inspiration from the SkipGram architecture. The vector for a word is simply the sum of the vectors of its n-gram subwords:\n$$v_w = \\sum_{g \\in G_w} z_g$$\nObjective: The objective is to predict context words given a target word.\n$$P(w_c | w_t) = \\frac{e^{s(v_{w_t} \\cdot v_{w_c})}}{\\sum_{w \\in V} e^{s(v_{w_t} \\cdot v_w)}}$$\nAnd the scoring function is defined as:\n$$s(w_t, w_c) = u_{w_t}^T (\\sum_{g \\in G_w} z_g)$$\n$u_{w_c}$: The vector for the context word (output vector). $z_g$: The vector for the n-gram $g$ (input vector). Modifications to Softmax: Calculating the full softmax for every word in the vocabulary is computationally expensive. FastText uses two methods to approximate this efficiently.\nOption A: Negative Sampling\nWe pick a few random \u0026ldquo;wrong\u0026rdquo; words every time to serve as \u0026ldquo;negative samples\u0026rdquo; and calculate the loss only for the correct word and these noise words.\n$$J = - \\log \\sigma(s(w_t, w_c)) - \\sum\\limits_{i=1}^{N} \\mathbb{E}_{n_i \\sim P_n(w)} [\\log \\sigma(-s(w_t, n_i))]$$\nThe sigmoid function $\\sigma$ squashes the score between 0 and 1. The first term pushes the probability of the correct word towards 1. The second term pushes the probability of the noise words towards 0. Option B: Hierarchical Softmax (Faster for infrequent words)\nInstead of a flat list of words, imagine the vocabulary arranged as a binary tree (Huffman Tree).\nRoot: Top of the tree. Leaves: The actual words. To calculate the probability of a word, we trace a path from the root to that leaf. At every branching node, we calculate the probability (sigmoid) of going left or right. The probability of a word $w$ is the product of the probabilities of the turns taken to reach it. Code Example from gensim.models import FastText from gensim.test.utils import common_texts # Initialize the FastText model with specific hyperparameters model = FastText( vector_size=100, # Dimensionality of the word vectors window=5, # Maximum distance between the current and predicted word within a sentence min_count=1, # Ignores all words with total frequency lower than this sg=1, # Training algorithm: 1 for skip-gram; 0 for CBOW hs=0, # If 0, and negative is non-zero, negative sampling will be used negative=5, # Number of \u0026#34;noise words\u0026#34; to be drawn for negative sampling workers=4, # Number of worker threads to train the model epochs=10, # Number of iterations over the corpus min_n=3, # Minimum length of character n-grams max_n=6 # Maximum length of character n-grams ) # Build the vocabulary from the provided text corpus model.build_vocab(common_texts) # Train the model on the corpus model.train( common_texts, total_examples=model.corpus_count, epochs=model.epochs ) # Calculate and print the cosine similarity between two words in the vocabulary print(f\u0026#34;Similarity between \u0026#39;computer\u0026#39; and \u0026#39;human\u0026#39;: {model.wv.similarity(\u0026#39;computer\u0026#39;, \u0026#39;human\u0026#39;)}\u0026#34;) # Demonstrate FastText\u0026#39;s ability to handle Out-Of-Vocabulary (OOV) words # Even if \u0026#39;computation\u0026#39; wasn\u0026#39;t in the training data, FastText constructs a vector using character n-grams oov_vector = model.wv[\u0026#39;computation\u0026#39;] # Check if a specific word exists in the model\u0026#39;s fixed vocabulary index word = \u0026#34;computer\u0026#34; is_in_vocab = word in model.wv.key_to_index print(f\u0026#34;\\nIs the word \u0026#39;{word}\u0026#39; in the model\u0026#39;s vocabulary? {is_in_vocab}\u0026#34;) ","permalink":"http://localhost:1313/posts/glove-and-fasttext/","summary":"How subword units and global co-occurrence matrices allow GloVe and FastText to capture nuances that Word2Vec missed.","title":"The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText"},{"content":" Before we had Large Language Models writing poetry, we had to teach computers that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; are related not just by spelling, but by meaning. This is the story of that breakthrough. It‚Äôs the moment we stopped counting words and started mapping their souls‚Äîturning raw text into a mathematical landscape where math can solve analogies. Welcome to the world of Word2Vec. üîÆ\nLanguage models require vector representations of words to capture semantic relationships. Before the 2010s, models used word count-based vector representations that captured only the frequency of words (e.g., One-Hot Encoding). The Problems: üöß\nThe Curse of Dimensionality Lack of meaning (Synonyms were treated as mathematically unrelated). In 2011, Mikolov et al. at Google introduced Word2Vec, which used a shallow neural network to learn vector representations of words. The best part? It could create much denser relationships between words than prior models, and it was unsupervised.\nThe training (prediction) is essentially \u0026ldquo;fake\u0026rdquo;‚Äîa pretext task. But the weight matrices we get on the side are a gold mine of fine-grained semantic relationships. ‚õèÔ∏è\nThere are two popular variants of Word2Vec:\nSkipGram Continuous Bag Of Words (CBOW) 1. Continuous Bag of Words (CBOW) üß© CBOW is a simple \u0026lsquo;fill in the blanks\u0026rsquo; machine. It takes the surrounding words as inputs and tries to predict the center word.\n\u0026ldquo;Continuous\u0026rdquo;: It operates in continuous vector space (unlike the discrete space of n-gram models).\n\u0026ldquo;Bag of words\u0026rdquo;: The order of the context does not matter. \u0026ldquo;The cat sat on the mat\u0026rdquo; and \u0026ldquo;The mat sat on the cat\u0026rdquo; are the same for CBOW. They produce exactly the same predictions.\nObjective: Maximize the probability of the target word $w_t$ given its context $w_{t-C}, \\dots, w_{t+C}$.\n2. Training Defining Dimensions:\n$V$: Vocabulary size (number of unique words in the corpus, e.g., 10,000) $N$: Dimension of the embedding space (e.g., 100) $C$: Number of context words (e.g., 2 words on left of target and 2 words on right) Step 1: Input Lookup \u0026amp; One-Hot Encoding\nMathematically, we represent each context word as a one-hot vector $x^{(c)}$. We have an Input Matrix $W_{in}$ of size $V \\times N$. For each context word (e.g., \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;), we select its corresponding row from $W_{in}$.\n$$ v_c = W_{in}^T x^{(c)} $$\nWhere $v_c$ is the $N$-dimensional vector for context word $c$.\nStep 2: Projection (The \u0026ldquo;Mixing\u0026rdquo;)\nWe combine the context vectors. In standard CBOW, we simply average them. This creates the hidden layer vector $h$.\n$$ h = \\frac{1}{2C} \\sum_{c=1}^{2C} v_c $$\n$h$ is a single vector of size $N \\times 1$. Note: This operation is linear and contains no activation function (like ReLU or Sigmoid).\nStep 3: Output Scoring (The \u0026ldquo;Dot Product\u0026rdquo;)\nWe have a second matrix, the Output Matrix $W_{out}$ of size $N \\times V$. We compute the raw score (logit) $u_j$ for every word $j$ in the vocabulary by taking the dot product of the hidden vector $h$ and the output vector $v\u0026rsquo;_j$.\n$$ u_j = {v\u0026rsquo;_j}^T h $$\nOr in matrix form:\n$$ u = W_{out}^T h $$\nResult $u$ is a vector of size $V \\times 1$.\nStep 4: Probability Conversion (Softmax)\nWe convert the raw scores $u$ into probabilities using the Softmax function. This tells us the probability that word $w_j$ is the center word.\n$$ y_j = P(w_j | \\text{context}) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 5: Loss Calculation (Cross-Entropy)\nWe compare the predicted distribution $y$ with the actual ground truth $t$ (a one-hot vector where the index of the true word is 1). The loss function $E$ is:\n$$ E = - \\sum_{j=1}^V t_j \\log(y_j) $$\nSince $t_j$ is 0 for all words except the target word (let\u0026rsquo;s call the target index $j^*$), this simplifies to:\n$$ E = - \\log(y_{j^*}) $$\n3. Backpropagation Details We need to update the weights to minimize $E$. We use the Chain Rule.\nA. Gradient w.r.t. Output Matrix ($W_{out}$)\nWe want to know how the error changes with respect to the raw score $u_j$.\n$$ \\frac{\\partial E}{\\partial u_j} = y_j - t_j $$\nLet\u0026rsquo;s call this error term $e_j$.\nIf $j$ is the target word: $e_j = y_j - 1$ (Result is negative; we boost the score). If $j$ is not target: $e_j = y_j - 0$ (Result is positive; we suppress the score). The update rule for the output vector $v\u0026rsquo;_j$ becomes:\n$$ v'_{j}(\\text{new}) = v'_{j}(\\text{old}) - \\eta \\cdot e_j \\cdot h $$ ($\\eta$ is the learning rate)\nB. Gradient w.r.t. Hidden Layer ($h$)\nWe backpropagate the error from the output layer to the hidden layer.\n$$ EH = \\sum_{j=1}^V e_j \\cdot v\u0026rsquo;_j $$\n$EH$ is an $N$-dimensional vector representing the aggregate error passed back to the projection layer.\nC. Gradient w.r.t. Input Matrix ($W_{in}$)\nSince $h$ was just an average of the input vectors, the error $EH$ is distributed to each context word\u0026rsquo;s input vector. For every word $w_c$ in the context:\n$$ v_{w_c}(\\text{new}) = v_{w_c}(\\text{old}) - \\eta \\cdot \\frac{1}{2C} \\cdot EH $$\nLet\u0026rsquo;s build a single pass through the network for a given context.\nimport torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # Define a simple Continuous Bag of Words (CBOW) style model class CBOWModel(nn.Module): def __init__(self, vocab_size, embedding_dim): super(CBOWModel, self).__init__() # The embedding layer stores the word vectors we want to learn self.embeddings = nn.Embedding(vocab_size, embedding_dim) # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word self.linear = nn.Linear(embedding_dim, vocab_size) def forward(self, inputs): # inputs: tensor of word indices for the context embeds = self.embeddings(inputs) # Aggregate context by calculating the mean of the word embeddings h = torch.mean(embeds, dim=1) # Produce logits (raw scores) for each word in the vocabulary logits = self.linear(h) return logits # Setup vocabulary and mappings word_to_ix = {\u0026#34;the\u0026#34;: 0, \u0026#34;quick\u0026#34;: 1, \u0026#34;brown\u0026#34;: 2, \u0026#34;fox\u0026#34;: 3, \u0026#34;jumps\u0026#34;: 4, \u0026#34;over\u0026#34;: 5, \u0026#34;lazy\u0026#34;: 6, \u0026#34;dog\u0026#34;: 7} ix_to_word = {v: k for k, v in word_to_ix.items()} # Configuration constants EMBEDDING_DIM = 5 VOCAB_SIZE = len(word_to_ix) LEARNING_RATE = 0.01 # Initialize the model, loss function, and optimizer model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM) loss_function = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE) # Prepare dummy training data: context words and the target word # Context: [\u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;over\u0026#34;] -\u0026gt; Target: \u0026#34;fox\u0026#34; context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long) target_idxs = torch.tensor([3], dtype=torch.long) # Perform a single optimization step model.zero_grad() # Forward pass: we reshape context to (1, -1) to simulate a batch of size 1 logits = model(context_idxs.view(1, -1)) # Calculate loss against the target word index loss = loss_function(logits, target_idxs) # Backpropagate and update weights loss.backward() optimizer.step() # Output results for the blog post print(\u0026#34;=== Model Training Snapshot ===\u0026#34;) print(f\u0026#34;Calculated Loss: {loss.item():.6f}\u0026#34;) print(f\u0026#34;\\n=== Learned Vector for \u0026#39;jumps\u0026#39; ===\u0026#34;) word_vec = model.embeddings(torch.tensor([word_to_ix[\u0026#39;jumps\u0026#39;]])) print(word_vec.detach().numpy()) print(\u0026#34;\\n=== Embedding Matrix (Weights) ===\u0026#34;) print(model.embeddings.weight.detach().numpy()) print(\u0026#34;\\n=== Linear Layer Weights ===\u0026#34;) print(model.linear.weight.detach().numpy()) Output shows single pass loss and learned vector for \u0026lsquo;jumps\u0026rsquo; word. This is a single training sample result but after multiple such passes both Embedding and linear weights should be very similar.\n=== Model Training Snapshot === Calculated Loss: 1.989656 === Learned Vector for \u0026#39;jumps\u0026#39; === [[-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775]] === Embedding Matrix (Weights) === [[ 1.66039 -0.11371879 0.6246518 0.35860053 -1.2504417 ] [-2.1847186 -0.77199775 -0.17050214 -0.38411248 -0.03913084] [ 0.11852697 0.90073633 0.8847807 0.7404524 0.900149 ] [ 0.07440972 -0.40259898 2.6246994 -0.08851447 0.02660969] [-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775] [-0.9245572 -0.5545908 0.9083091 -1.0755049 0.84047747] [-1.0237687 0.59466314 0.05621134 -0.6202532 1.3664424 ] [ 0.60998917 -1.0549186 1.6103884 0.8724912 -1.2486908 ]] === Linear Layer Weights === [[ 0.02165058 -0.28883642 0.14545658 -0.3442509 0.32704315] [-0.18731792 0.28583744 0.22635977 0.13245736 0.29019794] [ 0.3158916 -0.15826383 -0.03203773 0.16377363 -0.41457543] [-0.05080034 0.4180087 0.11228557 0.30218413 0.3025514 ] [-0.38419306 0.24475925 -0.39210224 -0.38660625 -0.2673145 ] [-0.32321444 0.12200444 -0.03569533 0.2891424 -0.07345333] [-0.33704326 0.2521956 0.31587374 0.22590035 0.29866052] [ 0.4117266 -0.44231793 0.24064957 -0.29684234 0.333821 ]] Interesting Facts About CBOW üß† The \u0026ldquo;Averaging Problem\u0026rdquo;: This is a common issue in CBOW models. The averaging and training is order-agnostic. All variants of word order in the context window will generate the same output. \u0026ldquo;Dog bit the man\u0026rdquo; and \u0026ldquo;Man bit the dog\u0026rdquo; are the same in CBOW\u0026rsquo;s eyes.\nSmoothing Effect: CBOW models are much faster to train than Skip-Gram but are generally smoother.\nRare Words: CBOW models struggle slightly with rare words because the presence of surrounding common words (like articles and prepositions) can skew the embeddings and dissolve their uniqueness.\nComputational Cost: Softmax requires summing over the entire vocabulary size, which can be computationally expensive if $V$ is very large. Solutions include Hierarchical Softmax and Negative Sampling (approximating the denominator by only checking the target word vs. 5 random noise words).\nVectors as Double Agents: Every word has a different representation in the embedding matrix and the output matrix. Most of the time $W_{out}$ is discarded, but some consider averaging both $W_{in}$ and $W_{out}$ for slightly better performance.\nLinear Relationships: The famous analogy King - Man + Woman = Queen emerges from the model because of the linear relationships between the words.\nInitialization: This is a no-brainer but worth mentioning. Initialization to zero vectors would mean no learning. We must initialize randomly.\n2. Skip-Gram (The \u0026ldquo;Burger Flip\u0026rdquo; of CBOW) üîÑ Skip-Gram flips the CBOW logic. While CBOW tries to predict the center word given the context, Skip-Gram tries to predict the scattered context given the center word.\nCore Concept: If a word like \u0026ldquo;monarch\u0026rdquo; can accurately predict words like \u0026ldquo;king\u0026rdquo;, \u0026ldquo;throne\u0026rdquo;, \u0026ldquo;crown\u0026rdquo;, \u0026ldquo;royalty\u0026rdquo;, and \u0026ldquo;power\u0026rdquo;, then it must effectively capture the concept of royalty.\nTraining Let\u0026rsquo;s define dimensions:\n$V$: Vocabulary size $D$: Embedding dimension $C$: Number of context words Step 1: Input Lookup\nUnlike CBOW, our input is just one word vector (the center word $w_t$). We grab the vector $v_c$ from the Input Matrix $W_{in}$.\n$$ h = v_{w_t} $$\nNote: There is no averaging here. The hidden layer $h$ is simply the raw vector of the center word.\nStep 2: Output Scoring (The Broadcast)\nThe model takes this single vector $h$ and compares it against the Output Matrix $W_{out}$ (the entire vocabulary). It computes a score $u_j$ for every word $j$ in the dictionary.\n$$ u = W_{out}^T h $$\nResult $u$: A vector of size $V \\times 1$ containing raw scores.\nStep 3: Probability Conversion (Softmax)\nWe apply Softmax to turn scores into probabilities.\n$$ P(w_j | w_t) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 4: The Multi-Target Loss\nHere is the major difference from CBOW. In CBOW, we had one target (the center). In Skip-Gram, we have $2C$ targets (all the surrounding words). For a sentence \u0026ldquo;The cat sat on mat\u0026rdquo; (Center: \u0026ldquo;sat\u0026rdquo;), the targets are \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;. We want to maximize the probability of all these true context words. The Loss function ($E$) sums the error for each context word:\n$$ E = - \\sum_{w_{ctx} \\in \\text{Window}} \\log P(w_{ctx} | w_t) $$\nStep 5: Backpropagation (Accumulating Gradients)\nSince one input word is responsible for predicting multiple output words, the error signals from all those context words add up.\nAt the Output Layer:\nIf \u0026ldquo;cat\u0026rdquo; was a target, the \u0026ldquo;cat\u0026rdquo; vector in $W_{out}$ gets a signal: \u0026ldquo;Move closer to \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; If \u0026ldquo;apple\u0026rdquo; was NOT a target, the \u0026ldquo;apple\u0026rdquo; vector gets a signal: \u0026ldquo;Move away from \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; At the Hidden Layer ($h$):\nThe center word \u0026ldquo;sat\u0026rdquo; receives feedback from all its neighbors simultaneously. Error signal = (Error from \u0026ldquo;The\u0026rdquo;) + (Error from \u0026ldquo;cat\u0026rdquo;) + (Error from \u0026ldquo;on\u0026rdquo;)\u0026hellip; The vector for \u0026ldquo;sat\u0026rdquo; in $W_{in}$ moves in the average direction of all its neighbors. 3. The \u0026ldquo;Computational Nightmare\u0026rdquo; \u0026amp; Negative Sampling The equations above describe Naive Softmax.\nThe Problem: If $V = 100,000$, computing the denominator $\\sum \\exp(u_k)$ takes 100,000 operations. Doing this for every word in a training set of billions of words is impossibly slow.\nThe Solution: Negative Sampling Instead of updating the entire vocabulary (1 correct word vs 99,999 wrong ones), we approximate the problem.\nPositive Pair: (sat, cat) $\\rightarrow$ Maximize probability (Label 1). Negative Pairs: We pick $K$ (e.g., 5) random words the model didn\u0026rsquo;t see, e.g., (sat, bulldozer), (sat, quantum). $\\rightarrow$ Minimize probability (Label 0). New Equation (Sigmoid instead of Softmax):\n$$ E = - \\log \\sigma({v'_{\\text{pos}}}^T v_{\\text{in}}) - \\sum_{k=1}^K \\log \\sigma(-{v'_{\\text{neg}_{k}}}^T v_{\\text{in}}) $$ Effect: We typically only update 6 vectors per step (1 pos + 5 negs) instead of 100,000.\nLet\u0026rsquo;s see this in code:\nimport torch import torch.nn as nn import torch.optim as optim class SkipGramNegativeSampling(nn.Module): \u0026#34;\u0026#34;\u0026#34; Skip-Gram with Negative Sampling (SGNS) implementation. SGNS approximates the softmax over the entire vocabulary by instead distinguishing between a real context word (positive) and K noise words (negative). \u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embedding_dim): super(SkipGramNegativeSampling, self).__init__() # Input embeddings: used when the word is the center word self.in_embed = nn.Embedding(vocab_size, embedding_dim) # Output embeddings: used when the word is a context or negative sample self.out_embed = nn.Embedding(vocab_size, embedding_dim) # Initialize weights with small values to prevent gradient saturation initrange = 0.5 / embedding_dim self.in_embed.weight.data.uniform_(-initrange, initrange) self.out_embed.weight.data.uniform_(-initrange, initrange) def forward(self, center_words, target_words, negative_words): \u0026#34;\u0026#34;\u0026#34; Computes the negative sampling loss. Args: center_words: (batch_size) target_words: (batch_size) negative_words: (batch_size, K) where K is number of negative samples \u0026#34;\u0026#34;\u0026#34; # Retrieve vectors v_c = self.in_embed(center_words) # (batch_size, embed_dim) u_o = self.out_embed(target_words) # (batch_size, embed_dim) u_n = self.out_embed(negative_words) # (batch_size, K, embed_dim) # 1. Positive Score: log(sigmoid(v_c ¬∑ u_o)) # Compute dot product: (batch, 1, dim) @ (batch, dim, 1) -\u0026gt; (batch, 1) pos_score = torch.bmm(u_o.unsqueeze(1), v_c.unsqueeze(2)).squeeze(2) pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-7) # 2. Negative Score: sum(log(sigmoid(-v_c ¬∑ u_n))) # Compute dot products for all K samples: (batch, K, dim) @ (batch, dim, 1) -\u0026gt; (batch, K) neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2) neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-7), dim=1, keepdim=True) # Total loss is the negative of the objective function loss = -(pos_loss + neg_loss) return torch.mean(loss) # --- Configuration \u0026amp; Mock Data --- VOCAB_SIZE = 100 EMBED_DIM = 10 word_to_ix = {\u0026#39;fox\u0026#39;: 0} # Example vocabulary mapping model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock inputs: 1 center word, 1 target word, 5 negative samples center_id = torch.tensor([0], dtype=torch.long) target_id = torch.tensor([1], dtype=torch.long) negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long) # Training Step model.zero_grad() loss = model(center_id, target_id, negative_ids) loss.backward() optimizer.step() # Output Results print(f\u0026#34;Loss after one step: {loss.item():.6f}\u0026#34;) word_vec = model.in_embed(torch.tensor([word_to_ix[\u0026#39;fox\u0026#39;]])) print(f\u0026#34;Vector for \u0026#39;fox\u0026#39;:\\n{word_vec.detach().numpy()}\u0026#34;) The outputs:\nLoss after one step: 4.158468 Vector for \u0026#39;fox\u0026#39;: [[ 0.04197385 0.02400733 -0.03800093 0.01672485 -0.03872231 -0.0061478 0.0121122 0.04057864 -0.036255 0.03861175]] Interesting Facts About Skip-Gram üí° Makes Rare Words Shine: Skip-Gram is more effective at capturing the meaning of rare words because it focuses on the context of the center word. In CBOW, a rare word\u0026rsquo;s vector is averaged with others, diluting its signal. In Skip-Gram, when the rare word appears as the center, it gets the full, undivided attention of the backpropagation update.\nSlow Training: This method creates more training samples. A window of size 2 creates 4 training pairs per center word, while CBOW creates only 1 training pair per center word.\nSemantic Focus: Skip-Gram puts more emphasis on the semantic relationship between words. It views the context as the primary signal, with the center word as the target. This makes it better at capturing the meaning of words in context. Skip-Gram tends to capture semantic relationships (King/Queen) slightly better, while CBOW captures syntactic ones (walk/walked) slightly better.\n","permalink":"http://localhost:1313/posts/word-embeddings/","summary":"Understanding the mathematics behind Word2Vec, CBOW, and Skip-Gram and how they map language to vector space.","title":"Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n5. Code Examples: We\u0026rsquo;ll use sentencepiece for tokenization and compare results for different models.\nimport sentencepiece as spm import os # Configuration for training the SentencePiece model # SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words. options = { # The source text file used for learning the vocabulary \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # The base name for the generated model (.model) and vocabulary (.vocab) files \u0026#39;model_prefix\u0026#39;: \u0026#39;bpe_model\u0026#39;, # Number of unique tokens in the final vocabulary \u0026#39;vocab_size\u0026#39;: 4000, # \u0026#39;bpe\u0026#39; (Byte Pair Encoding) merges frequent pairs of characters/sequences \u0026#39;model_type\u0026#39;: \u0026#39;bpe\u0026#39;, # Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets \u0026#39;character_coverage\u0026#39;: 0.9995, # When enabled, unknown characters are decomposed into UTF-8 bytes to avoid \u0026#39;unk\u0026#39; tokens \u0026#39;byte_fallback\u0026#39;: True, # Treats digits individually (0-9), preventing large numbers from being treated as single tokens \u0026#39;split_digits\u0026#39;: True, # Prevents adding a whitespace prefix to the first token; useful for fine-grained control \u0026#39;add_dummy_prefix\u0026#39;: False } try: print(\u0026#34;Starting the training process...\u0026#34;) # SentencePieceTrainer.train takes the dictionary of options to build the BPE model spm.SentencePieceTrainer.train(**options) print(\u0026#34;Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created.\u0026#34;) # Initialize the processor and load the newly trained model sp = spm.SentencePieceProcessor() sp.load(\u0026#39;bpe_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Model Metadata:\u0026#34;) # Retrieve the total number of tokens in the vocabulary print(f\u0026#34;Total Vocab Size: {sp.get_piece_size()}\u0026#34;) # Special tokens are used for sequence boundaries and handling unknown characters print(f\u0026#34;BOS (Beginning of Sentence) ID: {sp.bos_id()}\u0026#34;) print(f\u0026#34;EOS (End of Sentence) ID: {sp.eos_id()}\u0026#34;) print(f\u0026#34;UNK (Unknown) ID: {sp.unk_id()}\u0026#34;) print(f\u0026#34;PAD (Padding) ID: {sp.pad_id()}\u0026#34;) # Test the tokenizer on sample strings to see how it breaks down text test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: shows the actual subword units (tokens) print(f\u0026#34;Subword Tokens: {sp.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: shows the numerical mapping for each token print(f\u0026#34;Numerical IDs: {sp.encode_as_ids(text)}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred during training or processing: {e}\u0026#34;) You can see the following output:\nStarting training... Starting the training process... Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created. ------------------------------ Model Metadata: Total Vocab Size: 4000 BOS (Beginning of Sentence) ID: 1 EOS (End of Sentence) ID: 2 UNK (Unknown) ID: 0 PAD (Padding) ID: -1 --- Tokenization Test --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;el\u0026#39;, \u0026#39;lo\u0026#39;, \u0026#39;‚ñÅW\u0026#39;, \u0026#39;orld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 380, 309, 476, 619, 36, 3924, 3964, 3959, 3993, 3978, 3983, 57, 3975, 3976, 60, 3974] --- Tokenization Test --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅmo\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;inst\u0026#39;, \u0026#39;ag\u0026#39;, \u0026#39;ram\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅever\u0026#39;] Numerical IDs: [1894, 304, 3284, 376, 321, 313, 312, 1008, 3313, 1325, 428, 3937, 341, 304, 3284, 1495] The ! is fallen back to UTF bytes. and the word instagrammable is split into multiple subword tokens. Also, original text can be recreated right back by simply concatenating the tokens. And replacing underscore by whitespace. Now we can try Unigram model. and see the same sentences tokenized differently.\nfrom tokenizers import Tokenizer from tokenizers.models import WordPiece from tokenizers.trainers import WordPieceTrainer from tokenizers.pre_tokenizers import Whitespace try: # 1. Initialize the WordPiece Tokenizer # We specify the [UNK] token for handling words not found in the vocabulary. tokenizer = Tokenizer(WordPiece(unk_token=\u0026#34;[UNK]\u0026#34;)) # 2. Configure Pre-tokenization # Before the subword algorithm runs, we need to split the raw text into words. # Whitespace splitting is the standard first step for most English NLP tasks. tokenizer.pre_tokenizer = Whitespace() # 3. Initialize the Trainer # We define our target vocabulary size and the special tokens required for # downstream tasks (like BERT\u0026#39;s [CLS] for classification or [SEP] for separators). trainer = WordPieceTrainer( vocab_size=4000, special_tokens=[\u0026#34;[UNK]\u0026#34;, \u0026#34;[CLS]\u0026#34;, \u0026#34;[SEP]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[MASK]\u0026#34;] ) # 4. Train the Model # The tokenizer scans the training file to build a vocabulary of the most # frequent subword units. tokenizer.train(files=[\u0026#34;train_data.txt\u0026#34;], trainer=trainer) # 5. Persist the Model # Save the configuration and vocabulary to a JSON file for future inference. tokenizer.save(\u0026#34;wordpiece.json\u0026#34;) print(\u0026#34;Training complete. \u0026#39;wordpiece.json\u0026#39; created.\u0026#34;) # 6. Metadata Inspection print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;WordPiece Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {tokenizer.get_vocab_size()}\u0026#34;) # 7. Testing Subword Tokenization # WordPiece shines at handling rare words by breaking them into meaningful chunks. test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (WordPiece) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # Encode converts raw text into a Tokenizer object containing tokens and IDs output = tokenizer.encode(text) # \u0026#39;tokens\u0026#39; shows the subword breakdown (e.g., \u0026#39;un\u0026#39;, \u0026#39;##insta\u0026#39;, etc.) print(f\u0026#34;Subword Tokens: {output.tokens}\u0026#34;) # \u0026#39;ids\u0026#39; are the numerical indices mapped to the vocabulary print(f\u0026#34;Numerical IDs: {output.ids}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred with WordPiece model: {e}\u0026#34;) You can see the following output:\nWordPiece Model Metadata: Total Vocab Size: 2609 --- Tokenization Test (WordPiece) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;H\u0026#39;, \u0026#39;##el\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;##or\u0026#39;, \u0026#39;##ld\u0026#39;, \u0026#39;[UNK]\u0026#39;, \u0026#39;[UNK]\u0026#39;] Numerical IDs: [37, 180, 214, 52, 162, 418, 0, 0] --- Tokenization Test (WordPiece) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;##os\u0026#39;, \u0026#39;##t\u0026#39;, \u0026#39;un\u0026#39;, \u0026#39;##ins\u0026#39;, \u0026#39;##ta\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;##ra\u0026#39;, \u0026#39;##m\u0026#39;, \u0026#39;##ma\u0026#39;, \u0026#39;##ble\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;ever\u0026#39;] Numerical IDs: [691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240] We can see the special character ## in the output. We can also see the breakdowns not having any semantic meaning. They are likelihood based. We can try the Unigram model now.\n# \u0026#39;Unigram\u0026#39; is the default and usually recommended over BPE in SentencePiece. options_unigram = { \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # Path to the raw text file for training \u0026#39;model_prefix\u0026#39;: \u0026#39;unigram_model\u0026#39;, # Prefix for the output .model and .vocab files \u0026#39;vocab_size\u0026#39;: 1200, # Desired size of the final vocabulary \u0026#39;model_type\u0026#39;: \u0026#39;unigram\u0026#39;, # Specifies the Unigram language model algorithm \u0026#39;character_coverage\u0026#39;: 0.9995, # Percentage of characters covered by the model (0.9995 is standard for Latin scripts) \u0026#39;byte_fallback\u0026#39;: True, # Enables mapping unknown characters to UTF-8 bytes to avoid \u0026lt;unk\u0026gt; tokens \u0026#39;split_digits\u0026#39;: True, # Treats each digit as an individual token (useful for numerical data) \u0026#39;add_dummy_prefix\u0026#39;: False # Prevents adding a leading space (SentencePiece default is True) } try: # 1. Train the SentencePiece model using the defined options print(\u0026#34;Starting Unigram training...\u0026#34;) spm.SentencePieceTrainer.train(**options_unigram) print(\u0026#34;Training complete. \u0026#39;unigram_model.model\u0026#39; created.\u0026#34;) # 2. Load the trained model into a processor instance for inference sp_unigram = spm.SentencePieceProcessor() sp_unigram.load(\u0026#39;unigram_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Unigram Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {sp_unigram.get_piece_size()}\u0026#34;) # 3. Define test cases to evaluate how the model handles common and rare words test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] # 4. Iterate through test sentences to visualize subword segmentation for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (Unigram) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: Converts text into subword strings (visual representation) print(f\u0026#34;Subword Tokens: {sp_unigram.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: Converts text into numerical indices for model input print(f\u0026#34;Numerical IDs: {sp_unigram.encode_as_ids(text)}\u0026#34;) except Exception as e: # Handle potential errors during training or loading (e.g., missing input file) print(f\u0026#34;An error occurred with Unigram model: {e}\u0026#34;) The output is as follows:\nUnigram Model Metadata: Total Vocab Size: 1200 --- Tokenization Test (Unigram) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ll\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;ld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311] --- Tokenization Test (Unigram) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;T\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅm\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;sta\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;ra\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ver\u0026#39;] ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"A comprehensive guide to tokenization strategies: BPE, WordPiece, Unigram, and SentencePiece.","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"A brief introduction to Vectors \u0026amp; Verbs and formatting verification.","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"\nFeedforward network (FFN) blocks are the least talked about when in context to LLMs. But that is where more than 70% of the paramters come from. Something as simple and convinient as a couple of linear transforming matrices become a burden when models start scaling in Billions of parameters. So, getting the maximum juice of a lean structure is a challenge. Plus, mostly under represented, because the FFN sub-layer acts as the primary \u0026ldquo;knowledge\u0026rdquo; processing engine, often containing the majority of a model\u0026rsquo;s total parameters.\nStandard Position Wise Feedforward Network This is as simple as it can get, two-layered fully connected network of neurons with a single activation switch, most probably a ReLU variant (for LLMs). $$ FFN(x) = max(0, W_2(W_1(x)) + b_1)W_2 + b_2 $$ Same transformation is applied for each token in parallel, making it efficient for GPU computation. For Example, in BERT model the 512 dimensions of each token are expanded to 2048 to allow for \u0026ldquo;intricate interactions\u0026rdquo; before projecting back to 512 dimensions. This part is important, in every transformer stacking of layers is possible only because the output of every matches it\u0026rsquo;s input dimensions.\nSWiGLU Sigmoid Weighted Gated Linear Unit (SWiGLU) has become the Gold Standard for feed forward networks in LLMs like Llama 3 and PaLM. The primary reason for this choice is the Swish function (as shown in the first image) instead of ReLU or GELU are as following: Mathematical Linearity: Swish (SILU) is defined as $x\\cdot sigmoid(\\beta x)$, $\\beta$ is a learnable parameter. It is a small monotonic function that allowes small negetive values to pass through, making it mathematically linear. While GELU ($x \\cdot \\phi(x)$) is also smooth and nonmonotonic, Swish\u0026rsquo;s curve and equations have emperically improved the model\u0026rsquo;s performance. Computational Efficiency: SILU is simpler to compute. $x/(1+exp(-x))$ compared to the standard GELU, which is an approximation of the tanh function. In the massive scale of modern LLMs these shortcuts save significant training time. Performance: Research in the PaLM and Llama papers found that Swish-based Gated Linear Units consistently outperformed GELU-based versions in terms of perplexity and downstream task accuracy. $$SwiGLU(x, W, V, b, c) = (Swish(xW + b) \\otimes (xV + c))W_2$$ Where $W$ and $V$ are weight matrices for the gate and the data path, and $\\otimes$ is element-wise multiplication. More Complex Filtering: Instead of using ordinary two weight matrices in standard FFN, SWiGLU uses three weight matrices (gate, date, output). This allows it to gather more complex features out of the input without drastically increasing the number of parameters. This performance boost has resulted in modern implementations being low in dimensionality of hidden layers (2/3 of input dimensions) while retaining the performance benefits of gated activations. Zero-Gradient Avoidance: Unlike ReLU, which has a zero gradient for all negative values (the \u0026ldquo;dying ReLU\u0026rdquo; problem), SwiGLU\u0026rsquo;s smooth curve ensures that the model can still learn from negative inputs, improving training stability in deep stacks.\n","permalink":"http://localhost:1313/posts/feedforward-networks/","summary":"Exploring the hidden layers of trillion-parameter switchboards: Feedforward Neural Networks and Activation Functions.","title":"Anatomy of Trillion-Parameter Switchboards: Understanding Feedforward Blocks"},{"content":"\nWe are about to touch the holy grail of modern AI. The 2017 declaration was simple: Attention Is All You Need. But as we‚Äôve raced from GPT-1 to reasoning models, the definition of \u0026lsquo;Attention\u0026rsquo; has quietly transformed. Is the algorithm ruling the world today actually the same one we started with? Let‚Äôs look under the hood.\nüî• The Core Concept At its heart, the attention mechanism is a form of weighted aggregation. It is often described using a database retrieval analogy. Imagine you have a database of information (context).\nQuery ($Q$): What are you currently looking for? (The token being generated). Key ($K$): The indexing tag associated with each piece of information. Value ($V$): The actual information (The context token). The \u0026ldquo;Attention\u0026rdquo; is the process of matching your Query against all the Keys to determine how much \u0026ldquo;weight\u0026rdquo; to put on each Value. Attention scores are calculated in each transformer layer at every step of the sequence. This is a single unifying factor in all transformer models, from BERT to Gemini.\nüî• Standard Multi-Head Attention (MHA) The original mechanism introduced in the paper \u0026ldquo;Attention Is All You Need\u0026rdquo;.\nHere, instead of performing the Key-Query-Value (KQV) operation on the entire context only once, the attention is split into $N$ parallel heads. Each head learns a different aspect of the language (e.g., word meaning, syntax, semantic relationships, etc.). The equation is a simple culmination of dot products aided by a regularizer like Softmax.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nWhere:\n$QK^T$ computes the similarity scores between the Query and Key. $\\sqrt{d_k}$ is a scaling factor to prevent the dot product from growing too large, mitigating the vanishing/exploding gradient problem. The weighted sum is applied to the values of $V$. The Memory Bandwidth Bottleneck: \u0026gt; During inference, the model must store Key and Value matrices for all previous tokens. This is called the KV Cache. In MHA, every single head has its own unique set of Keys and Values. For large models with massive context windows, loading this massive KV cache from memory (HBM) to the computation chip (SRAM) becomes the primary bottleneck, slowing down generation significantly. Earlier models could often only generate ~8 tokens per second at inference due to this constraint.\nüî• Multi-Query Attention (MQA) This approach addresses the memory bottleneck by sharing the same set of Keys and Values across all heads while keeping separate Query heads.\nMHA: $H$ Query heads, $H$ Key heads, $H$ Value heads MQA: $H$ Query heads, 1 Key head, 1 Value head This drastically reduces the memory requirements of the KV cache by a factor of $H$ (often 8x or 16x). The GPU needs to load far less data, significantly speeding up inference. However, it does not reduce the computation (FLOPs)‚Äîso why is it faster?\nThis can be explained with the Kitchen Analogy: Imagine a kitchen (GPU) with 8 chefs (Heads).\nComputation (Math): The Chef chopping vegetables. Memory Access (Bandwidth): The Assistant running to the fridge to get ingredients. In MHA: Each chef demands unique vegetables from the fridge. The assistant runs back and forth 8 times, fetching 8 different crates. Most of the time is spent waiting for the assistant to return. This results in idle cores waiting for data to load from HBM.\nIn MQA: All chefs agree to use the same vegetables. The assistant runs back and forth only once, fetching one giant crate of potatoes and dumping it. This results in minimized memory transfer and no idle cores.\nThe Technical Reality: During text generation, the model is memory-bound, not compute-bound. The time it takes to move the KV Cache from HBM (High Bandwidth Memory) to the chip\u0026rsquo;s SRAM is the bottleneck. MQA reduces the data volume of that move by a factor of $H$.\nThe Capacity Gap: MQA is a trade-off between nuanced response and memory capacity. The model loses the ability to store different types of information, as a single $V$ matrix must now store all nuances of information flow in the language.\nThe Surgery Trick (Uptraining): Google Research figured out a way to take an existing model with MHA and convert it to MQA by averaging the weight matrices of $H$ heads. This is called Uptraining. You then train this \u0026ldquo;Frankenstein\u0026rdquo; model for ~5% of the original training steps for adjustments. This yields an MQA model that is nearly as good as the original MHA but vastly faster.\nüî• Grouped Query Attention (GQA) A \u0026ldquo;best of both worlds\u0026rdquo; solution used in modern models like Llama 2 and Llama 3.\nHow it works: GQA acts as an interpolation between MHA and MQA. Instead of having just 1 Key head (MQA) or $H$ Key heads (MHA), it divides the query heads into $G$ groups. Each group shares a single Key and Value head.\nEquation modification: If you have 8 query heads, you might create 4 groups (2 queries per group). This reduces the KV cache by 2x instead of 8x, preserving more quality than MQA while still speeding up inference.\nMHA: 8 people (heads) are working on a project. Each person has their own unique filing cabinet (KV cache). It takes forever to move 8 cabinets around. MQA: 8 people share one single filing cabinet. It\u0026rsquo;s very fast to move, but they fight over the organization and lose detail. GQA: The 8 people split into 4 teams of 2. Each team shares a cabinet. It\u0026rsquo;s a balance between speed and organizational detail. üî• Multi-Head Latent Attention (MLA) This is the most advanced mechanism for attention, used in models like DeepSeek-V3. It is a sophisticated design for massive efficiency that takes a radical approach: mathematically compressing the memory into a low-rank \u0026ldquo;latent\u0026rdquo; vector.\nKV Cache Explosion: As mentioned before, for a massive model like DeepSeek-V3 (671B parameters), storing the full Keys and Values for every token in a long conversation requires terabytes of high-speed memory (HBM), and chips spend more time waiting than churning.\nMHA (Standard) stores everything. MQA deletes redundancy but hurts quality. MLA keeps the quality of MHA but compresses the data 4x-6x smaller than even GQA.\nThe Zip File Approach: Low-Rank Joint Compression Let\u0026rsquo;s compare the math of MHA and MLA. In MHA, for every single token, we generate a unique Key ($k$) and Value ($v$) matrix for each attention head.\nThe Memory Problem: The $k$ is stored in the KV Cache. For 128 heads, for example, it calculates a massive vector of $128 \\times 128 = 16,384$ floats per token.\nFor MHA, the attention score looks like: $$\\text{Score} = q^T \\cdot k$$\nBut for MLA, instead of storing the full $k$, it projects the input down into a tiny latent vector $c_{KV}$, which is much smaller (e.g., 512 floats). $$c_{KV} = W_{DKV} \\cdot x$$\nNow here is the catch: Naive compression and decompression would mean constructing a small latent matrix for every token and deconstructing it every single time the information is required, which defeats the purpose of compression.\n$$k_{\\text{reconstructed}} = W_{UK} \\cdot c_{KV}$$ $$\\text{Score} = q^T \\cdot k_{\\text{reconstructed}}$$\nThe Matrix Absorption Trick (Optimization) Instead of reconstructing $k$ for every token, we can absorb the Up-Projection matrix ($W_{UK}$) into the Down-Projection matrix ($W_{DKV}$) and absorb the query $q$ into a new $Q$.\nFrom the original equation: $$\\text{Score} = q^T \\cdot (\\underbrace{W_{UK} \\cdot c_{KV}}_{\\text{This is } k})$$\nWe associate differently:\n$$\\text{Score} = (\\underbrace{q^T \\cdot W_{UK}}_{\\text{Absorbed Query}}) \\cdot c_{KV}$$ We change the order in which the matrix multiplication is performed.\nHow is this allowed? In linear algebra, matrix multiplication is associative:\n$$(A \\cdot B) \\cdot C = A \\cdot (B \\cdot C)$$ We can move the parentheses! Instead of grouping $(W_{UK} \\cdot c_{KV})$ to make the Key, we group $(q^T \\cdot W_{UK})$ to make a new Query.\nNote: The positional embedding information is left untouched. That rotational information from algorithms like RoPE is preserved outside the compression; it requires minimal space anyways.\n","permalink":"http://localhost:1313/posts/attention-mechanisms/","summary":"We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek\u0026rsquo;s MLA, how has the definition of \u0026lsquo;Attention\u0026rsquo; transformed?","title":"Attention Is All You Need, but exactly which one?: MHA, GQA and MLA"},{"content":"\nPositional Embeddings are the \u0026ldquo;voice\u0026rdquo; that tell Transformers where words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.\n\u0026ldquo;The dog chased the cat\u0026rdquo; and \u0026ldquo;cat chased the dog\u0026rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.\nüî• Sinusoidal Embeddings: The Odometer of Language Introduced in the seminal \u0026ldquo;Attention Is All You Need\u0026rdquo; paper, this was the industry standard for many years. This approach adds a vector to each word embedding based on its position. The vector is computed out of wavelike patterns that are periodic and smooth. Mathematically, for a position $pos$ and embedding dimension $i$:\n$$ PE_{(pos, i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $$\nThe model uses sine and cosine waves of different frequencies to capture both short and long-range dependencies. The frequency of the waves decreases as the embedding dimension increases, which helps capture long-range dependencies in the input sequence.\nThe creators wanted the model to calculate distance easily, which is why a linear function was used. The model simply rotates the current position vector using a rotation matrix to derive the next position.\n$$ \\begin{pmatrix} \\cos(k) \u0026 \\sin(k) \\\\ -\\sin(k) \u0026 \\cos(k) \\end{pmatrix} \\cdot PE(pos) \\approx PE(pos + k) $$ Low dimensions act like a fast-ticking clock (high frequency, changing rapidly) and high dimensions act like a slow clock (low frequency, changing slowly).\nThe number 10,000 is an arbitrarily chosen high number to reduce Aliasing. By choosing such a high number, the creators ensured that the wavelength of the slowest dimension is incredibly long. Consequently, the pattern practically never repeats, and each position remains unique.\nThis combination works like a mechanical odometer dial. Each subsequent dimension acts as a slower-moving gear. The model learns to read these \u0026ldquo;dials\u0026rdquo; to understand exactly where a word sits in the sequence.\nThis mitigated the challenge of exploding gradients that previous array-based positional embeddings suffered from (e.g., adding integers 1, 2, 3\u0026hellip;). This model keeps values between -1 and +1, adding stability while ensuring every position gets a distinct code.\nThe Problem: This approach pollutes the word embedding matrix (semantic information). While high-dimensional vectors are almost always orthogonal‚Äîmeaning the noise doesn\u0026rsquo;t fully destroy the word\u0026rsquo;s identity‚Äîit does not strictly guarantee purity.\nThis led to a more mature approach.\nüî• ALiBi: The Fading Streetlight Attention with Linear Biases (ALiBi) is a method where, instead of adding a vector to the embeddings at the start, the model injects positional information linearly, directly into the attention scores. If Sinusoidal Encoding asks, \u0026ldquo;Where am I on the map?\u0026rdquo;, ALiBi asks, \u0026ldquo;How much harder should I squint to see you?\u0026rdquo;\nThe biggest problem ALiBi solved is the \u0026ldquo;Invisible Wall\u0026rdquo;. If you train a Transformer on sequences of length 2048 and then feed it a book with 3000 words, it will crash or output garbage at word 2049. This is a failure of Extrapolation.\nALiBi throws away the idea of adding positional vectors entirely. Instead, it devises a \u0026ldquo;penalty\u0026rdquo; system for attention scores:\n$$ \\text{Score} = (q \\cdot k^\\top) - (m \\cdot \\text{distance}) $$\nDistance: If words A and B are neighbors, the distance is 0, so the penalty is 0. Although the distance is linear, it does not cause exploding gradients because the raw attention scores are normalized by a softmax function. This distance prioritizes nearby words (local context) over distant ones.\nSlope ($m$): If we always penalize distance equally, the model becomes myopic. ALiBi mitigates this by assigning a different slope value to each attention head:\nHead 1 (The Historian): Very low slope. The penalty grows slowly, allowing it to see far back. Head 2 (The Reader): Medium slope. Head 3 (The Myopic): Very high slope (e.g., 0.5); it effectively cannot see beyond 2-3 words. This mimics holding a lantern. Words close to you are bright, while words further away naturally fade into darkness.\nExtrapolation: If you walk 5 miles down the road (a longer sequence), your lantern works exactly the same way. The physics of light decay are constant, allowing the model to handle sequences longer than it was trained on.\nHowever, ALiBi forces a \u0026ldquo;sliding window\u0026rdquo; view where distant words eventually become invisible. This brings us to the modern standard.\nüî• RoPE: The Rotary Revolution Rotary Position Embeddings (RoPE) is the current industry standard (used in Llama 3, Mistral, PaLM) because it mathematically unifies \u0026ldquo;absolute position\u0026rdquo; with \u0026ldquo;relative position.\u0026rdquo; This method rotates the vectors in the Query and Key matrices based on their absolute position.\nIf a vector has 512 dimensions, RoPE breaks it into 256 pairs. Each pair is rotated by a different frequency:\n$$ \\theta_i = 10000^{-2i/d} $$\nThe First pair ($i = 0$) is rotated by the highest frequency. The Last pair ($i = 255$) is rotated by the lowest frequency. This spread ensures the model has \u0026ldquo;high precision\u0026rdquo; (fast rotation) to distinguish immediate neighbors, and \u0026ldquo;low precision\u0026rdquo; (slow rotation) to track global position without the pattern repeating.\nDoesn\u0026rsquo;t rotation change meaning? Yes and No. The vector for \u0026ldquo;Apple\u0026rdquo; at position 1 and \u0026ldquo;Apple\u0026rdquo; at position 256 will look completely different in terms of coordinates. However, rotation does not change the semantic strength (magnitude/norm) of the vector.\nThe Relativity Trick: Transformers compare words using dot products (angles). RoPE relies on the fact that the attention mechanism ignores the absolute rotation and only detects the difference in rotation (relative position). Thus, the \u0026ldquo;relative meaning\u0026rdquo; is encoded purely into the angle between words, leaving the semantic \u0026ldquo;magnitude\u0026rdquo; untouched.\nüî• LongRoPE: The Bifocal Lens While RoPE is powerful, it struggles when stretching context windows from 4k to 2 million tokens. The \u0026ldquo;fast\u0026rdquo; dimensions spin so rapidly over long distances that they become random noise. LongRoPE solves this using an evolutionary search algorithm to find a unique scaling factor ($\\lambda$) for each dimension.\nThe Equations Instead of rotating by the standard $\\theta_i$, LongRoPE rotates by a rescaled frequency:\n$$ \\theta\u0026rsquo;_i = \\lambda_i \\theta_i $$\nTo efficiently find these $\\lambda$ values without searching an infinite space, the algorithm enforces a monotonicity constraint based on NTK theory:\n$$ \\lambda_i \\le \\lambda_{i+1} $$\nThis ensures that low-frequency dimensions (global context) are stretched more than high-frequency dimensions (local context), creating a \u0026ldquo;bifocal\u0026rdquo; effect:\nHigh Frequencies (Local): Kept sharp ($\\lambda \\approx 1$) to maintain grammar. Low Frequencies (Global): Stretched ($\\lambda \u0026gt; 1$) to track massive distances without repeating. Not worth perusing more since the next one is much better at long context positional encoding. üî• HoPE: The Hyperbolic Slide Hyperbolic Rotary Positional Encoding (HoPE) moves from the geometry of a circle to the geometry of a hyperbola (inspired by Lorentz transformations). It was designed to fix a subtle flaw in RoPE: the \u0026ldquo;wobbly\u0026rdquo; or oscillatory nature of attention scores.\nThe Core Problem: RoPE\u0026rsquo;s \u0026ldquo;Wobble\u0026rdquo; In standard RoPE, we rotate vectors around a circle.\nThe Issue: Circles repeat. A dial at $361^\\circ$ looks identical to $1^\\circ$. Even with different frequencies, the dot product fluctuates up and down as distance increases. The Consequence: The model gets confused at long distances, potentially mistaking a word 1,000 tokens away for a neighbor just because the rotation cycles aligned, creating \u0026ldquo;noise.\u0026rdquo; The \u0026ldquo;Hyperbolic\u0026rdquo; Shift HoPE replaces trigonometric functions ($\\sin, \\cos$) with hyperbolic functions ($\\sinh, \\cosh$).\nCircular Rotation: Keeps distance from the center constant. Hyperbolic Rotation: Moves points along a hyperbola. Crucially, hyperbolic functions do not repeat; they grow or decay exponentially. By using hyperbolic rotation, HoPE guarantees that as words get further apart, the attention score decays monotonically (smoothly drops).\nThe Equations HoPE uses a hyperbolic matrix $B(\\theta, m)$ defined using the Lorentz Boost structure:\n$$ B(\\theta, m) = \\begin{pmatrix} \\cosh(m\\theta) \u0026 \\sinh(m\\theta) \\\\ \\sinh(m\\theta) \u0026 \\cosh(m\\theta) \\end{pmatrix} $$ Since hyperbolic functions grow to infinity, HoPE introduces a decay penalty ($e^{-m\\theta'}$). The final attention score simplifies beautifully to a function of relative distance: $$ \\text{Score} \\propto e^{-|m-n|(\\theta\u0026rsquo; - \\theta)} $$\nThis ensures the attention score exponentially decays as distance increases, eliminating the wobbles of standard RoPE. This the latest model that is taking up the space in the industry for extremely long context models.\n","permalink":"http://localhost:1313/posts/positional-embeddings/","summary":"From Sinusoidal to RoPE and HoPE: How Transformers learn to process word order and sequence length.","title":"The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE"},{"content":"Imagine trying to teach a computer the difference between \u0026ldquo;Apple\u0026rdquo; the fruit and \u0026ldquo;Apple\u0026rdquo; the company. To us, the distinction is intuitive. To a machine, it‚Äôs just a string of characters. How do we turn these strings into meaningful math? While early attempts like Word2Vec gave us a great start, they missed the forest for the trees‚Äîor in some cases, the twigs for the branches. Enter GloVe and FastText: two algorithms that revolutionized how machines understand the nuances of human language.\nPrevious static embedding models like Word2Vec successfully captured the local semantics of words, but they failed in addressing contexts between words that might not always appear in the same context window. Furthermore, they treated words as indivisible atomic units in continuous vector spaces, missing out on the internal structure of language.\nThe two simple but effective ideas behind GloVe and FastText are subword units and global co-occurrence. Let\u0026rsquo;s dive into them.\nGloVe: A Global Mapping Strategy Global Vectors for Word Representation (GloVe) was created at Stanford. The core idea was to train a model where word vectors are defined by how often they appear around other words across the entire corpus, rather than just locally.\nTraining Steps: Co-occurrence Matrix X: GloVe scans the entire corpus and creates a giant matrix (spreadsheet) recording the counts of how often words appear near each other. Factorization: It compresses this giant matrix into smaller, dense vectors. Objective: The function tries to equate the dot product of two word vectors with the logarithm of their probability of appearing together. The error between these values is then backpropagated to update the vectors. $$w_i \\cdot w_j + b_i + b_j = \\log(X_{ij})$$\n$w$: The word vector (what we want). $X_{ij}$: The count from our matrix. The Loss Function: We can\u0026rsquo;t satisfy the equation above perfectly for every word pair. So, we use a weighting function to minimize the squared error:\n$$J = \\sum_{i,j=1}^V f(X_{ij}) ({w_i}^T w_j + b_i + b_j - \\log(X_{ij}))^2$$\nIf we do not use the weighting function, the loss would be infinite for rare words (since $\\log(0) = \\infty$) and frequent words (like \u0026rsquo;the\u0026rsquo;) would overpower the model.\n$$ f(x) = \\begin{cases} (x/x_{max})^\\alpha \u0026amp; \\text{if } x \u0026lt; x_{max} \\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$\n(Typically $x_{max}=100$ and $\\alpha=0.75$)\nFastText: The Lego Bricks Strategy FastText was created by Facebook AI Research (FAIR). The idea was to treat words not as solid rocks, but as structures made of \u0026ldquo;Lego bricks\u0026rdquo; (subword units).\nWord2Vec struggled to create embeddings for words it hadn\u0026rsquo;t seen before (Out-Of-Vocabulary or OOV words). FastText solves this by breaking words down.\nGloVe treats a word like \u0026ldquo;Apple\u0026rdquo; as a single, atomic unit. FastText breaks \u0026ldquo;Apple\u0026rdquo; into character n-grams: \u0026lt;ap, app, ppl, ple, le\u0026gt; For a complex or new word like \u0026ldquo;Applesauce\u0026rdquo;, the model might not know the whole word, but it recognizes the Lego brick for \u0026ldquo;Apple\u0026rdquo; and the remaining bits from other subwords.\nFastText takes its inspiration from the SkipGram architecture. The vector for a word is simply the sum of the vectors of its n-gram subwords:\n$$v_w = \\sum_{g \\in G_w} z_g$$\nObjective: The objective is to predict context words given a target word.\n$$P(w_c | w_t) = \\frac{e^{s(v_{w_t} \\cdot v_{w_c})}}{\\sum_{w \\in V} e^{s(v_{w_t} \\cdot v_w)}}$$\nAnd the scoring function is defined as:\n$$s(w_t, w_c) = u_{w_t}^T (\\sum_{g \\in G_w} z_g)$$\n$u_{w_c}$: The vector for the context word (output vector). $z_g$: The vector for the n-gram $g$ (input vector). Modifications to Softmax: Calculating the full softmax for every word in the vocabulary is computationally expensive. FastText uses two methods to approximate this efficiently.\nOption A: Negative Sampling\nWe pick a few random \u0026ldquo;wrong\u0026rdquo; words every time to serve as \u0026ldquo;negative samples\u0026rdquo; and calculate the loss only for the correct word and these noise words.\n$$J = - \\log \\sigma(s(w_t, w_c)) - \\sum\\limits_{i=1}^{N} \\mathbb{E}_{n_i \\sim P_n(w)} [\\log \\sigma(-s(w_t, n_i))]$$\nThe sigmoid function $\\sigma$ squashes the score between 0 and 1. The first term pushes the probability of the correct word towards 1. The second term pushes the probability of the noise words towards 0. Option B: Hierarchical Softmax (Faster for infrequent words)\nInstead of a flat list of words, imagine the vocabulary arranged as a binary tree (Huffman Tree).\nRoot: Top of the tree. Leaves: The actual words. To calculate the probability of a word, we trace a path from the root to that leaf. At every branching node, we calculate the probability (sigmoid) of going left or right. The probability of a word $w$ is the product of the probabilities of the turns taken to reach it. Code Example from gensim.models import FastText from gensim.test.utils import common_texts # Initialize the FastText model with specific hyperparameters model = FastText( vector_size=100, # Dimensionality of the word vectors window=5, # Maximum distance between the current and predicted word within a sentence min_count=1, # Ignores all words with total frequency lower than this sg=1, # Training algorithm: 1 for skip-gram; 0 for CBOW hs=0, # If 0, and negative is non-zero, negative sampling will be used negative=5, # Number of \u0026#34;noise words\u0026#34; to be drawn for negative sampling workers=4, # Number of worker threads to train the model epochs=10, # Number of iterations over the corpus min_n=3, # Minimum length of character n-grams max_n=6 # Maximum length of character n-grams ) # Build the vocabulary from the provided text corpus model.build_vocab(common_texts) # Train the model on the corpus model.train( common_texts, total_examples=model.corpus_count, epochs=model.epochs ) # Calculate and print the cosine similarity between two words in the vocabulary print(f\u0026#34;Similarity between \u0026#39;computer\u0026#39; and \u0026#39;human\u0026#39;: {model.wv.similarity(\u0026#39;computer\u0026#39;, \u0026#39;human\u0026#39;)}\u0026#34;) # Demonstrate FastText\u0026#39;s ability to handle Out-Of-Vocabulary (OOV) words # Even if \u0026#39;computation\u0026#39; wasn\u0026#39;t in the training data, FastText constructs a vector using character n-grams oov_vector = model.wv[\u0026#39;computation\u0026#39;] # Check if a specific word exists in the model\u0026#39;s fixed vocabulary index word = \u0026#34;computer\u0026#34; is_in_vocab = word in model.wv.key_to_index print(f\u0026#34;\\nIs the word \u0026#39;{word}\u0026#39; in the model\u0026#39;s vocabulary? {is_in_vocab}\u0026#34;) ","permalink":"http://localhost:1313/posts/glove-and-fasttext/","summary":"How subword units and global co-occurrence matrices allow GloVe and FastText to capture nuances that Word2Vec missed.","title":"The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText"},{"content":" Before we had Large Language Models writing poetry, we had to teach computers that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; are related not just by spelling, but by meaning. This is the story of that breakthrough. It‚Äôs the moment we stopped counting words and started mapping their souls‚Äîturning raw text into a mathematical landscape where math can solve analogies. Welcome to the world of Word2Vec. üîÆ\nLanguage models require vector representations of words to capture semantic relationships. Before the 2010s, models used word count-based vector representations that captured only the frequency of words (e.g., One-Hot Encoding). The Problems: üöß\nThe Curse of Dimensionality Lack of meaning (Synonyms were treated as mathematically unrelated). In 2011, Mikolov et al. at Google introduced Word2Vec, which used a shallow neural network to learn vector representations of words. The best part? It could create much denser relationships between words than prior models, and it was unsupervised.\nThe training (prediction) is essentially \u0026ldquo;fake\u0026rdquo;‚Äîa pretext task. But the weight matrices we get on the side are a gold mine of fine-grained semantic relationships. ‚õèÔ∏è\nThere are two popular variants of Word2Vec:\nSkipGram Continuous Bag Of Words (CBOW) 1. Continuous Bag of Words (CBOW) üß© CBOW is a simple \u0026lsquo;fill in the blanks\u0026rsquo; machine. It takes the surrounding words as inputs and tries to predict the center word.\n\u0026ldquo;Continuous\u0026rdquo;: It operates in continuous vector space (unlike the discrete space of n-gram models).\n\u0026ldquo;Bag of words\u0026rdquo;: The order of the context does not matter. \u0026ldquo;The cat sat on the mat\u0026rdquo; and \u0026ldquo;The mat sat on the cat\u0026rdquo; are the same for CBOW. They produce exactly the same predictions.\nObjective: Maximize the probability of the target word $w_t$ given its context $w_{t-C}, \\dots, w_{t+C}$.\n2. Training Defining Dimensions:\n$V$: Vocabulary size (number of unique words in the corpus, e.g., 10,000) $N$: Dimension of the embedding space (e.g., 100) $C$: Number of context words (e.g., 2 words on left of target and 2 words on right) Step 1: Input Lookup \u0026amp; One-Hot Encoding\nMathematically, we represent each context word as a one-hot vector $x^{(c)}$. We have an Input Matrix $W_{in}$ of size $V \\times N$. For each context word (e.g., \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;), we select its corresponding row from $W_{in}$.\n$$ v_c = W_{in}^T x^{(c)} $$\nWhere $v_c$ is the $N$-dimensional vector for context word $c$.\nStep 2: Projection (The \u0026ldquo;Mixing\u0026rdquo;)\nWe combine the context vectors. In standard CBOW, we simply average them. This creates the hidden layer vector $h$.\n$$ h = \\frac{1}{2C} \\sum_{c=1}^{2C} v_c $$\n$h$ is a single vector of size $N \\times 1$. Note: This operation is linear and contains no activation function (like ReLU or Sigmoid).\nStep 3: Output Scoring (The \u0026ldquo;Dot Product\u0026rdquo;)\nWe have a second matrix, the Output Matrix $W_{out}$ of size $N \\times V$. We compute the raw score (logit) $u_j$ for every word $j$ in the vocabulary by taking the dot product of the hidden vector $h$ and the output vector $v\u0026rsquo;_j$.\n$$ u_j = {v\u0026rsquo;_j}^T h $$\nOr in matrix form:\n$$ u = W_{out}^T h $$\nResult $u$ is a vector of size $V \\times 1$.\nStep 4: Probability Conversion (Softmax)\nWe convert the raw scores $u$ into probabilities using the Softmax function. This tells us the probability that word $w_j$ is the center word.\n$$ y_j = P(w_j | \\text{context}) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 5: Loss Calculation (Cross-Entropy)\nWe compare the predicted distribution $y$ with the actual ground truth $t$ (a one-hot vector where the index of the true word is 1). The loss function $E$ is:\n$$ E = - \\sum_{j=1}^V t_j \\log(y_j) $$\nSince $t_j$ is 0 for all words except the target word (let\u0026rsquo;s call the target index $j^*$), this simplifies to:\n$$ E = - \\log(y_{j^*}) $$\n3. Backpropagation Details We need to update the weights to minimize $E$. We use the Chain Rule.\nA. Gradient w.r.t. Output Matrix ($W_{out}$)\nWe want to know how the error changes with respect to the raw score $u_j$.\n$$ \\frac{\\partial E}{\\partial u_j} = y_j - t_j $$\nLet\u0026rsquo;s call this error term $e_j$.\nIf $j$ is the target word: $e_j = y_j - 1$ (Result is negative; we boost the score). If $j$ is not target: $e_j = y_j - 0$ (Result is positive; we suppress the score). The update rule for the output vector $v\u0026rsquo;_j$ becomes:\n$$ v'_{j}(\\text{new}) = v'_{j}(\\text{old}) - \\eta \\cdot e_j \\cdot h $$ ($\\eta$ is the learning rate)\nB. Gradient w.r.t. Hidden Layer ($h$)\nWe backpropagate the error from the output layer to the hidden layer.\n$$ EH = \\sum_{j=1}^V e_j \\cdot v\u0026rsquo;_j $$\n$EH$ is an $N$-dimensional vector representing the aggregate error passed back to the projection layer.\nC. Gradient w.r.t. Input Matrix ($W_{in}$)\nSince $h$ was just an average of the input vectors, the error $EH$ is distributed to each context word\u0026rsquo;s input vector. For every word $w_c$ in the context:\n$$ v_{w_c}(\\text{new}) = v_{w_c}(\\text{old}) - \\eta \\cdot \\frac{1}{2C} \\cdot EH $$\nLet\u0026rsquo;s build a single pass through the network for a given context.\nimport torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # Define a simple Continuous Bag of Words (CBOW) style model class CBOWModel(nn.Module): def __init__(self, vocab_size, embedding_dim): super(CBOWModel, self).__init__() # The embedding layer stores the word vectors we want to learn self.embeddings = nn.Embedding(vocab_size, embedding_dim) # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word self.linear = nn.Linear(embedding_dim, vocab_size) def forward(self, inputs): # inputs: tensor of word indices for the context embeds = self.embeddings(inputs) # Aggregate context by calculating the mean of the word embeddings h = torch.mean(embeds, dim=1) # Produce logits (raw scores) for each word in the vocabulary logits = self.linear(h) return logits # Setup vocabulary and mappings word_to_ix = {\u0026#34;the\u0026#34;: 0, \u0026#34;quick\u0026#34;: 1, \u0026#34;brown\u0026#34;: 2, \u0026#34;fox\u0026#34;: 3, \u0026#34;jumps\u0026#34;: 4, \u0026#34;over\u0026#34;: 5, \u0026#34;lazy\u0026#34;: 6, \u0026#34;dog\u0026#34;: 7} ix_to_word = {v: k for k, v in word_to_ix.items()} # Configuration constants EMBEDDING_DIM = 5 VOCAB_SIZE = len(word_to_ix) LEARNING_RATE = 0.01 # Initialize the model, loss function, and optimizer model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM) loss_function = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE) # Prepare dummy training data: context words and the target word # Context: [\u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;over\u0026#34;] -\u0026gt; Target: \u0026#34;fox\u0026#34; context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long) target_idxs = torch.tensor([3], dtype=torch.long) # Perform a single optimization step model.zero_grad() # Forward pass: we reshape context to (1, -1) to simulate a batch of size 1 logits = model(context_idxs.view(1, -1)) # Calculate loss against the target word index loss = loss_function(logits, target_idxs) # Backpropagate and update weights loss.backward() optimizer.step() # Output results for the blog post print(\u0026#34;=== Model Training Snapshot ===\u0026#34;) print(f\u0026#34;Calculated Loss: {loss.item():.6f}\u0026#34;) print(f\u0026#34;\\n=== Learned Vector for \u0026#39;jumps\u0026#39; ===\u0026#34;) word_vec = model.embeddings(torch.tensor([word_to_ix[\u0026#39;jumps\u0026#39;]])) print(word_vec.detach().numpy()) print(\u0026#34;\\n=== Embedding Matrix (Weights) ===\u0026#34;) print(model.embeddings.weight.detach().numpy()) print(\u0026#34;\\n=== Linear Layer Weights ===\u0026#34;) print(model.linear.weight.detach().numpy()) Output shows single pass loss and learned vector for \u0026lsquo;jumps\u0026rsquo; word. This is a single training sample result but after multiple such passes both Embedding and linear weights should be very similar.\n=== Model Training Snapshot === Calculated Loss: 1.989656 === Learned Vector for \u0026#39;jumps\u0026#39; === [[-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775]] === Embedding Matrix (Weights) === [[ 1.66039 -0.11371879 0.6246518 0.35860053 -1.2504417 ] [-2.1847186 -0.77199775 -0.17050214 -0.38411248 -0.03913084] [ 0.11852697 0.90073633 0.8847807 0.7404524 0.900149 ] [ 0.07440972 -0.40259898 2.6246994 -0.08851447 0.02660969] [-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775] [-0.9245572 -0.5545908 0.9083091 -1.0755049 0.84047747] [-1.0237687 0.59466314 0.05621134 -0.6202532 1.3664424 ] [ 0.60998917 -1.0549186 1.6103884 0.8724912 -1.2486908 ]] === Linear Layer Weights === [[ 0.02165058 -0.28883642 0.14545658 -0.3442509 0.32704315] [-0.18731792 0.28583744 0.22635977 0.13245736 0.29019794] [ 0.3158916 -0.15826383 -0.03203773 0.16377363 -0.41457543] [-0.05080034 0.4180087 0.11228557 0.30218413 0.3025514 ] [-0.38419306 0.24475925 -0.39210224 -0.38660625 -0.2673145 ] [-0.32321444 0.12200444 -0.03569533 0.2891424 -0.07345333] [-0.33704326 0.2521956 0.31587374 0.22590035 0.29866052] [ 0.4117266 -0.44231793 0.24064957 -0.29684234 0.333821 ]] Interesting Facts About CBOW üß† The \u0026ldquo;Averaging Problem\u0026rdquo;: This is a common issue in CBOW models. The averaging and training is order-agnostic. All variants of word order in the context window will generate the same output. \u0026ldquo;Dog bit the man\u0026rdquo; and \u0026ldquo;Man bit the dog\u0026rdquo; are the same in CBOW\u0026rsquo;s eyes.\nSmoothing Effect: CBOW models are much faster to train than Skip-Gram but are generally smoother.\nRare Words: CBOW models struggle slightly with rare words because the presence of surrounding common words (like articles and prepositions) can skew the embeddings and dissolve their uniqueness.\nComputational Cost: Softmax requires summing over the entire vocabulary size, which can be computationally expensive if $V$ is very large. Solutions include Hierarchical Softmax and Negative Sampling (approximating the denominator by only checking the target word vs. 5 random noise words).\nVectors as Double Agents: Every word has a different representation in the embedding matrix and the output matrix. Most of the time $W_{out}$ is discarded, but some consider averaging both $W_{in}$ and $W_{out}$ for slightly better performance.\nLinear Relationships: The famous analogy King - Man + Woman = Queen emerges from the model because of the linear relationships between the words.\nInitialization: This is a no-brainer but worth mentioning. Initialization to zero vectors would mean no learning. We must initialize randomly.\n2. Skip-Gram (The \u0026ldquo;Burger Flip\u0026rdquo; of CBOW) üîÑ Skip-Gram flips the CBOW logic. While CBOW tries to predict the center word given the context, Skip-Gram tries to predict the scattered context given the center word.\nCore Concept: If a word like \u0026ldquo;monarch\u0026rdquo; can accurately predict words like \u0026ldquo;king\u0026rdquo;, \u0026ldquo;throne\u0026rdquo;, \u0026ldquo;crown\u0026rdquo;, \u0026ldquo;royalty\u0026rdquo;, and \u0026ldquo;power\u0026rdquo;, then it must effectively capture the concept of royalty.\nTraining Let\u0026rsquo;s define dimensions:\n$V$: Vocabulary size $D$: Embedding dimension $C$: Number of context words Step 1: Input Lookup\nUnlike CBOW, our input is just one word vector (the center word $w_t$). We grab the vector $v_c$ from the Input Matrix $W_{in}$.\n$$ h = v_{w_t} $$\nNote: There is no averaging here. The hidden layer $h$ is simply the raw vector of the center word.\nStep 2: Output Scoring (The Broadcast)\nThe model takes this single vector $h$ and compares it against the Output Matrix $W_{out}$ (the entire vocabulary). It computes a score $u_j$ for every word $j$ in the dictionary.\n$$ u = W_{out}^T h $$\nResult $u$: A vector of size $V \\times 1$ containing raw scores.\nStep 3: Probability Conversion (Softmax)\nWe apply Softmax to turn scores into probabilities.\n$$ P(w_j | w_t) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 4: The Multi-Target Loss\nHere is the major difference from CBOW. In CBOW, we had one target (the center). In Skip-Gram, we have $2C$ targets (all the surrounding words). For a sentence \u0026ldquo;The cat sat on mat\u0026rdquo; (Center: \u0026ldquo;sat\u0026rdquo;), the targets are \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;. We want to maximize the probability of all these true context words. The Loss function ($E$) sums the error for each context word:\n$$ E = - \\sum_{w_{ctx} \\in \\text{Window}} \\log P(w_{ctx} | w_t) $$\nStep 5: Backpropagation (Accumulating Gradients)\nSince one input word is responsible for predicting multiple output words, the error signals from all those context words add up.\nAt the Output Layer:\nIf \u0026ldquo;cat\u0026rdquo; was a target, the \u0026ldquo;cat\u0026rdquo; vector in $W_{out}$ gets a signal: \u0026ldquo;Move closer to \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; If \u0026ldquo;apple\u0026rdquo; was NOT a target, the \u0026ldquo;apple\u0026rdquo; vector gets a signal: \u0026ldquo;Move away from \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; At the Hidden Layer ($h$):\nThe center word \u0026ldquo;sat\u0026rdquo; receives feedback from all its neighbors simultaneously. Error signal = (Error from \u0026ldquo;The\u0026rdquo;) + (Error from \u0026ldquo;cat\u0026rdquo;) + (Error from \u0026ldquo;on\u0026rdquo;)\u0026hellip; The vector for \u0026ldquo;sat\u0026rdquo; in $W_{in}$ moves in the average direction of all its neighbors. 3. The \u0026ldquo;Computational Nightmare\u0026rdquo; \u0026amp; Negative Sampling The equations above describe Naive Softmax.\nThe Problem: If $V = 100,000$, computing the denominator $\\sum \\exp(u_k)$ takes 100,000 operations. Doing this for every word in a training set of billions of words is impossibly slow.\nThe Solution: Negative Sampling Instead of updating the entire vocabulary (1 correct word vs 99,999 wrong ones), we approximate the problem.\nPositive Pair: (sat, cat) $\\rightarrow$ Maximize probability (Label 1). Negative Pairs: We pick $K$ (e.g., 5) random words the model didn\u0026rsquo;t see, e.g., (sat, bulldozer), (sat, quantum). $\\rightarrow$ Minimize probability (Label 0). New Equation (Sigmoid instead of Softmax):\n$$ E = - \\log \\sigma({v'_{\\text{pos}}}^T v_{\\text{in}}) - \\sum_{k=1}^K \\log \\sigma(-{v'_{\\text{neg}_{k}}}^T v_{\\text{in}}) $$ Effect: We typically only update 6 vectors per step (1 pos + 5 negs) instead of 100,000.\nLet\u0026rsquo;s see this in code:\nimport torch import torch.nn as nn import torch.optim as optim class SkipGramNegativeSampling(nn.Module): \u0026#34;\u0026#34;\u0026#34; Skip-Gram with Negative Sampling (SGNS) implementation. SGNS approximates the softmax over the entire vocabulary by instead distinguishing between a real context word (positive) and K noise words (negative). \u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embedding_dim): super(SkipGramNegativeSampling, self).__init__() # Input embeddings: used when the word is the center word self.in_embed = nn.Embedding(vocab_size, embedding_dim) # Output embeddings: used when the word is a context or negative sample self.out_embed = nn.Embedding(vocab_size, embedding_dim) # Initialize weights with small values to prevent gradient saturation initrange = 0.5 / embedding_dim self.in_embed.weight.data.uniform_(-initrange, initrange) self.out_embed.weight.data.uniform_(-initrange, initrange) def forward(self, center_words, target_words, negative_words): \u0026#34;\u0026#34;\u0026#34; Computes the negative sampling loss. Args: center_words: (batch_size) target_words: (batch_size) negative_words: (batch_size, K) where K is number of negative samples \u0026#34;\u0026#34;\u0026#34; # Retrieve vectors v_c = self.in_embed(center_words) # (batch_size, embed_dim) u_o = self.out_embed(target_words) # (batch_size, embed_dim) u_n = self.out_embed(negative_words) # (batch_size, K, embed_dim) # 1. Positive Score: log(sigmoid(v_c ¬∑ u_o)) # Compute dot product: (batch, 1, dim) @ (batch, dim, 1) -\u0026gt; (batch, 1) pos_score = torch.bmm(u_o.unsqueeze(1), v_c.unsqueeze(2)).squeeze(2) pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-7) # 2. Negative Score: sum(log(sigmoid(-v_c ¬∑ u_n))) # Compute dot products for all K samples: (batch, K, dim) @ (batch, dim, 1) -\u0026gt; (batch, K) neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2) neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-7), dim=1, keepdim=True) # Total loss is the negative of the objective function loss = -(pos_loss + neg_loss) return torch.mean(loss) # --- Configuration \u0026amp; Mock Data --- VOCAB_SIZE = 100 EMBED_DIM = 10 word_to_ix = {\u0026#39;fox\u0026#39;: 0} # Example vocabulary mapping model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock inputs: 1 center word, 1 target word, 5 negative samples center_id = torch.tensor([0], dtype=torch.long) target_id = torch.tensor([1], dtype=torch.long) negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long) # Training Step model.zero_grad() loss = model(center_id, target_id, negative_ids) loss.backward() optimizer.step() # Output Results print(f\u0026#34;Loss after one step: {loss.item():.6f}\u0026#34;) word_vec = model.in_embed(torch.tensor([word_to_ix[\u0026#39;fox\u0026#39;]])) print(f\u0026#34;Vector for \u0026#39;fox\u0026#39;:\\n{word_vec.detach().numpy()}\u0026#34;) The outputs:\nLoss after one step: 4.158468 Vector for \u0026#39;fox\u0026#39;: [[ 0.04197385 0.02400733 -0.03800093 0.01672485 -0.03872231 -0.0061478 0.0121122 0.04057864 -0.036255 0.03861175]] Interesting Facts About Skip-Gram üí° Makes Rare Words Shine: Skip-Gram is more effective at capturing the meaning of rare words because it focuses on the context of the center word. In CBOW, a rare word\u0026rsquo;s vector is averaged with others, diluting its signal. In Skip-Gram, when the rare word appears as the center, it gets the full, undivided attention of the backpropagation update.\nSlow Training: This method creates more training samples. A window of size 2 creates 4 training pairs per center word, while CBOW creates only 1 training pair per center word.\nSemantic Focus: Skip-Gram puts more emphasis on the semantic relationship between words. It views the context as the primary signal, with the center word as the target. This makes it better at capturing the meaning of words in context. Skip-Gram tends to capture semantic relationships (King/Queen) slightly better, while CBOW captures syntactic ones (walk/walked) slightly better.\n","permalink":"http://localhost:1313/posts/word-embeddings/","summary":"Understanding the mathematics behind Word2Vec, CBOW, and Skip-Gram and how they map language to vector space.","title":"Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n5. Code Examples: We\u0026rsquo;ll use sentencepiece for tokenization and compare results for different models.\nimport sentencepiece as spm import os # Configuration for training the SentencePiece model # SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words. options = { # The source text file used for learning the vocabulary \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # The base name for the generated model (.model) and vocabulary (.vocab) files \u0026#39;model_prefix\u0026#39;: \u0026#39;bpe_model\u0026#39;, # Number of unique tokens in the final vocabulary \u0026#39;vocab_size\u0026#39;: 4000, # \u0026#39;bpe\u0026#39; (Byte Pair Encoding) merges frequent pairs of characters/sequences \u0026#39;model_type\u0026#39;: \u0026#39;bpe\u0026#39;, # Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets \u0026#39;character_coverage\u0026#39;: 0.9995, # When enabled, unknown characters are decomposed into UTF-8 bytes to avoid \u0026#39;unk\u0026#39; tokens \u0026#39;byte_fallback\u0026#39;: True, # Treats digits individually (0-9), preventing large numbers from being treated as single tokens \u0026#39;split_digits\u0026#39;: True, # Prevents adding a whitespace prefix to the first token; useful for fine-grained control \u0026#39;add_dummy_prefix\u0026#39;: False } try: print(\u0026#34;Starting the training process...\u0026#34;) # SentencePieceTrainer.train takes the dictionary of options to build the BPE model spm.SentencePieceTrainer.train(**options) print(\u0026#34;Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created.\u0026#34;) # Initialize the processor and load the newly trained model sp = spm.SentencePieceProcessor() sp.load(\u0026#39;bpe_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Model Metadata:\u0026#34;) # Retrieve the total number of tokens in the vocabulary print(f\u0026#34;Total Vocab Size: {sp.get_piece_size()}\u0026#34;) # Special tokens are used for sequence boundaries and handling unknown characters print(f\u0026#34;BOS (Beginning of Sentence) ID: {sp.bos_id()}\u0026#34;) print(f\u0026#34;EOS (End of Sentence) ID: {sp.eos_id()}\u0026#34;) print(f\u0026#34;UNK (Unknown) ID: {sp.unk_id()}\u0026#34;) print(f\u0026#34;PAD (Padding) ID: {sp.pad_id()}\u0026#34;) # Test the tokenizer on sample strings to see how it breaks down text test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: shows the actual subword units (tokens) print(f\u0026#34;Subword Tokens: {sp.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: shows the numerical mapping for each token print(f\u0026#34;Numerical IDs: {sp.encode_as_ids(text)}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred during training or processing: {e}\u0026#34;) You can see the following output:\nStarting training... Starting the training process... Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created. ------------------------------ Model Metadata: Total Vocab Size: 4000 BOS (Beginning of Sentence) ID: 1 EOS (End of Sentence) ID: 2 UNK (Unknown) ID: 0 PAD (Padding) ID: -1 --- Tokenization Test --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;el\u0026#39;, \u0026#39;lo\u0026#39;, \u0026#39;‚ñÅW\u0026#39;, \u0026#39;orld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 380, 309, 476, 619, 36, 3924, 3964, 3959, 3993, 3978, 3983, 57, 3975, 3976, 60, 3974] --- Tokenization Test --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅmo\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;inst\u0026#39;, \u0026#39;ag\u0026#39;, \u0026#39;ram\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅever\u0026#39;] Numerical IDs: [1894, 304, 3284, 376, 321, 313, 312, 1008, 3313, 1325, 428, 3937, 341, 304, 3284, 1495] The ! is fallen back to UTF bytes. and the word instagrammable is split into multiple subword tokens. Also, original text can be recreated right back by simply concatenating the tokens. And replacing underscore by whitespace. Now we can try Unigram model. and see the same sentences tokenized differently.\nfrom tokenizers import Tokenizer from tokenizers.models import WordPiece from tokenizers.trainers import WordPieceTrainer from tokenizers.pre_tokenizers import Whitespace try: # 1. Initialize the WordPiece Tokenizer # We specify the [UNK] token for handling words not found in the vocabulary. tokenizer = Tokenizer(WordPiece(unk_token=\u0026#34;[UNK]\u0026#34;)) # 2. Configure Pre-tokenization # Before the subword algorithm runs, we need to split the raw text into words. # Whitespace splitting is the standard first step for most English NLP tasks. tokenizer.pre_tokenizer = Whitespace() # 3. Initialize the Trainer # We define our target vocabulary size and the special tokens required for # downstream tasks (like BERT\u0026#39;s [CLS] for classification or [SEP] for separators). trainer = WordPieceTrainer( vocab_size=4000, special_tokens=[\u0026#34;[UNK]\u0026#34;, \u0026#34;[CLS]\u0026#34;, \u0026#34;[SEP]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[MASK]\u0026#34;] ) # 4. Train the Model # The tokenizer scans the training file to build a vocabulary of the most # frequent subword units. tokenizer.train(files=[\u0026#34;train_data.txt\u0026#34;], trainer=trainer) # 5. Persist the Model # Save the configuration and vocabulary to a JSON file for future inference. tokenizer.save(\u0026#34;wordpiece.json\u0026#34;) print(\u0026#34;Training complete. \u0026#39;wordpiece.json\u0026#39; created.\u0026#34;) # 6. Metadata Inspection print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;WordPiece Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {tokenizer.get_vocab_size()}\u0026#34;) # 7. Testing Subword Tokenization # WordPiece shines at handling rare words by breaking them into meaningful chunks. test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (WordPiece) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # Encode converts raw text into a Tokenizer object containing tokens and IDs output = tokenizer.encode(text) # \u0026#39;tokens\u0026#39; shows the subword breakdown (e.g., \u0026#39;un\u0026#39;, \u0026#39;##insta\u0026#39;, etc.) print(f\u0026#34;Subword Tokens: {output.tokens}\u0026#34;) # \u0026#39;ids\u0026#39; are the numerical indices mapped to the vocabulary print(f\u0026#34;Numerical IDs: {output.ids}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred with WordPiece model: {e}\u0026#34;) You can see the following output:\nWordPiece Model Metadata: Total Vocab Size: 2609 --- Tokenization Test (WordPiece) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;H\u0026#39;, \u0026#39;##el\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;##or\u0026#39;, \u0026#39;##ld\u0026#39;, \u0026#39;[UNK]\u0026#39;, \u0026#39;[UNK]\u0026#39;] Numerical IDs: [37, 180, 214, 52, 162, 418, 0, 0] --- Tokenization Test (WordPiece) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;##os\u0026#39;, \u0026#39;##t\u0026#39;, \u0026#39;un\u0026#39;, \u0026#39;##ins\u0026#39;, \u0026#39;##ta\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;##ra\u0026#39;, \u0026#39;##m\u0026#39;, \u0026#39;##ma\u0026#39;, \u0026#39;##ble\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;ever\u0026#39;] Numerical IDs: [691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240] We can see the special character ## in the output. We can also see the breakdowns not having any semantic meaning. They are likelihood based. We can try the Unigram model now.\n# \u0026#39;Unigram\u0026#39; is the default and usually recommended over BPE in SentencePiece. options_unigram = { \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # Path to the raw text file for training \u0026#39;model_prefix\u0026#39;: \u0026#39;unigram_model\u0026#39;, # Prefix for the output .model and .vocab files \u0026#39;vocab_size\u0026#39;: 1200, # Desired size of the final vocabulary \u0026#39;model_type\u0026#39;: \u0026#39;unigram\u0026#39;, # Specifies the Unigram language model algorithm \u0026#39;character_coverage\u0026#39;: 0.9995, # Percentage of characters covered by the model (0.9995 is standard for Latin scripts) \u0026#39;byte_fallback\u0026#39;: True, # Enables mapping unknown characters to UTF-8 bytes to avoid \u0026lt;unk\u0026gt; tokens \u0026#39;split_digits\u0026#39;: True, # Treats each digit as an individual token (useful for numerical data) \u0026#39;add_dummy_prefix\u0026#39;: False # Prevents adding a leading space (SentencePiece default is True) } try: # 1. Train the SentencePiece model using the defined options print(\u0026#34;Starting Unigram training...\u0026#34;) spm.SentencePieceTrainer.train(**options_unigram) print(\u0026#34;Training complete. \u0026#39;unigram_model.model\u0026#39; created.\u0026#34;) # 2. Load the trained model into a processor instance for inference sp_unigram = spm.SentencePieceProcessor() sp_unigram.load(\u0026#39;unigram_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Unigram Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {sp_unigram.get_piece_size()}\u0026#34;) # 3. Define test cases to evaluate how the model handles common and rare words test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] # 4. Iterate through test sentences to visualize subword segmentation for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (Unigram) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: Converts text into subword strings (visual representation) print(f\u0026#34;Subword Tokens: {sp_unigram.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: Converts text into numerical indices for model input print(f\u0026#34;Numerical IDs: {sp_unigram.encode_as_ids(text)}\u0026#34;) except Exception as e: # Handle potential errors during training or loading (e.g., missing input file) print(f\u0026#34;An error occurred with Unigram model: {e}\u0026#34;) The output is as follows:\nUnigram Model Metadata: Total Vocab Size: 1200 --- Tokenization Test (Unigram) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ll\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;ld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311] --- Tokenization Test (Unigram) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;T\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅm\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;sta\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;ra\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ver\u0026#39;] ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"A comprehensive guide to tokenization strategies: BPE, WordPiece, Unigram, and SentencePiece.","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"A brief introduction to Vectors \u0026amp; Verbs and formatting verification.","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"\nFeedforward network (FFN) blocks are the least talked about when in context to LLMs. But that is where more than 70% of the paramters come from. Something as simple and convinient as a couple of linear transforming matrices become a burden when models start scaling in Billions of parameters. So, getting the maximum juice of a lean structure is a challenge. Plus, mostly under represented, because the FFN sub-layer acts as the primary \u0026ldquo;knowledge\u0026rdquo; processing engine, often containing the majority of a model\u0026rsquo;s total parameters.\nStandard Position Wise Feedforward Network This is as simple as it can get, two-layered fully connected network of neurons with a single activation switch, most probably a ReLU variant (for LLMs). $$ FFN(x) = max(0, W_2(W_1(x)) + b_1)W_2 + b_2 $$ Same transformation is applied for each token in parallel, making it efficient for GPU computation. For Example, in BERT model the 512 dimensions of each token are expanded to 2048 to allow for \u0026ldquo;intricate interactions\u0026rdquo; before projecting back to 512 dimensions. This part is important, in every transformer stacking of layers is possible only because the output of every matches it\u0026rsquo;s input dimensions.\nSWiGLU Sigmoid Weighted Gated Linear Unit (SWiGLU) has become the Gold Standard for feed forward networks in LLMs like Llama 3 and PaLM. The primary reason for this choice is the Swish function (as shown in the first image) instead of ReLU or GELU are as following:\nMathematical Linearity: Swish (SILU) is defined as $x\\cdot sigmoid(\\beta x)$, $\\beta$ is a learnable parameter. It is a small monotonic function that allowes small negetive values to pass through, making it mathematically linear. While GELU ($x \\cdot \\phi(x)$) is also smooth and nonmonotonic, Swish\u0026rsquo;s curve and equations have emperically improved the model\u0026rsquo;s performance.\nComputational Efficiency: SILU is simpler to compute. $x/(1+exp(-x))$ compared to the standard GELU, which is an approximation of the tanh function. In the massive scale of modern LLMs these shortcuts save significant training time.\nPerformance: Research in the PaLM and Llama papers found that Swish-based Gated Linear Units consistently outperformed GELU-based versions in terms of perplexity and downstream task accuracy. $$SwiGLU(x, W, V, b, c) = (Swish(xW + b) \\otimes (xV + c))W_2$$ Where $W$ and $V$ are weight matrices for the gate and the data path, and $\\otimes$ is element-wise multiplication.\nMore Complex Filtering: Instead of using ordinary two weight matrices in standard FFN, SWiGLU uses three weight matrices (gate, date, output). This allows it to gather more complex features out of the input without drastically increasing the number of parameters. This performance boost has resulted in modern implementations being low in dimensionality of hidden layers (2/3 of input dimensions) while retaining the performance benefits of gated activations.\nZero-Gradient Avoidance: Unlike ReLU, which has a zero gradient for all negative values (the \u0026ldquo;dying ReLU\u0026rdquo; problem), SwiGLU\u0026rsquo;s smooth curve ensures that the model can still learn from negative inputs, improving training stability in deep stacks.\n","permalink":"http://localhost:1313/posts/feedforward-networks/","summary":"Exploring the hidden layers of trillion-parameter switchboards: Feedforward Neural Networks and Activation Functions.","title":"Anatomy of Trillion-Parameter Switchboards: Understanding Feedforward Blocks"},{"content":"\nWe are about to touch the holy grail of modern AI. The 2017 declaration was simple: Attention Is All You Need. But as we‚Äôve raced from GPT-1 to reasoning models, the definition of \u0026lsquo;Attention\u0026rsquo; has quietly transformed. Is the algorithm ruling the world today actually the same one we started with? Let‚Äôs look under the hood.\nüî• The Core Concept At its heart, the attention mechanism is a form of weighted aggregation. It is often described using a database retrieval analogy. Imagine you have a database of information (context).\nQuery ($Q$): What are you currently looking for? (The token being generated). Key ($K$): The indexing tag associated with each piece of information. Value ($V$): The actual information (The context token). The \u0026ldquo;Attention\u0026rdquo; is the process of matching your Query against all the Keys to determine how much \u0026ldquo;weight\u0026rdquo; to put on each Value. Attention scores are calculated in each transformer layer at every step of the sequence. This is a single unifying factor in all transformer models, from BERT to Gemini.\nüî• Standard Multi-Head Attention (MHA) The original mechanism introduced in the paper \u0026ldquo;Attention Is All You Need\u0026rdquo;.\nHere, instead of performing the Key-Query-Value (KQV) operation on the entire context only once, the attention is split into $N$ parallel heads. Each head learns a different aspect of the language (e.g., word meaning, syntax, semantic relationships, etc.). The equation is a simple culmination of dot products aided by a regularizer like Softmax.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nWhere:\n$QK^T$ computes the similarity scores between the Query and Key. $\\sqrt{d_k}$ is a scaling factor to prevent the dot product from growing too large, mitigating the vanishing/exploding gradient problem. The weighted sum is applied to the values of $V$. The Memory Bandwidth Bottleneck: \u0026gt; During inference, the model must store Key and Value matrices for all previous tokens. This is called the KV Cache. In MHA, every single head has its own unique set of Keys and Values. For large models with massive context windows, loading this massive KV cache from memory (HBM) to the computation chip (SRAM) becomes the primary bottleneck, slowing down generation significantly. Earlier models could often only generate ~8 tokens per second at inference due to this constraint.\nüî• Multi-Query Attention (MQA) This approach addresses the memory bottleneck by sharing the same set of Keys and Values across all heads while keeping separate Query heads.\nMHA: $H$ Query heads, $H$ Key heads, $H$ Value heads MQA: $H$ Query heads, 1 Key head, 1 Value head This drastically reduces the memory requirements of the KV cache by a factor of $H$ (often 8x or 16x). The GPU needs to load far less data, significantly speeding up inference. However, it does not reduce the computation (FLOPs)‚Äîso why is it faster?\nThis can be explained with the Kitchen Analogy: Imagine a kitchen (GPU) with 8 chefs (Heads).\nComputation (Math): The Chef chopping vegetables. Memory Access (Bandwidth): The Assistant running to the fridge to get ingredients. In MHA: Each chef demands unique vegetables from the fridge. The assistant runs back and forth 8 times, fetching 8 different crates. Most of the time is spent waiting for the assistant to return. This results in idle cores waiting for data to load from HBM.\nIn MQA: All chefs agree to use the same vegetables. The assistant runs back and forth only once, fetching one giant crate of potatoes and dumping it. This results in minimized memory transfer and no idle cores.\nThe Technical Reality: During text generation, the model is memory-bound, not compute-bound. The time it takes to move the KV Cache from HBM (High Bandwidth Memory) to the chip\u0026rsquo;s SRAM is the bottleneck. MQA reduces the data volume of that move by a factor of $H$.\nThe Capacity Gap: MQA is a trade-off between nuanced response and memory capacity. The model loses the ability to store different types of information, as a single $V$ matrix must now store all nuances of information flow in the language.\nThe Surgery Trick (Uptraining): Google Research figured out a way to take an existing model with MHA and convert it to MQA by averaging the weight matrices of $H$ heads. This is called Uptraining. You then train this \u0026ldquo;Frankenstein\u0026rdquo; model for ~5% of the original training steps for adjustments. This yields an MQA model that is nearly as good as the original MHA but vastly faster.\nüî• Grouped Query Attention (GQA) A \u0026ldquo;best of both worlds\u0026rdquo; solution used in modern models like Llama 2 and Llama 3.\nHow it works: GQA acts as an interpolation between MHA and MQA. Instead of having just 1 Key head (MQA) or $H$ Key heads (MHA), it divides the query heads into $G$ groups. Each group shares a single Key and Value head.\nEquation modification: If you have 8 query heads, you might create 4 groups (2 queries per group). This reduces the KV cache by 2x instead of 8x, preserving more quality than MQA while still speeding up inference.\nMHA: 8 people (heads) are working on a project. Each person has their own unique filing cabinet (KV cache). It takes forever to move 8 cabinets around. MQA: 8 people share one single filing cabinet. It\u0026rsquo;s very fast to move, but they fight over the organization and lose detail. GQA: The 8 people split into 4 teams of 2. Each team shares a cabinet. It\u0026rsquo;s a balance between speed and organizational detail. üî• Multi-Head Latent Attention (MLA) This is the most advanced mechanism for attention, used in models like DeepSeek-V3. It is a sophisticated design for massive efficiency that takes a radical approach: mathematically compressing the memory into a low-rank \u0026ldquo;latent\u0026rdquo; vector.\nKV Cache Explosion: As mentioned before, for a massive model like DeepSeek-V3 (671B parameters), storing the full Keys and Values for every token in a long conversation requires terabytes of high-speed memory (HBM), and chips spend more time waiting than churning.\nMHA (Standard) stores everything. MQA deletes redundancy but hurts quality. MLA keeps the quality of MHA but compresses the data 4x-6x smaller than even GQA.\nThe Zip File Approach: Low-Rank Joint Compression Let\u0026rsquo;s compare the math of MHA and MLA. In MHA, for every single token, we generate a unique Key ($k$) and Value ($v$) matrix for each attention head.\nThe Memory Problem: The $k$ is stored in the KV Cache. For 128 heads, for example, it calculates a massive vector of $128 \\times 128 = 16,384$ floats per token.\nFor MHA, the attention score looks like: $$\\text{Score} = q^T \\cdot k$$\nBut for MLA, instead of storing the full $k$, it projects the input down into a tiny latent vector $c_{KV}$, which is much smaller (e.g., 512 floats). $$c_{KV} = W_{DKV} \\cdot x$$\nNow here is the catch: Naive compression and decompression would mean constructing a small latent matrix for every token and deconstructing it every single time the information is required, which defeats the purpose of compression.\n$$k_{\\text{reconstructed}} = W_{UK} \\cdot c_{KV}$$ $$\\text{Score} = q^T \\cdot k_{\\text{reconstructed}}$$\nThe Matrix Absorption Trick (Optimization) Instead of reconstructing $k$ for every token, we can absorb the Up-Projection matrix ($W_{UK}$) into the Down-Projection matrix ($W_{DKV}$) and absorb the query $q$ into a new $Q$.\nFrom the original equation: $$\\text{Score} = q^T \\cdot (\\underbrace{W_{UK} \\cdot c_{KV}}_{\\text{This is } k})$$\nWe associate differently:\n$$\\text{Score} = (\\underbrace{q^T \\cdot W_{UK}}_{\\text{Absorbed Query}}) \\cdot c_{KV}$$ We change the order in which the matrix multiplication is performed.\nHow is this allowed? In linear algebra, matrix multiplication is associative:\n$$(A \\cdot B) \\cdot C = A \\cdot (B \\cdot C)$$ We can move the parentheses! Instead of grouping $(W_{UK} \\cdot c_{KV})$ to make the Key, we group $(q^T \\cdot W_{UK})$ to make a new Query.\nNote: The positional embedding information is left untouched. That rotational information from algorithms like RoPE is preserved outside the compression; it requires minimal space anyways.\n","permalink":"http://localhost:1313/posts/attention-mechanisms/","summary":"We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek\u0026rsquo;s MLA, how has the definition of \u0026lsquo;Attention\u0026rsquo; transformed?","title":"Attention Is All You Need, but exactly which one?: MHA, GQA and MLA"},{"content":"\nPositional Embeddings are the \u0026ldquo;voice\u0026rdquo; that tell Transformers where words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.\n\u0026ldquo;The dog chased the cat\u0026rdquo; and \u0026ldquo;cat chased the dog\u0026rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.\nüî• Sinusoidal Embeddings: The Odometer of Language Introduced in the seminal \u0026ldquo;Attention Is All You Need\u0026rdquo; paper, this was the industry standard for many years. This approach adds a vector to each word embedding based on its position. The vector is computed out of wavelike patterns that are periodic and smooth. Mathematically, for a position $pos$ and embedding dimension $i$:\n$$ PE_{(pos, i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $$\nThe model uses sine and cosine waves of different frequencies to capture both short and long-range dependencies. The frequency of the waves decreases as the embedding dimension increases, which helps capture long-range dependencies in the input sequence.\nThe creators wanted the model to calculate distance easily, which is why a linear function was used. The model simply rotates the current position vector using a rotation matrix to derive the next position.\n$$ \\begin{pmatrix} \\cos(k) \u0026 \\sin(k) \\\\ -\\sin(k) \u0026 \\cos(k) \\end{pmatrix} \\cdot PE(pos) \\approx PE(pos + k) $$ Low dimensions act like a fast-ticking clock (high frequency, changing rapidly) and high dimensions act like a slow clock (low frequency, changing slowly).\nThe number 10,000 is an arbitrarily chosen high number to reduce Aliasing. By choosing such a high number, the creators ensured that the wavelength of the slowest dimension is incredibly long. Consequently, the pattern practically never repeats, and each position remains unique.\nThis combination works like a mechanical odometer dial. Each subsequent dimension acts as a slower-moving gear. The model learns to read these \u0026ldquo;dials\u0026rdquo; to understand exactly where a word sits in the sequence.\nThis mitigated the challenge of exploding gradients that previous array-based positional embeddings suffered from (e.g., adding integers 1, 2, 3\u0026hellip;). This model keeps values between -1 and +1, adding stability while ensuring every position gets a distinct code.\nThe Problem: This approach pollutes the word embedding matrix (semantic information). While high-dimensional vectors are almost always orthogonal‚Äîmeaning the noise doesn\u0026rsquo;t fully destroy the word\u0026rsquo;s identity‚Äîit does not strictly guarantee purity.\nThis led to a more mature approach.\nüî• ALiBi: The Fading Streetlight Attention with Linear Biases (ALiBi) is a method where, instead of adding a vector to the embeddings at the start, the model injects positional information linearly, directly into the attention scores. If Sinusoidal Encoding asks, \u0026ldquo;Where am I on the map?\u0026rdquo;, ALiBi asks, \u0026ldquo;How much harder should I squint to see you?\u0026rdquo;\nThe biggest problem ALiBi solved is the \u0026ldquo;Invisible Wall\u0026rdquo;. If you train a Transformer on sequences of length 2048 and then feed it a book with 3000 words, it will crash or output garbage at word 2049. This is a failure of Extrapolation.\nALiBi throws away the idea of adding positional vectors entirely. Instead, it devises a \u0026ldquo;penalty\u0026rdquo; system for attention scores:\n$$ \\text{Score} = (q \\cdot k^\\top) - (m \\cdot \\text{distance}) $$\nDistance: If words A and B are neighbors, the distance is 0, so the penalty is 0. Although the distance is linear, it does not cause exploding gradients because the raw attention scores are normalized by a softmax function. This distance prioritizes nearby words (local context) over distant ones.\nSlope ($m$): If we always penalize distance equally, the model becomes myopic. ALiBi mitigates this by assigning a different slope value to each attention head:\nHead 1 (The Historian): Very low slope. The penalty grows slowly, allowing it to see far back. Head 2 (The Reader): Medium slope. Head 3 (The Myopic): Very high slope (e.g., 0.5); it effectively cannot see beyond 2-3 words. This mimics holding a lantern. Words close to you are bright, while words further away naturally fade into darkness.\nExtrapolation: If you walk 5 miles down the road (a longer sequence), your lantern works exactly the same way. The physics of light decay are constant, allowing the model to handle sequences longer than it was trained on.\nHowever, ALiBi forces a \u0026ldquo;sliding window\u0026rdquo; view where distant words eventually become invisible. This brings us to the modern standard.\nüî• RoPE: The Rotary Revolution Rotary Position Embeddings (RoPE) is the current industry standard (used in Llama 3, Mistral, PaLM) because it mathematically unifies \u0026ldquo;absolute position\u0026rdquo; with \u0026ldquo;relative position.\u0026rdquo; This method rotates the vectors in the Query and Key matrices based on their absolute position.\nIf a vector has 512 dimensions, RoPE breaks it into 256 pairs. Each pair is rotated by a different frequency:\n$$ \\theta_i = 10000^{-2i/d} $$\nThe First pair ($i = 0$) is rotated by the highest frequency. The Last pair ($i = 255$) is rotated by the lowest frequency. This spread ensures the model has \u0026ldquo;high precision\u0026rdquo; (fast rotation) to distinguish immediate neighbors, and \u0026ldquo;low precision\u0026rdquo; (slow rotation) to track global position without the pattern repeating.\nDoesn\u0026rsquo;t rotation change meaning? Yes and No. The vector for \u0026ldquo;Apple\u0026rdquo; at position 1 and \u0026ldquo;Apple\u0026rdquo; at position 256 will look completely different in terms of coordinates. However, rotation does not change the semantic strength (magnitude/norm) of the vector.\nThe Relativity Trick: Transformers compare words using dot products (angles). RoPE relies on the fact that the attention mechanism ignores the absolute rotation and only detects the difference in rotation (relative position). Thus, the \u0026ldquo;relative meaning\u0026rdquo; is encoded purely into the angle between words, leaving the semantic \u0026ldquo;magnitude\u0026rdquo; untouched.\nüî• LongRoPE: The Bifocal Lens While RoPE is powerful, it struggles when stretching context windows from 4k to 2 million tokens. The \u0026ldquo;fast\u0026rdquo; dimensions spin so rapidly over long distances that they become random noise. LongRoPE solves this using an evolutionary search algorithm to find a unique scaling factor ($\\lambda$) for each dimension.\nThe Equations Instead of rotating by the standard $\\theta_i$, LongRoPE rotates by a rescaled frequency:\n$$ \\theta\u0026rsquo;_i = \\lambda_i \\theta_i $$\nTo efficiently find these $\\lambda$ values without searching an infinite space, the algorithm enforces a monotonicity constraint based on NTK theory:\n$$ \\lambda_i \\le \\lambda_{i+1} $$\nThis ensures that low-frequency dimensions (global context) are stretched more than high-frequency dimensions (local context), creating a \u0026ldquo;bifocal\u0026rdquo; effect:\nHigh Frequencies (Local): Kept sharp ($\\lambda \\approx 1$) to maintain grammar. Low Frequencies (Global): Stretched ($\\lambda \u0026gt; 1$) to track massive distances without repeating. Not worth perusing more since the next one is much better at long context positional encoding. üî• HoPE: The Hyperbolic Slide Hyperbolic Rotary Positional Encoding (HoPE) moves from the geometry of a circle to the geometry of a hyperbola (inspired by Lorentz transformations). It was designed to fix a subtle flaw in RoPE: the \u0026ldquo;wobbly\u0026rdquo; or oscillatory nature of attention scores.\nThe Core Problem: RoPE\u0026rsquo;s \u0026ldquo;Wobble\u0026rdquo; In standard RoPE, we rotate vectors around a circle.\nThe Issue: Circles repeat. A dial at $361^\\circ$ looks identical to $1^\\circ$. Even with different frequencies, the dot product fluctuates up and down as distance increases. The Consequence: The model gets confused at long distances, potentially mistaking a word 1,000 tokens away for a neighbor just because the rotation cycles aligned, creating \u0026ldquo;noise.\u0026rdquo; The \u0026ldquo;Hyperbolic\u0026rdquo; Shift HoPE replaces trigonometric functions ($\\sin, \\cos$) with hyperbolic functions ($\\sinh, \\cosh$).\nCircular Rotation: Keeps distance from the center constant. Hyperbolic Rotation: Moves points along a hyperbola. Crucially, hyperbolic functions do not repeat; they grow or decay exponentially. By using hyperbolic rotation, HoPE guarantees that as words get further apart, the attention score decays monotonically (smoothly drops).\nThe Equations HoPE uses a hyperbolic matrix $B(\\theta, m)$ defined using the Lorentz Boost structure:\n$$ B(\\theta, m) = \\begin{pmatrix} \\cosh(m\\theta) \u0026 \\sinh(m\\theta) \\\\ \\sinh(m\\theta) \u0026 \\cosh(m\\theta) \\end{pmatrix} $$ Since hyperbolic functions grow to infinity, HoPE introduces a decay penalty ($e^{-m\\theta'}$). The final attention score simplifies beautifully to a function of relative distance: $$ \\text{Score} \\propto e^{-|m-n|(\\theta\u0026rsquo; - \\theta)} $$\nThis ensures the attention score exponentially decays as distance increases, eliminating the wobbles of standard RoPE. This the latest model that is taking up the space in the industry for extremely long context models.\n","permalink":"http://localhost:1313/posts/positional-embeddings/","summary":"From Sinusoidal to RoPE and HoPE: How Transformers learn to process word order and sequence length.","title":"The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE"},{"content":"Imagine trying to teach a computer the difference between \u0026ldquo;Apple\u0026rdquo; the fruit and \u0026ldquo;Apple\u0026rdquo; the company. To us, the distinction is intuitive. To a machine, it‚Äôs just a string of characters. How do we turn these strings into meaningful math? While early attempts like Word2Vec gave us a great start, they missed the forest for the trees‚Äîor in some cases, the twigs for the branches. Enter GloVe and FastText: two algorithms that revolutionized how machines understand the nuances of human language.\nPrevious static embedding models like Word2Vec successfully captured the local semantics of words, but they failed in addressing contexts between words that might not always appear in the same context window. Furthermore, they treated words as indivisible atomic units in continuous vector spaces, missing out on the internal structure of language.\nThe two simple but effective ideas behind GloVe and FastText are subword units and global co-occurrence. Let\u0026rsquo;s dive into them.\nGloVe: A Global Mapping Strategy Global Vectors for Word Representation (GloVe) was created at Stanford. The core idea was to train a model where word vectors are defined by how often they appear around other words across the entire corpus, rather than just locally.\nTraining Steps: Co-occurrence Matrix X: GloVe scans the entire corpus and creates a giant matrix (spreadsheet) recording the counts of how often words appear near each other. Factorization: It compresses this giant matrix into smaller, dense vectors. Objective: The function tries to equate the dot product of two word vectors with the logarithm of their probability of appearing together. The error between these values is then backpropagated to update the vectors. $$w_i \\cdot w_j + b_i + b_j = \\log(X_{ij})$$\n$w$: The word vector (what we want). $X_{ij}$: The count from our matrix. The Loss Function: We can\u0026rsquo;t satisfy the equation above perfectly for every word pair. So, we use a weighting function to minimize the squared error:\n$$J = \\sum_{i,j=1}^V f(X_{ij}) ({w_i}^T w_j + b_i + b_j - \\log(X_{ij}))^2$$\nIf we do not use the weighting function, the loss would be infinite for rare words (since $\\log(0) = \\infty$) and frequent words (like \u0026rsquo;the\u0026rsquo;) would overpower the model.\n$$ f(x) = \\begin{cases} (x/x_{max})^\\alpha \u0026amp; \\text{if } x \u0026lt; x_{max} \\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$\n(Typically $x_{max}=100$ and $\\alpha=0.75$)\nFastText: The Lego Bricks Strategy FastText was created by Facebook AI Research (FAIR). The idea was to treat words not as solid rocks, but as structures made of \u0026ldquo;Lego bricks\u0026rdquo; (subword units).\nWord2Vec struggled to create embeddings for words it hadn\u0026rsquo;t seen before (Out-Of-Vocabulary or OOV words). FastText solves this by breaking words down.\nGloVe treats a word like \u0026ldquo;Apple\u0026rdquo; as a single, atomic unit. FastText breaks \u0026ldquo;Apple\u0026rdquo; into character n-grams: \u0026lt;ap, app, ppl, ple, le\u0026gt; For a complex or new word like \u0026ldquo;Applesauce\u0026rdquo;, the model might not know the whole word, but it recognizes the Lego brick for \u0026ldquo;Apple\u0026rdquo; and the remaining bits from other subwords.\nFastText takes its inspiration from the SkipGram architecture. The vector for a word is simply the sum of the vectors of its n-gram subwords:\n$$v_w = \\sum_{g \\in G_w} z_g$$\nObjective: The objective is to predict context words given a target word.\n$$P(w_c | w_t) = \\frac{e^{s(v_{w_t} \\cdot v_{w_c})}}{\\sum_{w \\in V} e^{s(v_{w_t} \\cdot v_w)}}$$\nAnd the scoring function is defined as:\n$$s(w_t, w_c) = u_{w_t}^T (\\sum_{g \\in G_w} z_g)$$\n$u_{w_c}$: The vector for the context word (output vector). $z_g$: The vector for the n-gram $g$ (input vector). Modifications to Softmax: Calculating the full softmax for every word in the vocabulary is computationally expensive. FastText uses two methods to approximate this efficiently.\nOption A: Negative Sampling\nWe pick a few random \u0026ldquo;wrong\u0026rdquo; words every time to serve as \u0026ldquo;negative samples\u0026rdquo; and calculate the loss only for the correct word and these noise words.\n$$J = - \\log \\sigma(s(w_t, w_c)) - \\sum\\limits_{i=1}^{N} \\mathbb{E}_{n_i \\sim P_n(w)} [\\log \\sigma(-s(w_t, n_i))]$$\nThe sigmoid function $\\sigma$ squashes the score between 0 and 1. The first term pushes the probability of the correct word towards 1. The second term pushes the probability of the noise words towards 0. Option B: Hierarchical Softmax (Faster for infrequent words)\nInstead of a flat list of words, imagine the vocabulary arranged as a binary tree (Huffman Tree).\nRoot: Top of the tree. Leaves: The actual words. To calculate the probability of a word, we trace a path from the root to that leaf. At every branching node, we calculate the probability (sigmoid) of going left or right. The probability of a word $w$ is the product of the probabilities of the turns taken to reach it. Code Example from gensim.models import FastText from gensim.test.utils import common_texts # Initialize the FastText model with specific hyperparameters model = FastText( vector_size=100, # Dimensionality of the word vectors window=5, # Maximum distance between the current and predicted word within a sentence min_count=1, # Ignores all words with total frequency lower than this sg=1, # Training algorithm: 1 for skip-gram; 0 for CBOW hs=0, # If 0, and negative is non-zero, negative sampling will be used negative=5, # Number of \u0026#34;noise words\u0026#34; to be drawn for negative sampling workers=4, # Number of worker threads to train the model epochs=10, # Number of iterations over the corpus min_n=3, # Minimum length of character n-grams max_n=6 # Maximum length of character n-grams ) # Build the vocabulary from the provided text corpus model.build_vocab(common_texts) # Train the model on the corpus model.train( common_texts, total_examples=model.corpus_count, epochs=model.epochs ) # Calculate and print the cosine similarity between two words in the vocabulary print(f\u0026#34;Similarity between \u0026#39;computer\u0026#39; and \u0026#39;human\u0026#39;: {model.wv.similarity(\u0026#39;computer\u0026#39;, \u0026#39;human\u0026#39;)}\u0026#34;) # Demonstrate FastText\u0026#39;s ability to handle Out-Of-Vocabulary (OOV) words # Even if \u0026#39;computation\u0026#39; wasn\u0026#39;t in the training data, FastText constructs a vector using character n-grams oov_vector = model.wv[\u0026#39;computation\u0026#39;] # Check if a specific word exists in the model\u0026#39;s fixed vocabulary index word = \u0026#34;computer\u0026#34; is_in_vocab = word in model.wv.key_to_index print(f\u0026#34;\\nIs the word \u0026#39;{word}\u0026#39; in the model\u0026#39;s vocabulary? {is_in_vocab}\u0026#34;) ","permalink":"http://localhost:1313/posts/glove-and-fasttext/","summary":"How subword units and global co-occurrence matrices allow GloVe and FastText to capture nuances that Word2Vec missed.","title":"The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText"},{"content":" Before we had Large Language Models writing poetry, we had to teach computers that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; are related not just by spelling, but by meaning. This is the story of that breakthrough. It‚Äôs the moment we stopped counting words and started mapping their souls‚Äîturning raw text into a mathematical landscape where math can solve analogies. Welcome to the world of Word2Vec. üîÆ\nLanguage models require vector representations of words to capture semantic relationships. Before the 2010s, models used word count-based vector representations that captured only the frequency of words (e.g., One-Hot Encoding). The Problems: üöß\nThe Curse of Dimensionality Lack of meaning (Synonyms were treated as mathematically unrelated). In 2011, Mikolov et al. at Google introduced Word2Vec, which used a shallow neural network to learn vector representations of words. The best part? It could create much denser relationships between words than prior models, and it was unsupervised.\nThe training (prediction) is essentially \u0026ldquo;fake\u0026rdquo;‚Äîa pretext task. But the weight matrices we get on the side are a gold mine of fine-grained semantic relationships. ‚õèÔ∏è\nThere are two popular variants of Word2Vec:\nSkipGram Continuous Bag Of Words (CBOW) 1. Continuous Bag of Words (CBOW) üß© CBOW is a simple \u0026lsquo;fill in the blanks\u0026rsquo; machine. It takes the surrounding words as inputs and tries to predict the center word.\n\u0026ldquo;Continuous\u0026rdquo;: It operates in continuous vector space (unlike the discrete space of n-gram models).\n\u0026ldquo;Bag of words\u0026rdquo;: The order of the context does not matter. \u0026ldquo;The cat sat on the mat\u0026rdquo; and \u0026ldquo;The mat sat on the cat\u0026rdquo; are the same for CBOW. They produce exactly the same predictions.\nObjective: Maximize the probability of the target word $w_t$ given its context $w_{t-C}, \\dots, w_{t+C}$.\n2. Training Defining Dimensions:\n$V$: Vocabulary size (number of unique words in the corpus, e.g., 10,000) $N$: Dimension of the embedding space (e.g., 100) $C$: Number of context words (e.g., 2 words on left of target and 2 words on right) Step 1: Input Lookup \u0026amp; One-Hot Encoding\nMathematically, we represent each context word as a one-hot vector $x^{(c)}$. We have an Input Matrix $W_{in}$ of size $V \\times N$. For each context word (e.g., \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;), we select its corresponding row from $W_{in}$.\n$$ v_c = W_{in}^T x^{(c)} $$\nWhere $v_c$ is the $N$-dimensional vector for context word $c$.\nStep 2: Projection (The \u0026ldquo;Mixing\u0026rdquo;)\nWe combine the context vectors. In standard CBOW, we simply average them. This creates the hidden layer vector $h$.\n$$ h = \\frac{1}{2C} \\sum_{c=1}^{2C} v_c $$\n$h$ is a single vector of size $N \\times 1$. Note: This operation is linear and contains no activation function (like ReLU or Sigmoid).\nStep 3: Output Scoring (The \u0026ldquo;Dot Product\u0026rdquo;)\nWe have a second matrix, the Output Matrix $W_{out}$ of size $N \\times V$. We compute the raw score (logit) $u_j$ for every word $j$ in the vocabulary by taking the dot product of the hidden vector $h$ and the output vector $v\u0026rsquo;_j$.\n$$ u_j = {v\u0026rsquo;_j}^T h $$\nOr in matrix form:\n$$ u = W_{out}^T h $$\nResult $u$ is a vector of size $V \\times 1$.\nStep 4: Probability Conversion (Softmax)\nWe convert the raw scores $u$ into probabilities using the Softmax function. This tells us the probability that word $w_j$ is the center word.\n$$ y_j = P(w_j | \\text{context}) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 5: Loss Calculation (Cross-Entropy)\nWe compare the predicted distribution $y$ with the actual ground truth $t$ (a one-hot vector where the index of the true word is 1). The loss function $E$ is:\n$$ E = - \\sum_{j=1}^V t_j \\log(y_j) $$\nSince $t_j$ is 0 for all words except the target word (let\u0026rsquo;s call the target index $j^*$), this simplifies to:\n$$ E = - \\log(y_{j^*}) $$\n3. Backpropagation Details We need to update the weights to minimize $E$. We use the Chain Rule.\nA. Gradient w.r.t. Output Matrix ($W_{out}$)\nWe want to know how the error changes with respect to the raw score $u_j$.\n$$ \\frac{\\partial E}{\\partial u_j} = y_j - t_j $$\nLet\u0026rsquo;s call this error term $e_j$.\nIf $j$ is the target word: $e_j = y_j - 1$ (Result is negative; we boost the score). If $j$ is not target: $e_j = y_j - 0$ (Result is positive; we suppress the score). The update rule for the output vector $v\u0026rsquo;_j$ becomes:\n$$ v'_{j}(\\text{new}) = v'_{j}(\\text{old}) - \\eta \\cdot e_j \\cdot h $$ ($\\eta$ is the learning rate)\nB. Gradient w.r.t. Hidden Layer ($h$)\nWe backpropagate the error from the output layer to the hidden layer.\n$$ EH = \\sum_{j=1}^V e_j \\cdot v\u0026rsquo;_j $$\n$EH$ is an $N$-dimensional vector representing the aggregate error passed back to the projection layer.\nC. Gradient w.r.t. Input Matrix ($W_{in}$)\nSince $h$ was just an average of the input vectors, the error $EH$ is distributed to each context word\u0026rsquo;s input vector. For every word $w_c$ in the context:\n$$ v_{w_c}(\\text{new}) = v_{w_c}(\\text{old}) - \\eta \\cdot \\frac{1}{2C} \\cdot EH $$\nLet\u0026rsquo;s build a single pass through the network for a given context.\nimport torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # Define a simple Continuous Bag of Words (CBOW) style model class CBOWModel(nn.Module): def __init__(self, vocab_size, embedding_dim): super(CBOWModel, self).__init__() # The embedding layer stores the word vectors we want to learn self.embeddings = nn.Embedding(vocab_size, embedding_dim) # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word self.linear = nn.Linear(embedding_dim, vocab_size) def forward(self, inputs): # inputs: tensor of word indices for the context embeds = self.embeddings(inputs) # Aggregate context by calculating the mean of the word embeddings h = torch.mean(embeds, dim=1) # Produce logits (raw scores) for each word in the vocabulary logits = self.linear(h) return logits # Setup vocabulary and mappings word_to_ix = {\u0026#34;the\u0026#34;: 0, \u0026#34;quick\u0026#34;: 1, \u0026#34;brown\u0026#34;: 2, \u0026#34;fox\u0026#34;: 3, \u0026#34;jumps\u0026#34;: 4, \u0026#34;over\u0026#34;: 5, \u0026#34;lazy\u0026#34;: 6, \u0026#34;dog\u0026#34;: 7} ix_to_word = {v: k for k, v in word_to_ix.items()} # Configuration constants EMBEDDING_DIM = 5 VOCAB_SIZE = len(word_to_ix) LEARNING_RATE = 0.01 # Initialize the model, loss function, and optimizer model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM) loss_function = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE) # Prepare dummy training data: context words and the target word # Context: [\u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;over\u0026#34;] -\u0026gt; Target: \u0026#34;fox\u0026#34; context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long) target_idxs = torch.tensor([3], dtype=torch.long) # Perform a single optimization step model.zero_grad() # Forward pass: we reshape context to (1, -1) to simulate a batch of size 1 logits = model(context_idxs.view(1, -1)) # Calculate loss against the target word index loss = loss_function(logits, target_idxs) # Backpropagate and update weights loss.backward() optimizer.step() # Output results for the blog post print(\u0026#34;=== Model Training Snapshot ===\u0026#34;) print(f\u0026#34;Calculated Loss: {loss.item():.6f}\u0026#34;) print(f\u0026#34;\\n=== Learned Vector for \u0026#39;jumps\u0026#39; ===\u0026#34;) word_vec = model.embeddings(torch.tensor([word_to_ix[\u0026#39;jumps\u0026#39;]])) print(word_vec.detach().numpy()) print(\u0026#34;\\n=== Embedding Matrix (Weights) ===\u0026#34;) print(model.embeddings.weight.detach().numpy()) print(\u0026#34;\\n=== Linear Layer Weights ===\u0026#34;) print(model.linear.weight.detach().numpy()) Output shows single pass loss and learned vector for \u0026lsquo;jumps\u0026rsquo; word. This is a single training sample result but after multiple such passes both Embedding and linear weights should be very similar.\n=== Model Training Snapshot === Calculated Loss: 1.989656 === Learned Vector for \u0026#39;jumps\u0026#39; === [[-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775]] === Embedding Matrix (Weights) === [[ 1.66039 -0.11371879 0.6246518 0.35860053 -1.2504417 ] [-2.1847186 -0.77199775 -0.17050214 -0.38411248 -0.03913084] [ 0.11852697 0.90073633 0.8847807 0.7404524 0.900149 ] [ 0.07440972 -0.40259898 2.6246994 -0.08851447 0.02660969] [-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775] [-0.9245572 -0.5545908 0.9083091 -1.0755049 0.84047747] [-1.0237687 0.59466314 0.05621134 -0.6202532 1.3664424 ] [ 0.60998917 -1.0549186 1.6103884 0.8724912 -1.2486908 ]] === Linear Layer Weights === [[ 0.02165058 -0.28883642 0.14545658 -0.3442509 0.32704315] [-0.18731792 0.28583744 0.22635977 0.13245736 0.29019794] [ 0.3158916 -0.15826383 -0.03203773 0.16377363 -0.41457543] [-0.05080034 0.4180087 0.11228557 0.30218413 0.3025514 ] [-0.38419306 0.24475925 -0.39210224 -0.38660625 -0.2673145 ] [-0.32321444 0.12200444 -0.03569533 0.2891424 -0.07345333] [-0.33704326 0.2521956 0.31587374 0.22590035 0.29866052] [ 0.4117266 -0.44231793 0.24064957 -0.29684234 0.333821 ]] Interesting Facts About CBOW üß† The \u0026ldquo;Averaging Problem\u0026rdquo;: This is a common issue in CBOW models. The averaging and training is order-agnostic. All variants of word order in the context window will generate the same output. \u0026ldquo;Dog bit the man\u0026rdquo; and \u0026ldquo;Man bit the dog\u0026rdquo; are the same in CBOW\u0026rsquo;s eyes.\nSmoothing Effect: CBOW models are much faster to train than Skip-Gram but are generally smoother.\nRare Words: CBOW models struggle slightly with rare words because the presence of surrounding common words (like articles and prepositions) can skew the embeddings and dissolve their uniqueness.\nComputational Cost: Softmax requires summing over the entire vocabulary size, which can be computationally expensive if $V$ is very large. Solutions include Hierarchical Softmax and Negative Sampling (approximating the denominator by only checking the target word vs. 5 random noise words).\nVectors as Double Agents: Every word has a different representation in the embedding matrix and the output matrix. Most of the time $W_{out}$ is discarded, but some consider averaging both $W_{in}$ and $W_{out}$ for slightly better performance.\nLinear Relationships: The famous analogy King - Man + Woman = Queen emerges from the model because of the linear relationships between the words.\nInitialization: This is a no-brainer but worth mentioning. Initialization to zero vectors would mean no learning. We must initialize randomly.\n2. Skip-Gram (The \u0026ldquo;Burger Flip\u0026rdquo; of CBOW) üîÑ Skip-Gram flips the CBOW logic. While CBOW tries to predict the center word given the context, Skip-Gram tries to predict the scattered context given the center word.\nCore Concept: If a word like \u0026ldquo;monarch\u0026rdquo; can accurately predict words like \u0026ldquo;king\u0026rdquo;, \u0026ldquo;throne\u0026rdquo;, \u0026ldquo;crown\u0026rdquo;, \u0026ldquo;royalty\u0026rdquo;, and \u0026ldquo;power\u0026rdquo;, then it must effectively capture the concept of royalty.\nTraining Let\u0026rsquo;s define dimensions:\n$V$: Vocabulary size $D$: Embedding dimension $C$: Number of context words Step 1: Input Lookup\nUnlike CBOW, our input is just one word vector (the center word $w_t$). We grab the vector $v_c$ from the Input Matrix $W_{in}$.\n$$ h = v_{w_t} $$\nNote: There is no averaging here. The hidden layer $h$ is simply the raw vector of the center word.\nStep 2: Output Scoring (The Broadcast)\nThe model takes this single vector $h$ and compares it against the Output Matrix $W_{out}$ (the entire vocabulary). It computes a score $u_j$ for every word $j$ in the dictionary.\n$$ u = W_{out}^T h $$\nResult $u$: A vector of size $V \\times 1$ containing raw scores.\nStep 3: Probability Conversion (Softmax)\nWe apply Softmax to turn scores into probabilities.\n$$ P(w_j | w_t) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 4: The Multi-Target Loss\nHere is the major difference from CBOW. In CBOW, we had one target (the center). In Skip-Gram, we have $2C$ targets (all the surrounding words). For a sentence \u0026ldquo;The cat sat on mat\u0026rdquo; (Center: \u0026ldquo;sat\u0026rdquo;), the targets are \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;. We want to maximize the probability of all these true context words. The Loss function ($E$) sums the error for each context word:\n$$ E = - \\sum_{w_{ctx} \\in \\text{Window}} \\log P(w_{ctx} | w_t) $$\nStep 5: Backpropagation (Accumulating Gradients)\nSince one input word is responsible for predicting multiple output words, the error signals from all those context words add up.\nAt the Output Layer:\nIf \u0026ldquo;cat\u0026rdquo; was a target, the \u0026ldquo;cat\u0026rdquo; vector in $W_{out}$ gets a signal: \u0026ldquo;Move closer to \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; If \u0026ldquo;apple\u0026rdquo; was NOT a target, the \u0026ldquo;apple\u0026rdquo; vector gets a signal: \u0026ldquo;Move away from \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; At the Hidden Layer ($h$):\nThe center word \u0026ldquo;sat\u0026rdquo; receives feedback from all its neighbors simultaneously. Error signal = (Error from \u0026ldquo;The\u0026rdquo;) + (Error from \u0026ldquo;cat\u0026rdquo;) + (Error from \u0026ldquo;on\u0026rdquo;)\u0026hellip; The vector for \u0026ldquo;sat\u0026rdquo; in $W_{in}$ moves in the average direction of all its neighbors. 3. The \u0026ldquo;Computational Nightmare\u0026rdquo; \u0026amp; Negative Sampling The equations above describe Naive Softmax.\nThe Problem: If $V = 100,000$, computing the denominator $\\sum \\exp(u_k)$ takes 100,000 operations. Doing this for every word in a training set of billions of words is impossibly slow.\nThe Solution: Negative Sampling Instead of updating the entire vocabulary (1 correct word vs 99,999 wrong ones), we approximate the problem.\nPositive Pair: (sat, cat) $\\rightarrow$ Maximize probability (Label 1). Negative Pairs: We pick $K$ (e.g., 5) random words the model didn\u0026rsquo;t see, e.g., (sat, bulldozer), (sat, quantum). $\\rightarrow$ Minimize probability (Label 0). New Equation (Sigmoid instead of Softmax):\n$$ E = - \\log \\sigma({v'_{\\text{pos}}}^T v_{\\text{in}}) - \\sum_{k=1}^K \\log \\sigma(-{v'_{\\text{neg}_{k}}}^T v_{\\text{in}}) $$ Effect: We typically only update 6 vectors per step (1 pos + 5 negs) instead of 100,000.\nLet\u0026rsquo;s see this in code:\nimport torch import torch.nn as nn import torch.optim as optim class SkipGramNegativeSampling(nn.Module): \u0026#34;\u0026#34;\u0026#34; Skip-Gram with Negative Sampling (SGNS) implementation. SGNS approximates the softmax over the entire vocabulary by instead distinguishing between a real context word (positive) and K noise words (negative). \u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embedding_dim): super(SkipGramNegativeSampling, self).__init__() # Input embeddings: used when the word is the center word self.in_embed = nn.Embedding(vocab_size, embedding_dim) # Output embeddings: used when the word is a context or negative sample self.out_embed = nn.Embedding(vocab_size, embedding_dim) # Initialize weights with small values to prevent gradient saturation initrange = 0.5 / embedding_dim self.in_embed.weight.data.uniform_(-initrange, initrange) self.out_embed.weight.data.uniform_(-initrange, initrange) def forward(self, center_words, target_words, negative_words): \u0026#34;\u0026#34;\u0026#34; Computes the negative sampling loss. Args: center_words: (batch_size) target_words: (batch_size) negative_words: (batch_size, K) where K is number of negative samples \u0026#34;\u0026#34;\u0026#34; # Retrieve vectors v_c = self.in_embed(center_words) # (batch_size, embed_dim) u_o = self.out_embed(target_words) # (batch_size, embed_dim) u_n = self.out_embed(negative_words) # (batch_size, K, embed_dim) # 1. Positive Score: log(sigmoid(v_c ¬∑ u_o)) # Compute dot product: (batch, 1, dim) @ (batch, dim, 1) -\u0026gt; (batch, 1) pos_score = torch.bmm(u_o.unsqueeze(1), v_c.unsqueeze(2)).squeeze(2) pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-7) # 2. Negative Score: sum(log(sigmoid(-v_c ¬∑ u_n))) # Compute dot products for all K samples: (batch, K, dim) @ (batch, dim, 1) -\u0026gt; (batch, K) neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2) neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-7), dim=1, keepdim=True) # Total loss is the negative of the objective function loss = -(pos_loss + neg_loss) return torch.mean(loss) # --- Configuration \u0026amp; Mock Data --- VOCAB_SIZE = 100 EMBED_DIM = 10 word_to_ix = {\u0026#39;fox\u0026#39;: 0} # Example vocabulary mapping model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock inputs: 1 center word, 1 target word, 5 negative samples center_id = torch.tensor([0], dtype=torch.long) target_id = torch.tensor([1], dtype=torch.long) negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long) # Training Step model.zero_grad() loss = model(center_id, target_id, negative_ids) loss.backward() optimizer.step() # Output Results print(f\u0026#34;Loss after one step: {loss.item():.6f}\u0026#34;) word_vec = model.in_embed(torch.tensor([word_to_ix[\u0026#39;fox\u0026#39;]])) print(f\u0026#34;Vector for \u0026#39;fox\u0026#39;:\\n{word_vec.detach().numpy()}\u0026#34;) The outputs:\nLoss after one step: 4.158468 Vector for \u0026#39;fox\u0026#39;: [[ 0.04197385 0.02400733 -0.03800093 0.01672485 -0.03872231 -0.0061478 0.0121122 0.04057864 -0.036255 0.03861175]] Interesting Facts About Skip-Gram üí° Makes Rare Words Shine: Skip-Gram is more effective at capturing the meaning of rare words because it focuses on the context of the center word. In CBOW, a rare word\u0026rsquo;s vector is averaged with others, diluting its signal. In Skip-Gram, when the rare word appears as the center, it gets the full, undivided attention of the backpropagation update.\nSlow Training: This method creates more training samples. A window of size 2 creates 4 training pairs per center word, while CBOW creates only 1 training pair per center word.\nSemantic Focus: Skip-Gram puts more emphasis on the semantic relationship between words. It views the context as the primary signal, with the center word as the target. This makes it better at capturing the meaning of words in context. Skip-Gram tends to capture semantic relationships (King/Queen) slightly better, while CBOW captures syntactic ones (walk/walked) slightly better.\n","permalink":"http://localhost:1313/posts/word-embeddings/","summary":"Understanding the mathematics behind Word2Vec, CBOW, and Skip-Gram and how they map language to vector space.","title":"Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n5. Code Examples: We\u0026rsquo;ll use sentencepiece for tokenization and compare results for different models.\nimport sentencepiece as spm import os # Configuration for training the SentencePiece model # SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words. options = { # The source text file used for learning the vocabulary \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # The base name for the generated model (.model) and vocabulary (.vocab) files \u0026#39;model_prefix\u0026#39;: \u0026#39;bpe_model\u0026#39;, # Number of unique tokens in the final vocabulary \u0026#39;vocab_size\u0026#39;: 4000, # \u0026#39;bpe\u0026#39; (Byte Pair Encoding) merges frequent pairs of characters/sequences \u0026#39;model_type\u0026#39;: \u0026#39;bpe\u0026#39;, # Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets \u0026#39;character_coverage\u0026#39;: 0.9995, # When enabled, unknown characters are decomposed into UTF-8 bytes to avoid \u0026#39;unk\u0026#39; tokens \u0026#39;byte_fallback\u0026#39;: True, # Treats digits individually (0-9), preventing large numbers from being treated as single tokens \u0026#39;split_digits\u0026#39;: True, # Prevents adding a whitespace prefix to the first token; useful for fine-grained control \u0026#39;add_dummy_prefix\u0026#39;: False } try: print(\u0026#34;Starting the training process...\u0026#34;) # SentencePieceTrainer.train takes the dictionary of options to build the BPE model spm.SentencePieceTrainer.train(**options) print(\u0026#34;Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created.\u0026#34;) # Initialize the processor and load the newly trained model sp = spm.SentencePieceProcessor() sp.load(\u0026#39;bpe_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Model Metadata:\u0026#34;) # Retrieve the total number of tokens in the vocabulary print(f\u0026#34;Total Vocab Size: {sp.get_piece_size()}\u0026#34;) # Special tokens are used for sequence boundaries and handling unknown characters print(f\u0026#34;BOS (Beginning of Sentence) ID: {sp.bos_id()}\u0026#34;) print(f\u0026#34;EOS (End of Sentence) ID: {sp.eos_id()}\u0026#34;) print(f\u0026#34;UNK (Unknown) ID: {sp.unk_id()}\u0026#34;) print(f\u0026#34;PAD (Padding) ID: {sp.pad_id()}\u0026#34;) # Test the tokenizer on sample strings to see how it breaks down text test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: shows the actual subword units (tokens) print(f\u0026#34;Subword Tokens: {sp.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: shows the numerical mapping for each token print(f\u0026#34;Numerical IDs: {sp.encode_as_ids(text)}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred during training or processing: {e}\u0026#34;) You can see the following output:\nStarting training... Starting the training process... Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created. ------------------------------ Model Metadata: Total Vocab Size: 4000 BOS (Beginning of Sentence) ID: 1 EOS (End of Sentence) ID: 2 UNK (Unknown) ID: 0 PAD (Padding) ID: -1 --- Tokenization Test --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;el\u0026#39;, \u0026#39;lo\u0026#39;, \u0026#39;‚ñÅW\u0026#39;, \u0026#39;orld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 380, 309, 476, 619, 36, 3924, 3964, 3959, 3993, 3978, 3983, 57, 3975, 3976, 60, 3974] --- Tokenization Test --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅmo\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;inst\u0026#39;, \u0026#39;ag\u0026#39;, \u0026#39;ram\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅever\u0026#39;] Numerical IDs: [1894, 304, 3284, 376, 321, 313, 312, 1008, 3313, 1325, 428, 3937, 341, 304, 3284, 1495] The ! is fallen back to UTF bytes. and the word instagrammable is split into multiple subword tokens. Also, original text can be recreated right back by simply concatenating the tokens. And replacing underscore by whitespace. Now we can try Unigram model. and see the same sentences tokenized differently.\nfrom tokenizers import Tokenizer from tokenizers.models import WordPiece from tokenizers.trainers import WordPieceTrainer from tokenizers.pre_tokenizers import Whitespace try: # 1. Initialize the WordPiece Tokenizer # We specify the [UNK] token for handling words not found in the vocabulary. tokenizer = Tokenizer(WordPiece(unk_token=\u0026#34;[UNK]\u0026#34;)) # 2. Configure Pre-tokenization # Before the subword algorithm runs, we need to split the raw text into words. # Whitespace splitting is the standard first step for most English NLP tasks. tokenizer.pre_tokenizer = Whitespace() # 3. Initialize the Trainer # We define our target vocabulary size and the special tokens required for # downstream tasks (like BERT\u0026#39;s [CLS] for classification or [SEP] for separators). trainer = WordPieceTrainer( vocab_size=4000, special_tokens=[\u0026#34;[UNK]\u0026#34;, \u0026#34;[CLS]\u0026#34;, \u0026#34;[SEP]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[MASK]\u0026#34;] ) # 4. Train the Model # The tokenizer scans the training file to build a vocabulary of the most # frequent subword units. tokenizer.train(files=[\u0026#34;train_data.txt\u0026#34;], trainer=trainer) # 5. Persist the Model # Save the configuration and vocabulary to a JSON file for future inference. tokenizer.save(\u0026#34;wordpiece.json\u0026#34;) print(\u0026#34;Training complete. \u0026#39;wordpiece.json\u0026#39; created.\u0026#34;) # 6. Metadata Inspection print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;WordPiece Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {tokenizer.get_vocab_size()}\u0026#34;) # 7. Testing Subword Tokenization # WordPiece shines at handling rare words by breaking them into meaningful chunks. test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (WordPiece) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # Encode converts raw text into a Tokenizer object containing tokens and IDs output = tokenizer.encode(text) # \u0026#39;tokens\u0026#39; shows the subword breakdown (e.g., \u0026#39;un\u0026#39;, \u0026#39;##insta\u0026#39;, etc.) print(f\u0026#34;Subword Tokens: {output.tokens}\u0026#34;) # \u0026#39;ids\u0026#39; are the numerical indices mapped to the vocabulary print(f\u0026#34;Numerical IDs: {output.ids}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred with WordPiece model: {e}\u0026#34;) You can see the following output:\nWordPiece Model Metadata: Total Vocab Size: 2609 --- Tokenization Test (WordPiece) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;H\u0026#39;, \u0026#39;##el\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;##or\u0026#39;, \u0026#39;##ld\u0026#39;, \u0026#39;[UNK]\u0026#39;, \u0026#39;[UNK]\u0026#39;] Numerical IDs: [37, 180, 214, 52, 162, 418, 0, 0] --- Tokenization Test (WordPiece) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;##os\u0026#39;, \u0026#39;##t\u0026#39;, \u0026#39;un\u0026#39;, \u0026#39;##ins\u0026#39;, \u0026#39;##ta\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;##ra\u0026#39;, \u0026#39;##m\u0026#39;, \u0026#39;##ma\u0026#39;, \u0026#39;##ble\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;ever\u0026#39;] Numerical IDs: [691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240] We can see the special character ## in the output. We can also see the breakdowns not having any semantic meaning. They are likelihood based. We can try the Unigram model now.\n# \u0026#39;Unigram\u0026#39; is the default and usually recommended over BPE in SentencePiece. options_unigram = { \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # Path to the raw text file for training \u0026#39;model_prefix\u0026#39;: \u0026#39;unigram_model\u0026#39;, # Prefix for the output .model and .vocab files \u0026#39;vocab_size\u0026#39;: 1200, # Desired size of the final vocabulary \u0026#39;model_type\u0026#39;: \u0026#39;unigram\u0026#39;, # Specifies the Unigram language model algorithm \u0026#39;character_coverage\u0026#39;: 0.9995, # Percentage of characters covered by the model (0.9995 is standard for Latin scripts) \u0026#39;byte_fallback\u0026#39;: True, # Enables mapping unknown characters to UTF-8 bytes to avoid \u0026lt;unk\u0026gt; tokens \u0026#39;split_digits\u0026#39;: True, # Treats each digit as an individual token (useful for numerical data) \u0026#39;add_dummy_prefix\u0026#39;: False # Prevents adding a leading space (SentencePiece default is True) } try: # 1. Train the SentencePiece model using the defined options print(\u0026#34;Starting Unigram training...\u0026#34;) spm.SentencePieceTrainer.train(**options_unigram) print(\u0026#34;Training complete. \u0026#39;unigram_model.model\u0026#39; created.\u0026#34;) # 2. Load the trained model into a processor instance for inference sp_unigram = spm.SentencePieceProcessor() sp_unigram.load(\u0026#39;unigram_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Unigram Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {sp_unigram.get_piece_size()}\u0026#34;) # 3. Define test cases to evaluate how the model handles common and rare words test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] # 4. Iterate through test sentences to visualize subword segmentation for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (Unigram) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: Converts text into subword strings (visual representation) print(f\u0026#34;Subword Tokens: {sp_unigram.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: Converts text into numerical indices for model input print(f\u0026#34;Numerical IDs: {sp_unigram.encode_as_ids(text)}\u0026#34;) except Exception as e: # Handle potential errors during training or loading (e.g., missing input file) print(f\u0026#34;An error occurred with Unigram model: {e}\u0026#34;) The output is as follows:\nUnigram Model Metadata: Total Vocab Size: 1200 --- Tokenization Test (Unigram) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ll\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;ld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311] --- Tokenization Test (Unigram) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;T\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅm\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;sta\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;ra\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ver\u0026#39;] ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"A comprehensive guide to tokenization strategies: BPE, WordPiece, Unigram, and SentencePiece.","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"A brief introduction to Vectors \u0026amp; Verbs and formatting verification.","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"\nFeedforward network (FFN) blocks are the least talked about when in context to LLMs. But that is where more than 70% of the paramters come from. Something as simple and convinient as a couple of linear transforming matrices become a burden when models start scaling in Billions of parameters. So, getting the maximum juice of a lean structure is a challenge. Plus, mostly under represented, because the FFN sub-layer acts as the primary \u0026ldquo;knowledge\u0026rdquo; processing engine, often containing the majority of a model\u0026rsquo;s total parameters.\nStandard Position Wise Feedforward Network This is as simple as it can get, two-layered fully connected network of neurons with a single activation switch, most probably a ReLU variant (for LLMs). $$ FFN(x) = max(0, W_2(W_1(x)) + b_1)W_2 + b_2 $$ Same transformation is applied for each token in parallel, making it efficient for GPU computation. For Example, in BERT model the 512 dimensions of each token are expanded to 2048 to allow for \u0026ldquo;intricate interactions\u0026rdquo; before projecting back to 512 dimensions. This part is important, in every transformer stacking of layers is possible only because the output of every matches it\u0026rsquo;s input dimensions.\nSWiGLU Sigmoid Weighted Gated Linear Unit (SWiGLU) has become the Gold Standard for feed forward networks in LLMs like Llama 3 and PaLM. The primary reason for this choice is the Swish function (as shown in the first image) instead of ReLU or GELU are as following:\nMathematical Linearity: Swish (SILU) is defined as $x\\cdot sigmoid(\\beta x)$, $\\beta$ is a learnable parameter. It is a small monotonic function that allowes small negetive values to pass through, making it mathematically linear. While GELU ($x \\cdot \\phi(x)$) is also smooth and nonmonotonic, Swish\u0026rsquo;s curve and equations have emperically improved the model\u0026rsquo;s performance.\nComputational Efficiency: SILU gradients are simpler to compute. $x/(1+exp(-x))$ compared to the standard GELU, which is an approximation of the tanh function. In the massive scale of modern LLMs these shortcuts save significant training time.\nPerformance: Research in the PaLM and Llama papers found that Swish-based Gated Linear Units consistently outperformed GELU-based versions in terms of perplexity and downstream task accuracy. $$SwiGLU(x, W, V, b, c) = (Swish(xW + b) \\otimes (xV + c))W_2$$ Where $W$ and $V$ are weight matrices for the gate and the data path, and $\\otimes$ is element-wise multiplication.\nMore Complex Filtering: Instead of using ordinary two weight matrices in standard FFN, SWiGLU uses three weight matrices (gate, date, output). This allows it to gather more complex features out of the input without drastically increasing the number of parameters. This performance boost has resulted in modern implementations being low in dimensionality of hidden layers (2/3 of input dimensions) while retaining the performance benefits of gated activations.\nZero-Gradient Avoidance: Unlike ReLU, which has a zero gradient for all negative values (the \u0026ldquo;dying ReLU\u0026rdquo; problem), SwiGLU\u0026rsquo;s smooth curve ensures that the model can still learn from negative inputs, improving training stability in deep stacks.\n","permalink":"http://localhost:1313/posts/feedforward-networks/","summary":"Exploring the hidden layers of trillion-parameter switchboards: Feedforward Neural Networks and Activation Functions.","title":"Anatomy of Trillion-Parameter Switchboards: Understanding Feedforward Blocks"},{"content":"\nWe are about to touch the holy grail of modern AI. The 2017 declaration was simple: Attention Is All You Need. But as we‚Äôve raced from GPT-1 to reasoning models, the definition of \u0026lsquo;Attention\u0026rsquo; has quietly transformed. Is the algorithm ruling the world today actually the same one we started with? Let‚Äôs look under the hood.\nüî• The Core Concept At its heart, the attention mechanism is a form of weighted aggregation. It is often described using a database retrieval analogy. Imagine you have a database of information (context).\nQuery ($Q$): What are you currently looking for? (The token being generated). Key ($K$): The indexing tag associated with each piece of information. Value ($V$): The actual information (The context token). The \u0026ldquo;Attention\u0026rdquo; is the process of matching your Query against all the Keys to determine how much \u0026ldquo;weight\u0026rdquo; to put on each Value. Attention scores are calculated in each transformer layer at every step of the sequence. This is a single unifying factor in all transformer models, from BERT to Gemini.\nüî• Standard Multi-Head Attention (MHA) The original mechanism introduced in the paper \u0026ldquo;Attention Is All You Need\u0026rdquo;.\nHere, instead of performing the Key-Query-Value (KQV) operation on the entire context only once, the attention is split into $N$ parallel heads. Each head learns a different aspect of the language (e.g., word meaning, syntax, semantic relationships, etc.). The equation is a simple culmination of dot products aided by a regularizer like Softmax.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nWhere:\n$QK^T$ computes the similarity scores between the Query and Key. $\\sqrt{d_k}$ is a scaling factor to prevent the dot product from growing too large, mitigating the vanishing/exploding gradient problem. The weighted sum is applied to the values of $V$. The Memory Bandwidth Bottleneck: \u0026gt; During inference, the model must store Key and Value matrices for all previous tokens. This is called the KV Cache. In MHA, every single head has its own unique set of Keys and Values. For large models with massive context windows, loading this massive KV cache from memory (HBM) to the computation chip (SRAM) becomes the primary bottleneck, slowing down generation significantly. Earlier models could often only generate ~8 tokens per second at inference due to this constraint.\nüî• Multi-Query Attention (MQA) This approach addresses the memory bottleneck by sharing the same set of Keys and Values across all heads while keeping separate Query heads.\nMHA: $H$ Query heads, $H$ Key heads, $H$ Value heads MQA: $H$ Query heads, 1 Key head, 1 Value head This drastically reduces the memory requirements of the KV cache by a factor of $H$ (often 8x or 16x). The GPU needs to load far less data, significantly speeding up inference. However, it does not reduce the computation (FLOPs)‚Äîso why is it faster?\nThis can be explained with the Kitchen Analogy: Imagine a kitchen (GPU) with 8 chefs (Heads).\nComputation (Math): The Chef chopping vegetables. Memory Access (Bandwidth): The Assistant running to the fridge to get ingredients. In MHA: Each chef demands unique vegetables from the fridge. The assistant runs back and forth 8 times, fetching 8 different crates. Most of the time is spent waiting for the assistant to return. This results in idle cores waiting for data to load from HBM.\nIn MQA: All chefs agree to use the same vegetables. The assistant runs back and forth only once, fetching one giant crate of potatoes and dumping it. This results in minimized memory transfer and no idle cores.\nThe Technical Reality: During text generation, the model is memory-bound, not compute-bound. The time it takes to move the KV Cache from HBM (High Bandwidth Memory) to the chip\u0026rsquo;s SRAM is the bottleneck. MQA reduces the data volume of that move by a factor of $H$.\nThe Capacity Gap: MQA is a trade-off between nuanced response and memory capacity. The model loses the ability to store different types of information, as a single $V$ matrix must now store all nuances of information flow in the language.\nThe Surgery Trick (Uptraining): Google Research figured out a way to take an existing model with MHA and convert it to MQA by averaging the weight matrices of $H$ heads. This is called Uptraining. You then train this \u0026ldquo;Frankenstein\u0026rdquo; model for ~5% of the original training steps for adjustments. This yields an MQA model that is nearly as good as the original MHA but vastly faster.\nüî• Grouped Query Attention (GQA) A \u0026ldquo;best of both worlds\u0026rdquo; solution used in modern models like Llama 2 and Llama 3.\nHow it works: GQA acts as an interpolation between MHA and MQA. Instead of having just 1 Key head (MQA) or $H$ Key heads (MHA), it divides the query heads into $G$ groups. Each group shares a single Key and Value head.\nEquation modification: If you have 8 query heads, you might create 4 groups (2 queries per group). This reduces the KV cache by 2x instead of 8x, preserving more quality than MQA while still speeding up inference.\nMHA: 8 people (heads) are working on a project. Each person has their own unique filing cabinet (KV cache). It takes forever to move 8 cabinets around. MQA: 8 people share one single filing cabinet. It\u0026rsquo;s very fast to move, but they fight over the organization and lose detail. GQA: The 8 people split into 4 teams of 2. Each team shares a cabinet. It\u0026rsquo;s a balance between speed and organizational detail. üî• Multi-Head Latent Attention (MLA) This is the most advanced mechanism for attention, used in models like DeepSeek-V3. It is a sophisticated design for massive efficiency that takes a radical approach: mathematically compressing the memory into a low-rank \u0026ldquo;latent\u0026rdquo; vector.\nKV Cache Explosion: As mentioned before, for a massive model like DeepSeek-V3 (671B parameters), storing the full Keys and Values for every token in a long conversation requires terabytes of high-speed memory (HBM), and chips spend more time waiting than churning.\nMHA (Standard) stores everything. MQA deletes redundancy but hurts quality. MLA keeps the quality of MHA but compresses the data 4x-6x smaller than even GQA.\nThe Zip File Approach: Low-Rank Joint Compression Let\u0026rsquo;s compare the math of MHA and MLA. In MHA, for every single token, we generate a unique Key ($k$) and Value ($v$) matrix for each attention head.\nThe Memory Problem: The $k$ is stored in the KV Cache. For 128 heads, for example, it calculates a massive vector of $128 \\times 128 = 16,384$ floats per token.\nFor MHA, the attention score looks like: $$\\text{Score} = q^T \\cdot k$$\nBut for MLA, instead of storing the full $k$, it projects the input down into a tiny latent vector $c_{KV}$, which is much smaller (e.g., 512 floats). $$c_{KV} = W_{DKV} \\cdot x$$\nNow here is the catch: Naive compression and decompression would mean constructing a small latent matrix for every token and deconstructing it every single time the information is required, which defeats the purpose of compression.\n$$k_{\\text{reconstructed}} = W_{UK} \\cdot c_{KV}$$ $$\\text{Score} = q^T \\cdot k_{\\text{reconstructed}}$$\nThe Matrix Absorption Trick (Optimization) Instead of reconstructing $k$ for every token, we can absorb the Up-Projection matrix ($W_{UK}$) into the Down-Projection matrix ($W_{DKV}$) and absorb the query $q$ into a new $Q$.\nFrom the original equation: $$\\text{Score} = q^T \\cdot (\\underbrace{W_{UK} \\cdot c_{KV}}_{\\text{This is } k})$$\nWe associate differently:\n$$\\text{Score} = (\\underbrace{q^T \\cdot W_{UK}}_{\\text{Absorbed Query}}) \\cdot c_{KV}$$ We change the order in which the matrix multiplication is performed.\nHow is this allowed? In linear algebra, matrix multiplication is associative:\n$$(A \\cdot B) \\cdot C = A \\cdot (B \\cdot C)$$ We can move the parentheses! Instead of grouping $(W_{UK} \\cdot c_{KV})$ to make the Key, we group $(q^T \\cdot W_{UK})$ to make a new Query.\nNote: The positional embedding information is left untouched. That rotational information from algorithms like RoPE is preserved outside the compression; it requires minimal space anyways.\n","permalink":"http://localhost:1313/posts/attention-mechanisms/","summary":"We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek\u0026rsquo;s MLA, how has the definition of \u0026lsquo;Attention\u0026rsquo; transformed?","title":"Attention Is All You Need, but exactly which one?: MHA, GQA and MLA"},{"content":"\nPositional Embeddings are the \u0026ldquo;voice\u0026rdquo; that tell Transformers where words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.\n\u0026ldquo;The dog chased the cat\u0026rdquo; and \u0026ldquo;cat chased the dog\u0026rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.\nüî• Sinusoidal Embeddings: The Odometer of Language Introduced in the seminal \u0026ldquo;Attention Is All You Need\u0026rdquo; paper, this was the industry standard for many years. This approach adds a vector to each word embedding based on its position. The vector is computed out of wavelike patterns that are periodic and smooth. Mathematically, for a position $pos$ and embedding dimension $i$:\n$$ PE_{(pos, i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $$\nThe model uses sine and cosine waves of different frequencies to capture both short and long-range dependencies. The frequency of the waves decreases as the embedding dimension increases, which helps capture long-range dependencies in the input sequence.\nThe creators wanted the model to calculate distance easily, which is why a linear function was used. The model simply rotates the current position vector using a rotation matrix to derive the next position.\n$$ \\begin{pmatrix} \\cos(k) \u0026 \\sin(k) \\\\ -\\sin(k) \u0026 \\cos(k) \\end{pmatrix} \\cdot PE(pos) \\approx PE(pos + k) $$ Low dimensions act like a fast-ticking clock (high frequency, changing rapidly) and high dimensions act like a slow clock (low frequency, changing slowly).\nThe number 10,000 is an arbitrarily chosen high number to reduce Aliasing. By choosing such a high number, the creators ensured that the wavelength of the slowest dimension is incredibly long. Consequently, the pattern practically never repeats, and each position remains unique.\nThis combination works like a mechanical odometer dial. Each subsequent dimension acts as a slower-moving gear. The model learns to read these \u0026ldquo;dials\u0026rdquo; to understand exactly where a word sits in the sequence.\nThis mitigated the challenge of exploding gradients that previous array-based positional embeddings suffered from (e.g., adding integers 1, 2, 3\u0026hellip;). This model keeps values between -1 and +1, adding stability while ensuring every position gets a distinct code.\nThe Problem: This approach pollutes the word embedding matrix (semantic information). While high-dimensional vectors are almost always orthogonal‚Äîmeaning the noise doesn\u0026rsquo;t fully destroy the word\u0026rsquo;s identity‚Äîit does not strictly guarantee purity.\nThis led to a more mature approach.\nüî• ALiBi: The Fading Streetlight Attention with Linear Biases (ALiBi) is a method where, instead of adding a vector to the embeddings at the start, the model injects positional information linearly, directly into the attention scores. If Sinusoidal Encoding asks, \u0026ldquo;Where am I on the map?\u0026rdquo;, ALiBi asks, \u0026ldquo;How much harder should I squint to see you?\u0026rdquo;\nThe biggest problem ALiBi solved is the \u0026ldquo;Invisible Wall\u0026rdquo;. If you train a Transformer on sequences of length 2048 and then feed it a book with 3000 words, it will crash or output garbage at word 2049. This is a failure of Extrapolation.\nALiBi throws away the idea of adding positional vectors entirely. Instead, it devises a \u0026ldquo;penalty\u0026rdquo; system for attention scores:\n$$ \\text{Score} = (q \\cdot k^\\top) - (m \\cdot \\text{distance}) $$\nDistance: If words A and B are neighbors, the distance is 0, so the penalty is 0. Although the distance is linear, it does not cause exploding gradients because the raw attention scores are normalized by a softmax function. This distance prioritizes nearby words (local context) over distant ones.\nSlope ($m$): If we always penalize distance equally, the model becomes myopic. ALiBi mitigates this by assigning a different slope value to each attention head:\nHead 1 (The Historian): Very low slope. The penalty grows slowly, allowing it to see far back. Head 2 (The Reader): Medium slope. Head 3 (The Myopic): Very high slope (e.g., 0.5); it effectively cannot see beyond 2-3 words. This mimics holding a lantern. Words close to you are bright, while words further away naturally fade into darkness.\nExtrapolation: If you walk 5 miles down the road (a longer sequence), your lantern works exactly the same way. The physics of light decay are constant, allowing the model to handle sequences longer than it was trained on.\nHowever, ALiBi forces a \u0026ldquo;sliding window\u0026rdquo; view where distant words eventually become invisible. This brings us to the modern standard.\nüî• RoPE: The Rotary Revolution Rotary Position Embeddings (RoPE) is the current industry standard (used in Llama 3, Mistral, PaLM) because it mathematically unifies \u0026ldquo;absolute position\u0026rdquo; with \u0026ldquo;relative position.\u0026rdquo; This method rotates the vectors in the Query and Key matrices based on their absolute position.\nIf a vector has 512 dimensions, RoPE breaks it into 256 pairs. Each pair is rotated by a different frequency:\n$$ \\theta_i = 10000^{-2i/d} $$\nThe First pair ($i = 0$) is rotated by the highest frequency. The Last pair ($i = 255$) is rotated by the lowest frequency. This spread ensures the model has \u0026ldquo;high precision\u0026rdquo; (fast rotation) to distinguish immediate neighbors, and \u0026ldquo;low precision\u0026rdquo; (slow rotation) to track global position without the pattern repeating.\nDoesn\u0026rsquo;t rotation change meaning? Yes and No. The vector for \u0026ldquo;Apple\u0026rdquo; at position 1 and \u0026ldquo;Apple\u0026rdquo; at position 256 will look completely different in terms of coordinates. However, rotation does not change the semantic strength (magnitude/norm) of the vector.\nThe Relativity Trick: Transformers compare words using dot products (angles). RoPE relies on the fact that the attention mechanism ignores the absolute rotation and only detects the difference in rotation (relative position). Thus, the \u0026ldquo;relative meaning\u0026rdquo; is encoded purely into the angle between words, leaving the semantic \u0026ldquo;magnitude\u0026rdquo; untouched.\nüî• LongRoPE: The Bifocal Lens While RoPE is powerful, it struggles when stretching context windows from 4k to 2 million tokens. The \u0026ldquo;fast\u0026rdquo; dimensions spin so rapidly over long distances that they become random noise. LongRoPE solves this using an evolutionary search algorithm to find a unique scaling factor ($\\lambda$) for each dimension.\nThe Equations Instead of rotating by the standard $\\theta_i$, LongRoPE rotates by a rescaled frequency:\n$$ \\theta\u0026rsquo;_i = \\lambda_i \\theta_i $$\nTo efficiently find these $\\lambda$ values without searching an infinite space, the algorithm enforces a monotonicity constraint based on NTK theory:\n$$ \\lambda_i \\le \\lambda_{i+1} $$\nThis ensures that low-frequency dimensions (global context) are stretched more than high-frequency dimensions (local context), creating a \u0026ldquo;bifocal\u0026rdquo; effect:\nHigh Frequencies (Local): Kept sharp ($\\lambda \\approx 1$) to maintain grammar. Low Frequencies (Global): Stretched ($\\lambda \u0026gt; 1$) to track massive distances without repeating. Not worth perusing more since the next one is much better at long context positional encoding. üî• HoPE: The Hyperbolic Slide Hyperbolic Rotary Positional Encoding (HoPE) moves from the geometry of a circle to the geometry of a hyperbola (inspired by Lorentz transformations). It was designed to fix a subtle flaw in RoPE: the \u0026ldquo;wobbly\u0026rdquo; or oscillatory nature of attention scores.\nThe Core Problem: RoPE\u0026rsquo;s \u0026ldquo;Wobble\u0026rdquo; In standard RoPE, we rotate vectors around a circle.\nThe Issue: Circles repeat. A dial at $361^\\circ$ looks identical to $1^\\circ$. Even with different frequencies, the dot product fluctuates up and down as distance increases. The Consequence: The model gets confused at long distances, potentially mistaking a word 1,000 tokens away for a neighbor just because the rotation cycles aligned, creating \u0026ldquo;noise.\u0026rdquo; The \u0026ldquo;Hyperbolic\u0026rdquo; Shift HoPE replaces trigonometric functions ($\\sin, \\cos$) with hyperbolic functions ($\\sinh, \\cosh$).\nCircular Rotation: Keeps distance from the center constant. Hyperbolic Rotation: Moves points along a hyperbola. Crucially, hyperbolic functions do not repeat; they grow or decay exponentially. By using hyperbolic rotation, HoPE guarantees that as words get further apart, the attention score decays monotonically (smoothly drops).\nThe Equations HoPE uses a hyperbolic matrix $B(\\theta, m)$ defined using the Lorentz Boost structure:\n$$ B(\\theta, m) = \\begin{pmatrix} \\cosh(m\\theta) \u0026 \\sinh(m\\theta) \\\\ \\sinh(m\\theta) \u0026 \\cosh(m\\theta) \\end{pmatrix} $$ Since hyperbolic functions grow to infinity, HoPE introduces a decay penalty ($e^{-m\\theta'}$). The final attention score simplifies beautifully to a function of relative distance: $$ \\text{Score} \\propto e^{-|m-n|(\\theta\u0026rsquo; - \\theta)} $$\nThis ensures the attention score exponentially decays as distance increases, eliminating the wobbles of standard RoPE. This the latest model that is taking up the space in the industry for extremely long context models.\n","permalink":"http://localhost:1313/posts/positional-embeddings/","summary":"From Sinusoidal to RoPE and HoPE: How Transformers learn to process word order and sequence length.","title":"The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE"},{"content":"Imagine trying to teach a computer the difference between \u0026ldquo;Apple\u0026rdquo; the fruit and \u0026ldquo;Apple\u0026rdquo; the company. To us, the distinction is intuitive. To a machine, it‚Äôs just a string of characters. How do we turn these strings into meaningful math? While early attempts like Word2Vec gave us a great start, they missed the forest for the trees‚Äîor in some cases, the twigs for the branches. Enter GloVe and FastText: two algorithms that revolutionized how machines understand the nuances of human language.\nPrevious static embedding models like Word2Vec successfully captured the local semantics of words, but they failed in addressing contexts between words that might not always appear in the same context window. Furthermore, they treated words as indivisible atomic units in continuous vector spaces, missing out on the internal structure of language.\nThe two simple but effective ideas behind GloVe and FastText are subword units and global co-occurrence. Let\u0026rsquo;s dive into them.\nGloVe: A Global Mapping Strategy Global Vectors for Word Representation (GloVe) was created at Stanford. The core idea was to train a model where word vectors are defined by how often they appear around other words across the entire corpus, rather than just locally.\nTraining Steps: Co-occurrence Matrix X: GloVe scans the entire corpus and creates a giant matrix (spreadsheet) recording the counts of how often words appear near each other. Factorization: It compresses this giant matrix into smaller, dense vectors. Objective: The function tries to equate the dot product of two word vectors with the logarithm of their probability of appearing together. The error between these values is then backpropagated to update the vectors. $$w_i \\cdot w_j + b_i + b_j = \\log(X_{ij})$$\n$w$: The word vector (what we want). $X_{ij}$: The count from our matrix. The Loss Function: We can\u0026rsquo;t satisfy the equation above perfectly for every word pair. So, we use a weighting function to minimize the squared error:\n$$J = \\sum_{i,j=1}^V f(X_{ij}) ({w_i}^T w_j + b_i + b_j - \\log(X_{ij}))^2$$\nIf we do not use the weighting function, the loss would be infinite for rare words (since $\\log(0) = \\infty$) and frequent words (like \u0026rsquo;the\u0026rsquo;) would overpower the model.\n$$ f(x) = \\begin{cases} (x/x_{max})^\\alpha \u0026amp; \\text{if } x \u0026lt; x_{max} \\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$\n(Typically $x_{max}=100$ and $\\alpha=0.75$)\nFastText: The Lego Bricks Strategy FastText was created by Facebook AI Research (FAIR). The idea was to treat words not as solid rocks, but as structures made of \u0026ldquo;Lego bricks\u0026rdquo; (subword units).\nWord2Vec struggled to create embeddings for words it hadn\u0026rsquo;t seen before (Out-Of-Vocabulary or OOV words). FastText solves this by breaking words down.\nGloVe treats a word like \u0026ldquo;Apple\u0026rdquo; as a single, atomic unit. FastText breaks \u0026ldquo;Apple\u0026rdquo; into character n-grams: \u0026lt;ap, app, ppl, ple, le\u0026gt; For a complex or new word like \u0026ldquo;Applesauce\u0026rdquo;, the model might not know the whole word, but it recognizes the Lego brick for \u0026ldquo;Apple\u0026rdquo; and the remaining bits from other subwords.\nFastText takes its inspiration from the SkipGram architecture. The vector for a word is simply the sum of the vectors of its n-gram subwords:\n$$v_w = \\sum_{g \\in G_w} z_g$$\nObjective: The objective is to predict context words given a target word.\n$$P(w_c | w_t) = \\frac{e^{s(v_{w_t} \\cdot v_{w_c})}}{\\sum_{w \\in V} e^{s(v_{w_t} \\cdot v_w)}}$$\nAnd the scoring function is defined as:\n$$s(w_t, w_c) = u_{w_t}^T (\\sum_{g \\in G_w} z_g)$$\n$u_{w_c}$: The vector for the context word (output vector). $z_g$: The vector for the n-gram $g$ (input vector). Modifications to Softmax: Calculating the full softmax for every word in the vocabulary is computationally expensive. FastText uses two methods to approximate this efficiently.\nOption A: Negative Sampling\nWe pick a few random \u0026ldquo;wrong\u0026rdquo; words every time to serve as \u0026ldquo;negative samples\u0026rdquo; and calculate the loss only for the correct word and these noise words.\n$$J = - \\log \\sigma(s(w_t, w_c)) - \\sum\\limits_{i=1}^{N} \\mathbb{E}_{n_i \\sim P_n(w)} [\\log \\sigma(-s(w_t, n_i))]$$\nThe sigmoid function $\\sigma$ squashes the score between 0 and 1. The first term pushes the probability of the correct word towards 1. The second term pushes the probability of the noise words towards 0. Option B: Hierarchical Softmax (Faster for infrequent words)\nInstead of a flat list of words, imagine the vocabulary arranged as a binary tree (Huffman Tree).\nRoot: Top of the tree. Leaves: The actual words. To calculate the probability of a word, we trace a path from the root to that leaf. At every branching node, we calculate the probability (sigmoid) of going left or right. The probability of a word $w$ is the product of the probabilities of the turns taken to reach it. Code Example from gensim.models import FastText from gensim.test.utils import common_texts # Initialize the FastText model with specific hyperparameters model = FastText( vector_size=100, # Dimensionality of the word vectors window=5, # Maximum distance between the current and predicted word within a sentence min_count=1, # Ignores all words with total frequency lower than this sg=1, # Training algorithm: 1 for skip-gram; 0 for CBOW hs=0, # If 0, and negative is non-zero, negative sampling will be used negative=5, # Number of \u0026#34;noise words\u0026#34; to be drawn for negative sampling workers=4, # Number of worker threads to train the model epochs=10, # Number of iterations over the corpus min_n=3, # Minimum length of character n-grams max_n=6 # Maximum length of character n-grams ) # Build the vocabulary from the provided text corpus model.build_vocab(common_texts) # Train the model on the corpus model.train( common_texts, total_examples=model.corpus_count, epochs=model.epochs ) # Calculate and print the cosine similarity between two words in the vocabulary print(f\u0026#34;Similarity between \u0026#39;computer\u0026#39; and \u0026#39;human\u0026#39;: {model.wv.similarity(\u0026#39;computer\u0026#39;, \u0026#39;human\u0026#39;)}\u0026#34;) # Demonstrate FastText\u0026#39;s ability to handle Out-Of-Vocabulary (OOV) words # Even if \u0026#39;computation\u0026#39; wasn\u0026#39;t in the training data, FastText constructs a vector using character n-grams oov_vector = model.wv[\u0026#39;computation\u0026#39;] # Check if a specific word exists in the model\u0026#39;s fixed vocabulary index word = \u0026#34;computer\u0026#34; is_in_vocab = word in model.wv.key_to_index print(f\u0026#34;\\nIs the word \u0026#39;{word}\u0026#39; in the model\u0026#39;s vocabulary? {is_in_vocab}\u0026#34;) ","permalink":"http://localhost:1313/posts/glove-and-fasttext/","summary":"How subword units and global co-occurrence matrices allow GloVe and FastText to capture nuances that Word2Vec missed.","title":"The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText"},{"content":" Before we had Large Language Models writing poetry, we had to teach computers that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; are related not just by spelling, but by meaning. This is the story of that breakthrough. It‚Äôs the moment we stopped counting words and started mapping their souls‚Äîturning raw text into a mathematical landscape where math can solve analogies. Welcome to the world of Word2Vec. üîÆ\nLanguage models require vector representations of words to capture semantic relationships. Before the 2010s, models used word count-based vector representations that captured only the frequency of words (e.g., One-Hot Encoding). The Problems: üöß\nThe Curse of Dimensionality Lack of meaning (Synonyms were treated as mathematically unrelated). In 2011, Mikolov et al. at Google introduced Word2Vec, which used a shallow neural network to learn vector representations of words. The best part? It could create much denser relationships between words than prior models, and it was unsupervised.\nThe training (prediction) is essentially \u0026ldquo;fake\u0026rdquo;‚Äîa pretext task. But the weight matrices we get on the side are a gold mine of fine-grained semantic relationships. ‚õèÔ∏è\nThere are two popular variants of Word2Vec:\nSkipGram Continuous Bag Of Words (CBOW) 1. Continuous Bag of Words (CBOW) üß© CBOW is a simple \u0026lsquo;fill in the blanks\u0026rsquo; machine. It takes the surrounding words as inputs and tries to predict the center word.\n\u0026ldquo;Continuous\u0026rdquo;: It operates in continuous vector space (unlike the discrete space of n-gram models).\n\u0026ldquo;Bag of words\u0026rdquo;: The order of the context does not matter. \u0026ldquo;The cat sat on the mat\u0026rdquo; and \u0026ldquo;The mat sat on the cat\u0026rdquo; are the same for CBOW. They produce exactly the same predictions.\nObjective: Maximize the probability of the target word $w_t$ given its context $w_{t-C}, \\dots, w_{t+C}$.\n2. Training Defining Dimensions:\n$V$: Vocabulary size (number of unique words in the corpus, e.g., 10,000) $N$: Dimension of the embedding space (e.g., 100) $C$: Number of context words (e.g., 2 words on left of target and 2 words on right) Step 1: Input Lookup \u0026amp; One-Hot Encoding\nMathematically, we represent each context word as a one-hot vector $x^{(c)}$. We have an Input Matrix $W_{in}$ of size $V \\times N$. For each context word (e.g., \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;), we select its corresponding row from $W_{in}$.\n$$ v_c = W_{in}^T x^{(c)} $$\nWhere $v_c$ is the $N$-dimensional vector for context word $c$.\nStep 2: Projection (The \u0026ldquo;Mixing\u0026rdquo;)\nWe combine the context vectors. In standard CBOW, we simply average them. This creates the hidden layer vector $h$.\n$$ h = \\frac{1}{2C} \\sum_{c=1}^{2C} v_c $$\n$h$ is a single vector of size $N \\times 1$. Note: This operation is linear and contains no activation function (like ReLU or Sigmoid).\nStep 3: Output Scoring (The \u0026ldquo;Dot Product\u0026rdquo;)\nWe have a second matrix, the Output Matrix $W_{out}$ of size $N \\times V$. We compute the raw score (logit) $u_j$ for every word $j$ in the vocabulary by taking the dot product of the hidden vector $h$ and the output vector $v\u0026rsquo;_j$.\n$$ u_j = {v\u0026rsquo;_j}^T h $$\nOr in matrix form:\n$$ u = W_{out}^T h $$\nResult $u$ is a vector of size $V \\times 1$.\nStep 4: Probability Conversion (Softmax)\nWe convert the raw scores $u$ into probabilities using the Softmax function. This tells us the probability that word $w_j$ is the center word.\n$$ y_j = P(w_j | \\text{context}) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 5: Loss Calculation (Cross-Entropy)\nWe compare the predicted distribution $y$ with the actual ground truth $t$ (a one-hot vector where the index of the true word is 1). The loss function $E$ is:\n$$ E = - \\sum_{j=1}^V t_j \\log(y_j) $$\nSince $t_j$ is 0 for all words except the target word (let\u0026rsquo;s call the target index $j^*$), this simplifies to:\n$$ E = - \\log(y_{j^*}) $$\n3. Backpropagation Details We need to update the weights to minimize $E$. We use the Chain Rule.\nA. Gradient w.r.t. Output Matrix ($W_{out}$)\nWe want to know how the error changes with respect to the raw score $u_j$.\n$$ \\frac{\\partial E}{\\partial u_j} = y_j - t_j $$\nLet\u0026rsquo;s call this error term $e_j$.\nIf $j$ is the target word: $e_j = y_j - 1$ (Result is negative; we boost the score). If $j$ is not target: $e_j = y_j - 0$ (Result is positive; we suppress the score). The update rule for the output vector $v\u0026rsquo;_j$ becomes:\n$$ v'_{j}(\\text{new}) = v'_{j}(\\text{old}) - \\eta \\cdot e_j \\cdot h $$ ($\\eta$ is the learning rate)\nB. Gradient w.r.t. Hidden Layer ($h$)\nWe backpropagate the error from the output layer to the hidden layer.\n$$ EH = \\sum_{j=1}^V e_j \\cdot v\u0026rsquo;_j $$\n$EH$ is an $N$-dimensional vector representing the aggregate error passed back to the projection layer.\nC. Gradient w.r.t. Input Matrix ($W_{in}$)\nSince $h$ was just an average of the input vectors, the error $EH$ is distributed to each context word\u0026rsquo;s input vector. For every word $w_c$ in the context:\n$$ v_{w_c}(\\text{new}) = v_{w_c}(\\text{old}) - \\eta \\cdot \\frac{1}{2C} \\cdot EH $$\nLet\u0026rsquo;s build a single pass through the network for a given context.\nimport torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # Define a simple Continuous Bag of Words (CBOW) style model class CBOWModel(nn.Module): def __init__(self, vocab_size, embedding_dim): super(CBOWModel, self).__init__() # The embedding layer stores the word vectors we want to learn self.embeddings = nn.Embedding(vocab_size, embedding_dim) # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word self.linear = nn.Linear(embedding_dim, vocab_size) def forward(self, inputs): # inputs: tensor of word indices for the context embeds = self.embeddings(inputs) # Aggregate context by calculating the mean of the word embeddings h = torch.mean(embeds, dim=1) # Produce logits (raw scores) for each word in the vocabulary logits = self.linear(h) return logits # Setup vocabulary and mappings word_to_ix = {\u0026#34;the\u0026#34;: 0, \u0026#34;quick\u0026#34;: 1, \u0026#34;brown\u0026#34;: 2, \u0026#34;fox\u0026#34;: 3, \u0026#34;jumps\u0026#34;: 4, \u0026#34;over\u0026#34;: 5, \u0026#34;lazy\u0026#34;: 6, \u0026#34;dog\u0026#34;: 7} ix_to_word = {v: k for k, v in word_to_ix.items()} # Configuration constants EMBEDDING_DIM = 5 VOCAB_SIZE = len(word_to_ix) LEARNING_RATE = 0.01 # Initialize the model, loss function, and optimizer model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM) loss_function = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE) # Prepare dummy training data: context words and the target word # Context: [\u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;over\u0026#34;] -\u0026gt; Target: \u0026#34;fox\u0026#34; context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long) target_idxs = torch.tensor([3], dtype=torch.long) # Perform a single optimization step model.zero_grad() # Forward pass: we reshape context to (1, -1) to simulate a batch of size 1 logits = model(context_idxs.view(1, -1)) # Calculate loss against the target word index loss = loss_function(logits, target_idxs) # Backpropagate and update weights loss.backward() optimizer.step() # Output results for the blog post print(\u0026#34;=== Model Training Snapshot ===\u0026#34;) print(f\u0026#34;Calculated Loss: {loss.item():.6f}\u0026#34;) print(f\u0026#34;\\n=== Learned Vector for \u0026#39;jumps\u0026#39; ===\u0026#34;) word_vec = model.embeddings(torch.tensor([word_to_ix[\u0026#39;jumps\u0026#39;]])) print(word_vec.detach().numpy()) print(\u0026#34;\\n=== Embedding Matrix (Weights) ===\u0026#34;) print(model.embeddings.weight.detach().numpy()) print(\u0026#34;\\n=== Linear Layer Weights ===\u0026#34;) print(model.linear.weight.detach().numpy()) Output shows single pass loss and learned vector for \u0026lsquo;jumps\u0026rsquo; word. This is a single training sample result but after multiple such passes both Embedding and linear weights should be very similar.\n=== Model Training Snapshot === Calculated Loss: 1.989656 === Learned Vector for \u0026#39;jumps\u0026#39; === [[-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775]] === Embedding Matrix (Weights) === [[ 1.66039 -0.11371879 0.6246518 0.35860053 -1.2504417 ] [-2.1847186 -0.77199775 -0.17050214 -0.38411248 -0.03913084] [ 0.11852697 0.90073633 0.8847807 0.7404524 0.900149 ] [ 0.07440972 -0.40259898 2.6246994 -0.08851447 0.02660969] [-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775] [-0.9245572 -0.5545908 0.9083091 -1.0755049 0.84047747] [-1.0237687 0.59466314 0.05621134 -0.6202532 1.3664424 ] [ 0.60998917 -1.0549186 1.6103884 0.8724912 -1.2486908 ]] === Linear Layer Weights === [[ 0.02165058 -0.28883642 0.14545658 -0.3442509 0.32704315] [-0.18731792 0.28583744 0.22635977 0.13245736 0.29019794] [ 0.3158916 -0.15826383 -0.03203773 0.16377363 -0.41457543] [-0.05080034 0.4180087 0.11228557 0.30218413 0.3025514 ] [-0.38419306 0.24475925 -0.39210224 -0.38660625 -0.2673145 ] [-0.32321444 0.12200444 -0.03569533 0.2891424 -0.07345333] [-0.33704326 0.2521956 0.31587374 0.22590035 0.29866052] [ 0.4117266 -0.44231793 0.24064957 -0.29684234 0.333821 ]] Interesting Facts About CBOW üß† The \u0026ldquo;Averaging Problem\u0026rdquo;: This is a common issue in CBOW models. The averaging and training is order-agnostic. All variants of word order in the context window will generate the same output. \u0026ldquo;Dog bit the man\u0026rdquo; and \u0026ldquo;Man bit the dog\u0026rdquo; are the same in CBOW\u0026rsquo;s eyes.\nSmoothing Effect: CBOW models are much faster to train than Skip-Gram but are generally smoother.\nRare Words: CBOW models struggle slightly with rare words because the presence of surrounding common words (like articles and prepositions) can skew the embeddings and dissolve their uniqueness.\nComputational Cost: Softmax requires summing over the entire vocabulary size, which can be computationally expensive if $V$ is very large. Solutions include Hierarchical Softmax and Negative Sampling (approximating the denominator by only checking the target word vs. 5 random noise words).\nVectors as Double Agents: Every word has a different representation in the embedding matrix and the output matrix. Most of the time $W_{out}$ is discarded, but some consider averaging both $W_{in}$ and $W_{out}$ for slightly better performance.\nLinear Relationships: The famous analogy King - Man + Woman = Queen emerges from the model because of the linear relationships between the words.\nInitialization: This is a no-brainer but worth mentioning. Initialization to zero vectors would mean no learning. We must initialize randomly.\n2. Skip-Gram (The \u0026ldquo;Burger Flip\u0026rdquo; of CBOW) üîÑ Skip-Gram flips the CBOW logic. While CBOW tries to predict the center word given the context, Skip-Gram tries to predict the scattered context given the center word.\nCore Concept: If a word like \u0026ldquo;monarch\u0026rdquo; can accurately predict words like \u0026ldquo;king\u0026rdquo;, \u0026ldquo;throne\u0026rdquo;, \u0026ldquo;crown\u0026rdquo;, \u0026ldquo;royalty\u0026rdquo;, and \u0026ldquo;power\u0026rdquo;, then it must effectively capture the concept of royalty.\nTraining Let\u0026rsquo;s define dimensions:\n$V$: Vocabulary size $D$: Embedding dimension $C$: Number of context words Step 1: Input Lookup\nUnlike CBOW, our input is just one word vector (the center word $w_t$). We grab the vector $v_c$ from the Input Matrix $W_{in}$.\n$$ h = v_{w_t} $$\nNote: There is no averaging here. The hidden layer $h$ is simply the raw vector of the center word.\nStep 2: Output Scoring (The Broadcast)\nThe model takes this single vector $h$ and compares it against the Output Matrix $W_{out}$ (the entire vocabulary). It computes a score $u_j$ for every word $j$ in the dictionary.\n$$ u = W_{out}^T h $$\nResult $u$: A vector of size $V \\times 1$ containing raw scores.\nStep 3: Probability Conversion (Softmax)\nWe apply Softmax to turn scores into probabilities.\n$$ P(w_j | w_t) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 4: The Multi-Target Loss\nHere is the major difference from CBOW. In CBOW, we had one target (the center). In Skip-Gram, we have $2C$ targets (all the surrounding words). For a sentence \u0026ldquo;The cat sat on mat\u0026rdquo; (Center: \u0026ldquo;sat\u0026rdquo;), the targets are \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;. We want to maximize the probability of all these true context words. The Loss function ($E$) sums the error for each context word:\n$$ E = - \\sum_{w_{ctx} \\in \\text{Window}} \\log P(w_{ctx} | w_t) $$\nStep 5: Backpropagation (Accumulating Gradients)\nSince one input word is responsible for predicting multiple output words, the error signals from all those context words add up.\nAt the Output Layer:\nIf \u0026ldquo;cat\u0026rdquo; was a target, the \u0026ldquo;cat\u0026rdquo; vector in $W_{out}$ gets a signal: \u0026ldquo;Move closer to \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; If \u0026ldquo;apple\u0026rdquo; was NOT a target, the \u0026ldquo;apple\u0026rdquo; vector gets a signal: \u0026ldquo;Move away from \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; At the Hidden Layer ($h$):\nThe center word \u0026ldquo;sat\u0026rdquo; receives feedback from all its neighbors simultaneously. Error signal = (Error from \u0026ldquo;The\u0026rdquo;) + (Error from \u0026ldquo;cat\u0026rdquo;) + (Error from \u0026ldquo;on\u0026rdquo;)\u0026hellip; The vector for \u0026ldquo;sat\u0026rdquo; in $W_{in}$ moves in the average direction of all its neighbors. 3. The \u0026ldquo;Computational Nightmare\u0026rdquo; \u0026amp; Negative Sampling The equations above describe Naive Softmax.\nThe Problem: If $V = 100,000$, computing the denominator $\\sum \\exp(u_k)$ takes 100,000 operations. Doing this for every word in a training set of billions of words is impossibly slow.\nThe Solution: Negative Sampling Instead of updating the entire vocabulary (1 correct word vs 99,999 wrong ones), we approximate the problem.\nPositive Pair: (sat, cat) $\\rightarrow$ Maximize probability (Label 1). Negative Pairs: We pick $K$ (e.g., 5) random words the model didn\u0026rsquo;t see, e.g., (sat, bulldozer), (sat, quantum). $\\rightarrow$ Minimize probability (Label 0). New Equation (Sigmoid instead of Softmax):\n$$ E = - \\log \\sigma({v'_{\\text{pos}}}^T v_{\\text{in}}) - \\sum_{k=1}^K \\log \\sigma(-{v'_{\\text{neg}_{k}}}^T v_{\\text{in}}) $$ Effect: We typically only update 6 vectors per step (1 pos + 5 negs) instead of 100,000.\nLet\u0026rsquo;s see this in code:\nimport torch import torch.nn as nn import torch.optim as optim class SkipGramNegativeSampling(nn.Module): \u0026#34;\u0026#34;\u0026#34; Skip-Gram with Negative Sampling (SGNS) implementation. SGNS approximates the softmax over the entire vocabulary by instead distinguishing between a real context word (positive) and K noise words (negative). \u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embedding_dim): super(SkipGramNegativeSampling, self).__init__() # Input embeddings: used when the word is the center word self.in_embed = nn.Embedding(vocab_size, embedding_dim) # Output embeddings: used when the word is a context or negative sample self.out_embed = nn.Embedding(vocab_size, embedding_dim) # Initialize weights with small values to prevent gradient saturation initrange = 0.5 / embedding_dim self.in_embed.weight.data.uniform_(-initrange, initrange) self.out_embed.weight.data.uniform_(-initrange, initrange) def forward(self, center_words, target_words, negative_words): \u0026#34;\u0026#34;\u0026#34; Computes the negative sampling loss. Args: center_words: (batch_size) target_words: (batch_size) negative_words: (batch_size, K) where K is number of negative samples \u0026#34;\u0026#34;\u0026#34; # Retrieve vectors v_c = self.in_embed(center_words) # (batch_size, embed_dim) u_o = self.out_embed(target_words) # (batch_size, embed_dim) u_n = self.out_embed(negative_words) # (batch_size, K, embed_dim) # 1. Positive Score: log(sigmoid(v_c ¬∑ u_o)) # Compute dot product: (batch, 1, dim) @ (batch, dim, 1) -\u0026gt; (batch, 1) pos_score = torch.bmm(u_o.unsqueeze(1), v_c.unsqueeze(2)).squeeze(2) pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-7) # 2. Negative Score: sum(log(sigmoid(-v_c ¬∑ u_n))) # Compute dot products for all K samples: (batch, K, dim) @ (batch, dim, 1) -\u0026gt; (batch, K) neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2) neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-7), dim=1, keepdim=True) # Total loss is the negative of the objective function loss = -(pos_loss + neg_loss) return torch.mean(loss) # --- Configuration \u0026amp; Mock Data --- VOCAB_SIZE = 100 EMBED_DIM = 10 word_to_ix = {\u0026#39;fox\u0026#39;: 0} # Example vocabulary mapping model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock inputs: 1 center word, 1 target word, 5 negative samples center_id = torch.tensor([0], dtype=torch.long) target_id = torch.tensor([1], dtype=torch.long) negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long) # Training Step model.zero_grad() loss = model(center_id, target_id, negative_ids) loss.backward() optimizer.step() # Output Results print(f\u0026#34;Loss after one step: {loss.item():.6f}\u0026#34;) word_vec = model.in_embed(torch.tensor([word_to_ix[\u0026#39;fox\u0026#39;]])) print(f\u0026#34;Vector for \u0026#39;fox\u0026#39;:\\n{word_vec.detach().numpy()}\u0026#34;) The outputs:\nLoss after one step: 4.158468 Vector for \u0026#39;fox\u0026#39;: [[ 0.04197385 0.02400733 -0.03800093 0.01672485 -0.03872231 -0.0061478 0.0121122 0.04057864 -0.036255 0.03861175]] Interesting Facts About Skip-Gram üí° Makes Rare Words Shine: Skip-Gram is more effective at capturing the meaning of rare words because it focuses on the context of the center word. In CBOW, a rare word\u0026rsquo;s vector is averaged with others, diluting its signal. In Skip-Gram, when the rare word appears as the center, it gets the full, undivided attention of the backpropagation update.\nSlow Training: This method creates more training samples. A window of size 2 creates 4 training pairs per center word, while CBOW creates only 1 training pair per center word.\nSemantic Focus: Skip-Gram puts more emphasis on the semantic relationship between words. It views the context as the primary signal, with the center word as the target. This makes it better at capturing the meaning of words in context. Skip-Gram tends to capture semantic relationships (King/Queen) slightly better, while CBOW captures syntactic ones (walk/walked) slightly better.\n","permalink":"http://localhost:1313/posts/word-embeddings/","summary":"Understanding the mathematics behind Word2Vec, CBOW, and Skip-Gram and how they map language to vector space.","title":"Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n5. Code Examples: We\u0026rsquo;ll use sentencepiece for tokenization and compare results for different models.\nimport sentencepiece as spm import os # Configuration for training the SentencePiece model # SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words. options = { # The source text file used for learning the vocabulary \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # The base name for the generated model (.model) and vocabulary (.vocab) files \u0026#39;model_prefix\u0026#39;: \u0026#39;bpe_model\u0026#39;, # Number of unique tokens in the final vocabulary \u0026#39;vocab_size\u0026#39;: 4000, # \u0026#39;bpe\u0026#39; (Byte Pair Encoding) merges frequent pairs of characters/sequences \u0026#39;model_type\u0026#39;: \u0026#39;bpe\u0026#39;, # Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets \u0026#39;character_coverage\u0026#39;: 0.9995, # When enabled, unknown characters are decomposed into UTF-8 bytes to avoid \u0026#39;unk\u0026#39; tokens \u0026#39;byte_fallback\u0026#39;: True, # Treats digits individually (0-9), preventing large numbers from being treated as single tokens \u0026#39;split_digits\u0026#39;: True, # Prevents adding a whitespace prefix to the first token; useful for fine-grained control \u0026#39;add_dummy_prefix\u0026#39;: False } try: print(\u0026#34;Starting the training process...\u0026#34;) # SentencePieceTrainer.train takes the dictionary of options to build the BPE model spm.SentencePieceTrainer.train(**options) print(\u0026#34;Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created.\u0026#34;) # Initialize the processor and load the newly trained model sp = spm.SentencePieceProcessor() sp.load(\u0026#39;bpe_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Model Metadata:\u0026#34;) # Retrieve the total number of tokens in the vocabulary print(f\u0026#34;Total Vocab Size: {sp.get_piece_size()}\u0026#34;) # Special tokens are used for sequence boundaries and handling unknown characters print(f\u0026#34;BOS (Beginning of Sentence) ID: {sp.bos_id()}\u0026#34;) print(f\u0026#34;EOS (End of Sentence) ID: {sp.eos_id()}\u0026#34;) print(f\u0026#34;UNK (Unknown) ID: {sp.unk_id()}\u0026#34;) print(f\u0026#34;PAD (Padding) ID: {sp.pad_id()}\u0026#34;) # Test the tokenizer on sample strings to see how it breaks down text test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: shows the actual subword units (tokens) print(f\u0026#34;Subword Tokens: {sp.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: shows the numerical mapping for each token print(f\u0026#34;Numerical IDs: {sp.encode_as_ids(text)}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred during training or processing: {e}\u0026#34;) You can see the following output:\nStarting training... Starting the training process... Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created. ------------------------------ Model Metadata: Total Vocab Size: 4000 BOS (Beginning of Sentence) ID: 1 EOS (End of Sentence) ID: 2 UNK (Unknown) ID: 0 PAD (Padding) ID: -1 --- Tokenization Test --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;el\u0026#39;, \u0026#39;lo\u0026#39;, \u0026#39;‚ñÅW\u0026#39;, \u0026#39;orld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 380, 309, 476, 619, 36, 3924, 3964, 3959, 3993, 3978, 3983, 57, 3975, 3976, 60, 3974] --- Tokenization Test --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅmo\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;inst\u0026#39;, \u0026#39;ag\u0026#39;, \u0026#39;ram\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅever\u0026#39;] Numerical IDs: [1894, 304, 3284, 376, 321, 313, 312, 1008, 3313, 1325, 428, 3937, 341, 304, 3284, 1495] The ! is fallen back to UTF bytes. and the word instagrammable is split into multiple subword tokens. Also, original text can be recreated right back by simply concatenating the tokens. And replacing underscore by whitespace. Now we can try Unigram model. and see the same sentences tokenized differently.\nfrom tokenizers import Tokenizer from tokenizers.models import WordPiece from tokenizers.trainers import WordPieceTrainer from tokenizers.pre_tokenizers import Whitespace try: # 1. Initialize the WordPiece Tokenizer # We specify the [UNK] token for handling words not found in the vocabulary. tokenizer = Tokenizer(WordPiece(unk_token=\u0026#34;[UNK]\u0026#34;)) # 2. Configure Pre-tokenization # Before the subword algorithm runs, we need to split the raw text into words. # Whitespace splitting is the standard first step for most English NLP tasks. tokenizer.pre_tokenizer = Whitespace() # 3. Initialize the Trainer # We define our target vocabulary size and the special tokens required for # downstream tasks (like BERT\u0026#39;s [CLS] for classification or [SEP] for separators). trainer = WordPieceTrainer( vocab_size=4000, special_tokens=[\u0026#34;[UNK]\u0026#34;, \u0026#34;[CLS]\u0026#34;, \u0026#34;[SEP]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[MASK]\u0026#34;] ) # 4. Train the Model # The tokenizer scans the training file to build a vocabulary of the most # frequent subword units. tokenizer.train(files=[\u0026#34;train_data.txt\u0026#34;], trainer=trainer) # 5. Persist the Model # Save the configuration and vocabulary to a JSON file for future inference. tokenizer.save(\u0026#34;wordpiece.json\u0026#34;) print(\u0026#34;Training complete. \u0026#39;wordpiece.json\u0026#39; created.\u0026#34;) # 6. Metadata Inspection print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;WordPiece Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {tokenizer.get_vocab_size()}\u0026#34;) # 7. Testing Subword Tokenization # WordPiece shines at handling rare words by breaking them into meaningful chunks. test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (WordPiece) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # Encode converts raw text into a Tokenizer object containing tokens and IDs output = tokenizer.encode(text) # \u0026#39;tokens\u0026#39; shows the subword breakdown (e.g., \u0026#39;un\u0026#39;, \u0026#39;##insta\u0026#39;, etc.) print(f\u0026#34;Subword Tokens: {output.tokens}\u0026#34;) # \u0026#39;ids\u0026#39; are the numerical indices mapped to the vocabulary print(f\u0026#34;Numerical IDs: {output.ids}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred with WordPiece model: {e}\u0026#34;) You can see the following output:\nWordPiece Model Metadata: Total Vocab Size: 2609 --- Tokenization Test (WordPiece) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;H\u0026#39;, \u0026#39;##el\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;##or\u0026#39;, \u0026#39;##ld\u0026#39;, \u0026#39;[UNK]\u0026#39;, \u0026#39;[UNK]\u0026#39;] Numerical IDs: [37, 180, 214, 52, 162, 418, 0, 0] --- Tokenization Test (WordPiece) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;##os\u0026#39;, \u0026#39;##t\u0026#39;, \u0026#39;un\u0026#39;, \u0026#39;##ins\u0026#39;, \u0026#39;##ta\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;##ra\u0026#39;, \u0026#39;##m\u0026#39;, \u0026#39;##ma\u0026#39;, \u0026#39;##ble\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;ever\u0026#39;] Numerical IDs: [691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240] We can see the special character ## in the output. We can also see the breakdowns not having any semantic meaning. They are likelihood based. We can try the Unigram model now.\n# \u0026#39;Unigram\u0026#39; is the default and usually recommended over BPE in SentencePiece. options_unigram = { \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # Path to the raw text file for training \u0026#39;model_prefix\u0026#39;: \u0026#39;unigram_model\u0026#39;, # Prefix for the output .model and .vocab files \u0026#39;vocab_size\u0026#39;: 1200, # Desired size of the final vocabulary \u0026#39;model_type\u0026#39;: \u0026#39;unigram\u0026#39;, # Specifies the Unigram language model algorithm \u0026#39;character_coverage\u0026#39;: 0.9995, # Percentage of characters covered by the model (0.9995 is standard for Latin scripts) \u0026#39;byte_fallback\u0026#39;: True, # Enables mapping unknown characters to UTF-8 bytes to avoid \u0026lt;unk\u0026gt; tokens \u0026#39;split_digits\u0026#39;: True, # Treats each digit as an individual token (useful for numerical data) \u0026#39;add_dummy_prefix\u0026#39;: False # Prevents adding a leading space (SentencePiece default is True) } try: # 1. Train the SentencePiece model using the defined options print(\u0026#34;Starting Unigram training...\u0026#34;) spm.SentencePieceTrainer.train(**options_unigram) print(\u0026#34;Training complete. \u0026#39;unigram_model.model\u0026#39; created.\u0026#34;) # 2. Load the trained model into a processor instance for inference sp_unigram = spm.SentencePieceProcessor() sp_unigram.load(\u0026#39;unigram_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Unigram Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {sp_unigram.get_piece_size()}\u0026#34;) # 3. Define test cases to evaluate how the model handles common and rare words test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] # 4. Iterate through test sentences to visualize subword segmentation for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (Unigram) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: Converts text into subword strings (visual representation) print(f\u0026#34;Subword Tokens: {sp_unigram.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: Converts text into numerical indices for model input print(f\u0026#34;Numerical IDs: {sp_unigram.encode_as_ids(text)}\u0026#34;) except Exception as e: # Handle potential errors during training or loading (e.g., missing input file) print(f\u0026#34;An error occurred with Unigram model: {e}\u0026#34;) The output is as follows:\nUnigram Model Metadata: Total Vocab Size: 1200 --- Tokenization Test (Unigram) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ll\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;ld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311] --- Tokenization Test (Unigram) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;T\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅm\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;sta\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;ra\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ver\u0026#39;] ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"A comprehensive guide to tokenization strategies: BPE, WordPiece, Unigram, and SentencePiece.","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"A brief introduction to Vectors \u0026amp; Verbs and formatting verification.","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"\nFeedforward network (FFN) blocks are the least talked about when in context to LLMs. But that is where more than 70% of the paramters come from. Something as simple and convinient as a couple of linear transforming matrices become a burden when models start scaling in Billions of parameters. So, getting the maximum juice of a lean structure is a challenge. Plus, mostly under represented, because the FFN sub-layer acts as the primary \u0026ldquo;knowledge\u0026rdquo; processing engine, often containing the majority of a model\u0026rsquo;s total parameters.\nStandard Position Wise Feedforward Network This is as simple as it can get, two-layered fully connected network of neurons with a single activation switch, most probably a ReLU variant (for LLMs). $$ FFN(x) = max(0, W_2(W_1(x)) + b_1)W_2 + b_2 $$ Same transformation is applied for each token in parallel, making it efficient for GPU computation. For Example, in BERT model the 512 dimensions of each token are expanded to 2048 to allow for \u0026ldquo;intricate interactions\u0026rdquo; before projecting back to 512 dimensions. This part is important, in every transformer stacking of layers is possible only because the output of every matches it\u0026rsquo;s input dimensions.\nSWiGLU Sigmoid Weighted Gated Linear Unit (SWiGLU) has become the Gold Standard for feed forward networks in LLMs like Llama 3 and PaLM. The primary reason for this choice is the Swish function (as shown in the first image) instead of ReLU or GELU are as following:\nMathematical Linearity: Swish (SILU) is defined as $x\\cdot sigmoid(\\beta x)$, $\\beta$ is a learnable parameter. It is a small monotonic function that allowes small negetive values to pass through, making it mathematically linear. While GELU ($x \\cdot \\phi(x)$) is also smooth and nonmonotonic, Swish\u0026rsquo;s curve and equations have emperically improved the model\u0026rsquo;s performance.\nComputational Efficiency: SILU gradients are simpler to compute. $x/(1+exp(-x))$ compared to the standard GELU, which is an approximation of the tanh function. In the massive scale of modern LLMs these shortcuts save significant training time.\nPerformance: Research in the PaLM and Llama papers found that Swish-based Gated Linear Units consistently outperformed GELU-based versions in terms of perplexity and downstream task accuracy. $$SwiGLU(x, W, V, b, c) = (Swish(xW + b) \\otimes (xV + c))W_2$$ Where $W$ and $V$ are weight matrices for the gate and the data path, and $\\otimes$ is element-wise multiplication.\nMore Complex Filtering: Instead of using ordinary two weight matrices in standard FFN, SWiGLU uses three weight matrices (gate, date, output). This allows it to gather more complex features out of the input without drastically increasing the number of parameters. This performance boost has resulted in modern implementations being low in dimensionality of hidden layers (2/3 of input dimensions) while retaining the performance benefits of gated activations.\nZero-Gradient Avoidance: Unlike ReLU, which has a zero gradient for all negative values (the \u0026ldquo;dying ReLU\u0026rdquo; problem), SwiGLU\u0026rsquo;s smooth curve ensures that the model can still learn from negative inputs, improving training stability in deep stacks.\nMixture of Experts (MoE) This is an entire architecture rather than a single block with activation switch. It comes with it\u0026rsquo;s own challenges. It is a \u0026ldquo;divide and conquer\u0026rdquo; strategy. It became a dominant strategy for scaling insanely large models upto a trillion parameters like DeepSeek-V3 and Mistral. It is because it breaks down the direct and obvious link between model size and its speed. It is because it breaks down the direct and obvious link between model size (information capacity) and its speed (inference cost). Sparse Activation: MoE comes with a system of Manager (called Router in this case) and Experts. The Router is responsible for determining which smaller Experts (FFNs) are best suited for specific tokens. The Experts layer is actually split in different sublayers. For any token only a tiny part of the Experts are used. DeepSeek had 256 Expert layers in a single layer while activating not more than 8 for a single token.\n","permalink":"http://localhost:1313/posts/feedforward-networks/","summary":"Exploring the hidden layers of trillion-parameter switchboards: Feedforward Neural Networks and Activation Functions.","title":"Anatomy of Trillion-Parameter Switchboards: Understanding Feedforward Blocks"},{"content":"\nWe are about to touch the holy grail of modern AI. The 2017 declaration was simple: Attention Is All You Need. But as we‚Äôve raced from GPT-1 to reasoning models, the definition of \u0026lsquo;Attention\u0026rsquo; has quietly transformed. Is the algorithm ruling the world today actually the same one we started with? Let‚Äôs look under the hood.\nüî• The Core Concept At its heart, the attention mechanism is a form of weighted aggregation. It is often described using a database retrieval analogy. Imagine you have a database of information (context).\nQuery ($Q$): What are you currently looking for? (The token being generated). Key ($K$): The indexing tag associated with each piece of information. Value ($V$): The actual information (The context token). The \u0026ldquo;Attention\u0026rdquo; is the process of matching your Query against all the Keys to determine how much \u0026ldquo;weight\u0026rdquo; to put on each Value. Attention scores are calculated in each transformer layer at every step of the sequence. This is a single unifying factor in all transformer models, from BERT to Gemini.\nüî• Standard Multi-Head Attention (MHA) The original mechanism introduced in the paper \u0026ldquo;Attention Is All You Need\u0026rdquo;.\nHere, instead of performing the Key-Query-Value (KQV) operation on the entire context only once, the attention is split into $N$ parallel heads. Each head learns a different aspect of the language (e.g., word meaning, syntax, semantic relationships, etc.). The equation is a simple culmination of dot products aided by a regularizer like Softmax.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nWhere:\n$QK^T$ computes the similarity scores between the Query and Key. $\\sqrt{d_k}$ is a scaling factor to prevent the dot product from growing too large, mitigating the vanishing/exploding gradient problem. The weighted sum is applied to the values of $V$. The Memory Bandwidth Bottleneck: \u0026gt; During inference, the model must store Key and Value matrices for all previous tokens. This is called the KV Cache. In MHA, every single head has its own unique set of Keys and Values. For large models with massive context windows, loading this massive KV cache from memory (HBM) to the computation chip (SRAM) becomes the primary bottleneck, slowing down generation significantly. Earlier models could often only generate ~8 tokens per second at inference due to this constraint.\nüî• Multi-Query Attention (MQA) This approach addresses the memory bottleneck by sharing the same set of Keys and Values across all heads while keeping separate Query heads.\nMHA: $H$ Query heads, $H$ Key heads, $H$ Value heads MQA: $H$ Query heads, 1 Key head, 1 Value head This drastically reduces the memory requirements of the KV cache by a factor of $H$ (often 8x or 16x). The GPU needs to load far less data, significantly speeding up inference. However, it does not reduce the computation (FLOPs)‚Äîso why is it faster?\nThis can be explained with the Kitchen Analogy: Imagine a kitchen (GPU) with 8 chefs (Heads).\nComputation (Math): The Chef chopping vegetables. Memory Access (Bandwidth): The Assistant running to the fridge to get ingredients. In MHA: Each chef demands unique vegetables from the fridge. The assistant runs back and forth 8 times, fetching 8 different crates. Most of the time is spent waiting for the assistant to return. This results in idle cores waiting for data to load from HBM.\nIn MQA: All chefs agree to use the same vegetables. The assistant runs back and forth only once, fetching one giant crate of potatoes and dumping it. This results in minimized memory transfer and no idle cores.\nThe Technical Reality: During text generation, the model is memory-bound, not compute-bound. The time it takes to move the KV Cache from HBM (High Bandwidth Memory) to the chip\u0026rsquo;s SRAM is the bottleneck. MQA reduces the data volume of that move by a factor of $H$.\nThe Capacity Gap: MQA is a trade-off between nuanced response and memory capacity. The model loses the ability to store different types of information, as a single $V$ matrix must now store all nuances of information flow in the language.\nThe Surgery Trick (Uptraining): Google Research figured out a way to take an existing model with MHA and convert it to MQA by averaging the weight matrices of $H$ heads. This is called Uptraining. You then train this \u0026ldquo;Frankenstein\u0026rdquo; model for ~5% of the original training steps for adjustments. This yields an MQA model that is nearly as good as the original MHA but vastly faster.\nüî• Grouped Query Attention (GQA) A \u0026ldquo;best of both worlds\u0026rdquo; solution used in modern models like Llama 2 and Llama 3.\nHow it works: GQA acts as an interpolation between MHA and MQA. Instead of having just 1 Key head (MQA) or $H$ Key heads (MHA), it divides the query heads into $G$ groups. Each group shares a single Key and Value head.\nEquation modification: If you have 8 query heads, you might create 4 groups (2 queries per group). This reduces the KV cache by 2x instead of 8x, preserving more quality than MQA while still speeding up inference.\nMHA: 8 people (heads) are working on a project. Each person has their own unique filing cabinet (KV cache). It takes forever to move 8 cabinets around. MQA: 8 people share one single filing cabinet. It\u0026rsquo;s very fast to move, but they fight over the organization and lose detail. GQA: The 8 people split into 4 teams of 2. Each team shares a cabinet. It\u0026rsquo;s a balance between speed and organizational detail. üî• Multi-Head Latent Attention (MLA) This is the most advanced mechanism for attention, used in models like DeepSeek-V3. It is a sophisticated design for massive efficiency that takes a radical approach: mathematically compressing the memory into a low-rank \u0026ldquo;latent\u0026rdquo; vector.\nKV Cache Explosion: As mentioned before, for a massive model like DeepSeek-V3 (671B parameters), storing the full Keys and Values for every token in a long conversation requires terabytes of high-speed memory (HBM), and chips spend more time waiting than churning.\nMHA (Standard) stores everything. MQA deletes redundancy but hurts quality. MLA keeps the quality of MHA but compresses the data 4x-6x smaller than even GQA.\nThe Zip File Approach: Low-Rank Joint Compression Let\u0026rsquo;s compare the math of MHA and MLA. In MHA, for every single token, we generate a unique Key ($k$) and Value ($v$) matrix for each attention head.\nThe Memory Problem: The $k$ is stored in the KV Cache. For 128 heads, for example, it calculates a massive vector of $128 \\times 128 = 16,384$ floats per token.\nFor MHA, the attention score looks like: $$\\text{Score} = q^T \\cdot k$$\nBut for MLA, instead of storing the full $k$, it projects the input down into a tiny latent vector $c_{KV}$, which is much smaller (e.g., 512 floats). $$c_{KV} = W_{DKV} \\cdot x$$\nNow here is the catch: Naive compression and decompression would mean constructing a small latent matrix for every token and deconstructing it every single time the information is required, which defeats the purpose of compression.\n$$k_{\\text{reconstructed}} = W_{UK} \\cdot c_{KV}$$ $$\\text{Score} = q^T \\cdot k_{\\text{reconstructed}}$$\nThe Matrix Absorption Trick (Optimization) Instead of reconstructing $k$ for every token, we can absorb the Up-Projection matrix ($W_{UK}$) into the Down-Projection matrix ($W_{DKV}$) and absorb the query $q$ into a new $Q$.\nFrom the original equation: $$\\text{Score} = q^T \\cdot (\\underbrace{W_{UK} \\cdot c_{KV}}_{\\text{This is } k})$$\nWe associate differently:\n$$\\text{Score} = (\\underbrace{q^T \\cdot W_{UK}}_{\\text{Absorbed Query}}) \\cdot c_{KV}$$ We change the order in which the matrix multiplication is performed.\nHow is this allowed? In linear algebra, matrix multiplication is associative:\n$$(A \\cdot B) \\cdot C = A \\cdot (B \\cdot C)$$ We can move the parentheses! Instead of grouping $(W_{UK} \\cdot c_{KV})$ to make the Key, we group $(q^T \\cdot W_{UK})$ to make a new Query.\nNote: The positional embedding information is left untouched. That rotational information from algorithms like RoPE is preserved outside the compression; it requires minimal space anyways.\n","permalink":"http://localhost:1313/posts/attention-mechanisms/","summary":"We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek\u0026rsquo;s MLA, how has the definition of \u0026lsquo;Attention\u0026rsquo; transformed?","title":"Attention Is All You Need, but exactly which one?: MHA, GQA and MLA"},{"content":"\nPositional Embeddings are the \u0026ldquo;voice\u0026rdquo; that tell Transformers where words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.\n\u0026ldquo;The dog chased the cat\u0026rdquo; and \u0026ldquo;cat chased the dog\u0026rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.\nüî• Sinusoidal Embeddings: The Odometer of Language Introduced in the seminal \u0026ldquo;Attention Is All You Need\u0026rdquo; paper, this was the industry standard for many years. This approach adds a vector to each word embedding based on its position. The vector is computed out of wavelike patterns that are periodic and smooth. Mathematically, for a position $pos$ and embedding dimension $i$:\n$$ PE_{(pos, i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $$\nThe model uses sine and cosine waves of different frequencies to capture both short and long-range dependencies. The frequency of the waves decreases as the embedding dimension increases, which helps capture long-range dependencies in the input sequence.\nThe creators wanted the model to calculate distance easily, which is why a linear function was used. The model simply rotates the current position vector using a rotation matrix to derive the next position.\n$$ \\begin{pmatrix} \\cos(k) \u0026 \\sin(k) \\\\ -\\sin(k) \u0026 \\cos(k) \\end{pmatrix} \\cdot PE(pos) \\approx PE(pos + k) $$ Low dimensions act like a fast-ticking clock (high frequency, changing rapidly) and high dimensions act like a slow clock (low frequency, changing slowly).\nThe number 10,000 is an arbitrarily chosen high number to reduce Aliasing. By choosing such a high number, the creators ensured that the wavelength of the slowest dimension is incredibly long. Consequently, the pattern practically never repeats, and each position remains unique.\nThis combination works like a mechanical odometer dial. Each subsequent dimension acts as a slower-moving gear. The model learns to read these \u0026ldquo;dials\u0026rdquo; to understand exactly where a word sits in the sequence.\nThis mitigated the challenge of exploding gradients that previous array-based positional embeddings suffered from (e.g., adding integers 1, 2, 3\u0026hellip;). This model keeps values between -1 and +1, adding stability while ensuring every position gets a distinct code.\nThe Problem: This approach pollutes the word embedding matrix (semantic information). While high-dimensional vectors are almost always orthogonal‚Äîmeaning the noise doesn\u0026rsquo;t fully destroy the word\u0026rsquo;s identity‚Äîit does not strictly guarantee purity.\nThis led to a more mature approach.\nüî• ALiBi: The Fading Streetlight Attention with Linear Biases (ALiBi) is a method where, instead of adding a vector to the embeddings at the start, the model injects positional information linearly, directly into the attention scores. If Sinusoidal Encoding asks, \u0026ldquo;Where am I on the map?\u0026rdquo;, ALiBi asks, \u0026ldquo;How much harder should I squint to see you?\u0026rdquo;\nThe biggest problem ALiBi solved is the \u0026ldquo;Invisible Wall\u0026rdquo;. If you train a Transformer on sequences of length 2048 and then feed it a book with 3000 words, it will crash or output garbage at word 2049. This is a failure of Extrapolation.\nALiBi throws away the idea of adding positional vectors entirely. Instead, it devises a \u0026ldquo;penalty\u0026rdquo; system for attention scores:\n$$ \\text{Score} = (q \\cdot k^\\top) - (m \\cdot \\text{distance}) $$\nDistance: If words A and B are neighbors, the distance is 0, so the penalty is 0. Although the distance is linear, it does not cause exploding gradients because the raw attention scores are normalized by a softmax function. This distance prioritizes nearby words (local context) over distant ones.\nSlope ($m$): If we always penalize distance equally, the model becomes myopic. ALiBi mitigates this by assigning a different slope value to each attention head:\nHead 1 (The Historian): Very low slope. The penalty grows slowly, allowing it to see far back. Head 2 (The Reader): Medium slope. Head 3 (The Myopic): Very high slope (e.g., 0.5); it effectively cannot see beyond 2-3 words. This mimics holding a lantern. Words close to you are bright, while words further away naturally fade into darkness.\nExtrapolation: If you walk 5 miles down the road (a longer sequence), your lantern works exactly the same way. The physics of light decay are constant, allowing the model to handle sequences longer than it was trained on.\nHowever, ALiBi forces a \u0026ldquo;sliding window\u0026rdquo; view where distant words eventually become invisible. This brings us to the modern standard.\nüî• RoPE: The Rotary Revolution Rotary Position Embeddings (RoPE) is the current industry standard (used in Llama 3, Mistral, PaLM) because it mathematically unifies \u0026ldquo;absolute position\u0026rdquo; with \u0026ldquo;relative position.\u0026rdquo; This method rotates the vectors in the Query and Key matrices based on their absolute position.\nIf a vector has 512 dimensions, RoPE breaks it into 256 pairs. Each pair is rotated by a different frequency:\n$$ \\theta_i = 10000^{-2i/d} $$\nThe First pair ($i = 0$) is rotated by the highest frequency. The Last pair ($i = 255$) is rotated by the lowest frequency. This spread ensures the model has \u0026ldquo;high precision\u0026rdquo; (fast rotation) to distinguish immediate neighbors, and \u0026ldquo;low precision\u0026rdquo; (slow rotation) to track global position without the pattern repeating.\nDoesn\u0026rsquo;t rotation change meaning? Yes and No. The vector for \u0026ldquo;Apple\u0026rdquo; at position 1 and \u0026ldquo;Apple\u0026rdquo; at position 256 will look completely different in terms of coordinates. However, rotation does not change the semantic strength (magnitude/norm) of the vector.\nThe Relativity Trick: Transformers compare words using dot products (angles). RoPE relies on the fact that the attention mechanism ignores the absolute rotation and only detects the difference in rotation (relative position). Thus, the \u0026ldquo;relative meaning\u0026rdquo; is encoded purely into the angle between words, leaving the semantic \u0026ldquo;magnitude\u0026rdquo; untouched.\nüî• LongRoPE: The Bifocal Lens While RoPE is powerful, it struggles when stretching context windows from 4k to 2 million tokens. The \u0026ldquo;fast\u0026rdquo; dimensions spin so rapidly over long distances that they become random noise. LongRoPE solves this using an evolutionary search algorithm to find a unique scaling factor ($\\lambda$) for each dimension.\nThe Equations Instead of rotating by the standard $\\theta_i$, LongRoPE rotates by a rescaled frequency:\n$$ \\theta\u0026rsquo;_i = \\lambda_i \\theta_i $$\nTo efficiently find these $\\lambda$ values without searching an infinite space, the algorithm enforces a monotonicity constraint based on NTK theory:\n$$ \\lambda_i \\le \\lambda_{i+1} $$\nThis ensures that low-frequency dimensions (global context) are stretched more than high-frequency dimensions (local context), creating a \u0026ldquo;bifocal\u0026rdquo; effect:\nHigh Frequencies (Local): Kept sharp ($\\lambda \\approx 1$) to maintain grammar. Low Frequencies (Global): Stretched ($\\lambda \u0026gt; 1$) to track massive distances without repeating. Not worth perusing more since the next one is much better at long context positional encoding. üî• HoPE: The Hyperbolic Slide Hyperbolic Rotary Positional Encoding (HoPE) moves from the geometry of a circle to the geometry of a hyperbola (inspired by Lorentz transformations). It was designed to fix a subtle flaw in RoPE: the \u0026ldquo;wobbly\u0026rdquo; or oscillatory nature of attention scores.\nThe Core Problem: RoPE\u0026rsquo;s \u0026ldquo;Wobble\u0026rdquo; In standard RoPE, we rotate vectors around a circle.\nThe Issue: Circles repeat. A dial at $361^\\circ$ looks identical to $1^\\circ$. Even with different frequencies, the dot product fluctuates up and down as distance increases. The Consequence: The model gets confused at long distances, potentially mistaking a word 1,000 tokens away for a neighbor just because the rotation cycles aligned, creating \u0026ldquo;noise.\u0026rdquo; The \u0026ldquo;Hyperbolic\u0026rdquo; Shift HoPE replaces trigonometric functions ($\\sin, \\cos$) with hyperbolic functions ($\\sinh, \\cosh$).\nCircular Rotation: Keeps distance from the center constant. Hyperbolic Rotation: Moves points along a hyperbola. Crucially, hyperbolic functions do not repeat; they grow or decay exponentially. By using hyperbolic rotation, HoPE guarantees that as words get further apart, the attention score decays monotonically (smoothly drops).\nThe Equations HoPE uses a hyperbolic matrix $B(\\theta, m)$ defined using the Lorentz Boost structure:\n$$ B(\\theta, m) = \\begin{pmatrix} \\cosh(m\\theta) \u0026 \\sinh(m\\theta) \\\\ \\sinh(m\\theta) \u0026 \\cosh(m\\theta) \\end{pmatrix} $$ Since hyperbolic functions grow to infinity, HoPE introduces a decay penalty ($e^{-m\\theta'}$). The final attention score simplifies beautifully to a function of relative distance: $$ \\text{Score} \\propto e^{-|m-n|(\\theta\u0026rsquo; - \\theta)} $$\nThis ensures the attention score exponentially decays as distance increases, eliminating the wobbles of standard RoPE. This the latest model that is taking up the space in the industry for extremely long context models.\n","permalink":"http://localhost:1313/posts/positional-embeddings/","summary":"From Sinusoidal to RoPE and HoPE: How Transformers learn to process word order and sequence length.","title":"The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE"},{"content":"Imagine trying to teach a computer the difference between \u0026ldquo;Apple\u0026rdquo; the fruit and \u0026ldquo;Apple\u0026rdquo; the company. To us, the distinction is intuitive. To a machine, it‚Äôs just a string of characters. How do we turn these strings into meaningful math? While early attempts like Word2Vec gave us a great start, they missed the forest for the trees‚Äîor in some cases, the twigs for the branches. Enter GloVe and FastText: two algorithms that revolutionized how machines understand the nuances of human language.\nPrevious static embedding models like Word2Vec successfully captured the local semantics of words, but they failed in addressing contexts between words that might not always appear in the same context window. Furthermore, they treated words as indivisible atomic units in continuous vector spaces, missing out on the internal structure of language.\nThe two simple but effective ideas behind GloVe and FastText are subword units and global co-occurrence. Let\u0026rsquo;s dive into them.\nGloVe: A Global Mapping Strategy Global Vectors for Word Representation (GloVe) was created at Stanford. The core idea was to train a model where word vectors are defined by how often they appear around other words across the entire corpus, rather than just locally.\nTraining Steps: Co-occurrence Matrix X: GloVe scans the entire corpus and creates a giant matrix (spreadsheet) recording the counts of how often words appear near each other. Factorization: It compresses this giant matrix into smaller, dense vectors. Objective: The function tries to equate the dot product of two word vectors with the logarithm of their probability of appearing together. The error between these values is then backpropagated to update the vectors. $$w_i \\cdot w_j + b_i + b_j = \\log(X_{ij})$$\n$w$: The word vector (what we want). $X_{ij}$: The count from our matrix. The Loss Function: We can\u0026rsquo;t satisfy the equation above perfectly for every word pair. So, we use a weighting function to minimize the squared error:\n$$J = \\sum_{i,j=1}^V f(X_{ij}) ({w_i}^T w_j + b_i + b_j - \\log(X_{ij}))^2$$\nIf we do not use the weighting function, the loss would be infinite for rare words (since $\\log(0) = \\infty$) and frequent words (like \u0026rsquo;the\u0026rsquo;) would overpower the model.\n$$ f(x) = \\begin{cases} (x/x_{max})^\\alpha \u0026amp; \\text{if } x \u0026lt; x_{max} \\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$\n(Typically $x_{max}=100$ and $\\alpha=0.75$)\nFastText: The Lego Bricks Strategy FastText was created by Facebook AI Research (FAIR). The idea was to treat words not as solid rocks, but as structures made of \u0026ldquo;Lego bricks\u0026rdquo; (subword units).\nWord2Vec struggled to create embeddings for words it hadn\u0026rsquo;t seen before (Out-Of-Vocabulary or OOV words). FastText solves this by breaking words down.\nGloVe treats a word like \u0026ldquo;Apple\u0026rdquo; as a single, atomic unit. FastText breaks \u0026ldquo;Apple\u0026rdquo; into character n-grams: \u0026lt;ap, app, ppl, ple, le\u0026gt; For a complex or new word like \u0026ldquo;Applesauce\u0026rdquo;, the model might not know the whole word, but it recognizes the Lego brick for \u0026ldquo;Apple\u0026rdquo; and the remaining bits from other subwords.\nFastText takes its inspiration from the SkipGram architecture. The vector for a word is simply the sum of the vectors of its n-gram subwords:\n$$v_w = \\sum_{g \\in G_w} z_g$$\nObjective: The objective is to predict context words given a target word.\n$$P(w_c | w_t) = \\frac{e^{s(v_{w_t} \\cdot v_{w_c})}}{\\sum_{w \\in V} e^{s(v_{w_t} \\cdot v_w)}}$$\nAnd the scoring function is defined as:\n$$s(w_t, w_c) = u_{w_t}^T (\\sum_{g \\in G_w} z_g)$$\n$u_{w_c}$: The vector for the context word (output vector). $z_g$: The vector for the n-gram $g$ (input vector). Modifications to Softmax: Calculating the full softmax for every word in the vocabulary is computationally expensive. FastText uses two methods to approximate this efficiently.\nOption A: Negative Sampling\nWe pick a few random \u0026ldquo;wrong\u0026rdquo; words every time to serve as \u0026ldquo;negative samples\u0026rdquo; and calculate the loss only for the correct word and these noise words.\n$$J = - \\log \\sigma(s(w_t, w_c)) - \\sum\\limits_{i=1}^{N} \\mathbb{E}_{n_i \\sim P_n(w)} [\\log \\sigma(-s(w_t, n_i))]$$\nThe sigmoid function $\\sigma$ squashes the score between 0 and 1. The first term pushes the probability of the correct word towards 1. The second term pushes the probability of the noise words towards 0. Option B: Hierarchical Softmax (Faster for infrequent words)\nInstead of a flat list of words, imagine the vocabulary arranged as a binary tree (Huffman Tree).\nRoot: Top of the tree. Leaves: The actual words. To calculate the probability of a word, we trace a path from the root to that leaf. At every branching node, we calculate the probability (sigmoid) of going left or right. The probability of a word $w$ is the product of the probabilities of the turns taken to reach it. Code Example from gensim.models import FastText from gensim.test.utils import common_texts # Initialize the FastText model with specific hyperparameters model = FastText( vector_size=100, # Dimensionality of the word vectors window=5, # Maximum distance between the current and predicted word within a sentence min_count=1, # Ignores all words with total frequency lower than this sg=1, # Training algorithm: 1 for skip-gram; 0 for CBOW hs=0, # If 0, and negative is non-zero, negative sampling will be used negative=5, # Number of \u0026#34;noise words\u0026#34; to be drawn for negative sampling workers=4, # Number of worker threads to train the model epochs=10, # Number of iterations over the corpus min_n=3, # Minimum length of character n-grams max_n=6 # Maximum length of character n-grams ) # Build the vocabulary from the provided text corpus model.build_vocab(common_texts) # Train the model on the corpus model.train( common_texts, total_examples=model.corpus_count, epochs=model.epochs ) # Calculate and print the cosine similarity between two words in the vocabulary print(f\u0026#34;Similarity between \u0026#39;computer\u0026#39; and \u0026#39;human\u0026#39;: {model.wv.similarity(\u0026#39;computer\u0026#39;, \u0026#39;human\u0026#39;)}\u0026#34;) # Demonstrate FastText\u0026#39;s ability to handle Out-Of-Vocabulary (OOV) words # Even if \u0026#39;computation\u0026#39; wasn\u0026#39;t in the training data, FastText constructs a vector using character n-grams oov_vector = model.wv[\u0026#39;computation\u0026#39;] # Check if a specific word exists in the model\u0026#39;s fixed vocabulary index word = \u0026#34;computer\u0026#34; is_in_vocab = word in model.wv.key_to_index print(f\u0026#34;\\nIs the word \u0026#39;{word}\u0026#39; in the model\u0026#39;s vocabulary? {is_in_vocab}\u0026#34;) ","permalink":"http://localhost:1313/posts/glove-and-fasttext/","summary":"How subword units and global co-occurrence matrices allow GloVe and FastText to capture nuances that Word2Vec missed.","title":"The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText"},{"content":" Before we had Large Language Models writing poetry, we had to teach computers that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; are related not just by spelling, but by meaning. This is the story of that breakthrough. It‚Äôs the moment we stopped counting words and started mapping their souls‚Äîturning raw text into a mathematical landscape where math can solve analogies. Welcome to the world of Word2Vec. üîÆ\nLanguage models require vector representations of words to capture semantic relationships. Before the 2010s, models used word count-based vector representations that captured only the frequency of words (e.g., One-Hot Encoding). The Problems: üöß\nThe Curse of Dimensionality Lack of meaning (Synonyms were treated as mathematically unrelated). In 2011, Mikolov et al. at Google introduced Word2Vec, which used a shallow neural network to learn vector representations of words. The best part? It could create much denser relationships between words than prior models, and it was unsupervised.\nThe training (prediction) is essentially \u0026ldquo;fake\u0026rdquo;‚Äîa pretext task. But the weight matrices we get on the side are a gold mine of fine-grained semantic relationships. ‚õèÔ∏è\nThere are two popular variants of Word2Vec:\nSkipGram Continuous Bag Of Words (CBOW) 1. Continuous Bag of Words (CBOW) üß© CBOW is a simple \u0026lsquo;fill in the blanks\u0026rsquo; machine. It takes the surrounding words as inputs and tries to predict the center word.\n\u0026ldquo;Continuous\u0026rdquo;: It operates in continuous vector space (unlike the discrete space of n-gram models).\n\u0026ldquo;Bag of words\u0026rdquo;: The order of the context does not matter. \u0026ldquo;The cat sat on the mat\u0026rdquo; and \u0026ldquo;The mat sat on the cat\u0026rdquo; are the same for CBOW. They produce exactly the same predictions.\nObjective: Maximize the probability of the target word $w_t$ given its context $w_{t-C}, \\dots, w_{t+C}$.\n2. Training Defining Dimensions:\n$V$: Vocabulary size (number of unique words in the corpus, e.g., 10,000) $N$: Dimension of the embedding space (e.g., 100) $C$: Number of context words (e.g., 2 words on left of target and 2 words on right) Step 1: Input Lookup \u0026amp; One-Hot Encoding\nMathematically, we represent each context word as a one-hot vector $x^{(c)}$. We have an Input Matrix $W_{in}$ of size $V \\times N$. For each context word (e.g., \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;), we select its corresponding row from $W_{in}$.\n$$ v_c = W_{in}^T x^{(c)} $$\nWhere $v_c$ is the $N$-dimensional vector for context word $c$.\nStep 2: Projection (The \u0026ldquo;Mixing\u0026rdquo;)\nWe combine the context vectors. In standard CBOW, we simply average them. This creates the hidden layer vector $h$.\n$$ h = \\frac{1}{2C} \\sum_{c=1}^{2C} v_c $$\n$h$ is a single vector of size $N \\times 1$. Note: This operation is linear and contains no activation function (like ReLU or Sigmoid).\nStep 3: Output Scoring (The \u0026ldquo;Dot Product\u0026rdquo;)\nWe have a second matrix, the Output Matrix $W_{out}$ of size $N \\times V$. We compute the raw score (logit) $u_j$ for every word $j$ in the vocabulary by taking the dot product of the hidden vector $h$ and the output vector $v\u0026rsquo;_j$.\n$$ u_j = {v\u0026rsquo;_j}^T h $$\nOr in matrix form:\n$$ u = W_{out}^T h $$\nResult $u$ is a vector of size $V \\times 1$.\nStep 4: Probability Conversion (Softmax)\nWe convert the raw scores $u$ into probabilities using the Softmax function. This tells us the probability that word $w_j$ is the center word.\n$$ y_j = P(w_j | \\text{context}) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 5: Loss Calculation (Cross-Entropy)\nWe compare the predicted distribution $y$ with the actual ground truth $t$ (a one-hot vector where the index of the true word is 1). The loss function $E$ is:\n$$ E = - \\sum_{j=1}^V t_j \\log(y_j) $$\nSince $t_j$ is 0 for all words except the target word (let\u0026rsquo;s call the target index $j^*$), this simplifies to:\n$$ E = - \\log(y_{j^*}) $$\n3. Backpropagation Details We need to update the weights to minimize $E$. We use the Chain Rule.\nA. Gradient w.r.t. Output Matrix ($W_{out}$)\nWe want to know how the error changes with respect to the raw score $u_j$.\n$$ \\frac{\\partial E}{\\partial u_j} = y_j - t_j $$\nLet\u0026rsquo;s call this error term $e_j$.\nIf $j$ is the target word: $e_j = y_j - 1$ (Result is negative; we boost the score). If $j$ is not target: $e_j = y_j - 0$ (Result is positive; we suppress the score). The update rule for the output vector $v\u0026rsquo;_j$ becomes:\n$$ v'_{j}(\\text{new}) = v'_{j}(\\text{old}) - \\eta \\cdot e_j \\cdot h $$ ($\\eta$ is the learning rate)\nB. Gradient w.r.t. Hidden Layer ($h$)\nWe backpropagate the error from the output layer to the hidden layer.\n$$ EH = \\sum_{j=1}^V e_j \\cdot v\u0026rsquo;_j $$\n$EH$ is an $N$-dimensional vector representing the aggregate error passed back to the projection layer.\nC. Gradient w.r.t. Input Matrix ($W_{in}$)\nSince $h$ was just an average of the input vectors, the error $EH$ is distributed to each context word\u0026rsquo;s input vector. For every word $w_c$ in the context:\n$$ v_{w_c}(\\text{new}) = v_{w_c}(\\text{old}) - \\eta \\cdot \\frac{1}{2C} \\cdot EH $$\nLet\u0026rsquo;s build a single pass through the network for a given context.\nimport torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # Define a simple Continuous Bag of Words (CBOW) style model class CBOWModel(nn.Module): def __init__(self, vocab_size, embedding_dim): super(CBOWModel, self).__init__() # The embedding layer stores the word vectors we want to learn self.embeddings = nn.Embedding(vocab_size, embedding_dim) # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word self.linear = nn.Linear(embedding_dim, vocab_size) def forward(self, inputs): # inputs: tensor of word indices for the context embeds = self.embeddings(inputs) # Aggregate context by calculating the mean of the word embeddings h = torch.mean(embeds, dim=1) # Produce logits (raw scores) for each word in the vocabulary logits = self.linear(h) return logits # Setup vocabulary and mappings word_to_ix = {\u0026#34;the\u0026#34;: 0, \u0026#34;quick\u0026#34;: 1, \u0026#34;brown\u0026#34;: 2, \u0026#34;fox\u0026#34;: 3, \u0026#34;jumps\u0026#34;: 4, \u0026#34;over\u0026#34;: 5, \u0026#34;lazy\u0026#34;: 6, \u0026#34;dog\u0026#34;: 7} ix_to_word = {v: k for k, v in word_to_ix.items()} # Configuration constants EMBEDDING_DIM = 5 VOCAB_SIZE = len(word_to_ix) LEARNING_RATE = 0.01 # Initialize the model, loss function, and optimizer model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM) loss_function = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE) # Prepare dummy training data: context words and the target word # Context: [\u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;over\u0026#34;] -\u0026gt; Target: \u0026#34;fox\u0026#34; context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long) target_idxs = torch.tensor([3], dtype=torch.long) # Perform a single optimization step model.zero_grad() # Forward pass: we reshape context to (1, -1) to simulate a batch of size 1 logits = model(context_idxs.view(1, -1)) # Calculate loss against the target word index loss = loss_function(logits, target_idxs) # Backpropagate and update weights loss.backward() optimizer.step() # Output results for the blog post print(\u0026#34;=== Model Training Snapshot ===\u0026#34;) print(f\u0026#34;Calculated Loss: {loss.item():.6f}\u0026#34;) print(f\u0026#34;\\n=== Learned Vector for \u0026#39;jumps\u0026#39; ===\u0026#34;) word_vec = model.embeddings(torch.tensor([word_to_ix[\u0026#39;jumps\u0026#39;]])) print(word_vec.detach().numpy()) print(\u0026#34;\\n=== Embedding Matrix (Weights) ===\u0026#34;) print(model.embeddings.weight.detach().numpy()) print(\u0026#34;\\n=== Linear Layer Weights ===\u0026#34;) print(model.linear.weight.detach().numpy()) Output shows single pass loss and learned vector for \u0026lsquo;jumps\u0026rsquo; word. This is a single training sample result but after multiple such passes both Embedding and linear weights should be very similar.\n=== Model Training Snapshot === Calculated Loss: 1.989656 === Learned Vector for \u0026#39;jumps\u0026#39; === [[-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775]] === Embedding Matrix (Weights) === [[ 1.66039 -0.11371879 0.6246518 0.35860053 -1.2504417 ] [-2.1847186 -0.77199775 -0.17050214 -0.38411248 -0.03913084] [ 0.11852697 0.90073633 0.8847807 0.7404524 0.900149 ] [ 0.07440972 -0.40259898 2.6246994 -0.08851447 0.02660969] [-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775] [-0.9245572 -0.5545908 0.9083091 -1.0755049 0.84047747] [-1.0237687 0.59466314 0.05621134 -0.6202532 1.3664424 ] [ 0.60998917 -1.0549186 1.6103884 0.8724912 -1.2486908 ]] === Linear Layer Weights === [[ 0.02165058 -0.28883642 0.14545658 -0.3442509 0.32704315] [-0.18731792 0.28583744 0.22635977 0.13245736 0.29019794] [ 0.3158916 -0.15826383 -0.03203773 0.16377363 -0.41457543] [-0.05080034 0.4180087 0.11228557 0.30218413 0.3025514 ] [-0.38419306 0.24475925 -0.39210224 -0.38660625 -0.2673145 ] [-0.32321444 0.12200444 -0.03569533 0.2891424 -0.07345333] [-0.33704326 0.2521956 0.31587374 0.22590035 0.29866052] [ 0.4117266 -0.44231793 0.24064957 -0.29684234 0.333821 ]] Interesting Facts About CBOW üß† The \u0026ldquo;Averaging Problem\u0026rdquo;: This is a common issue in CBOW models. The averaging and training is order-agnostic. All variants of word order in the context window will generate the same output. \u0026ldquo;Dog bit the man\u0026rdquo; and \u0026ldquo;Man bit the dog\u0026rdquo; are the same in CBOW\u0026rsquo;s eyes.\nSmoothing Effect: CBOW models are much faster to train than Skip-Gram but are generally smoother.\nRare Words: CBOW models struggle slightly with rare words because the presence of surrounding common words (like articles and prepositions) can skew the embeddings and dissolve their uniqueness.\nComputational Cost: Softmax requires summing over the entire vocabulary size, which can be computationally expensive if $V$ is very large. Solutions include Hierarchical Softmax and Negative Sampling (approximating the denominator by only checking the target word vs. 5 random noise words).\nVectors as Double Agents: Every word has a different representation in the embedding matrix and the output matrix. Most of the time $W_{out}$ is discarded, but some consider averaging both $W_{in}$ and $W_{out}$ for slightly better performance.\nLinear Relationships: The famous analogy King - Man + Woman = Queen emerges from the model because of the linear relationships between the words.\nInitialization: This is a no-brainer but worth mentioning. Initialization to zero vectors would mean no learning. We must initialize randomly.\n2. Skip-Gram (The \u0026ldquo;Burger Flip\u0026rdquo; of CBOW) üîÑ Skip-Gram flips the CBOW logic. While CBOW tries to predict the center word given the context, Skip-Gram tries to predict the scattered context given the center word.\nCore Concept: If a word like \u0026ldquo;monarch\u0026rdquo; can accurately predict words like \u0026ldquo;king\u0026rdquo;, \u0026ldquo;throne\u0026rdquo;, \u0026ldquo;crown\u0026rdquo;, \u0026ldquo;royalty\u0026rdquo;, and \u0026ldquo;power\u0026rdquo;, then it must effectively capture the concept of royalty.\nTraining Let\u0026rsquo;s define dimensions:\n$V$: Vocabulary size $D$: Embedding dimension $C$: Number of context words Step 1: Input Lookup\nUnlike CBOW, our input is just one word vector (the center word $w_t$). We grab the vector $v_c$ from the Input Matrix $W_{in}$.\n$$ h = v_{w_t} $$\nNote: There is no averaging here. The hidden layer $h$ is simply the raw vector of the center word.\nStep 2: Output Scoring (The Broadcast)\nThe model takes this single vector $h$ and compares it against the Output Matrix $W_{out}$ (the entire vocabulary). It computes a score $u_j$ for every word $j$ in the dictionary.\n$$ u = W_{out}^T h $$\nResult $u$: A vector of size $V \\times 1$ containing raw scores.\nStep 3: Probability Conversion (Softmax)\nWe apply Softmax to turn scores into probabilities.\n$$ P(w_j | w_t) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 4: The Multi-Target Loss\nHere is the major difference from CBOW. In CBOW, we had one target (the center). In Skip-Gram, we have $2C$ targets (all the surrounding words). For a sentence \u0026ldquo;The cat sat on mat\u0026rdquo; (Center: \u0026ldquo;sat\u0026rdquo;), the targets are \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;. We want to maximize the probability of all these true context words. The Loss function ($E$) sums the error for each context word:\n$$ E = - \\sum_{w_{ctx} \\in \\text{Window}} \\log P(w_{ctx} | w_t) $$\nStep 5: Backpropagation (Accumulating Gradients)\nSince one input word is responsible for predicting multiple output words, the error signals from all those context words add up.\nAt the Output Layer:\nIf \u0026ldquo;cat\u0026rdquo; was a target, the \u0026ldquo;cat\u0026rdquo; vector in $W_{out}$ gets a signal: \u0026ldquo;Move closer to \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; If \u0026ldquo;apple\u0026rdquo; was NOT a target, the \u0026ldquo;apple\u0026rdquo; vector gets a signal: \u0026ldquo;Move away from \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; At the Hidden Layer ($h$):\nThe center word \u0026ldquo;sat\u0026rdquo; receives feedback from all its neighbors simultaneously. Error signal = (Error from \u0026ldquo;The\u0026rdquo;) + (Error from \u0026ldquo;cat\u0026rdquo;) + (Error from \u0026ldquo;on\u0026rdquo;)\u0026hellip; The vector for \u0026ldquo;sat\u0026rdquo; in $W_{in}$ moves in the average direction of all its neighbors. 3. The \u0026ldquo;Computational Nightmare\u0026rdquo; \u0026amp; Negative Sampling The equations above describe Naive Softmax.\nThe Problem: If $V = 100,000$, computing the denominator $\\sum \\exp(u_k)$ takes 100,000 operations. Doing this for every word in a training set of billions of words is impossibly slow.\nThe Solution: Negative Sampling Instead of updating the entire vocabulary (1 correct word vs 99,999 wrong ones), we approximate the problem.\nPositive Pair: (sat, cat) $\\rightarrow$ Maximize probability (Label 1). Negative Pairs: We pick $K$ (e.g., 5) random words the model didn\u0026rsquo;t see, e.g., (sat, bulldozer), (sat, quantum). $\\rightarrow$ Minimize probability (Label 0). New Equation (Sigmoid instead of Softmax):\n$$ E = - \\log \\sigma({v'_{\\text{pos}}}^T v_{\\text{in}}) - \\sum_{k=1}^K \\log \\sigma(-{v'_{\\text{neg}_{k}}}^T v_{\\text{in}}) $$ Effect: We typically only update 6 vectors per step (1 pos + 5 negs) instead of 100,000.\nLet\u0026rsquo;s see this in code:\nimport torch import torch.nn as nn import torch.optim as optim class SkipGramNegativeSampling(nn.Module): \u0026#34;\u0026#34;\u0026#34; Skip-Gram with Negative Sampling (SGNS) implementation. SGNS approximates the softmax over the entire vocabulary by instead distinguishing between a real context word (positive) and K noise words (negative). \u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embedding_dim): super(SkipGramNegativeSampling, self).__init__() # Input embeddings: used when the word is the center word self.in_embed = nn.Embedding(vocab_size, embedding_dim) # Output embeddings: used when the word is a context or negative sample self.out_embed = nn.Embedding(vocab_size, embedding_dim) # Initialize weights with small values to prevent gradient saturation initrange = 0.5 / embedding_dim self.in_embed.weight.data.uniform_(-initrange, initrange) self.out_embed.weight.data.uniform_(-initrange, initrange) def forward(self, center_words, target_words, negative_words): \u0026#34;\u0026#34;\u0026#34; Computes the negative sampling loss. Args: center_words: (batch_size) target_words: (batch_size) negative_words: (batch_size, K) where K is number of negative samples \u0026#34;\u0026#34;\u0026#34; # Retrieve vectors v_c = self.in_embed(center_words) # (batch_size, embed_dim) u_o = self.out_embed(target_words) # (batch_size, embed_dim) u_n = self.out_embed(negative_words) # (batch_size, K, embed_dim) # 1. Positive Score: log(sigmoid(v_c ¬∑ u_o)) # Compute dot product: (batch, 1, dim) @ (batch, dim, 1) -\u0026gt; (batch, 1) pos_score = torch.bmm(u_o.unsqueeze(1), v_c.unsqueeze(2)).squeeze(2) pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-7) # 2. Negative Score: sum(log(sigmoid(-v_c ¬∑ u_n))) # Compute dot products for all K samples: (batch, K, dim) @ (batch, dim, 1) -\u0026gt; (batch, K) neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2) neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-7), dim=1, keepdim=True) # Total loss is the negative of the objective function loss = -(pos_loss + neg_loss) return torch.mean(loss) # --- Configuration \u0026amp; Mock Data --- VOCAB_SIZE = 100 EMBED_DIM = 10 word_to_ix = {\u0026#39;fox\u0026#39;: 0} # Example vocabulary mapping model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock inputs: 1 center word, 1 target word, 5 negative samples center_id = torch.tensor([0], dtype=torch.long) target_id = torch.tensor([1], dtype=torch.long) negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long) # Training Step model.zero_grad() loss = model(center_id, target_id, negative_ids) loss.backward() optimizer.step() # Output Results print(f\u0026#34;Loss after one step: {loss.item():.6f}\u0026#34;) word_vec = model.in_embed(torch.tensor([word_to_ix[\u0026#39;fox\u0026#39;]])) print(f\u0026#34;Vector for \u0026#39;fox\u0026#39;:\\n{word_vec.detach().numpy()}\u0026#34;) The outputs:\nLoss after one step: 4.158468 Vector for \u0026#39;fox\u0026#39;: [[ 0.04197385 0.02400733 -0.03800093 0.01672485 -0.03872231 -0.0061478 0.0121122 0.04057864 -0.036255 0.03861175]] Interesting Facts About Skip-Gram üí° Makes Rare Words Shine: Skip-Gram is more effective at capturing the meaning of rare words because it focuses on the context of the center word. In CBOW, a rare word\u0026rsquo;s vector is averaged with others, diluting its signal. In Skip-Gram, when the rare word appears as the center, it gets the full, undivided attention of the backpropagation update.\nSlow Training: This method creates more training samples. A window of size 2 creates 4 training pairs per center word, while CBOW creates only 1 training pair per center word.\nSemantic Focus: Skip-Gram puts more emphasis on the semantic relationship between words. It views the context as the primary signal, with the center word as the target. This makes it better at capturing the meaning of words in context. Skip-Gram tends to capture semantic relationships (King/Queen) slightly better, while CBOW captures syntactic ones (walk/walked) slightly better.\n","permalink":"http://localhost:1313/posts/word-embeddings/","summary":"Understanding the mathematics behind Word2Vec, CBOW, and Skip-Gram and how they map language to vector space.","title":"Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n5. Code Examples: We\u0026rsquo;ll use sentencepiece for tokenization and compare results for different models.\nimport sentencepiece as spm import os # Configuration for training the SentencePiece model # SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words. options = { # The source text file used for learning the vocabulary \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # The base name for the generated model (.model) and vocabulary (.vocab) files \u0026#39;model_prefix\u0026#39;: \u0026#39;bpe_model\u0026#39;, # Number of unique tokens in the final vocabulary \u0026#39;vocab_size\u0026#39;: 4000, # \u0026#39;bpe\u0026#39; (Byte Pair Encoding) merges frequent pairs of characters/sequences \u0026#39;model_type\u0026#39;: \u0026#39;bpe\u0026#39;, # Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets \u0026#39;character_coverage\u0026#39;: 0.9995, # When enabled, unknown characters are decomposed into UTF-8 bytes to avoid \u0026#39;unk\u0026#39; tokens \u0026#39;byte_fallback\u0026#39;: True, # Treats digits individually (0-9), preventing large numbers from being treated as single tokens \u0026#39;split_digits\u0026#39;: True, # Prevents adding a whitespace prefix to the first token; useful for fine-grained control \u0026#39;add_dummy_prefix\u0026#39;: False } try: print(\u0026#34;Starting the training process...\u0026#34;) # SentencePieceTrainer.train takes the dictionary of options to build the BPE model spm.SentencePieceTrainer.train(**options) print(\u0026#34;Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created.\u0026#34;) # Initialize the processor and load the newly trained model sp = spm.SentencePieceProcessor() sp.load(\u0026#39;bpe_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Model Metadata:\u0026#34;) # Retrieve the total number of tokens in the vocabulary print(f\u0026#34;Total Vocab Size: {sp.get_piece_size()}\u0026#34;) # Special tokens are used for sequence boundaries and handling unknown characters print(f\u0026#34;BOS (Beginning of Sentence) ID: {sp.bos_id()}\u0026#34;) print(f\u0026#34;EOS (End of Sentence) ID: {sp.eos_id()}\u0026#34;) print(f\u0026#34;UNK (Unknown) ID: {sp.unk_id()}\u0026#34;) print(f\u0026#34;PAD (Padding) ID: {sp.pad_id()}\u0026#34;) # Test the tokenizer on sample strings to see how it breaks down text test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: shows the actual subword units (tokens) print(f\u0026#34;Subword Tokens: {sp.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: shows the numerical mapping for each token print(f\u0026#34;Numerical IDs: {sp.encode_as_ids(text)}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred during training or processing: {e}\u0026#34;) You can see the following output:\nStarting training... Starting the training process... Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created. ------------------------------ Model Metadata: Total Vocab Size: 4000 BOS (Beginning of Sentence) ID: 1 EOS (End of Sentence) ID: 2 UNK (Unknown) ID: 0 PAD (Padding) ID: -1 --- Tokenization Test --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;el\u0026#39;, \u0026#39;lo\u0026#39;, \u0026#39;‚ñÅW\u0026#39;, \u0026#39;orld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 380, 309, 476, 619, 36, 3924, 3964, 3959, 3993, 3978, 3983, 57, 3975, 3976, 60, 3974] --- Tokenization Test --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅmo\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;inst\u0026#39;, \u0026#39;ag\u0026#39;, \u0026#39;ram\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅever\u0026#39;] Numerical IDs: [1894, 304, 3284, 376, 321, 313, 312, 1008, 3313, 1325, 428, 3937, 341, 304, 3284, 1495] The ! is fallen back to UTF bytes. and the word instagrammable is split into multiple subword tokens. Also, original text can be recreated right back by simply concatenating the tokens. And replacing underscore by whitespace. Now we can try Unigram model. and see the same sentences tokenized differently.\nfrom tokenizers import Tokenizer from tokenizers.models import WordPiece from tokenizers.trainers import WordPieceTrainer from tokenizers.pre_tokenizers import Whitespace try: # 1. Initialize the WordPiece Tokenizer # We specify the [UNK] token for handling words not found in the vocabulary. tokenizer = Tokenizer(WordPiece(unk_token=\u0026#34;[UNK]\u0026#34;)) # 2. Configure Pre-tokenization # Before the subword algorithm runs, we need to split the raw text into words. # Whitespace splitting is the standard first step for most English NLP tasks. tokenizer.pre_tokenizer = Whitespace() # 3. Initialize the Trainer # We define our target vocabulary size and the special tokens required for # downstream tasks (like BERT\u0026#39;s [CLS] for classification or [SEP] for separators). trainer = WordPieceTrainer( vocab_size=4000, special_tokens=[\u0026#34;[UNK]\u0026#34;, \u0026#34;[CLS]\u0026#34;, \u0026#34;[SEP]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[MASK]\u0026#34;] ) # 4. Train the Model # The tokenizer scans the training file to build a vocabulary of the most # frequent subword units. tokenizer.train(files=[\u0026#34;train_data.txt\u0026#34;], trainer=trainer) # 5. Persist the Model # Save the configuration and vocabulary to a JSON file for future inference. tokenizer.save(\u0026#34;wordpiece.json\u0026#34;) print(\u0026#34;Training complete. \u0026#39;wordpiece.json\u0026#39; created.\u0026#34;) # 6. Metadata Inspection print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;WordPiece Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {tokenizer.get_vocab_size()}\u0026#34;) # 7. Testing Subword Tokenization # WordPiece shines at handling rare words by breaking them into meaningful chunks. test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (WordPiece) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # Encode converts raw text into a Tokenizer object containing tokens and IDs output = tokenizer.encode(text) # \u0026#39;tokens\u0026#39; shows the subword breakdown (e.g., \u0026#39;un\u0026#39;, \u0026#39;##insta\u0026#39;, etc.) print(f\u0026#34;Subword Tokens: {output.tokens}\u0026#34;) # \u0026#39;ids\u0026#39; are the numerical indices mapped to the vocabulary print(f\u0026#34;Numerical IDs: {output.ids}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred with WordPiece model: {e}\u0026#34;) You can see the following output:\nWordPiece Model Metadata: Total Vocab Size: 2609 --- Tokenization Test (WordPiece) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;H\u0026#39;, \u0026#39;##el\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;##or\u0026#39;, \u0026#39;##ld\u0026#39;, \u0026#39;[UNK]\u0026#39;, \u0026#39;[UNK]\u0026#39;] Numerical IDs: [37, 180, 214, 52, 162, 418, 0, 0] --- Tokenization Test (WordPiece) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;##os\u0026#39;, \u0026#39;##t\u0026#39;, \u0026#39;un\u0026#39;, \u0026#39;##ins\u0026#39;, \u0026#39;##ta\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;##ra\u0026#39;, \u0026#39;##m\u0026#39;, \u0026#39;##ma\u0026#39;, \u0026#39;##ble\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;ever\u0026#39;] Numerical IDs: [691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240] We can see the special character ## in the output. We can also see the breakdowns not having any semantic meaning. They are likelihood based. We can try the Unigram model now.\n# \u0026#39;Unigram\u0026#39; is the default and usually recommended over BPE in SentencePiece. options_unigram = { \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # Path to the raw text file for training \u0026#39;model_prefix\u0026#39;: \u0026#39;unigram_model\u0026#39;, # Prefix for the output .model and .vocab files \u0026#39;vocab_size\u0026#39;: 1200, # Desired size of the final vocabulary \u0026#39;model_type\u0026#39;: \u0026#39;unigram\u0026#39;, # Specifies the Unigram language model algorithm \u0026#39;character_coverage\u0026#39;: 0.9995, # Percentage of characters covered by the model (0.9995 is standard for Latin scripts) \u0026#39;byte_fallback\u0026#39;: True, # Enables mapping unknown characters to UTF-8 bytes to avoid \u0026lt;unk\u0026gt; tokens \u0026#39;split_digits\u0026#39;: True, # Treats each digit as an individual token (useful for numerical data) \u0026#39;add_dummy_prefix\u0026#39;: False # Prevents adding a leading space (SentencePiece default is True) } try: # 1. Train the SentencePiece model using the defined options print(\u0026#34;Starting Unigram training...\u0026#34;) spm.SentencePieceTrainer.train(**options_unigram) print(\u0026#34;Training complete. \u0026#39;unigram_model.model\u0026#39; created.\u0026#34;) # 2. Load the trained model into a processor instance for inference sp_unigram = spm.SentencePieceProcessor() sp_unigram.load(\u0026#39;unigram_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Unigram Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {sp_unigram.get_piece_size()}\u0026#34;) # 3. Define test cases to evaluate how the model handles common and rare words test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] # 4. Iterate through test sentences to visualize subword segmentation for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (Unigram) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: Converts text into subword strings (visual representation) print(f\u0026#34;Subword Tokens: {sp_unigram.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: Converts text into numerical indices for model input print(f\u0026#34;Numerical IDs: {sp_unigram.encode_as_ids(text)}\u0026#34;) except Exception as e: # Handle potential errors during training or loading (e.g., missing input file) print(f\u0026#34;An error occurred with Unigram model: {e}\u0026#34;) The output is as follows:\nUnigram Model Metadata: Total Vocab Size: 1200 --- Tokenization Test (Unigram) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ll\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;ld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311] --- Tokenization Test (Unigram) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;T\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅm\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;sta\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;ra\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ver\u0026#39;] ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"A comprehensive guide to tokenization strategies: BPE, WordPiece, Unigram, and SentencePiece.","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"A brief introduction to Vectors \u0026amp; Verbs and formatting verification.","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"\nFeedforward network (FFN) blocks are the least talked about when in context to LLMs. But that is where more than 70% of the paramters come from. Something as simple and convinient as a couple of linear transforming matrices become a burden when models start scaling in Billions of parameters. So, getting the maximum juice of a lean structure is a challenge. Plus, mostly under represented, because the FFN sub-layer acts as the primary \u0026ldquo;knowledge\u0026rdquo; processing engine, often containing the majority of a model\u0026rsquo;s total parameters.\nStandard Position Wise Feedforward Network This is as simple as it can get, two-layered fully connected network of neurons with a single activation switch, most probably a ReLU variant (for LLMs). $$ FFN(x) = max(0, W_2(W_1(x)) + b_1)W_2 + b_2 $$ Same transformation is applied for each token in parallel, making it efficient for GPU computation. For Example, in BERT model the 512 dimensions of each token are expanded to 2048 to allow for \u0026ldquo;intricate interactions\u0026rdquo; before projecting back to 512 dimensions. This part is important, in every transformer stacking of layers is possible only because the output of every matches it\u0026rsquo;s input dimensions.\nSWiGLU Sigmoid Weighted Gated Linear Unit (SWiGLU) has become the Gold Standard for feed forward networks in LLMs like Llama 3 and PaLM. The primary reason for this choice is the Swish function (as shown in the first image) instead of ReLU or GELU are as following:\nMathematical Linearity: Swish (SILU) is defined as $x\\cdot sigmoid(\\beta x)$, $\\beta$ is a learnable parameter. It is a small monotonic function that allowes small negetive values to pass through, making it mathematically linear. While GELU ($x \\cdot \\phi(x)$) is also smooth and nonmonotonic, Swish\u0026rsquo;s curve and equations have emperically improved the model\u0026rsquo;s performance.\nComputational Efficiency: SILU gradients are simpler to compute. $x/(1+exp(-x))$ compared to the standard GELU, which is an approximation of the tanh function. In the massive scale of modern LLMs these shortcuts save significant training time.\nPerformance: Research in the PaLM and Llama papers found that Swish-based Gated Linear Units consistently outperformed GELU-based versions in terms of perplexity and downstream task accuracy. $$SwiGLU(x, W, V, b, c) = (Swish(xW + b) \\otimes (xV + c))W_2$$ Where $W$ and $V$ are weight matrices for the gate and the data path, and $\\otimes$ is element-wise multiplication.\nMore Complex Filtering: Instead of using ordinary two weight matrices in standard FFN, SWiGLU uses three weight matrices (gate, date, output). This allows it to gather more complex features out of the input without drastically increasing the number of parameters. This performance boost has resulted in modern implementations being low in dimensionality of hidden layers (2/3 of input dimensions) while retaining the performance benefits of gated activations.\nZero-Gradient Avoidance: Unlike ReLU, which has a zero gradient for all negative values (the \u0026ldquo;dying ReLU\u0026rdquo; problem), SwiGLU\u0026rsquo;s smooth curve ensures that the model can still learn from negative inputs, improving training stability in deep stacks.\nMixture of Experts (MoE) This is an entire architecture rather than a single block with activation switch. It comes with it\u0026rsquo;s own challenges. It is a \u0026ldquo;divide and conquer\u0026rdquo; strategy. It became a dominant strategy for scaling insanely large models upto a trillion parameters like DeepSeek-V3 and Mistral. It is because it breaks down the direct and obvious link between model size and its speed. It is because it breaks down the direct and obvious link between model size (information capacity) and its speed (inference cost). Sparse Activation: MoE comes with a system of Manager (called Router in this case) and Experts. The Router is responsible for determining which smaller Experts (FFNs) are best suited for specific tokens. The Experts layer is actually split in different sublayers. For any token only a tiny part of the Experts are used. DeepSeek had 256 Expert layers in a single layer while activating not more than 8 for a single token. Big Scale, Small Compute:* MoE runs with a \u0026ldquo;trillion parameter intelligence\u0026rdquo; that runs at a speed of a much smaller model. DeepSeek-V3 has 671 billion total parameters, which dictates its knowledge base. However, during inference, it only uses 37 billion parameters per token. You get the reasoning performance of a massive model with the latency and cost of a significantly smaller one. Expert Specialization: Because different experts handle different tokens, they naturally specialize. DeepSeek\u0026rsquo;s analysis shows that certain experts become \u0026ldquo;specialists\u0026rdquo; in mathematics or coding, while others handle general language, without human intervention. Training Efficiency: Since you only update a fraction of the weights for each token, MoE models can process many more tokens per second during training compared to a dense model of the same total size. Some points on training: The router gives probabilities for each expert stack to be used for a token. While training, in order to avoid overfitting over specific experts a little amount of noise is added to the router\u0026rsquo;s output. This is just to skew the model from selecting a few experts in small amount of iterations. In some models only one expert is chosen and in others more than are chose and then averaged. This is not as widely used as SwiGLU and for the reasons below.\nThe RAM wall (Memory Capacity):* Even though the model computes only small percentage of parameters at once it should hold all 671B parameters in GPU memory. This is a major challenge for MoE models to run on consumer hardware or single GPUs. The model even running at a fast speed will not work without a big GPU cluster. Training Instability: MoEs struggle from load balancing issues where the router might \u0026ldquo;collapse\u0026rdquo; (sending all tokens to just one expert, ignoring the others). Models often use auxiliary losses to force balance, but this can hurt performance. DeepSeek-V3 introduced an \u0026ldquo;Auxiliary-Loss-Free\u0026rdquo; strategy that balances load dynamically without degrading the model\u0026rsquo;s accuracy. Communication Overhead: In distributed training, experts are often split across different GPUs. If the router decides a token on GPU 1 needs an expert located on GPU 4, that data must travel over the network. The Fix: DeepSeek-V3 employs \u0026ldquo;Node-Limited Routing\u0026rdquo;, which restricts the router to send tokens only to a maximum of 4 nodes, preventing network congestion. Summary: MoE is \u0026ldquo;better\u0026rdquo; if you are a major lab (like DeepSeek, Google, OpenAI) trying to maximize intelligence per dollar of training cost. It is \u0026ldquo;worse\u0026rdquo; if you are a developer with limited VRAM trying to deploy a model locally. You use MoE to push the frontier of intelligence; you use Dense SwiGLU for reliability and ease of deployment.\n","permalink":"http://localhost:1313/posts/feedforward-networks/","summary":"Exploring the hidden layers of trillion-parameter switchboards: Feedforward Neural Networks and Activation Functions.","title":"Anatomy of Trillion-Parameter Switchboards: Understanding Feedforward Blocks"},{"content":"\nWe are about to touch the holy grail of modern AI. The 2017 declaration was simple: Attention Is All You Need. But as we‚Äôve raced from GPT-1 to reasoning models, the definition of \u0026lsquo;Attention\u0026rsquo; has quietly transformed. Is the algorithm ruling the world today actually the same one we started with? Let‚Äôs look under the hood.\nüî• The Core Concept At its heart, the attention mechanism is a form of weighted aggregation. It is often described using a database retrieval analogy. Imagine you have a database of information (context).\nQuery ($Q$): What are you currently looking for? (The token being generated). Key ($K$): The indexing tag associated with each piece of information. Value ($V$): The actual information (The context token). The \u0026ldquo;Attention\u0026rdquo; is the process of matching your Query against all the Keys to determine how much \u0026ldquo;weight\u0026rdquo; to put on each Value. Attention scores are calculated in each transformer layer at every step of the sequence. This is a single unifying factor in all transformer models, from BERT to Gemini.\nüî• Standard Multi-Head Attention (MHA) The original mechanism introduced in the paper \u0026ldquo;Attention Is All You Need\u0026rdquo;.\nHere, instead of performing the Key-Query-Value (KQV) operation on the entire context only once, the attention is split into $N$ parallel heads. Each head learns a different aspect of the language (e.g., word meaning, syntax, semantic relationships, etc.). The equation is a simple culmination of dot products aided by a regularizer like Softmax.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nWhere:\n$QK^T$ computes the similarity scores between the Query and Key. $\\sqrt{d_k}$ is a scaling factor to prevent the dot product from growing too large, mitigating the vanishing/exploding gradient problem. The weighted sum is applied to the values of $V$. The Memory Bandwidth Bottleneck: \u0026gt; During inference, the model must store Key and Value matrices for all previous tokens. This is called the KV Cache. In MHA, every single head has its own unique set of Keys and Values. For large models with massive context windows, loading this massive KV cache from memory (HBM) to the computation chip (SRAM) becomes the primary bottleneck, slowing down generation significantly. Earlier models could often only generate ~8 tokens per second at inference due to this constraint.\nüî• Multi-Query Attention (MQA) This approach addresses the memory bottleneck by sharing the same set of Keys and Values across all heads while keeping separate Query heads.\nMHA: $H$ Query heads, $H$ Key heads, $H$ Value heads MQA: $H$ Query heads, 1 Key head, 1 Value head This drastically reduces the memory requirements of the KV cache by a factor of $H$ (often 8x or 16x). The GPU needs to load far less data, significantly speeding up inference. However, it does not reduce the computation (FLOPs)‚Äîso why is it faster?\nThis can be explained with the Kitchen Analogy: Imagine a kitchen (GPU) with 8 chefs (Heads).\nComputation (Math): The Chef chopping vegetables. Memory Access (Bandwidth): The Assistant running to the fridge to get ingredients. In MHA: Each chef demands unique vegetables from the fridge. The assistant runs back and forth 8 times, fetching 8 different crates. Most of the time is spent waiting for the assistant to return. This results in idle cores waiting for data to load from HBM.\nIn MQA: All chefs agree to use the same vegetables. The assistant runs back and forth only once, fetching one giant crate of potatoes and dumping it. This results in minimized memory transfer and no idle cores.\nThe Technical Reality: During text generation, the model is memory-bound, not compute-bound. The time it takes to move the KV Cache from HBM (High Bandwidth Memory) to the chip\u0026rsquo;s SRAM is the bottleneck. MQA reduces the data volume of that move by a factor of $H$.\nThe Capacity Gap: MQA is a trade-off between nuanced response and memory capacity. The model loses the ability to store different types of information, as a single $V$ matrix must now store all nuances of information flow in the language.\nThe Surgery Trick (Uptraining): Google Research figured out a way to take an existing model with MHA and convert it to MQA by averaging the weight matrices of $H$ heads. This is called Uptraining. You then train this \u0026ldquo;Frankenstein\u0026rdquo; model for ~5% of the original training steps for adjustments. This yields an MQA model that is nearly as good as the original MHA but vastly faster.\nüî• Grouped Query Attention (GQA) A \u0026ldquo;best of both worlds\u0026rdquo; solution used in modern models like Llama 2 and Llama 3.\nHow it works: GQA acts as an interpolation between MHA and MQA. Instead of having just 1 Key head (MQA) or $H$ Key heads (MHA), it divides the query heads into $G$ groups. Each group shares a single Key and Value head.\nEquation modification: If you have 8 query heads, you might create 4 groups (2 queries per group). This reduces the KV cache by 2x instead of 8x, preserving more quality than MQA while still speeding up inference.\nMHA: 8 people (heads) are working on a project. Each person has their own unique filing cabinet (KV cache). It takes forever to move 8 cabinets around. MQA: 8 people share one single filing cabinet. It\u0026rsquo;s very fast to move, but they fight over the organization and lose detail. GQA: The 8 people split into 4 teams of 2. Each team shares a cabinet. It\u0026rsquo;s a balance between speed and organizational detail. üî• Multi-Head Latent Attention (MLA) This is the most advanced mechanism for attention, used in models like DeepSeek-V3. It is a sophisticated design for massive efficiency that takes a radical approach: mathematically compressing the memory into a low-rank \u0026ldquo;latent\u0026rdquo; vector.\nKV Cache Explosion: As mentioned before, for a massive model like DeepSeek-V3 (671B parameters), storing the full Keys and Values for every token in a long conversation requires terabytes of high-speed memory (HBM), and chips spend more time waiting than churning.\nMHA (Standard) stores everything. MQA deletes redundancy but hurts quality. MLA keeps the quality of MHA but compresses the data 4x-6x smaller than even GQA.\nThe Zip File Approach: Low-Rank Joint Compression Let\u0026rsquo;s compare the math of MHA and MLA. In MHA, for every single token, we generate a unique Key ($k$) and Value ($v$) matrix for each attention head.\nThe Memory Problem: The $k$ is stored in the KV Cache. For 128 heads, for example, it calculates a massive vector of $128 \\times 128 = 16,384$ floats per token.\nFor MHA, the attention score looks like: $$\\text{Score} = q^T \\cdot k$$\nBut for MLA, instead of storing the full $k$, it projects the input down into a tiny latent vector $c_{KV}$, which is much smaller (e.g., 512 floats). $$c_{KV} = W_{DKV} \\cdot x$$\nNow here is the catch: Naive compression and decompression would mean constructing a small latent matrix for every token and deconstructing it every single time the information is required, which defeats the purpose of compression.\n$$k_{\\text{reconstructed}} = W_{UK} \\cdot c_{KV}$$ $$\\text{Score} = q^T \\cdot k_{\\text{reconstructed}}$$\nThe Matrix Absorption Trick (Optimization) Instead of reconstructing $k$ for every token, we can absorb the Up-Projection matrix ($W_{UK}$) into the Down-Projection matrix ($W_{DKV}$) and absorb the query $q$ into a new $Q$.\nFrom the original equation: $$\\text{Score} = q^T \\cdot (\\underbrace{W_{UK} \\cdot c_{KV}}_{\\text{This is } k})$$\nWe associate differently:\n$$\\text{Score} = (\\underbrace{q^T \\cdot W_{UK}}_{\\text{Absorbed Query}}) \\cdot c_{KV}$$ We change the order in which the matrix multiplication is performed.\nHow is this allowed? In linear algebra, matrix multiplication is associative:\n$$(A \\cdot B) \\cdot C = A \\cdot (B \\cdot C)$$ We can move the parentheses! Instead of grouping $(W_{UK} \\cdot c_{KV})$ to make the Key, we group $(q^T \\cdot W_{UK})$ to make a new Query.\nNote: The positional embedding information is left untouched. That rotational information from algorithms like RoPE is preserved outside the compression; it requires minimal space anyways.\n","permalink":"http://localhost:1313/posts/attention-mechanisms/","summary":"We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek\u0026rsquo;s MLA, how has the definition of \u0026lsquo;Attention\u0026rsquo; transformed?","title":"Attention Is All You Need, but exactly which one?: MHA, GQA and MLA"},{"content":"\nPositional Embeddings are the \u0026ldquo;voice\u0026rdquo; that tell Transformers where words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.\n\u0026ldquo;The dog chased the cat\u0026rdquo; and \u0026ldquo;cat chased the dog\u0026rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.\nüî• Sinusoidal Embeddings: The Odometer of Language Introduced in the seminal \u0026ldquo;Attention Is All You Need\u0026rdquo; paper, this was the industry standard for many years. This approach adds a vector to each word embedding based on its position. The vector is computed out of wavelike patterns that are periodic and smooth. Mathematically, for a position $pos$ and embedding dimension $i$:\n$$ PE_{(pos, i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $$\nThe model uses sine and cosine waves of different frequencies to capture both short and long-range dependencies. The frequency of the waves decreases as the embedding dimension increases, which helps capture long-range dependencies in the input sequence.\nThe creators wanted the model to calculate distance easily, which is why a linear function was used. The model simply rotates the current position vector using a rotation matrix to derive the next position.\n$$ \\begin{pmatrix} \\cos(k) \u0026 \\sin(k) \\\\ -\\sin(k) \u0026 \\cos(k) \\end{pmatrix} \\cdot PE(pos) \\approx PE(pos + k) $$ Low dimensions act like a fast-ticking clock (high frequency, changing rapidly) and high dimensions act like a slow clock (low frequency, changing slowly).\nThe number 10,000 is an arbitrarily chosen high number to reduce Aliasing. By choosing such a high number, the creators ensured that the wavelength of the slowest dimension is incredibly long. Consequently, the pattern practically never repeats, and each position remains unique.\nThis combination works like a mechanical odometer dial. Each subsequent dimension acts as a slower-moving gear. The model learns to read these \u0026ldquo;dials\u0026rdquo; to understand exactly where a word sits in the sequence.\nThis mitigated the challenge of exploding gradients that previous array-based positional embeddings suffered from (e.g., adding integers 1, 2, 3\u0026hellip;). This model keeps values between -1 and +1, adding stability while ensuring every position gets a distinct code.\nThe Problem: This approach pollutes the word embedding matrix (semantic information). While high-dimensional vectors are almost always orthogonal‚Äîmeaning the noise doesn\u0026rsquo;t fully destroy the word\u0026rsquo;s identity‚Äîit does not strictly guarantee purity.\nThis led to a more mature approach.\nüî• ALiBi: The Fading Streetlight Attention with Linear Biases (ALiBi) is a method where, instead of adding a vector to the embeddings at the start, the model injects positional information linearly, directly into the attention scores. If Sinusoidal Encoding asks, \u0026ldquo;Where am I on the map?\u0026rdquo;, ALiBi asks, \u0026ldquo;How much harder should I squint to see you?\u0026rdquo;\nThe biggest problem ALiBi solved is the \u0026ldquo;Invisible Wall\u0026rdquo;. If you train a Transformer on sequences of length 2048 and then feed it a book with 3000 words, it will crash or output garbage at word 2049. This is a failure of Extrapolation.\nALiBi throws away the idea of adding positional vectors entirely. Instead, it devises a \u0026ldquo;penalty\u0026rdquo; system for attention scores:\n$$ \\text{Score} = (q \\cdot k^\\top) - (m \\cdot \\text{distance}) $$\nDistance: If words A and B are neighbors, the distance is 0, so the penalty is 0. Although the distance is linear, it does not cause exploding gradients because the raw attention scores are normalized by a softmax function. This distance prioritizes nearby words (local context) over distant ones.\nSlope ($m$): If we always penalize distance equally, the model becomes myopic. ALiBi mitigates this by assigning a different slope value to each attention head:\nHead 1 (The Historian): Very low slope. The penalty grows slowly, allowing it to see far back. Head 2 (The Reader): Medium slope. Head 3 (The Myopic): Very high slope (e.g., 0.5); it effectively cannot see beyond 2-3 words. This mimics holding a lantern. Words close to you are bright, while words further away naturally fade into darkness.\nExtrapolation: If you walk 5 miles down the road (a longer sequence), your lantern works exactly the same way. The physics of light decay are constant, allowing the model to handle sequences longer than it was trained on.\nHowever, ALiBi forces a \u0026ldquo;sliding window\u0026rdquo; view where distant words eventually become invisible. This brings us to the modern standard.\nüî• RoPE: The Rotary Revolution Rotary Position Embeddings (RoPE) is the current industry standard (used in Llama 3, Mistral, PaLM) because it mathematically unifies \u0026ldquo;absolute position\u0026rdquo; with \u0026ldquo;relative position.\u0026rdquo; This method rotates the vectors in the Query and Key matrices based on their absolute position.\nIf a vector has 512 dimensions, RoPE breaks it into 256 pairs. Each pair is rotated by a different frequency:\n$$ \\theta_i = 10000^{-2i/d} $$\nThe First pair ($i = 0$) is rotated by the highest frequency. The Last pair ($i = 255$) is rotated by the lowest frequency. This spread ensures the model has \u0026ldquo;high precision\u0026rdquo; (fast rotation) to distinguish immediate neighbors, and \u0026ldquo;low precision\u0026rdquo; (slow rotation) to track global position without the pattern repeating.\nDoesn\u0026rsquo;t rotation change meaning? Yes and No. The vector for \u0026ldquo;Apple\u0026rdquo; at position 1 and \u0026ldquo;Apple\u0026rdquo; at position 256 will look completely different in terms of coordinates. However, rotation does not change the semantic strength (magnitude/norm) of the vector.\nThe Relativity Trick: Transformers compare words using dot products (angles). RoPE relies on the fact that the attention mechanism ignores the absolute rotation and only detects the difference in rotation (relative position). Thus, the \u0026ldquo;relative meaning\u0026rdquo; is encoded purely into the angle between words, leaving the semantic \u0026ldquo;magnitude\u0026rdquo; untouched.\nüî• LongRoPE: The Bifocal Lens While RoPE is powerful, it struggles when stretching context windows from 4k to 2 million tokens. The \u0026ldquo;fast\u0026rdquo; dimensions spin so rapidly over long distances that they become random noise. LongRoPE solves this using an evolutionary search algorithm to find a unique scaling factor ($\\lambda$) for each dimension.\nThe Equations Instead of rotating by the standard $\\theta_i$, LongRoPE rotates by a rescaled frequency:\n$$ \\theta\u0026rsquo;_i = \\lambda_i \\theta_i $$\nTo efficiently find these $\\lambda$ values without searching an infinite space, the algorithm enforces a monotonicity constraint based on NTK theory:\n$$ \\lambda_i \\le \\lambda_{i+1} $$\nThis ensures that low-frequency dimensions (global context) are stretched more than high-frequency dimensions (local context), creating a \u0026ldquo;bifocal\u0026rdquo; effect:\nHigh Frequencies (Local): Kept sharp ($\\lambda \\approx 1$) to maintain grammar. Low Frequencies (Global): Stretched ($\\lambda \u0026gt; 1$) to track massive distances without repeating. Not worth perusing more since the next one is much better at long context positional encoding. üî• HoPE: The Hyperbolic Slide Hyperbolic Rotary Positional Encoding (HoPE) moves from the geometry of a circle to the geometry of a hyperbola (inspired by Lorentz transformations). It was designed to fix a subtle flaw in RoPE: the \u0026ldquo;wobbly\u0026rdquo; or oscillatory nature of attention scores.\nThe Core Problem: RoPE\u0026rsquo;s \u0026ldquo;Wobble\u0026rdquo; In standard RoPE, we rotate vectors around a circle.\nThe Issue: Circles repeat. A dial at $361^\\circ$ looks identical to $1^\\circ$. Even with different frequencies, the dot product fluctuates up and down as distance increases. The Consequence: The model gets confused at long distances, potentially mistaking a word 1,000 tokens away for a neighbor just because the rotation cycles aligned, creating \u0026ldquo;noise.\u0026rdquo; The \u0026ldquo;Hyperbolic\u0026rdquo; Shift HoPE replaces trigonometric functions ($\\sin, \\cos$) with hyperbolic functions ($\\sinh, \\cosh$).\nCircular Rotation: Keeps distance from the center constant. Hyperbolic Rotation: Moves points along a hyperbola. Crucially, hyperbolic functions do not repeat; they grow or decay exponentially. By using hyperbolic rotation, HoPE guarantees that as words get further apart, the attention score decays monotonically (smoothly drops).\nThe Equations HoPE uses a hyperbolic matrix $B(\\theta, m)$ defined using the Lorentz Boost structure:\n$$ B(\\theta, m) = \\begin{pmatrix} \\cosh(m\\theta) \u0026 \\sinh(m\\theta) \\\\ \\sinh(m\\theta) \u0026 \\cosh(m\\theta) \\end{pmatrix} $$ Since hyperbolic functions grow to infinity, HoPE introduces a decay penalty ($e^{-m\\theta'}$). The final attention score simplifies beautifully to a function of relative distance: $$ \\text{Score} \\propto e^{-|m-n|(\\theta\u0026rsquo; - \\theta)} $$\nThis ensures the attention score exponentially decays as distance increases, eliminating the wobbles of standard RoPE. This the latest model that is taking up the space in the industry for extremely long context models.\n","permalink":"http://localhost:1313/posts/positional-embeddings/","summary":"From Sinusoidal to RoPE and HoPE: How Transformers learn to process word order and sequence length.","title":"The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE"},{"content":"Imagine trying to teach a computer the difference between \u0026ldquo;Apple\u0026rdquo; the fruit and \u0026ldquo;Apple\u0026rdquo; the company. To us, the distinction is intuitive. To a machine, it‚Äôs just a string of characters. How do we turn these strings into meaningful math? While early attempts like Word2Vec gave us a great start, they missed the forest for the trees‚Äîor in some cases, the twigs for the branches. Enter GloVe and FastText: two algorithms that revolutionized how machines understand the nuances of human language.\nPrevious static embedding models like Word2Vec successfully captured the local semantics of words, but they failed in addressing contexts between words that might not always appear in the same context window. Furthermore, they treated words as indivisible atomic units in continuous vector spaces, missing out on the internal structure of language.\nThe two simple but effective ideas behind GloVe and FastText are subword units and global co-occurrence. Let\u0026rsquo;s dive into them.\nGloVe: A Global Mapping Strategy Global Vectors for Word Representation (GloVe) was created at Stanford. The core idea was to train a model where word vectors are defined by how often they appear around other words across the entire corpus, rather than just locally.\nTraining Steps: Co-occurrence Matrix X: GloVe scans the entire corpus and creates a giant matrix (spreadsheet) recording the counts of how often words appear near each other. Factorization: It compresses this giant matrix into smaller, dense vectors. Objective: The function tries to equate the dot product of two word vectors with the logarithm of their probability of appearing together. The error between these values is then backpropagated to update the vectors. $$w_i \\cdot w_j + b_i + b_j = \\log(X_{ij})$$\n$w$: The word vector (what we want). $X_{ij}$: The count from our matrix. The Loss Function: We can\u0026rsquo;t satisfy the equation above perfectly for every word pair. So, we use a weighting function to minimize the squared error:\n$$J = \\sum_{i,j=1}^V f(X_{ij}) ({w_i}^T w_j + b_i + b_j - \\log(X_{ij}))^2$$\nIf we do not use the weighting function, the loss would be infinite for rare words (since $\\log(0) = \\infty$) and frequent words (like \u0026rsquo;the\u0026rsquo;) would overpower the model.\n$$ f(x) = \\begin{cases} (x/x_{max})^\\alpha \u0026amp; \\text{if } x \u0026lt; x_{max} \\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$\n(Typically $x_{max}=100$ and $\\alpha=0.75$)\nFastText: The Lego Bricks Strategy FastText was created by Facebook AI Research (FAIR). The idea was to treat words not as solid rocks, but as structures made of \u0026ldquo;Lego bricks\u0026rdquo; (subword units).\nWord2Vec struggled to create embeddings for words it hadn\u0026rsquo;t seen before (Out-Of-Vocabulary or OOV words). FastText solves this by breaking words down.\nGloVe treats a word like \u0026ldquo;Apple\u0026rdquo; as a single, atomic unit. FastText breaks \u0026ldquo;Apple\u0026rdquo; into character n-grams: \u0026lt;ap, app, ppl, ple, le\u0026gt; For a complex or new word like \u0026ldquo;Applesauce\u0026rdquo;, the model might not know the whole word, but it recognizes the Lego brick for \u0026ldquo;Apple\u0026rdquo; and the remaining bits from other subwords.\nFastText takes its inspiration from the SkipGram architecture. The vector for a word is simply the sum of the vectors of its n-gram subwords:\n$$v_w = \\sum_{g \\in G_w} z_g$$\nObjective: The objective is to predict context words given a target word.\n$$P(w_c | w_t) = \\frac{e^{s(v_{w_t} \\cdot v_{w_c})}}{\\sum_{w \\in V} e^{s(v_{w_t} \\cdot v_w)}}$$\nAnd the scoring function is defined as:\n$$s(w_t, w_c) = u_{w_t}^T (\\sum_{g \\in G_w} z_g)$$\n$u_{w_c}$: The vector for the context word (output vector). $z_g$: The vector for the n-gram $g$ (input vector). Modifications to Softmax: Calculating the full softmax for every word in the vocabulary is computationally expensive. FastText uses two methods to approximate this efficiently.\nOption A: Negative Sampling\nWe pick a few random \u0026ldquo;wrong\u0026rdquo; words every time to serve as \u0026ldquo;negative samples\u0026rdquo; and calculate the loss only for the correct word and these noise words.\n$$J = - \\log \\sigma(s(w_t, w_c)) - \\sum\\limits_{i=1}^{N} \\mathbb{E}_{n_i \\sim P_n(w)} [\\log \\sigma(-s(w_t, n_i))]$$\nThe sigmoid function $\\sigma$ squashes the score between 0 and 1. The first term pushes the probability of the correct word towards 1. The second term pushes the probability of the noise words towards 0. Option B: Hierarchical Softmax (Faster for infrequent words)\nInstead of a flat list of words, imagine the vocabulary arranged as a binary tree (Huffman Tree).\nRoot: Top of the tree. Leaves: The actual words. To calculate the probability of a word, we trace a path from the root to that leaf. At every branching node, we calculate the probability (sigmoid) of going left or right. The probability of a word $w$ is the product of the probabilities of the turns taken to reach it. Code Example from gensim.models import FastText from gensim.test.utils import common_texts # Initialize the FastText model with specific hyperparameters model = FastText( vector_size=100, # Dimensionality of the word vectors window=5, # Maximum distance between the current and predicted word within a sentence min_count=1, # Ignores all words with total frequency lower than this sg=1, # Training algorithm: 1 for skip-gram; 0 for CBOW hs=0, # If 0, and negative is non-zero, negative sampling will be used negative=5, # Number of \u0026#34;noise words\u0026#34; to be drawn for negative sampling workers=4, # Number of worker threads to train the model epochs=10, # Number of iterations over the corpus min_n=3, # Minimum length of character n-grams max_n=6 # Maximum length of character n-grams ) # Build the vocabulary from the provided text corpus model.build_vocab(common_texts) # Train the model on the corpus model.train( common_texts, total_examples=model.corpus_count, epochs=model.epochs ) # Calculate and print the cosine similarity between two words in the vocabulary print(f\u0026#34;Similarity between \u0026#39;computer\u0026#39; and \u0026#39;human\u0026#39;: {model.wv.similarity(\u0026#39;computer\u0026#39;, \u0026#39;human\u0026#39;)}\u0026#34;) # Demonstrate FastText\u0026#39;s ability to handle Out-Of-Vocabulary (OOV) words # Even if \u0026#39;computation\u0026#39; wasn\u0026#39;t in the training data, FastText constructs a vector using character n-grams oov_vector = model.wv[\u0026#39;computation\u0026#39;] # Check if a specific word exists in the model\u0026#39;s fixed vocabulary index word = \u0026#34;computer\u0026#34; is_in_vocab = word in model.wv.key_to_index print(f\u0026#34;\\nIs the word \u0026#39;{word}\u0026#39; in the model\u0026#39;s vocabulary? {is_in_vocab}\u0026#34;) ","permalink":"http://localhost:1313/posts/glove-and-fasttext/","summary":"How subword units and global co-occurrence matrices allow GloVe and FastText to capture nuances that Word2Vec missed.","title":"The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText"},{"content":" Before we had Large Language Models writing poetry, we had to teach computers that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; are related not just by spelling, but by meaning. This is the story of that breakthrough. It‚Äôs the moment we stopped counting words and started mapping their souls‚Äîturning raw text into a mathematical landscape where math can solve analogies. Welcome to the world of Word2Vec. üîÆ\nLanguage models require vector representations of words to capture semantic relationships. Before the 2010s, models used word count-based vector representations that captured only the frequency of words (e.g., One-Hot Encoding). The Problems: üöß\nThe Curse of Dimensionality Lack of meaning (Synonyms were treated as mathematically unrelated). In 2011, Mikolov et al. at Google introduced Word2Vec, which used a shallow neural network to learn vector representations of words. The best part? It could create much denser relationships between words than prior models, and it was unsupervised.\nThe training (prediction) is essentially \u0026ldquo;fake\u0026rdquo;‚Äîa pretext task. But the weight matrices we get on the side are a gold mine of fine-grained semantic relationships. ‚õèÔ∏è\nThere are two popular variants of Word2Vec:\nSkipGram Continuous Bag Of Words (CBOW) 1. Continuous Bag of Words (CBOW) üß© CBOW is a simple \u0026lsquo;fill in the blanks\u0026rsquo; machine. It takes the surrounding words as inputs and tries to predict the center word.\n\u0026ldquo;Continuous\u0026rdquo;: It operates in continuous vector space (unlike the discrete space of n-gram models).\n\u0026ldquo;Bag of words\u0026rdquo;: The order of the context does not matter. \u0026ldquo;The cat sat on the mat\u0026rdquo; and \u0026ldquo;The mat sat on the cat\u0026rdquo; are the same for CBOW. They produce exactly the same predictions.\nObjective: Maximize the probability of the target word $w_t$ given its context $w_{t-C}, \\dots, w_{t+C}$.\n2. Training Defining Dimensions:\n$V$: Vocabulary size (number of unique words in the corpus, e.g., 10,000) $N$: Dimension of the embedding space (e.g., 100) $C$: Number of context words (e.g., 2 words on left of target and 2 words on right) Step 1: Input Lookup \u0026amp; One-Hot Encoding\nMathematically, we represent each context word as a one-hot vector $x^{(c)}$. We have an Input Matrix $W_{in}$ of size $V \\times N$. For each context word (e.g., \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;), we select its corresponding row from $W_{in}$.\n$$ v_c = W_{in}^T x^{(c)} $$\nWhere $v_c$ is the $N$-dimensional vector for context word $c$.\nStep 2: Projection (The \u0026ldquo;Mixing\u0026rdquo;)\nWe combine the context vectors. In standard CBOW, we simply average them. This creates the hidden layer vector $h$.\n$$ h = \\frac{1}{2C} \\sum_{c=1}^{2C} v_c $$\n$h$ is a single vector of size $N \\times 1$. Note: This operation is linear and contains no activation function (like ReLU or Sigmoid).\nStep 3: Output Scoring (The \u0026ldquo;Dot Product\u0026rdquo;)\nWe have a second matrix, the Output Matrix $W_{out}$ of size $N \\times V$. We compute the raw score (logit) $u_j$ for every word $j$ in the vocabulary by taking the dot product of the hidden vector $h$ and the output vector $v\u0026rsquo;_j$.\n$$ u_j = {v\u0026rsquo;_j}^T h $$\nOr in matrix form:\n$$ u = W_{out}^T h $$\nResult $u$ is a vector of size $V \\times 1$.\nStep 4: Probability Conversion (Softmax)\nWe convert the raw scores $u$ into probabilities using the Softmax function. This tells us the probability that word $w_j$ is the center word.\n$$ y_j = P(w_j | \\text{context}) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 5: Loss Calculation (Cross-Entropy)\nWe compare the predicted distribution $y$ with the actual ground truth $t$ (a one-hot vector where the index of the true word is 1). The loss function $E$ is:\n$$ E = - \\sum_{j=1}^V t_j \\log(y_j) $$\nSince $t_j$ is 0 for all words except the target word (let\u0026rsquo;s call the target index $j^*$), this simplifies to:\n$$ E = - \\log(y_{j^*}) $$\n3. Backpropagation Details We need to update the weights to minimize $E$. We use the Chain Rule.\nA. Gradient w.r.t. Output Matrix ($W_{out}$)\nWe want to know how the error changes with respect to the raw score $u_j$.\n$$ \\frac{\\partial E}{\\partial u_j} = y_j - t_j $$\nLet\u0026rsquo;s call this error term $e_j$.\nIf $j$ is the target word: $e_j = y_j - 1$ (Result is negative; we boost the score). If $j$ is not target: $e_j = y_j - 0$ (Result is positive; we suppress the score). The update rule for the output vector $v\u0026rsquo;_j$ becomes:\n$$ v'_{j}(\\text{new}) = v'_{j}(\\text{old}) - \\eta \\cdot e_j \\cdot h $$ ($\\eta$ is the learning rate)\nB. Gradient w.r.t. Hidden Layer ($h$)\nWe backpropagate the error from the output layer to the hidden layer.\n$$ EH = \\sum_{j=1}^V e_j \\cdot v\u0026rsquo;_j $$\n$EH$ is an $N$-dimensional vector representing the aggregate error passed back to the projection layer.\nC. Gradient w.r.t. Input Matrix ($W_{in}$)\nSince $h$ was just an average of the input vectors, the error $EH$ is distributed to each context word\u0026rsquo;s input vector. For every word $w_c$ in the context:\n$$ v_{w_c}(\\text{new}) = v_{w_c}(\\text{old}) - \\eta \\cdot \\frac{1}{2C} \\cdot EH $$\nLet\u0026rsquo;s build a single pass through the network for a given context.\nimport torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # Define a simple Continuous Bag of Words (CBOW) style model class CBOWModel(nn.Module): def __init__(self, vocab_size, embedding_dim): super(CBOWModel, self).__init__() # The embedding layer stores the word vectors we want to learn self.embeddings = nn.Embedding(vocab_size, embedding_dim) # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word self.linear = nn.Linear(embedding_dim, vocab_size) def forward(self, inputs): # inputs: tensor of word indices for the context embeds = self.embeddings(inputs) # Aggregate context by calculating the mean of the word embeddings h = torch.mean(embeds, dim=1) # Produce logits (raw scores) for each word in the vocabulary logits = self.linear(h) return logits # Setup vocabulary and mappings word_to_ix = {\u0026#34;the\u0026#34;: 0, \u0026#34;quick\u0026#34;: 1, \u0026#34;brown\u0026#34;: 2, \u0026#34;fox\u0026#34;: 3, \u0026#34;jumps\u0026#34;: 4, \u0026#34;over\u0026#34;: 5, \u0026#34;lazy\u0026#34;: 6, \u0026#34;dog\u0026#34;: 7} ix_to_word = {v: k for k, v in word_to_ix.items()} # Configuration constants EMBEDDING_DIM = 5 VOCAB_SIZE = len(word_to_ix) LEARNING_RATE = 0.01 # Initialize the model, loss function, and optimizer model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM) loss_function = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE) # Prepare dummy training data: context words and the target word # Context: [\u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;over\u0026#34;] -\u0026gt; Target: \u0026#34;fox\u0026#34; context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long) target_idxs = torch.tensor([3], dtype=torch.long) # Perform a single optimization step model.zero_grad() # Forward pass: we reshape context to (1, -1) to simulate a batch of size 1 logits = model(context_idxs.view(1, -1)) # Calculate loss against the target word index loss = loss_function(logits, target_idxs) # Backpropagate and update weights loss.backward() optimizer.step() # Output results for the blog post print(\u0026#34;=== Model Training Snapshot ===\u0026#34;) print(f\u0026#34;Calculated Loss: {loss.item():.6f}\u0026#34;) print(f\u0026#34;\\n=== Learned Vector for \u0026#39;jumps\u0026#39; ===\u0026#34;) word_vec = model.embeddings(torch.tensor([word_to_ix[\u0026#39;jumps\u0026#39;]])) print(word_vec.detach().numpy()) print(\u0026#34;\\n=== Embedding Matrix (Weights) ===\u0026#34;) print(model.embeddings.weight.detach().numpy()) print(\u0026#34;\\n=== Linear Layer Weights ===\u0026#34;) print(model.linear.weight.detach().numpy()) Output shows single pass loss and learned vector for \u0026lsquo;jumps\u0026rsquo; word. This is a single training sample result but after multiple such passes both Embedding and linear weights should be very similar.\n=== Model Training Snapshot === Calculated Loss: 1.989656 === Learned Vector for \u0026#39;jumps\u0026#39; === [[-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775]] === Embedding Matrix (Weights) === [[ 1.66039 -0.11371879 0.6246518 0.35860053 -1.2504417 ] [-2.1847186 -0.77199775 -0.17050214 -0.38411248 -0.03913084] [ 0.11852697 0.90073633 0.8847807 0.7404524 0.900149 ] [ 0.07440972 -0.40259898 2.6246994 -0.08851447 0.02660969] [-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775] [-0.9245572 -0.5545908 0.9083091 -1.0755049 0.84047747] [-1.0237687 0.59466314 0.05621134 -0.6202532 1.3664424 ] [ 0.60998917 -1.0549186 1.6103884 0.8724912 -1.2486908 ]] === Linear Layer Weights === [[ 0.02165058 -0.28883642 0.14545658 -0.3442509 0.32704315] [-0.18731792 0.28583744 0.22635977 0.13245736 0.29019794] [ 0.3158916 -0.15826383 -0.03203773 0.16377363 -0.41457543] [-0.05080034 0.4180087 0.11228557 0.30218413 0.3025514 ] [-0.38419306 0.24475925 -0.39210224 -0.38660625 -0.2673145 ] [-0.32321444 0.12200444 -0.03569533 0.2891424 -0.07345333] [-0.33704326 0.2521956 0.31587374 0.22590035 0.29866052] [ 0.4117266 -0.44231793 0.24064957 -0.29684234 0.333821 ]] Interesting Facts About CBOW üß† The \u0026ldquo;Averaging Problem\u0026rdquo;: This is a common issue in CBOW models. The averaging and training is order-agnostic. All variants of word order in the context window will generate the same output. \u0026ldquo;Dog bit the man\u0026rdquo; and \u0026ldquo;Man bit the dog\u0026rdquo; are the same in CBOW\u0026rsquo;s eyes.\nSmoothing Effect: CBOW models are much faster to train than Skip-Gram but are generally smoother.\nRare Words: CBOW models struggle slightly with rare words because the presence of surrounding common words (like articles and prepositions) can skew the embeddings and dissolve their uniqueness.\nComputational Cost: Softmax requires summing over the entire vocabulary size, which can be computationally expensive if $V$ is very large. Solutions include Hierarchical Softmax and Negative Sampling (approximating the denominator by only checking the target word vs. 5 random noise words).\nVectors as Double Agents: Every word has a different representation in the embedding matrix and the output matrix. Most of the time $W_{out}$ is discarded, but some consider averaging both $W_{in}$ and $W_{out}$ for slightly better performance.\nLinear Relationships: The famous analogy King - Man + Woman = Queen emerges from the model because of the linear relationships between the words.\nInitialization: This is a no-brainer but worth mentioning. Initialization to zero vectors would mean no learning. We must initialize randomly.\n2. Skip-Gram (The \u0026ldquo;Burger Flip\u0026rdquo; of CBOW) üîÑ Skip-Gram flips the CBOW logic. While CBOW tries to predict the center word given the context, Skip-Gram tries to predict the scattered context given the center word.\nCore Concept: If a word like \u0026ldquo;monarch\u0026rdquo; can accurately predict words like \u0026ldquo;king\u0026rdquo;, \u0026ldquo;throne\u0026rdquo;, \u0026ldquo;crown\u0026rdquo;, \u0026ldquo;royalty\u0026rdquo;, and \u0026ldquo;power\u0026rdquo;, then it must effectively capture the concept of royalty.\nTraining Let\u0026rsquo;s define dimensions:\n$V$: Vocabulary size $D$: Embedding dimension $C$: Number of context words Step 1: Input Lookup\nUnlike CBOW, our input is just one word vector (the center word $w_t$). We grab the vector $v_c$ from the Input Matrix $W_{in}$.\n$$ h = v_{w_t} $$\nNote: There is no averaging here. The hidden layer $h$ is simply the raw vector of the center word.\nStep 2: Output Scoring (The Broadcast)\nThe model takes this single vector $h$ and compares it against the Output Matrix $W_{out}$ (the entire vocabulary). It computes a score $u_j$ for every word $j$ in the dictionary.\n$$ u = W_{out}^T h $$\nResult $u$: A vector of size $V \\times 1$ containing raw scores.\nStep 3: Probability Conversion (Softmax)\nWe apply Softmax to turn scores into probabilities.\n$$ P(w_j | w_t) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 4: The Multi-Target Loss\nHere is the major difference from CBOW. In CBOW, we had one target (the center). In Skip-Gram, we have $2C$ targets (all the surrounding words). For a sentence \u0026ldquo;The cat sat on mat\u0026rdquo; (Center: \u0026ldquo;sat\u0026rdquo;), the targets are \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;. We want to maximize the probability of all these true context words. The Loss function ($E$) sums the error for each context word:\n$$ E = - \\sum_{w_{ctx} \\in \\text{Window}} \\log P(w_{ctx} | w_t) $$\nStep 5: Backpropagation (Accumulating Gradients)\nSince one input word is responsible for predicting multiple output words, the error signals from all those context words add up.\nAt the Output Layer:\nIf \u0026ldquo;cat\u0026rdquo; was a target, the \u0026ldquo;cat\u0026rdquo; vector in $W_{out}$ gets a signal: \u0026ldquo;Move closer to \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; If \u0026ldquo;apple\u0026rdquo; was NOT a target, the \u0026ldquo;apple\u0026rdquo; vector gets a signal: \u0026ldquo;Move away from \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; At the Hidden Layer ($h$):\nThe center word \u0026ldquo;sat\u0026rdquo; receives feedback from all its neighbors simultaneously. Error signal = (Error from \u0026ldquo;The\u0026rdquo;) + (Error from \u0026ldquo;cat\u0026rdquo;) + (Error from \u0026ldquo;on\u0026rdquo;)\u0026hellip; The vector for \u0026ldquo;sat\u0026rdquo; in $W_{in}$ moves in the average direction of all its neighbors. 3. The \u0026ldquo;Computational Nightmare\u0026rdquo; \u0026amp; Negative Sampling The equations above describe Naive Softmax.\nThe Problem: If $V = 100,000$, computing the denominator $\\sum \\exp(u_k)$ takes 100,000 operations. Doing this for every word in a training set of billions of words is impossibly slow.\nThe Solution: Negative Sampling Instead of updating the entire vocabulary (1 correct word vs 99,999 wrong ones), we approximate the problem.\nPositive Pair: (sat, cat) $\\rightarrow$ Maximize probability (Label 1). Negative Pairs: We pick $K$ (e.g., 5) random words the model didn\u0026rsquo;t see, e.g., (sat, bulldozer), (sat, quantum). $\\rightarrow$ Minimize probability (Label 0). New Equation (Sigmoid instead of Softmax):\n$$ E = - \\log \\sigma({v'_{\\text{pos}}}^T v_{\\text{in}}) - \\sum_{k=1}^K \\log \\sigma(-{v'_{\\text{neg}_{k}}}^T v_{\\text{in}}) $$ Effect: We typically only update 6 vectors per step (1 pos + 5 negs) instead of 100,000.\nLet\u0026rsquo;s see this in code:\nimport torch import torch.nn as nn import torch.optim as optim class SkipGramNegativeSampling(nn.Module): \u0026#34;\u0026#34;\u0026#34; Skip-Gram with Negative Sampling (SGNS) implementation. SGNS approximates the softmax over the entire vocabulary by instead distinguishing between a real context word (positive) and K noise words (negative). \u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embedding_dim): super(SkipGramNegativeSampling, self).__init__() # Input embeddings: used when the word is the center word self.in_embed = nn.Embedding(vocab_size, embedding_dim) # Output embeddings: used when the word is a context or negative sample self.out_embed = nn.Embedding(vocab_size, embedding_dim) # Initialize weights with small values to prevent gradient saturation initrange = 0.5 / embedding_dim self.in_embed.weight.data.uniform_(-initrange, initrange) self.out_embed.weight.data.uniform_(-initrange, initrange) def forward(self, center_words, target_words, negative_words): \u0026#34;\u0026#34;\u0026#34; Computes the negative sampling loss. Args: center_words: (batch_size) target_words: (batch_size) negative_words: (batch_size, K) where K is number of negative samples \u0026#34;\u0026#34;\u0026#34; # Retrieve vectors v_c = self.in_embed(center_words) # (batch_size, embed_dim) u_o = self.out_embed(target_words) # (batch_size, embed_dim) u_n = self.out_embed(negative_words) # (batch_size, K, embed_dim) # 1. Positive Score: log(sigmoid(v_c ¬∑ u_o)) # Compute dot product: (batch, 1, dim) @ (batch, dim, 1) -\u0026gt; (batch, 1) pos_score = torch.bmm(u_o.unsqueeze(1), v_c.unsqueeze(2)).squeeze(2) pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-7) # 2. Negative Score: sum(log(sigmoid(-v_c ¬∑ u_n))) # Compute dot products for all K samples: (batch, K, dim) @ (batch, dim, 1) -\u0026gt; (batch, K) neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2) neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-7), dim=1, keepdim=True) # Total loss is the negative of the objective function loss = -(pos_loss + neg_loss) return torch.mean(loss) # --- Configuration \u0026amp; Mock Data --- VOCAB_SIZE = 100 EMBED_DIM = 10 word_to_ix = {\u0026#39;fox\u0026#39;: 0} # Example vocabulary mapping model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock inputs: 1 center word, 1 target word, 5 negative samples center_id = torch.tensor([0], dtype=torch.long) target_id = torch.tensor([1], dtype=torch.long) negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long) # Training Step model.zero_grad() loss = model(center_id, target_id, negative_ids) loss.backward() optimizer.step() # Output Results print(f\u0026#34;Loss after one step: {loss.item():.6f}\u0026#34;) word_vec = model.in_embed(torch.tensor([word_to_ix[\u0026#39;fox\u0026#39;]])) print(f\u0026#34;Vector for \u0026#39;fox\u0026#39;:\\n{word_vec.detach().numpy()}\u0026#34;) The outputs:\nLoss after one step: 4.158468 Vector for \u0026#39;fox\u0026#39;: [[ 0.04197385 0.02400733 -0.03800093 0.01672485 -0.03872231 -0.0061478 0.0121122 0.04057864 -0.036255 0.03861175]] Interesting Facts About Skip-Gram üí° Makes Rare Words Shine: Skip-Gram is more effective at capturing the meaning of rare words because it focuses on the context of the center word. In CBOW, a rare word\u0026rsquo;s vector is averaged with others, diluting its signal. In Skip-Gram, when the rare word appears as the center, it gets the full, undivided attention of the backpropagation update.\nSlow Training: This method creates more training samples. A window of size 2 creates 4 training pairs per center word, while CBOW creates only 1 training pair per center word.\nSemantic Focus: Skip-Gram puts more emphasis on the semantic relationship between words. It views the context as the primary signal, with the center word as the target. This makes it better at capturing the meaning of words in context. Skip-Gram tends to capture semantic relationships (King/Queen) slightly better, while CBOW captures syntactic ones (walk/walked) slightly better.\n","permalink":"http://localhost:1313/posts/word-embeddings/","summary":"Understanding the mathematics behind Word2Vec, CBOW, and Skip-Gram and how they map language to vector space.","title":"Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n5. Code Examples: We\u0026rsquo;ll use sentencepiece for tokenization and compare results for different models.\nimport sentencepiece as spm import os # Configuration for training the SentencePiece model # SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words. options = { # The source text file used for learning the vocabulary \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # The base name for the generated model (.model) and vocabulary (.vocab) files \u0026#39;model_prefix\u0026#39;: \u0026#39;bpe_model\u0026#39;, # Number of unique tokens in the final vocabulary \u0026#39;vocab_size\u0026#39;: 4000, # \u0026#39;bpe\u0026#39; (Byte Pair Encoding) merges frequent pairs of characters/sequences \u0026#39;model_type\u0026#39;: \u0026#39;bpe\u0026#39;, # Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets \u0026#39;character_coverage\u0026#39;: 0.9995, # When enabled, unknown characters are decomposed into UTF-8 bytes to avoid \u0026#39;unk\u0026#39; tokens \u0026#39;byte_fallback\u0026#39;: True, # Treats digits individually (0-9), preventing large numbers from being treated as single tokens \u0026#39;split_digits\u0026#39;: True, # Prevents adding a whitespace prefix to the first token; useful for fine-grained control \u0026#39;add_dummy_prefix\u0026#39;: False } try: print(\u0026#34;Starting the training process...\u0026#34;) # SentencePieceTrainer.train takes the dictionary of options to build the BPE model spm.SentencePieceTrainer.train(**options) print(\u0026#34;Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created.\u0026#34;) # Initialize the processor and load the newly trained model sp = spm.SentencePieceProcessor() sp.load(\u0026#39;bpe_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Model Metadata:\u0026#34;) # Retrieve the total number of tokens in the vocabulary print(f\u0026#34;Total Vocab Size: {sp.get_piece_size()}\u0026#34;) # Special tokens are used for sequence boundaries and handling unknown characters print(f\u0026#34;BOS (Beginning of Sentence) ID: {sp.bos_id()}\u0026#34;) print(f\u0026#34;EOS (End of Sentence) ID: {sp.eos_id()}\u0026#34;) print(f\u0026#34;UNK (Unknown) ID: {sp.unk_id()}\u0026#34;) print(f\u0026#34;PAD (Padding) ID: {sp.pad_id()}\u0026#34;) # Test the tokenizer on sample strings to see how it breaks down text test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: shows the actual subword units (tokens) print(f\u0026#34;Subword Tokens: {sp.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: shows the numerical mapping for each token print(f\u0026#34;Numerical IDs: {sp.encode_as_ids(text)}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred during training or processing: {e}\u0026#34;) You can see the following output:\nStarting training... Starting the training process... Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created. ------------------------------ Model Metadata: Total Vocab Size: 4000 BOS (Beginning of Sentence) ID: 1 EOS (End of Sentence) ID: 2 UNK (Unknown) ID: 0 PAD (Padding) ID: -1 --- Tokenization Test --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;el\u0026#39;, \u0026#39;lo\u0026#39;, \u0026#39;‚ñÅW\u0026#39;, \u0026#39;orld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 380, 309, 476, 619, 36, 3924, 3964, 3959, 3993, 3978, 3983, 57, 3975, 3976, 60, 3974] --- Tokenization Test --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅmo\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;inst\u0026#39;, \u0026#39;ag\u0026#39;, \u0026#39;ram\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅever\u0026#39;] Numerical IDs: [1894, 304, 3284, 376, 321, 313, 312, 1008, 3313, 1325, 428, 3937, 341, 304, 3284, 1495] The ! is fallen back to UTF bytes. and the word instagrammable is split into multiple subword tokens. Also, original text can be recreated right back by simply concatenating the tokens. And replacing underscore by whitespace. Now we can try Unigram model. and see the same sentences tokenized differently.\nfrom tokenizers import Tokenizer from tokenizers.models import WordPiece from tokenizers.trainers import WordPieceTrainer from tokenizers.pre_tokenizers import Whitespace try: # 1. Initialize the WordPiece Tokenizer # We specify the [UNK] token for handling words not found in the vocabulary. tokenizer = Tokenizer(WordPiece(unk_token=\u0026#34;[UNK]\u0026#34;)) # 2. Configure Pre-tokenization # Before the subword algorithm runs, we need to split the raw text into words. # Whitespace splitting is the standard first step for most English NLP tasks. tokenizer.pre_tokenizer = Whitespace() # 3. Initialize the Trainer # We define our target vocabulary size and the special tokens required for # downstream tasks (like BERT\u0026#39;s [CLS] for classification or [SEP] for separators). trainer = WordPieceTrainer( vocab_size=4000, special_tokens=[\u0026#34;[UNK]\u0026#34;, \u0026#34;[CLS]\u0026#34;, \u0026#34;[SEP]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[MASK]\u0026#34;] ) # 4. Train the Model # The tokenizer scans the training file to build a vocabulary of the most # frequent subword units. tokenizer.train(files=[\u0026#34;train_data.txt\u0026#34;], trainer=trainer) # 5. Persist the Model # Save the configuration and vocabulary to a JSON file for future inference. tokenizer.save(\u0026#34;wordpiece.json\u0026#34;) print(\u0026#34;Training complete. \u0026#39;wordpiece.json\u0026#39; created.\u0026#34;) # 6. Metadata Inspection print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;WordPiece Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {tokenizer.get_vocab_size()}\u0026#34;) # 7. Testing Subword Tokenization # WordPiece shines at handling rare words by breaking them into meaningful chunks. test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (WordPiece) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # Encode converts raw text into a Tokenizer object containing tokens and IDs output = tokenizer.encode(text) # \u0026#39;tokens\u0026#39; shows the subword breakdown (e.g., \u0026#39;un\u0026#39;, \u0026#39;##insta\u0026#39;, etc.) print(f\u0026#34;Subword Tokens: {output.tokens}\u0026#34;) # \u0026#39;ids\u0026#39; are the numerical indices mapped to the vocabulary print(f\u0026#34;Numerical IDs: {output.ids}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred with WordPiece model: {e}\u0026#34;) You can see the following output:\nWordPiece Model Metadata: Total Vocab Size: 2609 --- Tokenization Test (WordPiece) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;H\u0026#39;, \u0026#39;##el\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;##or\u0026#39;, \u0026#39;##ld\u0026#39;, \u0026#39;[UNK]\u0026#39;, \u0026#39;[UNK]\u0026#39;] Numerical IDs: [37, 180, 214, 52, 162, 418, 0, 0] --- Tokenization Test (WordPiece) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;##os\u0026#39;, \u0026#39;##t\u0026#39;, \u0026#39;un\u0026#39;, \u0026#39;##ins\u0026#39;, \u0026#39;##ta\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;##ra\u0026#39;, \u0026#39;##m\u0026#39;, \u0026#39;##ma\u0026#39;, \u0026#39;##ble\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;ever\u0026#39;] Numerical IDs: [691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240] We can see the special character ## in the output. We can also see the breakdowns not having any semantic meaning. They are likelihood based. We can try the Unigram model now.\n# \u0026#39;Unigram\u0026#39; is the default and usually recommended over BPE in SentencePiece. options_unigram = { \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # Path to the raw text file for training \u0026#39;model_prefix\u0026#39;: \u0026#39;unigram_model\u0026#39;, # Prefix for the output .model and .vocab files \u0026#39;vocab_size\u0026#39;: 1200, # Desired size of the final vocabulary \u0026#39;model_type\u0026#39;: \u0026#39;unigram\u0026#39;, # Specifies the Unigram language model algorithm \u0026#39;character_coverage\u0026#39;: 0.9995, # Percentage of characters covered by the model (0.9995 is standard for Latin scripts) \u0026#39;byte_fallback\u0026#39;: True, # Enables mapping unknown characters to UTF-8 bytes to avoid \u0026lt;unk\u0026gt; tokens \u0026#39;split_digits\u0026#39;: True, # Treats each digit as an individual token (useful for numerical data) \u0026#39;add_dummy_prefix\u0026#39;: False # Prevents adding a leading space (SentencePiece default is True) } try: # 1. Train the SentencePiece model using the defined options print(\u0026#34;Starting Unigram training...\u0026#34;) spm.SentencePieceTrainer.train(**options_unigram) print(\u0026#34;Training complete. \u0026#39;unigram_model.model\u0026#39; created.\u0026#34;) # 2. Load the trained model into a processor instance for inference sp_unigram = spm.SentencePieceProcessor() sp_unigram.load(\u0026#39;unigram_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Unigram Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {sp_unigram.get_piece_size()}\u0026#34;) # 3. Define test cases to evaluate how the model handles common and rare words test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] # 4. Iterate through test sentences to visualize subword segmentation for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (Unigram) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: Converts text into subword strings (visual representation) print(f\u0026#34;Subword Tokens: {sp_unigram.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: Converts text into numerical indices for model input print(f\u0026#34;Numerical IDs: {sp_unigram.encode_as_ids(text)}\u0026#34;) except Exception as e: # Handle potential errors during training or loading (e.g., missing input file) print(f\u0026#34;An error occurred with Unigram model: {e}\u0026#34;) The output is as follows:\nUnigram Model Metadata: Total Vocab Size: 1200 --- Tokenization Test (Unigram) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ll\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;ld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311] --- Tokenization Test (Unigram) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;T\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅm\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;sta\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;ra\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ver\u0026#39;] ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"A comprehensive guide to tokenization strategies: BPE, WordPiece, Unigram, and SentencePiece.","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"A brief introduction to Vectors \u0026amp; Verbs and formatting verification.","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"\nFeedforward network (FFN) blocks are the least talked about when in context to LLMs. But that is where more than 70% of the paramters come from. Something as simple and convinient as a couple of linear transforming matrices become a burden when models start scaling in Billions of parameters. So, getting the maximum juice of a lean structure is a challenge. Plus, mostly under represented, because the FFN sub-layer acts as the primary \u0026ldquo;knowledge\u0026rdquo; processing engine, often containing the majority of a model\u0026rsquo;s total parameters.\nStandard Position Wise Feedforward Network This is as simple as it can get, two-layered fully connected network of neurons with a single activation switch, most probably a ReLU variant (for LLMs). $$ FFN(x) = max(0, W_2(W_1(x)) + b_1)W_2 + b_2 $$ Same transformation is applied for each token in parallel, making it efficient for GPU computation. For Example, in BERT model the 512 dimensions of each token are expanded to 2048 to allow for \u0026ldquo;intricate interactions\u0026rdquo; before projecting back to 512 dimensions. This part is important, in every transformer stacking of layers is possible only because the output of every matches it\u0026rsquo;s input dimensions.\nSWiGLU Sigmoid Weighted Gated Linear Unit (SWiGLU) has become the Gold Standard for feed forward networks in LLMs like Llama 3 and PaLM. The primary reason for this choice is the Swish function (as shown in the first image) instead of ReLU or GELU are as following:\nMathematical Linearity: Swish (SILU) is defined as $x\\cdot sigmoid(\\beta x)$, $\\beta$ is a learnable parameter. It is a small monotonic function that allowes small negetive values to pass through, making it mathematically linear. While GELU ($x \\cdot \\phi(x)$) is also smooth and nonmonotonic, Swish\u0026rsquo;s curve and equations have emperically improved the model\u0026rsquo;s performance.\nComputational Efficiency: SILU gradients are simpler to compute. $x/(1+exp(-x))$ compared to the standard GELU, which is an approximation of the tanh function. In the massive scale of modern LLMs these shortcuts save significant training time.\nPerformance: Research in the PaLM and Llama papers found that Swish-based Gated Linear Units consistently outperformed GELU-based versions in terms of perplexity and downstream task accuracy. $$SwiGLU(x, W, V, b, c) = (Swish(xW + b) \\otimes (xV + c))W_2$$ Where $W$ and $V$ are weight matrices for the gate and the data path, and $\\otimes$ is element-wise multiplication.\nMore Complex Filtering: Instead of using ordinary two weight matrices in standard FFN, SWiGLU uses three weight matrices (gate, date, output). This allows it to gather more complex features out of the input without drastically increasing the number of parameters. This performance boost has resulted in modern implementations being low in dimensionality of hidden layers (2/3 of input dimensions) while retaining the performance benefits of gated activations.\nZero-Gradient Avoidance: Unlike ReLU, which has a zero gradient for all negative values (the \u0026ldquo;dying ReLU\u0026rdquo; problem), SwiGLU\u0026rsquo;s smooth curve ensures that the model can still learn from negative inputs, improving training stability in deep stacks.\nMixture of Experts (MoE) This is an entire architecture rather than a single block with activation switch. It comes with it\u0026rsquo;s own challenges. It is a \u0026ldquo;divide and conquer\u0026rdquo; strategy. It became a dominant strategy for scaling insanely large models upto a trillion parameters like DeepSeek-V3 and Mistral. It is because it breaks down the direct and obvious link between model size and its speed. It is because it breaks down the direct and obvious link between model size (information capacity) and its speed (inference cost). Sparse Activation: MoE comes with a system of Manager (called Router in this case) and Experts. The Router is responsible for determining which smaller Experts (FFNs) are best suited for specific tokens. The Experts layer is actually split in different sublayers. For any token only a tiny part of the Experts are used. DeepSeek had 256 Expert layers in a single layer while activating not more than 8 for a single token. Big Scale, Small Compute:* MoE runs with a \u0026ldquo;trillion parameter intelligence\u0026rdquo; that runs at a speed of a much smaller model. DeepSeek-V3 has 671 billion total parameters, which dictates its knowledge base. However, during inference, it only uses 37 billion parameters per token. You get the reasoning performance of a massive model with the latency and cost of a significantly smaller one. Expert Specialization: Because different experts handle different tokens, they naturally specialize. DeepSeek\u0026rsquo;s analysis shows that certain experts become \u0026ldquo;specialists\u0026rdquo; in mathematics or coding, while others handle general language, without human intervention. Training Efficiency: Since you only update a fraction of the weights for each token, MoE models can process many more tokens per second during training compared to a dense model of the same total size. Some points on training: The router gives probabilities for each expert stack to be used for a token. While training, in order to avoid overfitting over specific experts a little amount of noise is added to the router\u0026rsquo;s output. This is just to skew the model from selecting a few experts in small amount of iterations. In some models only one expert is chosen and in others more than are chose and then averaged. This is not as widely used as SwiGLU and for the reasons below.\nThe RAM wall (Memory Capacity):* Even though the model computes only small percentage of parameters at once it should hold all 671B parameters in GPU memory. This is a major challenge for MoE models to run on consumer hardware or single GPUs. The model even running at a fast speed will not work without a big GPU cluster. Training Instability: MoEs struggle from load balancing issues where the router might \u0026ldquo;collapse\u0026rdquo; (sending all tokens to just one expert, ignoring the others). Models often use auxiliary losses to force balance, but this can hurt performance. DeepSeek-V3 introduced an \u0026ldquo;Auxiliary-Loss-Free\u0026rdquo; strategy that balances load dynamically without degrading the model\u0026rsquo;s accuracy. Communication Overhead: In distributed training, experts are often split across different GPUs. If the router decides a token on GPU 1 needs an expert located on GPU 4, that data must travel over the network. The Fix: DeepSeek-V3 employs \u0026ldquo;Node-Limited Routing\u0026rdquo;, which restricts the router to send tokens only to a maximum of 4 nodes, preventing network congestion. Summary: MoE is \u0026ldquo;better\u0026rdquo; if you are a major lab (like DeepSeek, Google, OpenAI) trying to maximize intelligence per dollar of training cost. It is \u0026ldquo;worse\u0026rdquo; if you are a developer with limited VRAM trying to deploy a model locally. You use MoE to push the frontier of intelligence; you use Dense SwiGLU for reliability and ease of deployment.\n","permalink":"http://localhost:1313/posts/feedforward-networks/","summary":"Exploring the hidden layers of trillion-parameter switchboards: Feedforward Neural Networks and Activation Functions.","title":"Anatomy of Trillion-Parameter Switchboards: Understanding Feedforward Blocks"},{"content":"\nWe are about to touch the holy grail of modern AI. The 2017 declaration was simple: Attention Is All You Need. But as we‚Äôve raced from GPT-1 to reasoning models, the definition of \u0026lsquo;Attention\u0026rsquo; has quietly transformed. Is the algorithm ruling the world today actually the same one we started with? Let‚Äôs look under the hood.\nüî• The Core Concept At its heart, the attention mechanism is a form of weighted aggregation. It is often described using a database retrieval analogy. Imagine you have a database of information (context).\nQuery ($Q$): What are you currently looking for? (The token being generated). Key ($K$): The indexing tag associated with each piece of information. Value ($V$): The actual information (The context token). The \u0026ldquo;Attention\u0026rdquo; is the process of matching your Query against all the Keys to determine how much \u0026ldquo;weight\u0026rdquo; to put on each Value. Attention scores are calculated in each transformer layer at every step of the sequence. This is a single unifying factor in all transformer models, from BERT to Gemini.\nüî• Standard Multi-Head Attention (MHA) The original mechanism introduced in the paper \u0026ldquo;Attention Is All You Need\u0026rdquo;.\nHere, instead of performing the Key-Query-Value (KQV) operation on the entire context only once, the attention is split into $N$ parallel heads. Each head learns a different aspect of the language (e.g., word meaning, syntax, semantic relationships, etc.). The equation is a simple culmination of dot products aided by a regularizer like Softmax.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nWhere:\n$QK^T$ computes the similarity scores between the Query and Key. $\\sqrt{d_k}$ is a scaling factor to prevent the dot product from growing too large, mitigating the vanishing/exploding gradient problem. The weighted sum is applied to the values of $V$. The Memory Bandwidth Bottleneck: \u0026gt; During inference, the model must store Key and Value matrices for all previous tokens. This is called the KV Cache. In MHA, every single head has its own unique set of Keys and Values. For large models with massive context windows, loading this massive KV cache from memory (HBM) to the computation chip (SRAM) becomes the primary bottleneck, slowing down generation significantly. Earlier models could often only generate ~8 tokens per second at inference due to this constraint.\nüî• Multi-Query Attention (MQA) This approach addresses the memory bottleneck by sharing the same set of Keys and Values across all heads while keeping separate Query heads.\nMHA: $H$ Query heads, $H$ Key heads, $H$ Value heads MQA: $H$ Query heads, 1 Key head, 1 Value head This drastically reduces the memory requirements of the KV cache by a factor of $H$ (often 8x or 16x). The GPU needs to load far less data, significantly speeding up inference. However, it does not reduce the computation (FLOPs)‚Äîso why is it faster?\nThis can be explained with the Kitchen Analogy: Imagine a kitchen (GPU) with 8 chefs (Heads).\nComputation (Math): The Chef chopping vegetables. Memory Access (Bandwidth): The Assistant running to the fridge to get ingredients. In MHA: Each chef demands unique vegetables from the fridge. The assistant runs back and forth 8 times, fetching 8 different crates. Most of the time is spent waiting for the assistant to return. This results in idle cores waiting for data to load from HBM.\nIn MQA: All chefs agree to use the same vegetables. The assistant runs back and forth only once, fetching one giant crate of potatoes and dumping it. This results in minimized memory transfer and no idle cores.\nThe Technical Reality: During text generation, the model is memory-bound, not compute-bound. The time it takes to move the KV Cache from HBM (High Bandwidth Memory) to the chip\u0026rsquo;s SRAM is the bottleneck. MQA reduces the data volume of that move by a factor of $H$.\nThe Capacity Gap: MQA is a trade-off between nuanced response and memory capacity. The model loses the ability to store different types of information, as a single $V$ matrix must now store all nuances of information flow in the language.\nThe Surgery Trick (Uptraining): Google Research figured out a way to take an existing model with MHA and convert it to MQA by averaging the weight matrices of $H$ heads. This is called Uptraining. You then train this \u0026ldquo;Frankenstein\u0026rdquo; model for ~5% of the original training steps for adjustments. This yields an MQA model that is nearly as good as the original MHA but vastly faster.\nüî• Grouped Query Attention (GQA) A \u0026ldquo;best of both worlds\u0026rdquo; solution used in modern models like Llama 2 and Llama 3.\nHow it works: GQA acts as an interpolation between MHA and MQA. Instead of having just 1 Key head (MQA) or $H$ Key heads (MHA), it divides the query heads into $G$ groups. Each group shares a single Key and Value head.\nEquation modification: If you have 8 query heads, you might create 4 groups (2 queries per group). This reduces the KV cache by 2x instead of 8x, preserving more quality than MQA while still speeding up inference.\nMHA: 8 people (heads) are working on a project. Each person has their own unique filing cabinet (KV cache). It takes forever to move 8 cabinets around. MQA: 8 people share one single filing cabinet. It\u0026rsquo;s very fast to move, but they fight over the organization and lose detail. GQA: The 8 people split into 4 teams of 2. Each team shares a cabinet. It\u0026rsquo;s a balance between speed and organizational detail. üî• Multi-Head Latent Attention (MLA) This is the most advanced mechanism for attention, used in models like DeepSeek-V3. It is a sophisticated design for massive efficiency that takes a radical approach: mathematically compressing the memory into a low-rank \u0026ldquo;latent\u0026rdquo; vector.\nKV Cache Explosion: As mentioned before, for a massive model like DeepSeek-V3 (671B parameters), storing the full Keys and Values for every token in a long conversation requires terabytes of high-speed memory (HBM), and chips spend more time waiting than churning.\nMHA (Standard) stores everything. MQA deletes redundancy but hurts quality. MLA keeps the quality of MHA but compresses the data 4x-6x smaller than even GQA.\nThe Zip File Approach: Low-Rank Joint Compression Let\u0026rsquo;s compare the math of MHA and MLA. In MHA, for every single token, we generate a unique Key ($k$) and Value ($v$) matrix for each attention head.\nThe Memory Problem: The $k$ is stored in the KV Cache. For 128 heads, for example, it calculates a massive vector of $128 \\times 128 = 16,384$ floats per token.\nFor MHA, the attention score looks like: $$\\text{Score} = q^T \\cdot k$$\nBut for MLA, instead of storing the full $k$, it projects the input down into a tiny latent vector $c_{KV}$, which is much smaller (e.g., 512 floats). $$c_{KV} = W_{DKV} \\cdot x$$\nNow here is the catch: Naive compression and decompression would mean constructing a small latent matrix for every token and deconstructing it every single time the information is required, which defeats the purpose of compression.\n$$k_{\\text{reconstructed}} = W_{UK} \\cdot c_{KV}$$ $$\\text{Score} = q^T \\cdot k_{\\text{reconstructed}}$$\nThe Matrix Absorption Trick (Optimization) Instead of reconstructing $k$ for every token, we can absorb the Up-Projection matrix ($W_{UK}$) into the Down-Projection matrix ($W_{DKV}$) and absorb the query $q$ into a new $Q$.\nFrom the original equation: $$\\text{Score} = q^T \\cdot (\\underbrace{W_{UK} \\cdot c_{KV}}_{\\text{This is } k})$$\nWe associate differently:\n$$\\text{Score} = (\\underbrace{q^T \\cdot W_{UK}}_{\\text{Absorbed Query}}) \\cdot c_{KV}$$ We change the order in which the matrix multiplication is performed.\nHow is this allowed? In linear algebra, matrix multiplication is associative:\n$$(A \\cdot B) \\cdot C = A \\cdot (B \\cdot C)$$ We can move the parentheses! Instead of grouping $(W_{UK} \\cdot c_{KV})$ to make the Key, we group $(q^T \\cdot W_{UK})$ to make a new Query.\nNote: The positional embedding information is left untouched. That rotational information from algorithms like RoPE is preserved outside the compression; it requires minimal space anyways.\n","permalink":"http://localhost:1313/posts/attention-mechanisms/","summary":"We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek\u0026rsquo;s MLA, how has the definition of \u0026lsquo;Attention\u0026rsquo; transformed?","title":"Attention Is All You Need, but exactly which one?: MHA, GQA and MLA"},{"content":"\nPositional Embeddings are the \u0026ldquo;voice\u0026rdquo; that tell Transformers where words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.\n\u0026ldquo;The dog chased the cat\u0026rdquo; and \u0026ldquo;cat chased the dog\u0026rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.\nüî• Sinusoidal Embeddings: The Odometer of Language Introduced in the seminal \u0026ldquo;Attention Is All You Need\u0026rdquo; paper, this was the industry standard for many years. This approach adds a vector to each word embedding based on its position. The vector is computed out of wavelike patterns that are periodic and smooth. Mathematically, for a position $pos$ and embedding dimension $i$:\n$$ PE_{(pos, i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $$\nThe model uses sine and cosine waves of different frequencies to capture both short and long-range dependencies. The frequency of the waves decreases as the embedding dimension increases, which helps capture long-range dependencies in the input sequence.\nThe creators wanted the model to calculate distance easily, which is why a linear function was used. The model simply rotates the current position vector using a rotation matrix to derive the next position.\n$$ \\begin{pmatrix} \\cos(k) \u0026 \\sin(k) \\\\ -\\sin(k) \u0026 \\cos(k) \\end{pmatrix} \\cdot PE(pos) \\approx PE(pos + k) $$ Low dimensions act like a fast-ticking clock (high frequency, changing rapidly) and high dimensions act like a slow clock (low frequency, changing slowly).\nThe number 10,000 is an arbitrarily chosen high number to reduce Aliasing. By choosing such a high number, the creators ensured that the wavelength of the slowest dimension is incredibly long. Consequently, the pattern practically never repeats, and each position remains unique.\nThis combination works like a mechanical odometer dial. Each subsequent dimension acts as a slower-moving gear. The model learns to read these \u0026ldquo;dials\u0026rdquo; to understand exactly where a word sits in the sequence.\nThis mitigated the challenge of exploding gradients that previous array-based positional embeddings suffered from (e.g., adding integers 1, 2, 3\u0026hellip;). This model keeps values between -1 and +1, adding stability while ensuring every position gets a distinct code.\nThe Problem: This approach pollutes the word embedding matrix (semantic information). While high-dimensional vectors are almost always orthogonal‚Äîmeaning the noise doesn\u0026rsquo;t fully destroy the word\u0026rsquo;s identity‚Äîit does not strictly guarantee purity.\nThis led to a more mature approach.\nüî• ALiBi: The Fading Streetlight Attention with Linear Biases (ALiBi) is a method where, instead of adding a vector to the embeddings at the start, the model injects positional information linearly, directly into the attention scores. If Sinusoidal Encoding asks, \u0026ldquo;Where am I on the map?\u0026rdquo;, ALiBi asks, \u0026ldquo;How much harder should I squint to see you?\u0026rdquo;\nThe biggest problem ALiBi solved is the \u0026ldquo;Invisible Wall\u0026rdquo;. If you train a Transformer on sequences of length 2048 and then feed it a book with 3000 words, it will crash or output garbage at word 2049. This is a failure of Extrapolation.\nALiBi throws away the idea of adding positional vectors entirely. Instead, it devises a \u0026ldquo;penalty\u0026rdquo; system for attention scores:\n$$ \\text{Score} = (q \\cdot k^\\top) - (m \\cdot \\text{distance}) $$\nDistance: If words A and B are neighbors, the distance is 0, so the penalty is 0. Although the distance is linear, it does not cause exploding gradients because the raw attention scores are normalized by a softmax function. This distance prioritizes nearby words (local context) over distant ones.\nSlope ($m$): If we always penalize distance equally, the model becomes myopic. ALiBi mitigates this by assigning a different slope value to each attention head:\nHead 1 (The Historian): Very low slope. The penalty grows slowly, allowing it to see far back. Head 2 (The Reader): Medium slope. Head 3 (The Myopic): Very high slope (e.g., 0.5); it effectively cannot see beyond 2-3 words. This mimics holding a lantern. Words close to you are bright, while words further away naturally fade into darkness.\nExtrapolation: If you walk 5 miles down the road (a longer sequence), your lantern works exactly the same way. The physics of light decay are constant, allowing the model to handle sequences longer than it was trained on.\nHowever, ALiBi forces a \u0026ldquo;sliding window\u0026rdquo; view where distant words eventually become invisible. This brings us to the modern standard.\nüî• RoPE: The Rotary Revolution Rotary Position Embeddings (RoPE) is the current industry standard (used in Llama 3, Mistral, PaLM) because it mathematically unifies \u0026ldquo;absolute position\u0026rdquo; with \u0026ldquo;relative position.\u0026rdquo; This method rotates the vectors in the Query and Key matrices based on their absolute position.\nIf a vector has 512 dimensions, RoPE breaks it into 256 pairs. Each pair is rotated by a different frequency:\n$$ \\theta_i = 10000^{-2i/d} $$\nThe First pair ($i = 0$) is rotated by the highest frequency. The Last pair ($i = 255$) is rotated by the lowest frequency. This spread ensures the model has \u0026ldquo;high precision\u0026rdquo; (fast rotation) to distinguish immediate neighbors, and \u0026ldquo;low precision\u0026rdquo; (slow rotation) to track global position without the pattern repeating.\nDoesn\u0026rsquo;t rotation change meaning? Yes and No. The vector for \u0026ldquo;Apple\u0026rdquo; at position 1 and \u0026ldquo;Apple\u0026rdquo; at position 256 will look completely different in terms of coordinates. However, rotation does not change the semantic strength (magnitude/norm) of the vector.\nThe Relativity Trick: Transformers compare words using dot products (angles). RoPE relies on the fact that the attention mechanism ignores the absolute rotation and only detects the difference in rotation (relative position). Thus, the \u0026ldquo;relative meaning\u0026rdquo; is encoded purely into the angle between words, leaving the semantic \u0026ldquo;magnitude\u0026rdquo; untouched.\nüî• LongRoPE: The Bifocal Lens While RoPE is powerful, it struggles when stretching context windows from 4k to 2 million tokens. The \u0026ldquo;fast\u0026rdquo; dimensions spin so rapidly over long distances that they become random noise. LongRoPE solves this using an evolutionary search algorithm to find a unique scaling factor ($\\lambda$) for each dimension.\nThe Equations Instead of rotating by the standard $\\theta_i$, LongRoPE rotates by a rescaled frequency:\n$$ \\theta\u0026rsquo;_i = \\lambda_i \\theta_i $$\nTo efficiently find these $\\lambda$ values without searching an infinite space, the algorithm enforces a monotonicity constraint based on NTK theory:\n$$ \\lambda_i \\le \\lambda_{i+1} $$\nThis ensures that low-frequency dimensions (global context) are stretched more than high-frequency dimensions (local context), creating a \u0026ldquo;bifocal\u0026rdquo; effect:\nHigh Frequencies (Local): Kept sharp ($\\lambda \\approx 1$) to maintain grammar. Low Frequencies (Global): Stretched ($\\lambda \u0026gt; 1$) to track massive distances without repeating. Not worth perusing more since the next one is much better at long context positional encoding. üî• HoPE: The Hyperbolic Slide Hyperbolic Rotary Positional Encoding (HoPE) moves from the geometry of a circle to the geometry of a hyperbola (inspired by Lorentz transformations). It was designed to fix a subtle flaw in RoPE: the \u0026ldquo;wobbly\u0026rdquo; or oscillatory nature of attention scores.\nThe Core Problem: RoPE\u0026rsquo;s \u0026ldquo;Wobble\u0026rdquo; In standard RoPE, we rotate vectors around a circle.\nThe Issue: Circles repeat. A dial at $361^\\circ$ looks identical to $1^\\circ$. Even with different frequencies, the dot product fluctuates up and down as distance increases. The Consequence: The model gets confused at long distances, potentially mistaking a word 1,000 tokens away for a neighbor just because the rotation cycles aligned, creating \u0026ldquo;noise.\u0026rdquo; The \u0026ldquo;Hyperbolic\u0026rdquo; Shift HoPE replaces trigonometric functions ($\\sin, \\cos$) with hyperbolic functions ($\\sinh, \\cosh$).\nCircular Rotation: Keeps distance from the center constant. Hyperbolic Rotation: Moves points along a hyperbola. Crucially, hyperbolic functions do not repeat; they grow or decay exponentially. By using hyperbolic rotation, HoPE guarantees that as words get further apart, the attention score decays monotonically (smoothly drops).\nThe Equations HoPE uses a hyperbolic matrix $B(\\theta, m)$ defined using the Lorentz Boost structure:\n$$ B(\\theta, m) = \\begin{pmatrix} \\cosh(m\\theta) \u0026 \\sinh(m\\theta) \\\\ \\sinh(m\\theta) \u0026 \\cosh(m\\theta) \\end{pmatrix} $$ Since hyperbolic functions grow to infinity, HoPE introduces a decay penalty ($e^{-m\\theta'}$). The final attention score simplifies beautifully to a function of relative distance: $$ \\text{Score} \\propto e^{-|m-n|(\\theta\u0026rsquo; - \\theta)} $$\nThis ensures the attention score exponentially decays as distance increases, eliminating the wobbles of standard RoPE. This the latest model that is taking up the space in the industry for extremely long context models.\n","permalink":"http://localhost:1313/posts/positional-embeddings/","summary":"From Sinusoidal to RoPE and HoPE: How Transformers learn to process word order and sequence length.","title":"The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE"},{"content":"Imagine trying to teach a computer the difference between \u0026ldquo;Apple\u0026rdquo; the fruit and \u0026ldquo;Apple\u0026rdquo; the company. To us, the distinction is intuitive. To a machine, it‚Äôs just a string of characters. How do we turn these strings into meaningful math? While early attempts like Word2Vec gave us a great start, they missed the forest for the trees‚Äîor in some cases, the twigs for the branches. Enter GloVe and FastText: two algorithms that revolutionized how machines understand the nuances of human language.\nPrevious static embedding models like Word2Vec successfully captured the local semantics of words, but they failed in addressing contexts between words that might not always appear in the same context window. Furthermore, they treated words as indivisible atomic units in continuous vector spaces, missing out on the internal structure of language.\nThe two simple but effective ideas behind GloVe and FastText are subword units and global co-occurrence. Let\u0026rsquo;s dive into them.\nGloVe: A Global Mapping Strategy Global Vectors for Word Representation (GloVe) was created at Stanford. The core idea was to train a model where word vectors are defined by how often they appear around other words across the entire corpus, rather than just locally.\nTraining Steps: Co-occurrence Matrix X: GloVe scans the entire corpus and creates a giant matrix (spreadsheet) recording the counts of how often words appear near each other. Factorization: It compresses this giant matrix into smaller, dense vectors. Objective: The function tries to equate the dot product of two word vectors with the logarithm of their probability of appearing together. The error between these values is then backpropagated to update the vectors. $$w_i \\cdot w_j + b_i + b_j = \\log(X_{ij})$$\n$w$: The word vector (what we want). $X_{ij}$: The count from our matrix. The Loss Function: We can\u0026rsquo;t satisfy the equation above perfectly for every word pair. So, we use a weighting function to minimize the squared error:\n$$J = \\sum_{i,j=1}^V f(X_{ij}) ({w_i}^T w_j + b_i + b_j - \\log(X_{ij}))^2$$\nIf we do not use the weighting function, the loss would be infinite for rare words (since $\\log(0) = \\infty$) and frequent words (like \u0026rsquo;the\u0026rsquo;) would overpower the model.\n$$ f(x) = \\begin{cases} (x/x_{max})^\\alpha \u0026amp; \\text{if } x \u0026lt; x_{max} \\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$\n(Typically $x_{max}=100$ and $\\alpha=0.75$)\nFastText: The Lego Bricks Strategy FastText was created by Facebook AI Research (FAIR). The idea was to treat words not as solid rocks, but as structures made of \u0026ldquo;Lego bricks\u0026rdquo; (subword units).\nWord2Vec struggled to create embeddings for words it hadn\u0026rsquo;t seen before (Out-Of-Vocabulary or OOV words). FastText solves this by breaking words down.\nGloVe treats a word like \u0026ldquo;Apple\u0026rdquo; as a single, atomic unit. FastText breaks \u0026ldquo;Apple\u0026rdquo; into character n-grams: \u0026lt;ap, app, ppl, ple, le\u0026gt; For a complex or new word like \u0026ldquo;Applesauce\u0026rdquo;, the model might not know the whole word, but it recognizes the Lego brick for \u0026ldquo;Apple\u0026rdquo; and the remaining bits from other subwords.\nFastText takes its inspiration from the SkipGram architecture. The vector for a word is simply the sum of the vectors of its n-gram subwords:\n$$v_w = \\sum_{g \\in G_w} z_g$$\nObjective: The objective is to predict context words given a target word.\n$$P(w_c | w_t) = \\frac{e^{s(v_{w_t} \\cdot v_{w_c})}}{\\sum_{w \\in V} e^{s(v_{w_t} \\cdot v_w)}}$$\nAnd the scoring function is defined as:\n$$s(w_t, w_c) = u_{w_t}^T (\\sum_{g \\in G_w} z_g)$$\n$u_{w_c}$: The vector for the context word (output vector). $z_g$: The vector for the n-gram $g$ (input vector). Modifications to Softmax: Calculating the full softmax for every word in the vocabulary is computationally expensive. FastText uses two methods to approximate this efficiently.\nOption A: Negative Sampling\nWe pick a few random \u0026ldquo;wrong\u0026rdquo; words every time to serve as \u0026ldquo;negative samples\u0026rdquo; and calculate the loss only for the correct word and these noise words.\n$$J = - \\log \\sigma(s(w_t, w_c)) - \\sum\\limits_{i=1}^{N} \\mathbb{E}_{n_i \\sim P_n(w)} [\\log \\sigma(-s(w_t, n_i))]$$\nThe sigmoid function $\\sigma$ squashes the score between 0 and 1. The first term pushes the probability of the correct word towards 1. The second term pushes the probability of the noise words towards 0. Option B: Hierarchical Softmax (Faster for infrequent words)\nInstead of a flat list of words, imagine the vocabulary arranged as a binary tree (Huffman Tree).\nRoot: Top of the tree. Leaves: The actual words. To calculate the probability of a word, we trace a path from the root to that leaf. At every branching node, we calculate the probability (sigmoid) of going left or right. The probability of a word $w$ is the product of the probabilities of the turns taken to reach it. Code Example from gensim.models import FastText from gensim.test.utils import common_texts # Initialize the FastText model with specific hyperparameters model = FastText( vector_size=100, # Dimensionality of the word vectors window=5, # Maximum distance between the current and predicted word within a sentence min_count=1, # Ignores all words with total frequency lower than this sg=1, # Training algorithm: 1 for skip-gram; 0 for CBOW hs=0, # If 0, and negative is non-zero, negative sampling will be used negative=5, # Number of \u0026#34;noise words\u0026#34; to be drawn for negative sampling workers=4, # Number of worker threads to train the model epochs=10, # Number of iterations over the corpus min_n=3, # Minimum length of character n-grams max_n=6 # Maximum length of character n-grams ) # Build the vocabulary from the provided text corpus model.build_vocab(common_texts) # Train the model on the corpus model.train( common_texts, total_examples=model.corpus_count, epochs=model.epochs ) # Calculate and print the cosine similarity between two words in the vocabulary print(f\u0026#34;Similarity between \u0026#39;computer\u0026#39; and \u0026#39;human\u0026#39;: {model.wv.similarity(\u0026#39;computer\u0026#39;, \u0026#39;human\u0026#39;)}\u0026#34;) # Demonstrate FastText\u0026#39;s ability to handle Out-Of-Vocabulary (OOV) words # Even if \u0026#39;computation\u0026#39; wasn\u0026#39;t in the training data, FastText constructs a vector using character n-grams oov_vector = model.wv[\u0026#39;computation\u0026#39;] # Check if a specific word exists in the model\u0026#39;s fixed vocabulary index word = \u0026#34;computer\u0026#34; is_in_vocab = word in model.wv.key_to_index print(f\u0026#34;\\nIs the word \u0026#39;{word}\u0026#39; in the model\u0026#39;s vocabulary? {is_in_vocab}\u0026#34;) ","permalink":"http://localhost:1313/posts/glove-and-fasttext/","summary":"How subword units and global co-occurrence matrices allow GloVe and FastText to capture nuances that Word2Vec missed.","title":"The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText"},{"content":" Before we had Large Language Models writing poetry, we had to teach computers that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; are related not just by spelling, but by meaning. This is the story of that breakthrough. It‚Äôs the moment we stopped counting words and started mapping their souls‚Äîturning raw text into a mathematical landscape where math can solve analogies. Welcome to the world of Word2Vec. üîÆ\nLanguage models require vector representations of words to capture semantic relationships. Before the 2010s, models used word count-based vector representations that captured only the frequency of words (e.g., One-Hot Encoding). The Problems: üöß\nThe Curse of Dimensionality Lack of meaning (Synonyms were treated as mathematically unrelated). In 2011, Mikolov et al. at Google introduced Word2Vec, which used a shallow neural network to learn vector representations of words. The best part? It could create much denser relationships between words than prior models, and it was unsupervised.\nThe training (prediction) is essentially \u0026ldquo;fake\u0026rdquo;‚Äîa pretext task. But the weight matrices we get on the side are a gold mine of fine-grained semantic relationships. ‚õèÔ∏è\nThere are two popular variants of Word2Vec:\nSkipGram Continuous Bag Of Words (CBOW) 1. Continuous Bag of Words (CBOW) üß© CBOW is a simple \u0026lsquo;fill in the blanks\u0026rsquo; machine. It takes the surrounding words as inputs and tries to predict the center word.\n\u0026ldquo;Continuous\u0026rdquo;: It operates in continuous vector space (unlike the discrete space of n-gram models).\n\u0026ldquo;Bag of words\u0026rdquo;: The order of the context does not matter. \u0026ldquo;The cat sat on the mat\u0026rdquo; and \u0026ldquo;The mat sat on the cat\u0026rdquo; are the same for CBOW. They produce exactly the same predictions.\nObjective: Maximize the probability of the target word $w_t$ given its context $w_{t-C}, \\dots, w_{t+C}$.\n2. Training Defining Dimensions:\n$V$: Vocabulary size (number of unique words in the corpus, e.g., 10,000) $N$: Dimension of the embedding space (e.g., 100) $C$: Number of context words (e.g., 2 words on left of target and 2 words on right) Step 1: Input Lookup \u0026amp; One-Hot Encoding\nMathematically, we represent each context word as a one-hot vector $x^{(c)}$. We have an Input Matrix $W_{in}$ of size $V \\times N$. For each context word (e.g., \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;), we select its corresponding row from $W_{in}$.\n$$ v_c = W_{in}^T x^{(c)} $$\nWhere $v_c$ is the $N$-dimensional vector for context word $c$.\nStep 2: Projection (The \u0026ldquo;Mixing\u0026rdquo;)\nWe combine the context vectors. In standard CBOW, we simply average them. This creates the hidden layer vector $h$.\n$$ h = \\frac{1}{2C} \\sum_{c=1}^{2C} v_c $$\n$h$ is a single vector of size $N \\times 1$. Note: This operation is linear and contains no activation function (like ReLU or Sigmoid).\nStep 3: Output Scoring (The \u0026ldquo;Dot Product\u0026rdquo;)\nWe have a second matrix, the Output Matrix $W_{out}$ of size $N \\times V$. We compute the raw score (logit) $u_j$ for every word $j$ in the vocabulary by taking the dot product of the hidden vector $h$ and the output vector $v\u0026rsquo;_j$.\n$$ u_j = {v\u0026rsquo;_j}^T h $$\nOr in matrix form:\n$$ u = W_{out}^T h $$\nResult $u$ is a vector of size $V \\times 1$.\nStep 4: Probability Conversion (Softmax)\nWe convert the raw scores $u$ into probabilities using the Softmax function. This tells us the probability that word $w_j$ is the center word.\n$$ y_j = P(w_j | \\text{context}) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 5: Loss Calculation (Cross-Entropy)\nWe compare the predicted distribution $y$ with the actual ground truth $t$ (a one-hot vector where the index of the true word is 1). The loss function $E$ is:\n$$ E = - \\sum_{j=1}^V t_j \\log(y_j) $$\nSince $t_j$ is 0 for all words except the target word (let\u0026rsquo;s call the target index $j^*$), this simplifies to:\n$$ E = - \\log(y_{j^*}) $$\n3. Backpropagation Details We need to update the weights to minimize $E$. We use the Chain Rule.\nA. Gradient w.r.t. Output Matrix ($W_{out}$)\nWe want to know how the error changes with respect to the raw score $u_j$.\n$$ \\frac{\\partial E}{\\partial u_j} = y_j - t_j $$\nLet\u0026rsquo;s call this error term $e_j$.\nIf $j$ is the target word: $e_j = y_j - 1$ (Result is negative; we boost the score). If $j$ is not target: $e_j = y_j - 0$ (Result is positive; we suppress the score). The update rule for the output vector $v\u0026rsquo;_j$ becomes:\n$$ v'_{j}(\\text{new}) = v'_{j}(\\text{old}) - \\eta \\cdot e_j \\cdot h $$ ($\\eta$ is the learning rate)\nB. Gradient w.r.t. Hidden Layer ($h$)\nWe backpropagate the error from the output layer to the hidden layer.\n$$ EH = \\sum_{j=1}^V e_j \\cdot v\u0026rsquo;_j $$\n$EH$ is an $N$-dimensional vector representing the aggregate error passed back to the projection layer.\nC. Gradient w.r.t. Input Matrix ($W_{in}$)\nSince $h$ was just an average of the input vectors, the error $EH$ is distributed to each context word\u0026rsquo;s input vector. For every word $w_c$ in the context:\n$$ v_{w_c}(\\text{new}) = v_{w_c}(\\text{old}) - \\eta \\cdot \\frac{1}{2C} \\cdot EH $$\nLet\u0026rsquo;s build a single pass through the network for a given context.\nimport torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # Define a simple Continuous Bag of Words (CBOW) style model class CBOWModel(nn.Module): def __init__(self, vocab_size, embedding_dim): super(CBOWModel, self).__init__() # The embedding layer stores the word vectors we want to learn self.embeddings = nn.Embedding(vocab_size, embedding_dim) # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word self.linear = nn.Linear(embedding_dim, vocab_size) def forward(self, inputs): # inputs: tensor of word indices for the context embeds = self.embeddings(inputs) # Aggregate context by calculating the mean of the word embeddings h = torch.mean(embeds, dim=1) # Produce logits (raw scores) for each word in the vocabulary logits = self.linear(h) return logits # Setup vocabulary and mappings word_to_ix = {\u0026#34;the\u0026#34;: 0, \u0026#34;quick\u0026#34;: 1, \u0026#34;brown\u0026#34;: 2, \u0026#34;fox\u0026#34;: 3, \u0026#34;jumps\u0026#34;: 4, \u0026#34;over\u0026#34;: 5, \u0026#34;lazy\u0026#34;: 6, \u0026#34;dog\u0026#34;: 7} ix_to_word = {v: k for k, v in word_to_ix.items()} # Configuration constants EMBEDDING_DIM = 5 VOCAB_SIZE = len(word_to_ix) LEARNING_RATE = 0.01 # Initialize the model, loss function, and optimizer model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM) loss_function = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE) # Prepare dummy training data: context words and the target word # Context: [\u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;over\u0026#34;] -\u0026gt; Target: \u0026#34;fox\u0026#34; context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long) target_idxs = torch.tensor([3], dtype=torch.long) # Perform a single optimization step model.zero_grad() # Forward pass: we reshape context to (1, -1) to simulate a batch of size 1 logits = model(context_idxs.view(1, -1)) # Calculate loss against the target word index loss = loss_function(logits, target_idxs) # Backpropagate and update weights loss.backward() optimizer.step() # Output results for the blog post print(\u0026#34;=== Model Training Snapshot ===\u0026#34;) print(f\u0026#34;Calculated Loss: {loss.item():.6f}\u0026#34;) print(f\u0026#34;\\n=== Learned Vector for \u0026#39;jumps\u0026#39; ===\u0026#34;) word_vec = model.embeddings(torch.tensor([word_to_ix[\u0026#39;jumps\u0026#39;]])) print(word_vec.detach().numpy()) print(\u0026#34;\\n=== Embedding Matrix (Weights) ===\u0026#34;) print(model.embeddings.weight.detach().numpy()) print(\u0026#34;\\n=== Linear Layer Weights ===\u0026#34;) print(model.linear.weight.detach().numpy()) Output shows single pass loss and learned vector for \u0026lsquo;jumps\u0026rsquo; word. This is a single training sample result but after multiple such passes both Embedding and linear weights should be very similar.\n=== Model Training Snapshot === Calculated Loss: 1.989656 === Learned Vector for \u0026#39;jumps\u0026#39; === [[-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775]] === Embedding Matrix (Weights) === [[ 1.66039 -0.11371879 0.6246518 0.35860053 -1.2504417 ] [-2.1847186 -0.77199775 -0.17050214 -0.38411248 -0.03913084] [ 0.11852697 0.90073633 0.8847807 0.7404524 0.900149 ] [ 0.07440972 -0.40259898 2.6246994 -0.08851447 0.02660969] [-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775] [-0.9245572 -0.5545908 0.9083091 -1.0755049 0.84047747] [-1.0237687 0.59466314 0.05621134 -0.6202532 1.3664424 ] [ 0.60998917 -1.0549186 1.6103884 0.8724912 -1.2486908 ]] === Linear Layer Weights === [[ 0.02165058 -0.28883642 0.14545658 -0.3442509 0.32704315] [-0.18731792 0.28583744 0.22635977 0.13245736 0.29019794] [ 0.3158916 -0.15826383 -0.03203773 0.16377363 -0.41457543] [-0.05080034 0.4180087 0.11228557 0.30218413 0.3025514 ] [-0.38419306 0.24475925 -0.39210224 -0.38660625 -0.2673145 ] [-0.32321444 0.12200444 -0.03569533 0.2891424 -0.07345333] [-0.33704326 0.2521956 0.31587374 0.22590035 0.29866052] [ 0.4117266 -0.44231793 0.24064957 -0.29684234 0.333821 ]] Interesting Facts About CBOW üß† The \u0026ldquo;Averaging Problem\u0026rdquo;: This is a common issue in CBOW models. The averaging and training is order-agnostic. All variants of word order in the context window will generate the same output. \u0026ldquo;Dog bit the man\u0026rdquo; and \u0026ldquo;Man bit the dog\u0026rdquo; are the same in CBOW\u0026rsquo;s eyes.\nSmoothing Effect: CBOW models are much faster to train than Skip-Gram but are generally smoother.\nRare Words: CBOW models struggle slightly with rare words because the presence of surrounding common words (like articles and prepositions) can skew the embeddings and dissolve their uniqueness.\nComputational Cost: Softmax requires summing over the entire vocabulary size, which can be computationally expensive if $V$ is very large. Solutions include Hierarchical Softmax and Negative Sampling (approximating the denominator by only checking the target word vs. 5 random noise words).\nVectors as Double Agents: Every word has a different representation in the embedding matrix and the output matrix. Most of the time $W_{out}$ is discarded, but some consider averaging both $W_{in}$ and $W_{out}$ for slightly better performance.\nLinear Relationships: The famous analogy King - Man + Woman = Queen emerges from the model because of the linear relationships between the words.\nInitialization: This is a no-brainer but worth mentioning. Initialization to zero vectors would mean no learning. We must initialize randomly.\n2. Skip-Gram (The \u0026ldquo;Burger Flip\u0026rdquo; of CBOW) üîÑ Skip-Gram flips the CBOW logic. While CBOW tries to predict the center word given the context, Skip-Gram tries to predict the scattered context given the center word.\nCore Concept: If a word like \u0026ldquo;monarch\u0026rdquo; can accurately predict words like \u0026ldquo;king\u0026rdquo;, \u0026ldquo;throne\u0026rdquo;, \u0026ldquo;crown\u0026rdquo;, \u0026ldquo;royalty\u0026rdquo;, and \u0026ldquo;power\u0026rdquo;, then it must effectively capture the concept of royalty.\nTraining Let\u0026rsquo;s define dimensions:\n$V$: Vocabulary size $D$: Embedding dimension $C$: Number of context words Step 1: Input Lookup\nUnlike CBOW, our input is just one word vector (the center word $w_t$). We grab the vector $v_c$ from the Input Matrix $W_{in}$.\n$$ h = v_{w_t} $$\nNote: There is no averaging here. The hidden layer $h$ is simply the raw vector of the center word.\nStep 2: Output Scoring (The Broadcast)\nThe model takes this single vector $h$ and compares it against the Output Matrix $W_{out}$ (the entire vocabulary). It computes a score $u_j$ for every word $j$ in the dictionary.\n$$ u = W_{out}^T h $$\nResult $u$: A vector of size $V \\times 1$ containing raw scores.\nStep 3: Probability Conversion (Softmax)\nWe apply Softmax to turn scores into probabilities.\n$$ P(w_j | w_t) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 4: The Multi-Target Loss\nHere is the major difference from CBOW. In CBOW, we had one target (the center). In Skip-Gram, we have $2C$ targets (all the surrounding words). For a sentence \u0026ldquo;The cat sat on mat\u0026rdquo; (Center: \u0026ldquo;sat\u0026rdquo;), the targets are \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;. We want to maximize the probability of all these true context words. The Loss function ($E$) sums the error for each context word:\n$$ E = - \\sum_{w_{ctx} \\in \\text{Window}} \\log P(w_{ctx} | w_t) $$\nStep 5: Backpropagation (Accumulating Gradients)\nSince one input word is responsible for predicting multiple output words, the error signals from all those context words add up.\nAt the Output Layer:\nIf \u0026ldquo;cat\u0026rdquo; was a target, the \u0026ldquo;cat\u0026rdquo; vector in $W_{out}$ gets a signal: \u0026ldquo;Move closer to \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; If \u0026ldquo;apple\u0026rdquo; was NOT a target, the \u0026ldquo;apple\u0026rdquo; vector gets a signal: \u0026ldquo;Move away from \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; At the Hidden Layer ($h$):\nThe center word \u0026ldquo;sat\u0026rdquo; receives feedback from all its neighbors simultaneously. Error signal = (Error from \u0026ldquo;The\u0026rdquo;) + (Error from \u0026ldquo;cat\u0026rdquo;) + (Error from \u0026ldquo;on\u0026rdquo;)\u0026hellip; The vector for \u0026ldquo;sat\u0026rdquo; in $W_{in}$ moves in the average direction of all its neighbors. 3. The \u0026ldquo;Computational Nightmare\u0026rdquo; \u0026amp; Negative Sampling The equations above describe Naive Softmax.\nThe Problem: If $V = 100,000$, computing the denominator $\\sum \\exp(u_k)$ takes 100,000 operations. Doing this for every word in a training set of billions of words is impossibly slow.\nThe Solution: Negative Sampling Instead of updating the entire vocabulary (1 correct word vs 99,999 wrong ones), we approximate the problem.\nPositive Pair: (sat, cat) $\\rightarrow$ Maximize probability (Label 1). Negative Pairs: We pick $K$ (e.g., 5) random words the model didn\u0026rsquo;t see, e.g., (sat, bulldozer), (sat, quantum). $\\rightarrow$ Minimize probability (Label 0). New Equation (Sigmoid instead of Softmax):\n$$ E = - \\log \\sigma({v'_{\\text{pos}}}^T v_{\\text{in}}) - \\sum_{k=1}^K \\log \\sigma(-{v'_{\\text{neg}_{k}}}^T v_{\\text{in}}) $$ Effect: We typically only update 6 vectors per step (1 pos + 5 negs) instead of 100,000.\nLet\u0026rsquo;s see this in code:\nimport torch import torch.nn as nn import torch.optim as optim class SkipGramNegativeSampling(nn.Module): \u0026#34;\u0026#34;\u0026#34; Skip-Gram with Negative Sampling (SGNS) implementation. SGNS approximates the softmax over the entire vocabulary by instead distinguishing between a real context word (positive) and K noise words (negative). \u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embedding_dim): super(SkipGramNegativeSampling, self).__init__() # Input embeddings: used when the word is the center word self.in_embed = nn.Embedding(vocab_size, embedding_dim) # Output embeddings: used when the word is a context or negative sample self.out_embed = nn.Embedding(vocab_size, embedding_dim) # Initialize weights with small values to prevent gradient saturation initrange = 0.5 / embedding_dim self.in_embed.weight.data.uniform_(-initrange, initrange) self.out_embed.weight.data.uniform_(-initrange, initrange) def forward(self, center_words, target_words, negative_words): \u0026#34;\u0026#34;\u0026#34; Computes the negative sampling loss. Args: center_words: (batch_size) target_words: (batch_size) negative_words: (batch_size, K) where K is number of negative samples \u0026#34;\u0026#34;\u0026#34; # Retrieve vectors v_c = self.in_embed(center_words) # (batch_size, embed_dim) u_o = self.out_embed(target_words) # (batch_size, embed_dim) u_n = self.out_embed(negative_words) # (batch_size, K, embed_dim) # 1. Positive Score: log(sigmoid(v_c ¬∑ u_o)) # Compute dot product: (batch, 1, dim) @ (batch, dim, 1) -\u0026gt; (batch, 1) pos_score = torch.bmm(u_o.unsqueeze(1), v_c.unsqueeze(2)).squeeze(2) pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-7) # 2. Negative Score: sum(log(sigmoid(-v_c ¬∑ u_n))) # Compute dot products for all K samples: (batch, K, dim) @ (batch, dim, 1) -\u0026gt; (batch, K) neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2) neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-7), dim=1, keepdim=True) # Total loss is the negative of the objective function loss = -(pos_loss + neg_loss) return torch.mean(loss) # --- Configuration \u0026amp; Mock Data --- VOCAB_SIZE = 100 EMBED_DIM = 10 word_to_ix = {\u0026#39;fox\u0026#39;: 0} # Example vocabulary mapping model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock inputs: 1 center word, 1 target word, 5 negative samples center_id = torch.tensor([0], dtype=torch.long) target_id = torch.tensor([1], dtype=torch.long) negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long) # Training Step model.zero_grad() loss = model(center_id, target_id, negative_ids) loss.backward() optimizer.step() # Output Results print(f\u0026#34;Loss after one step: {loss.item():.6f}\u0026#34;) word_vec = model.in_embed(torch.tensor([word_to_ix[\u0026#39;fox\u0026#39;]])) print(f\u0026#34;Vector for \u0026#39;fox\u0026#39;:\\n{word_vec.detach().numpy()}\u0026#34;) The outputs:\nLoss after one step: 4.158468 Vector for \u0026#39;fox\u0026#39;: [[ 0.04197385 0.02400733 -0.03800093 0.01672485 -0.03872231 -0.0061478 0.0121122 0.04057864 -0.036255 0.03861175]] Interesting Facts About Skip-Gram üí° Makes Rare Words Shine: Skip-Gram is more effective at capturing the meaning of rare words because it focuses on the context of the center word. In CBOW, a rare word\u0026rsquo;s vector is averaged with others, diluting its signal. In Skip-Gram, when the rare word appears as the center, it gets the full, undivided attention of the backpropagation update.\nSlow Training: This method creates more training samples. A window of size 2 creates 4 training pairs per center word, while CBOW creates only 1 training pair per center word.\nSemantic Focus: Skip-Gram puts more emphasis on the semantic relationship between words. It views the context as the primary signal, with the center word as the target. This makes it better at capturing the meaning of words in context. Skip-Gram tends to capture semantic relationships (King/Queen) slightly better, while CBOW captures syntactic ones (walk/walked) slightly better.\n","permalink":"http://localhost:1313/posts/word-embeddings/","summary":"Understanding the mathematics behind Word2Vec, CBOW, and Skip-Gram and how they map language to vector space.","title":"Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n5. Code Examples: We\u0026rsquo;ll use sentencepiece for tokenization and compare results for different models.\nimport sentencepiece as spm import os # Configuration for training the SentencePiece model # SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words. options = { # The source text file used for learning the vocabulary \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # The base name for the generated model (.model) and vocabulary (.vocab) files \u0026#39;model_prefix\u0026#39;: \u0026#39;bpe_model\u0026#39;, # Number of unique tokens in the final vocabulary \u0026#39;vocab_size\u0026#39;: 4000, # \u0026#39;bpe\u0026#39; (Byte Pair Encoding) merges frequent pairs of characters/sequences \u0026#39;model_type\u0026#39;: \u0026#39;bpe\u0026#39;, # Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets \u0026#39;character_coverage\u0026#39;: 0.9995, # When enabled, unknown characters are decomposed into UTF-8 bytes to avoid \u0026#39;unk\u0026#39; tokens \u0026#39;byte_fallback\u0026#39;: True, # Treats digits individually (0-9), preventing large numbers from being treated as single tokens \u0026#39;split_digits\u0026#39;: True, # Prevents adding a whitespace prefix to the first token; useful for fine-grained control \u0026#39;add_dummy_prefix\u0026#39;: False } try: print(\u0026#34;Starting the training process...\u0026#34;) # SentencePieceTrainer.train takes the dictionary of options to build the BPE model spm.SentencePieceTrainer.train(**options) print(\u0026#34;Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created.\u0026#34;) # Initialize the processor and load the newly trained model sp = spm.SentencePieceProcessor() sp.load(\u0026#39;bpe_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Model Metadata:\u0026#34;) # Retrieve the total number of tokens in the vocabulary print(f\u0026#34;Total Vocab Size: {sp.get_piece_size()}\u0026#34;) # Special tokens are used for sequence boundaries and handling unknown characters print(f\u0026#34;BOS (Beginning of Sentence) ID: {sp.bos_id()}\u0026#34;) print(f\u0026#34;EOS (End of Sentence) ID: {sp.eos_id()}\u0026#34;) print(f\u0026#34;UNK (Unknown) ID: {sp.unk_id()}\u0026#34;) print(f\u0026#34;PAD (Padding) ID: {sp.pad_id()}\u0026#34;) # Test the tokenizer on sample strings to see how it breaks down text test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: shows the actual subword units (tokens) print(f\u0026#34;Subword Tokens: {sp.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: shows the numerical mapping for each token print(f\u0026#34;Numerical IDs: {sp.encode_as_ids(text)}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred during training or processing: {e}\u0026#34;) You can see the following output:\nStarting training... Starting the training process... Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created. ------------------------------ Model Metadata: Total Vocab Size: 4000 BOS (Beginning of Sentence) ID: 1 EOS (End of Sentence) ID: 2 UNK (Unknown) ID: 0 PAD (Padding) ID: -1 --- Tokenization Test --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;el\u0026#39;, \u0026#39;lo\u0026#39;, \u0026#39;‚ñÅW\u0026#39;, \u0026#39;orld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 380, 309, 476, 619, 36, 3924, 3964, 3959, 3993, 3978, 3983, 57, 3975, 3976, 60, 3974] --- Tokenization Test --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅmo\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;inst\u0026#39;, \u0026#39;ag\u0026#39;, \u0026#39;ram\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅever\u0026#39;] Numerical IDs: [1894, 304, 3284, 376, 321, 313, 312, 1008, 3313, 1325, 428, 3937, 341, 304, 3284, 1495] The ! is fallen back to UTF bytes. and the word instagrammable is split into multiple subword tokens. Also, original text can be recreated right back by simply concatenating the tokens. And replacing underscore by whitespace. Now we can try Unigram model. and see the same sentences tokenized differently.\nfrom tokenizers import Tokenizer from tokenizers.models import WordPiece from tokenizers.trainers import WordPieceTrainer from tokenizers.pre_tokenizers import Whitespace try: # 1. Initialize the WordPiece Tokenizer # We specify the [UNK] token for handling words not found in the vocabulary. tokenizer = Tokenizer(WordPiece(unk_token=\u0026#34;[UNK]\u0026#34;)) # 2. Configure Pre-tokenization # Before the subword algorithm runs, we need to split the raw text into words. # Whitespace splitting is the standard first step for most English NLP tasks. tokenizer.pre_tokenizer = Whitespace() # 3. Initialize the Trainer # We define our target vocabulary size and the special tokens required for # downstream tasks (like BERT\u0026#39;s [CLS] for classification or [SEP] for separators). trainer = WordPieceTrainer( vocab_size=4000, special_tokens=[\u0026#34;[UNK]\u0026#34;, \u0026#34;[CLS]\u0026#34;, \u0026#34;[SEP]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[MASK]\u0026#34;] ) # 4. Train the Model # The tokenizer scans the training file to build a vocabulary of the most # frequent subword units. tokenizer.train(files=[\u0026#34;train_data.txt\u0026#34;], trainer=trainer) # 5. Persist the Model # Save the configuration and vocabulary to a JSON file for future inference. tokenizer.save(\u0026#34;wordpiece.json\u0026#34;) print(\u0026#34;Training complete. \u0026#39;wordpiece.json\u0026#39; created.\u0026#34;) # 6. Metadata Inspection print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;WordPiece Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {tokenizer.get_vocab_size()}\u0026#34;) # 7. Testing Subword Tokenization # WordPiece shines at handling rare words by breaking them into meaningful chunks. test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (WordPiece) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # Encode converts raw text into a Tokenizer object containing tokens and IDs output = tokenizer.encode(text) # \u0026#39;tokens\u0026#39; shows the subword breakdown (e.g., \u0026#39;un\u0026#39;, \u0026#39;##insta\u0026#39;, etc.) print(f\u0026#34;Subword Tokens: {output.tokens}\u0026#34;) # \u0026#39;ids\u0026#39; are the numerical indices mapped to the vocabulary print(f\u0026#34;Numerical IDs: {output.ids}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred with WordPiece model: {e}\u0026#34;) You can see the following output:\nWordPiece Model Metadata: Total Vocab Size: 2609 --- Tokenization Test (WordPiece) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;H\u0026#39;, \u0026#39;##el\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;##or\u0026#39;, \u0026#39;##ld\u0026#39;, \u0026#39;[UNK]\u0026#39;, \u0026#39;[UNK]\u0026#39;] Numerical IDs: [37, 180, 214, 52, 162, 418, 0, 0] --- Tokenization Test (WordPiece) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;##os\u0026#39;, \u0026#39;##t\u0026#39;, \u0026#39;un\u0026#39;, \u0026#39;##ins\u0026#39;, \u0026#39;##ta\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;##ra\u0026#39;, \u0026#39;##m\u0026#39;, \u0026#39;##ma\u0026#39;, \u0026#39;##ble\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;ever\u0026#39;] Numerical IDs: [691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240] We can see the special character ## in the output. We can also see the breakdowns not having any semantic meaning. They are likelihood based. We can try the Unigram model now.\n# \u0026#39;Unigram\u0026#39; is the default and usually recommended over BPE in SentencePiece. options_unigram = { \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # Path to the raw text file for training \u0026#39;model_prefix\u0026#39;: \u0026#39;unigram_model\u0026#39;, # Prefix for the output .model and .vocab files \u0026#39;vocab_size\u0026#39;: 1200, # Desired size of the final vocabulary \u0026#39;model_type\u0026#39;: \u0026#39;unigram\u0026#39;, # Specifies the Unigram language model algorithm \u0026#39;character_coverage\u0026#39;: 0.9995, # Percentage of characters covered by the model (0.9995 is standard for Latin scripts) \u0026#39;byte_fallback\u0026#39;: True, # Enables mapping unknown characters to UTF-8 bytes to avoid \u0026lt;unk\u0026gt; tokens \u0026#39;split_digits\u0026#39;: True, # Treats each digit as an individual token (useful for numerical data) \u0026#39;add_dummy_prefix\u0026#39;: False # Prevents adding a leading space (SentencePiece default is True) } try: # 1. Train the SentencePiece model using the defined options print(\u0026#34;Starting Unigram training...\u0026#34;) spm.SentencePieceTrainer.train(**options_unigram) print(\u0026#34;Training complete. \u0026#39;unigram_model.model\u0026#39; created.\u0026#34;) # 2. Load the trained model into a processor instance for inference sp_unigram = spm.SentencePieceProcessor() sp_unigram.load(\u0026#39;unigram_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Unigram Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {sp_unigram.get_piece_size()}\u0026#34;) # 3. Define test cases to evaluate how the model handles common and rare words test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] # 4. Iterate through test sentences to visualize subword segmentation for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (Unigram) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: Converts text into subword strings (visual representation) print(f\u0026#34;Subword Tokens: {sp_unigram.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: Converts text into numerical indices for model input print(f\u0026#34;Numerical IDs: {sp_unigram.encode_as_ids(text)}\u0026#34;) except Exception as e: # Handle potential errors during training or loading (e.g., missing input file) print(f\u0026#34;An error occurred with Unigram model: {e}\u0026#34;) The output is as follows:\nUnigram Model Metadata: Total Vocab Size: 1200 --- Tokenization Test (Unigram) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ll\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;ld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311] --- Tokenization Test (Unigram) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;T\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅm\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;sta\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;ra\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ver\u0026#39;] ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"A comprehensive guide to tokenization strategies: BPE, WordPiece, Unigram, and SentencePiece.","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"A brief introduction to Vectors \u0026amp; Verbs and formatting verification.","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"\nFeedforward network (FFN) blocks are the least talked about when in context to LLMs. But that is where more than 70% of the paramters come from. Something as simple and convinient as a couple of linear transforming matrices become a burden when models start scaling in Billions of parameters. So, getting the maximum juice of a lean structure is a challenge. Plus, mostly under represented, because the FFN sub-layer acts as the primary \u0026ldquo;knowledge\u0026rdquo; processing engine, often containing the majority of a model\u0026rsquo;s total parameters.\nStandard Position Wise Feedforward Network This is as simple as it can get, two-layered fully connected network of neurons with a single activation switch, most probably a ReLU variant (for LLMs). $$ FFN(x) = max(0, W_2(W_1(x)) + b_1)W_2 + b_2 $$ Same transformation is applied for each token in parallel, making it efficient for GPU computation. For Example, in BERT model the 512 dimensions of each token are expanded to 2048 to allow for \u0026ldquo;intricate interactions\u0026rdquo; before projecting back to 512 dimensions. This part is important, in every transformer stacking of layers is possible only because the output of every matches it\u0026rsquo;s input dimensions.\nSWiGLU Sigmoid Weighted Gated Linear Unit (SWiGLU) has become the Gold Standard for feed forward networks in LLMs like Llama 3 and PaLM. The primary reason for this choice is the Swish function (as shown in the first image) instead of ReLU or GELU are as following:\nMathematical Linearity: Swish (SILU) is defined as $x\\cdot sigmoid(\\beta x)$, $\\beta$ is a learnable parameter. It is a small monotonic function that allowes small negetive values to pass through, making it mathematically linear. While GELU ($x \\cdot \\phi(x)$) is also smooth and nonmonotonic, Swish\u0026rsquo;s curve and equations have emperically improved the model\u0026rsquo;s performance.\nComputational Efficiency: SILU gradients are simpler to compute. $x/(1+exp(-x))$ compared to the standard GELU, which is an approximation of the tanh function. In the massive scale of modern LLMs these shortcuts save significant training time.\nPerformance: Research in the PaLM and Llama papers found that Swish-based Gated Linear Units consistently outperformed GELU-based versions in terms of perplexity and downstream task accuracy. $$SwiGLU(x, W, V, b, c) = (Swish(xW + b) \\otimes (xV + c))W_2$$ Where $W$ and $V$ are weight matrices for the gate and the data path, and $\\otimes$ is element-wise multiplication.\nMore Complex Filtering: Instead of using ordinary two weight matrices in standard FFN, SWiGLU uses three weight matrices (gate, date, output). This allows it to gather more complex features out of the input without drastically increasing the number of parameters. This performance boost has resulted in modern implementations being low in dimensionality of hidden layers (2/3 of input dimensions) while retaining the performance benefits of gated activations.\nZero-Gradient Avoidance: Unlike ReLU, which has a zero gradient for all negative values (the \u0026ldquo;dying ReLU\u0026rdquo; problem), SwiGLU\u0026rsquo;s smooth curve ensures that the model can still learn from negative inputs, improving training stability in deep stacks.\nMixture of Experts (MoE) This is an entire architecture rather than a single block with activation switch. It comes with it\u0026rsquo;s own challenges. It is a \u0026ldquo;divide and conquer\u0026rdquo; strategy. It became a dominant strategy for scaling insanely large models upto a trillion parameters like DeepSeek-V3 and Mistral. It is because it breaks down the direct and obvious link between model size and its speed. It is because it breaks down the direct and obvious link between model size (information capacity) and its speed (inference cost). Sparse Activation: MoE comes with a system of Manager (called Router in this case) and Experts. The Router is responsible for determining which smaller Experts (FFNs) are best suited for specific tokens. The Experts layer is actually split in different sublayers. For any token only a tiny part of the Experts are used. DeepSeek had 256 Expert layers in a single layer while activating not more than 8 for a single token. Big Scale, Small Compute:* MoE runs with a \u0026ldquo;trillion parameter intelligence\u0026rdquo; that runs at a speed of a much smaller model. DeepSeek-V3 has 671 billion total parameters, which dictates its knowledge base. However, during inference, it only uses 37 billion parameters per token. You get the reasoning performance of a massive model with the latency and cost of a significantly smaller one. Expert Specialization: Because different experts handle different tokens, they naturally specialize. DeepSeek\u0026rsquo;s analysis shows that certain experts become \u0026ldquo;specialists\u0026rdquo; in mathematics or coding, while others handle general language, without human intervention. Training Efficiency: Since you only update a fraction of the weights for each token, MoE models can process many more tokens per second during training compared to a dense model of the same total size. Some points on training: The router gives probabilities for each expert stack to be used for a token. While training, in order to avoid overfitting over specific experts a little amount of noise is added to the router\u0026rsquo;s output. This is just to skew the model from selecting a few experts in small amount of iterations. In some models only one expert is chosen and in others more than are chose and then averaged. This is not as widely used as SwiGLU and for the reasons below.\nThe RAM wall (Memory Capacity):* Even though the model computes only small percentage of parameters at once it should hold all 671B parameters in GPU memory. This is a major challenge for MoE models to run on consumer hardware or single GPUs. The model even running at a fast speed will not work without a big GPU cluster. Training Instability: MoEs struggle from load balancing issues where the router might \u0026ldquo;collapse\u0026rdquo; (sending all tokens to just one expert, ignoring the others). Models often use auxiliary losses to force balance, but this can hurt performance. DeepSeek-V3 introduced an \u0026ldquo;Auxiliary-Loss-Free\u0026rdquo; strategy that balances load dynamically without degrading the model\u0026rsquo;s accuracy. Communication Overhead: In distributed training, experts are often split across different GPUs. If the router decides a token on GPU 1 needs an expert located on GPU 4, that data must travel over the network. The Fix: DeepSeek-V3 employs \u0026ldquo;Node-Limited Routing\u0026rdquo;, which restricts the router to send tokens only to a maximum of 4 nodes, preventing network congestion. Summary: MoE is \u0026ldquo;better\u0026rdquo; if you are a major lab (like DeepSeek, Google, OpenAI) trying to maximize intelligence per dollar of training cost. It is \u0026ldquo;worse\u0026rdquo; if you are a developer with limited VRAM trying to deploy a model locally. You use MoE to push the frontier of intelligence; you use Dense SwiGLU for reliability and ease of deployment.\n","permalink":"http://localhost:1313/posts/feedforward-networks/","summary":"Exploring the hidden layers of trillion-parameter switchboards: Feedforward Neural Networks and Activation Functions.","title":"Anatomy of Trillion-Parameter Switchboards: Understanding Feedforward Blocks"},{"content":"\nWe are about to touch the holy grail of modern AI. The 2017 declaration was simple: Attention Is All You Need. But as we‚Äôve raced from GPT-1 to reasoning models, the definition of \u0026lsquo;Attention\u0026rsquo; has quietly transformed. Is the algorithm ruling the world today actually the same one we started with? Let‚Äôs look under the hood.\nüî• The Core Concept At its heart, the attention mechanism is a form of weighted aggregation. It is often described using a database retrieval analogy. Imagine you have a database of information (context).\nQuery ($Q$): What are you currently looking for? (The token being generated). Key ($K$): The indexing tag associated with each piece of information. Value ($V$): The actual information (The context token). The \u0026ldquo;Attention\u0026rdquo; is the process of matching your Query against all the Keys to determine how much \u0026ldquo;weight\u0026rdquo; to put on each Value. Attention scores are calculated in each transformer layer at every step of the sequence. This is a single unifying factor in all transformer models, from BERT to Gemini.\nüî• Standard Multi-Head Attention (MHA) The original mechanism introduced in the paper \u0026ldquo;Attention Is All You Need\u0026rdquo;.\nHere, instead of performing the Key-Query-Value (KQV) operation on the entire context only once, the attention is split into $N$ parallel heads. Each head learns a different aspect of the language (e.g., word meaning, syntax, semantic relationships, etc.). The equation is a simple culmination of dot products aided by a regularizer like Softmax.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nWhere:\n$QK^T$ computes the similarity scores between the Query and Key. $\\sqrt{d_k}$ is a scaling factor to prevent the dot product from growing too large, mitigating the vanishing/exploding gradient problem. The weighted sum is applied to the values of $V$. The Memory Bandwidth Bottleneck: \u0026gt; During inference, the model must store Key and Value matrices for all previous tokens. This is called the KV Cache. In MHA, every single head has its own unique set of Keys and Values. For large models with massive context windows, loading this massive KV cache from memory (HBM) to the computation chip (SRAM) becomes the primary bottleneck, slowing down generation significantly. Earlier models could often only generate ~8 tokens per second at inference due to this constraint.\nüî• Multi-Query Attention (MQA) This approach addresses the memory bottleneck by sharing the same set of Keys and Values across all heads while keeping separate Query heads.\nMHA: $H$ Query heads, $H$ Key heads, $H$ Value heads MQA: $H$ Query heads, 1 Key head, 1 Value head This drastically reduces the memory requirements of the KV cache by a factor of $H$ (often 8x or 16x). The GPU needs to load far less data, significantly speeding up inference. However, it does not reduce the computation (FLOPs)‚Äîso why is it faster?\nThis can be explained with the Kitchen Analogy: Imagine a kitchen (GPU) with 8 chefs (Heads).\nComputation (Math): The Chef chopping vegetables. Memory Access (Bandwidth): The Assistant running to the fridge to get ingredients. In MHA: Each chef demands unique vegetables from the fridge. The assistant runs back and forth 8 times, fetching 8 different crates. Most of the time is spent waiting for the assistant to return. This results in idle cores waiting for data to load from HBM.\nIn MQA: All chefs agree to use the same vegetables. The assistant runs back and forth only once, fetching one giant crate of potatoes and dumping it. This results in minimized memory transfer and no idle cores.\nThe Technical Reality: During text generation, the model is memory-bound, not compute-bound. The time it takes to move the KV Cache from HBM (High Bandwidth Memory) to the chip\u0026rsquo;s SRAM is the bottleneck. MQA reduces the data volume of that move by a factor of $H$.\nThe Capacity Gap: MQA is a trade-off between nuanced response and memory capacity. The model loses the ability to store different types of information, as a single $V$ matrix must now store all nuances of information flow in the language.\nThe Surgery Trick (Uptraining): Google Research figured out a way to take an existing model with MHA and convert it to MQA by averaging the weight matrices of $H$ heads. This is called Uptraining. You then train this \u0026ldquo;Frankenstein\u0026rdquo; model for ~5% of the original training steps for adjustments. This yields an MQA model that is nearly as good as the original MHA but vastly faster.\nüî• Grouped Query Attention (GQA) A \u0026ldquo;best of both worlds\u0026rdquo; solution used in modern models like Llama 2 and Llama 3.\nHow it works: GQA acts as an interpolation between MHA and MQA. Instead of having just 1 Key head (MQA) or $H$ Key heads (MHA), it divides the query heads into $G$ groups. Each group shares a single Key and Value head.\nEquation modification: If you have 8 query heads, you might create 4 groups (2 queries per group). This reduces the KV cache by 2x instead of 8x, preserving more quality than MQA while still speeding up inference.\nMHA: 8 people (heads) are working on a project. Each person has their own unique filing cabinet (KV cache). It takes forever to move 8 cabinets around. MQA: 8 people share one single filing cabinet. It\u0026rsquo;s very fast to move, but they fight over the organization and lose detail. GQA: The 8 people split into 4 teams of 2. Each team shares a cabinet. It\u0026rsquo;s a balance between speed and organizational detail. üî• Multi-Head Latent Attention (MLA) This is the most advanced mechanism for attention, used in models like DeepSeek-V3. It is a sophisticated design for massive efficiency that takes a radical approach: mathematically compressing the memory into a low-rank \u0026ldquo;latent\u0026rdquo; vector.\nKV Cache Explosion: As mentioned before, for a massive model like DeepSeek-V3 (671B parameters), storing the full Keys and Values for every token in a long conversation requires terabytes of high-speed memory (HBM), and chips spend more time waiting than churning.\nMHA (Standard) stores everything. MQA deletes redundancy but hurts quality. MLA keeps the quality of MHA but compresses the data 4x-6x smaller than even GQA.\nThe Zip File Approach: Low-Rank Joint Compression Let\u0026rsquo;s compare the math of MHA and MLA. In MHA, for every single token, we generate a unique Key ($k$) and Value ($v$) matrix for each attention head.\nThe Memory Problem: The $k$ is stored in the KV Cache. For 128 heads, for example, it calculates a massive vector of $128 \\times 128 = 16,384$ floats per token.\nFor MHA, the attention score looks like: $$\\text{Score} = q^T \\cdot k$$\nBut for MLA, instead of storing the full $k$, it projects the input down into a tiny latent vector $c_{KV}$, which is much smaller (e.g., 512 floats). $$c_{KV} = W_{DKV} \\cdot x$$\nNow here is the catch: Naive compression and decompression would mean constructing a small latent matrix for every token and deconstructing it every single time the information is required, which defeats the purpose of compression.\n$$k_{\\text{reconstructed}} = W_{UK} \\cdot c_{KV}$$ $$\\text{Score} = q^T \\cdot k_{\\text{reconstructed}}$$\nThe Matrix Absorption Trick (Optimization) Instead of reconstructing $k$ for every token, we can absorb the Up-Projection matrix ($W_{UK}$) into the Down-Projection matrix ($W_{DKV}$) and absorb the query $q$ into a new $Q$.\nFrom the original equation: $$\\text{Score} = q^T \\cdot (\\underbrace{W_{UK} \\cdot c_{KV}}_{\\text{This is } k})$$\nWe associate differently:\n$$\\text{Score} = (\\underbrace{q^T \\cdot W_{UK}}_{\\text{Absorbed Query}}) \\cdot c_{KV}$$ We change the order in which the matrix multiplication is performed.\nHow is this allowed? In linear algebra, matrix multiplication is associative:\n$$(A \\cdot B) \\cdot C = A \\cdot (B \\cdot C)$$ We can move the parentheses! Instead of grouping $(W_{UK} \\cdot c_{KV})$ to make the Key, we group $(q^T \\cdot W_{UK})$ to make a new Query.\nNote: The positional embedding information is left untouched. That rotational information from algorithms like RoPE is preserved outside the compression; it requires minimal space anyways.\n","permalink":"http://localhost:1313/posts/attention-mechanisms/","summary":"We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek\u0026rsquo;s MLA, how has the definition of \u0026lsquo;Attention\u0026rsquo; transformed?","title":"Attention Is All You Need, but exactly which one?: MHA, GQA and MLA"},{"content":"\nPositional Embeddings are the \u0026ldquo;voice\u0026rdquo; that tell Transformers where words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.\n\u0026ldquo;The dog chased the cat\u0026rdquo; and \u0026ldquo;cat chased the dog\u0026rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.\nüî• Sinusoidal Embeddings: The Odometer of Language Introduced in the seminal \u0026ldquo;Attention Is All You Need\u0026rdquo; paper, this was the industry standard for many years. This approach adds a vector to each word embedding based on its position. The vector is computed out of wavelike patterns that are periodic and smooth. Mathematically, for a position $pos$ and embedding dimension $i$:\n$$ PE_{(pos, i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $$\nThe model uses sine and cosine waves of different frequencies to capture both short and long-range dependencies. The frequency of the waves decreases as the embedding dimension increases, which helps capture long-range dependencies in the input sequence.\nThe creators wanted the model to calculate distance easily, which is why a linear function was used. The model simply rotates the current position vector using a rotation matrix to derive the next position.\n$$ \\begin{pmatrix} \\cos(k) \u0026 \\sin(k) \\\\ -\\sin(k) \u0026 \\cos(k) \\end{pmatrix} \\cdot PE(pos) \\approx PE(pos + k) $$ Low dimensions act like a fast-ticking clock (high frequency, changing rapidly) and high dimensions act like a slow clock (low frequency, changing slowly).\nThe number 10,000 is an arbitrarily chosen high number to reduce Aliasing. By choosing such a high number, the creators ensured that the wavelength of the slowest dimension is incredibly long. Consequently, the pattern practically never repeats, and each position remains unique.\nThis combination works like a mechanical odometer dial. Each subsequent dimension acts as a slower-moving gear. The model learns to read these \u0026ldquo;dials\u0026rdquo; to understand exactly where a word sits in the sequence.\nThis mitigated the challenge of exploding gradients that previous array-based positional embeddings suffered from (e.g., adding integers 1, 2, 3\u0026hellip;). This model keeps values between -1 and +1, adding stability while ensuring every position gets a distinct code.\nThe Problem: This approach pollutes the word embedding matrix (semantic information). While high-dimensional vectors are almost always orthogonal‚Äîmeaning the noise doesn\u0026rsquo;t fully destroy the word\u0026rsquo;s identity‚Äîit does not strictly guarantee purity.\nThis led to a more mature approach.\nüî• ALiBi: The Fading Streetlight Attention with Linear Biases (ALiBi) is a method where, instead of adding a vector to the embeddings at the start, the model injects positional information linearly, directly into the attention scores. If Sinusoidal Encoding asks, \u0026ldquo;Where am I on the map?\u0026rdquo;, ALiBi asks, \u0026ldquo;How much harder should I squint to see you?\u0026rdquo;\nThe biggest problem ALiBi solved is the \u0026ldquo;Invisible Wall\u0026rdquo;. If you train a Transformer on sequences of length 2048 and then feed it a book with 3000 words, it will crash or output garbage at word 2049. This is a failure of Extrapolation.\nALiBi throws away the idea of adding positional vectors entirely. Instead, it devises a \u0026ldquo;penalty\u0026rdquo; system for attention scores:\n$$ \\text{Score} = (q \\cdot k^\\top) - (m \\cdot \\text{distance}) $$\nDistance: If words A and B are neighbors, the distance is 0, so the penalty is 0. Although the distance is linear, it does not cause exploding gradients because the raw attention scores are normalized by a softmax function. This distance prioritizes nearby words (local context) over distant ones.\nSlope ($m$): If we always penalize distance equally, the model becomes myopic. ALiBi mitigates this by assigning a different slope value to each attention head:\nHead 1 (The Historian): Very low slope. The penalty grows slowly, allowing it to see far back. Head 2 (The Reader): Medium slope. Head 3 (The Myopic): Very high slope (e.g., 0.5); it effectively cannot see beyond 2-3 words. This mimics holding a lantern. Words close to you are bright, while words further away naturally fade into darkness.\nExtrapolation: If you walk 5 miles down the road (a longer sequence), your lantern works exactly the same way. The physics of light decay are constant, allowing the model to handle sequences longer than it was trained on.\nHowever, ALiBi forces a \u0026ldquo;sliding window\u0026rdquo; view where distant words eventually become invisible. This brings us to the modern standard.\nüî• RoPE: The Rotary Revolution Rotary Position Embeddings (RoPE) is the current industry standard (used in Llama 3, Mistral, PaLM) because it mathematically unifies \u0026ldquo;absolute position\u0026rdquo; with \u0026ldquo;relative position.\u0026rdquo; This method rotates the vectors in the Query and Key matrices based on their absolute position.\nIf a vector has 512 dimensions, RoPE breaks it into 256 pairs. Each pair is rotated by a different frequency:\n$$ \\theta_i = 10000^{-2i/d} $$\nThe First pair ($i = 0$) is rotated by the highest frequency. The Last pair ($i = 255$) is rotated by the lowest frequency. This spread ensures the model has \u0026ldquo;high precision\u0026rdquo; (fast rotation) to distinguish immediate neighbors, and \u0026ldquo;low precision\u0026rdquo; (slow rotation) to track global position without the pattern repeating.\nDoesn\u0026rsquo;t rotation change meaning? Yes and No. The vector for \u0026ldquo;Apple\u0026rdquo; at position 1 and \u0026ldquo;Apple\u0026rdquo; at position 256 will look completely different in terms of coordinates. However, rotation does not change the semantic strength (magnitude/norm) of the vector.\nThe Relativity Trick: Transformers compare words using dot products (angles). RoPE relies on the fact that the attention mechanism ignores the absolute rotation and only detects the difference in rotation (relative position). Thus, the \u0026ldquo;relative meaning\u0026rdquo; is encoded purely into the angle between words, leaving the semantic \u0026ldquo;magnitude\u0026rdquo; untouched.\nüî• LongRoPE: The Bifocal Lens While RoPE is powerful, it struggles when stretching context windows from 4k to 2 million tokens. The \u0026ldquo;fast\u0026rdquo; dimensions spin so rapidly over long distances that they become random noise. LongRoPE solves this using an evolutionary search algorithm to find a unique scaling factor ($\\lambda$) for each dimension.\nThe Equations Instead of rotating by the standard $\\theta_i$, LongRoPE rotates by a rescaled frequency:\n$$ \\theta\u0026rsquo;_i = \\lambda_i \\theta_i $$\nTo efficiently find these $\\lambda$ values without searching an infinite space, the algorithm enforces a monotonicity constraint based on NTK theory:\n$$ \\lambda_i \\le \\lambda_{i+1} $$\nThis ensures that low-frequency dimensions (global context) are stretched more than high-frequency dimensions (local context), creating a \u0026ldquo;bifocal\u0026rdquo; effect:\nHigh Frequencies (Local): Kept sharp ($\\lambda \\approx 1$) to maintain grammar. Low Frequencies (Global): Stretched ($\\lambda \u0026gt; 1$) to track massive distances without repeating. Not worth perusing more since the next one is much better at long context positional encoding. üî• HoPE: The Hyperbolic Slide Hyperbolic Rotary Positional Encoding (HoPE) moves from the geometry of a circle to the geometry of a hyperbola (inspired by Lorentz transformations). It was designed to fix a subtle flaw in RoPE: the \u0026ldquo;wobbly\u0026rdquo; or oscillatory nature of attention scores.\nThe Core Problem: RoPE\u0026rsquo;s \u0026ldquo;Wobble\u0026rdquo; In standard RoPE, we rotate vectors around a circle.\nThe Issue: Circles repeat. A dial at $361^\\circ$ looks identical to $1^\\circ$. Even with different frequencies, the dot product fluctuates up and down as distance increases. The Consequence: The model gets confused at long distances, potentially mistaking a word 1,000 tokens away for a neighbor just because the rotation cycles aligned, creating \u0026ldquo;noise.\u0026rdquo; The \u0026ldquo;Hyperbolic\u0026rdquo; Shift HoPE replaces trigonometric functions ($\\sin, \\cos$) with hyperbolic functions ($\\sinh, \\cosh$).\nCircular Rotation: Keeps distance from the center constant. Hyperbolic Rotation: Moves points along a hyperbola. Crucially, hyperbolic functions do not repeat; they grow or decay exponentially. By using hyperbolic rotation, HoPE guarantees that as words get further apart, the attention score decays monotonically (smoothly drops).\nThe Equations HoPE uses a hyperbolic matrix $B(\\theta, m)$ defined using the Lorentz Boost structure:\n$$ B(\\theta, m) = \\begin{pmatrix} \\cosh(m\\theta) \u0026 \\sinh(m\\theta) \\\\ \\sinh(m\\theta) \u0026 \\cosh(m\\theta) \\end{pmatrix} $$ Since hyperbolic functions grow to infinity, HoPE introduces a decay penalty ($e^{-m\\theta'}$). The final attention score simplifies beautifully to a function of relative distance: $$ \\text{Score} \\propto e^{-|m-n|(\\theta\u0026rsquo; - \\theta)} $$\nThis ensures the attention score exponentially decays as distance increases, eliminating the wobbles of standard RoPE. This the latest model that is taking up the space in the industry for extremely long context models.\n","permalink":"http://localhost:1313/posts/positional-embeddings/","summary":"From Sinusoidal to RoPE and HoPE: How Transformers learn to process word order and sequence length.","title":"The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE"},{"content":"Imagine trying to teach a computer the difference between \u0026ldquo;Apple\u0026rdquo; the fruit and \u0026ldquo;Apple\u0026rdquo; the company. To us, the distinction is intuitive. To a machine, it‚Äôs just a string of characters. How do we turn these strings into meaningful math? While early attempts like Word2Vec gave us a great start, they missed the forest for the trees‚Äîor in some cases, the twigs for the branches. Enter GloVe and FastText: two algorithms that revolutionized how machines understand the nuances of human language.\nPrevious static embedding models like Word2Vec successfully captured the local semantics of words, but they failed in addressing contexts between words that might not always appear in the same context window. Furthermore, they treated words as indivisible atomic units in continuous vector spaces, missing out on the internal structure of language.\nThe two simple but effective ideas behind GloVe and FastText are subword units and global co-occurrence. Let\u0026rsquo;s dive into them.\nGloVe: A Global Mapping Strategy Global Vectors for Word Representation (GloVe) was created at Stanford. The core idea was to train a model where word vectors are defined by how often they appear around other words across the entire corpus, rather than just locally.\nTraining Steps: Co-occurrence Matrix X: GloVe scans the entire corpus and creates a giant matrix (spreadsheet) recording the counts of how often words appear near each other. Factorization: It compresses this giant matrix into smaller, dense vectors. Objective: The function tries to equate the dot product of two word vectors with the logarithm of their probability of appearing together. The error between these values is then backpropagated to update the vectors. $$w_i \\cdot w_j + b_i + b_j = \\log(X_{ij})$$\n$w$: The word vector (what we want). $X_{ij}$: The count from our matrix. The Loss Function: We can\u0026rsquo;t satisfy the equation above perfectly for every word pair. So, we use a weighting function to minimize the squared error:\n$$J = \\sum_{i,j=1}^V f(X_{ij}) ({w_i}^T w_j + b_i + b_j - \\log(X_{ij}))^2$$\nIf we do not use the weighting function, the loss would be infinite for rare words (since $\\log(0) = \\infty$) and frequent words (like \u0026rsquo;the\u0026rsquo;) would overpower the model.\n$$ f(x) = \\begin{cases} (x/x_{max})^\\alpha \u0026amp; \\text{if } x \u0026lt; x_{max} \\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$\n(Typically $x_{max}=100$ and $\\alpha=0.75$)\nFastText: The Lego Bricks Strategy FastText was created by Facebook AI Research (FAIR). The idea was to treat words not as solid rocks, but as structures made of \u0026ldquo;Lego bricks\u0026rdquo; (subword units).\nWord2Vec struggled to create embeddings for words it hadn\u0026rsquo;t seen before (Out-Of-Vocabulary or OOV words). FastText solves this by breaking words down.\nGloVe treats a word like \u0026ldquo;Apple\u0026rdquo; as a single, atomic unit. FastText breaks \u0026ldquo;Apple\u0026rdquo; into character n-grams: \u0026lt;ap, app, ppl, ple, le\u0026gt; For a complex or new word like \u0026ldquo;Applesauce\u0026rdquo;, the model might not know the whole word, but it recognizes the Lego brick for \u0026ldquo;Apple\u0026rdquo; and the remaining bits from other subwords.\nFastText takes its inspiration from the SkipGram architecture. The vector for a word is simply the sum of the vectors of its n-gram subwords:\n$$v_w = \\sum_{g \\in G_w} z_g$$\nObjective: The objective is to predict context words given a target word.\n$$P(w_c | w_t) = \\frac{e^{s(v_{w_t} \\cdot v_{w_c})}}{\\sum_{w \\in V} e^{s(v_{w_t} \\cdot v_w)}}$$\nAnd the scoring function is defined as:\n$$s(w_t, w_c) = u_{w_t}^T (\\sum_{g \\in G_w} z_g)$$\n$u_{w_c}$: The vector for the context word (output vector). $z_g$: The vector for the n-gram $g$ (input vector). Modifications to Softmax: Calculating the full softmax for every word in the vocabulary is computationally expensive. FastText uses two methods to approximate this efficiently.\nOption A: Negative Sampling\nWe pick a few random \u0026ldquo;wrong\u0026rdquo; words every time to serve as \u0026ldquo;negative samples\u0026rdquo; and calculate the loss only for the correct word and these noise words.\n$$J = - \\log \\sigma(s(w_t, w_c)) - \\sum\\limits_{i=1}^{N} \\mathbb{E}_{n_i \\sim P_n(w)} [\\log \\sigma(-s(w_t, n_i))]$$\nThe sigmoid function $\\sigma$ squashes the score between 0 and 1. The first term pushes the probability of the correct word towards 1. The second term pushes the probability of the noise words towards 0. Option B: Hierarchical Softmax (Faster for infrequent words)\nInstead of a flat list of words, imagine the vocabulary arranged as a binary tree (Huffman Tree).\nRoot: Top of the tree. Leaves: The actual words. To calculate the probability of a word, we trace a path from the root to that leaf. At every branching node, we calculate the probability (sigmoid) of going left or right. The probability of a word $w$ is the product of the probabilities of the turns taken to reach it. Code Example from gensim.models import FastText from gensim.test.utils import common_texts # Initialize the FastText model with specific hyperparameters model = FastText( vector_size=100, # Dimensionality of the word vectors window=5, # Maximum distance between the current and predicted word within a sentence min_count=1, # Ignores all words with total frequency lower than this sg=1, # Training algorithm: 1 for skip-gram; 0 for CBOW hs=0, # If 0, and negative is non-zero, negative sampling will be used negative=5, # Number of \u0026#34;noise words\u0026#34; to be drawn for negative sampling workers=4, # Number of worker threads to train the model epochs=10, # Number of iterations over the corpus min_n=3, # Minimum length of character n-grams max_n=6 # Maximum length of character n-grams ) # Build the vocabulary from the provided text corpus model.build_vocab(common_texts) # Train the model on the corpus model.train( common_texts, total_examples=model.corpus_count, epochs=model.epochs ) # Calculate and print the cosine similarity between two words in the vocabulary print(f\u0026#34;Similarity between \u0026#39;computer\u0026#39; and \u0026#39;human\u0026#39;: {model.wv.similarity(\u0026#39;computer\u0026#39;, \u0026#39;human\u0026#39;)}\u0026#34;) # Demonstrate FastText\u0026#39;s ability to handle Out-Of-Vocabulary (OOV) words # Even if \u0026#39;computation\u0026#39; wasn\u0026#39;t in the training data, FastText constructs a vector using character n-grams oov_vector = model.wv[\u0026#39;computation\u0026#39;] # Check if a specific word exists in the model\u0026#39;s fixed vocabulary index word = \u0026#34;computer\u0026#34; is_in_vocab = word in model.wv.key_to_index print(f\u0026#34;\\nIs the word \u0026#39;{word}\u0026#39; in the model\u0026#39;s vocabulary? {is_in_vocab}\u0026#34;) ","permalink":"http://localhost:1313/posts/glove-and-fasttext/","summary":"How subword units and global co-occurrence matrices allow GloVe and FastText to capture nuances that Word2Vec missed.","title":"The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText"},{"content":" Before we had Large Language Models writing poetry, we had to teach computers that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; are related not just by spelling, but by meaning. This is the story of that breakthrough. It‚Äôs the moment we stopped counting words and started mapping their souls‚Äîturning raw text into a mathematical landscape where math can solve analogies. Welcome to the world of Word2Vec. üîÆ\nLanguage models require vector representations of words to capture semantic relationships. Before the 2010s, models used word count-based vector representations that captured only the frequency of words (e.g., One-Hot Encoding). The Problems: üöß\nThe Curse of Dimensionality Lack of meaning (Synonyms were treated as mathematically unrelated). In 2011, Mikolov et al. at Google introduced Word2Vec, which used a shallow neural network to learn vector representations of words. The best part? It could create much denser relationships between words than prior models, and it was unsupervised.\nThe training (prediction) is essentially \u0026ldquo;fake\u0026rdquo;‚Äîa pretext task. But the weight matrices we get on the side are a gold mine of fine-grained semantic relationships. ‚õèÔ∏è\nThere are two popular variants of Word2Vec:\nSkipGram Continuous Bag Of Words (CBOW) 1. Continuous Bag of Words (CBOW) üß© CBOW is a simple \u0026lsquo;fill in the blanks\u0026rsquo; machine. It takes the surrounding words as inputs and tries to predict the center word.\n\u0026ldquo;Continuous\u0026rdquo;: It operates in continuous vector space (unlike the discrete space of n-gram models).\n\u0026ldquo;Bag of words\u0026rdquo;: The order of the context does not matter. \u0026ldquo;The cat sat on the mat\u0026rdquo; and \u0026ldquo;The mat sat on the cat\u0026rdquo; are the same for CBOW. They produce exactly the same predictions.\nObjective: Maximize the probability of the target word $w_t$ given its context $w_{t-C}, \\dots, w_{t+C}$.\n2. Training Defining Dimensions:\n$V$: Vocabulary size (number of unique words in the corpus, e.g., 10,000) $N$: Dimension of the embedding space (e.g., 100) $C$: Number of context words (e.g., 2 words on left of target and 2 words on right) Step 1: Input Lookup \u0026amp; One-Hot Encoding\nMathematically, we represent each context word as a one-hot vector $x^{(c)}$. We have an Input Matrix $W_{in}$ of size $V \\times N$. For each context word (e.g., \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;), we select its corresponding row from $W_{in}$.\n$$ v_c = W_{in}^T x^{(c)} $$\nWhere $v_c$ is the $N$-dimensional vector for context word $c$.\nStep 2: Projection (The \u0026ldquo;Mixing\u0026rdquo;)\nWe combine the context vectors. In standard CBOW, we simply average them. This creates the hidden layer vector $h$.\n$$ h = \\frac{1}{2C} \\sum_{c=1}^{2C} v_c $$\n$h$ is a single vector of size $N \\times 1$. Note: This operation is linear and contains no activation function (like ReLU or Sigmoid).\nStep 3: Output Scoring (The \u0026ldquo;Dot Product\u0026rdquo;)\nWe have a second matrix, the Output Matrix $W_{out}$ of size $N \\times V$. We compute the raw score (logit) $u_j$ for every word $j$ in the vocabulary by taking the dot product of the hidden vector $h$ and the output vector $v\u0026rsquo;_j$.\n$$ u_j = {v\u0026rsquo;_j}^T h $$\nOr in matrix form:\n$$ u = W_{out}^T h $$\nResult $u$ is a vector of size $V \\times 1$.\nStep 4: Probability Conversion (Softmax)\nWe convert the raw scores $u$ into probabilities using the Softmax function. This tells us the probability that word $w_j$ is the center word.\n$$ y_j = P(w_j | \\text{context}) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 5: Loss Calculation (Cross-Entropy)\nWe compare the predicted distribution $y$ with the actual ground truth $t$ (a one-hot vector where the index of the true word is 1). The loss function $E$ is:\n$$ E = - \\sum_{j=1}^V t_j \\log(y_j) $$\nSince $t_j$ is 0 for all words except the target word (let\u0026rsquo;s call the target index $j^*$), this simplifies to:\n$$ E = - \\log(y_{j^*}) $$\n3. Backpropagation Details We need to update the weights to minimize $E$. We use the Chain Rule.\nA. Gradient w.r.t. Output Matrix ($W_{out}$)\nWe want to know how the error changes with respect to the raw score $u_j$.\n$$ \\frac{\\partial E}{\\partial u_j} = y_j - t_j $$\nLet\u0026rsquo;s call this error term $e_j$.\nIf $j$ is the target word: $e_j = y_j - 1$ (Result is negative; we boost the score). If $j$ is not target: $e_j = y_j - 0$ (Result is positive; we suppress the score). The update rule for the output vector $v\u0026rsquo;_j$ becomes:\n$$ v'_{j}(\\text{new}) = v'_{j}(\\text{old}) - \\eta \\cdot e_j \\cdot h $$ ($\\eta$ is the learning rate)\nB. Gradient w.r.t. Hidden Layer ($h$)\nWe backpropagate the error from the output layer to the hidden layer.\n$$ EH = \\sum_{j=1}^V e_j \\cdot v\u0026rsquo;_j $$\n$EH$ is an $N$-dimensional vector representing the aggregate error passed back to the projection layer.\nC. Gradient w.r.t. Input Matrix ($W_{in}$)\nSince $h$ was just an average of the input vectors, the error $EH$ is distributed to each context word\u0026rsquo;s input vector. For every word $w_c$ in the context:\n$$ v_{w_c}(\\text{new}) = v_{w_c}(\\text{old}) - \\eta \\cdot \\frac{1}{2C} \\cdot EH $$\nLet\u0026rsquo;s build a single pass through the network for a given context.\nimport torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # Define a simple Continuous Bag of Words (CBOW) style model class CBOWModel(nn.Module): def __init__(self, vocab_size, embedding_dim): super(CBOWModel, self).__init__() # The embedding layer stores the word vectors we want to learn self.embeddings = nn.Embedding(vocab_size, embedding_dim) # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word self.linear = nn.Linear(embedding_dim, vocab_size) def forward(self, inputs): # inputs: tensor of word indices for the context embeds = self.embeddings(inputs) # Aggregate context by calculating the mean of the word embeddings h = torch.mean(embeds, dim=1) # Produce logits (raw scores) for each word in the vocabulary logits = self.linear(h) return logits # Setup vocabulary and mappings word_to_ix = {\u0026#34;the\u0026#34;: 0, \u0026#34;quick\u0026#34;: 1, \u0026#34;brown\u0026#34;: 2, \u0026#34;fox\u0026#34;: 3, \u0026#34;jumps\u0026#34;: 4, \u0026#34;over\u0026#34;: 5, \u0026#34;lazy\u0026#34;: 6, \u0026#34;dog\u0026#34;: 7} ix_to_word = {v: k for k, v in word_to_ix.items()} # Configuration constants EMBEDDING_DIM = 5 VOCAB_SIZE = len(word_to_ix) LEARNING_RATE = 0.01 # Initialize the model, loss function, and optimizer model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM) loss_function = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE) # Prepare dummy training data: context words and the target word # Context: [\u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;over\u0026#34;] -\u0026gt; Target: \u0026#34;fox\u0026#34; context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long) target_idxs = torch.tensor([3], dtype=torch.long) # Perform a single optimization step model.zero_grad() # Forward pass: we reshape context to (1, -1) to simulate a batch of size 1 logits = model(context_idxs.view(1, -1)) # Calculate loss against the target word index loss = loss_function(logits, target_idxs) # Backpropagate and update weights loss.backward() optimizer.step() # Output results for the blog post print(\u0026#34;=== Model Training Snapshot ===\u0026#34;) print(f\u0026#34;Calculated Loss: {loss.item():.6f}\u0026#34;) print(f\u0026#34;\\n=== Learned Vector for \u0026#39;jumps\u0026#39; ===\u0026#34;) word_vec = model.embeddings(torch.tensor([word_to_ix[\u0026#39;jumps\u0026#39;]])) print(word_vec.detach().numpy()) print(\u0026#34;\\n=== Embedding Matrix (Weights) ===\u0026#34;) print(model.embeddings.weight.detach().numpy()) print(\u0026#34;\\n=== Linear Layer Weights ===\u0026#34;) print(model.linear.weight.detach().numpy()) Output shows single pass loss and learned vector for \u0026lsquo;jumps\u0026rsquo; word. This is a single training sample result but after multiple such passes both Embedding and linear weights should be very similar.\n=== Model Training Snapshot === Calculated Loss: 1.989656 === Learned Vector for \u0026#39;jumps\u0026#39; === [[-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775]] === Embedding Matrix (Weights) === [[ 1.66039 -0.11371879 0.6246518 0.35860053 -1.2504417 ] [-2.1847186 -0.77199775 -0.17050214 -0.38411248 -0.03913084] [ 0.11852697 0.90073633 0.8847807 0.7404524 0.900149 ] [ 0.07440972 -0.40259898 2.6246994 -0.08851447 0.02660969] [-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775] [-0.9245572 -0.5545908 0.9083091 -1.0755049 0.84047747] [-1.0237687 0.59466314 0.05621134 -0.6202532 1.3664424 ] [ 0.60998917 -1.0549186 1.6103884 0.8724912 -1.2486908 ]] === Linear Layer Weights === [[ 0.02165058 -0.28883642 0.14545658 -0.3442509 0.32704315] [-0.18731792 0.28583744 0.22635977 0.13245736 0.29019794] [ 0.3158916 -0.15826383 -0.03203773 0.16377363 -0.41457543] [-0.05080034 0.4180087 0.11228557 0.30218413 0.3025514 ] [-0.38419306 0.24475925 -0.39210224 -0.38660625 -0.2673145 ] [-0.32321444 0.12200444 -0.03569533 0.2891424 -0.07345333] [-0.33704326 0.2521956 0.31587374 0.22590035 0.29866052] [ 0.4117266 -0.44231793 0.24064957 -0.29684234 0.333821 ]] Interesting Facts About CBOW üß† The \u0026ldquo;Averaging Problem\u0026rdquo;: This is a common issue in CBOW models. The averaging and training is order-agnostic. All variants of word order in the context window will generate the same output. \u0026ldquo;Dog bit the man\u0026rdquo; and \u0026ldquo;Man bit the dog\u0026rdquo; are the same in CBOW\u0026rsquo;s eyes.\nSmoothing Effect: CBOW models are much faster to train than Skip-Gram but are generally smoother.\nRare Words: CBOW models struggle slightly with rare words because the presence of surrounding common words (like articles and prepositions) can skew the embeddings and dissolve their uniqueness.\nComputational Cost: Softmax requires summing over the entire vocabulary size, which can be computationally expensive if $V$ is very large. Solutions include Hierarchical Softmax and Negative Sampling (approximating the denominator by only checking the target word vs. 5 random noise words).\nVectors as Double Agents: Every word has a different representation in the embedding matrix and the output matrix. Most of the time $W_{out}$ is discarded, but some consider averaging both $W_{in}$ and $W_{out}$ for slightly better performance.\nLinear Relationships: The famous analogy King - Man + Woman = Queen emerges from the model because of the linear relationships between the words.\nInitialization: This is a no-brainer but worth mentioning. Initialization to zero vectors would mean no learning. We must initialize randomly.\n2. Skip-Gram (The \u0026ldquo;Burger Flip\u0026rdquo; of CBOW) üîÑ Skip-Gram flips the CBOW logic. While CBOW tries to predict the center word given the context, Skip-Gram tries to predict the scattered context given the center word.\nCore Concept: If a word like \u0026ldquo;monarch\u0026rdquo; can accurately predict words like \u0026ldquo;king\u0026rdquo;, \u0026ldquo;throne\u0026rdquo;, \u0026ldquo;crown\u0026rdquo;, \u0026ldquo;royalty\u0026rdquo;, and \u0026ldquo;power\u0026rdquo;, then it must effectively capture the concept of royalty.\nTraining Let\u0026rsquo;s define dimensions:\n$V$: Vocabulary size $D$: Embedding dimension $C$: Number of context words Step 1: Input Lookup\nUnlike CBOW, our input is just one word vector (the center word $w_t$). We grab the vector $v_c$ from the Input Matrix $W_{in}$.\n$$ h = v_{w_t} $$\nNote: There is no averaging here. The hidden layer $h$ is simply the raw vector of the center word.\nStep 2: Output Scoring (The Broadcast)\nThe model takes this single vector $h$ and compares it against the Output Matrix $W_{out}$ (the entire vocabulary). It computes a score $u_j$ for every word $j$ in the dictionary.\n$$ u = W_{out}^T h $$\nResult $u$: A vector of size $V \\times 1$ containing raw scores.\nStep 3: Probability Conversion (Softmax)\nWe apply Softmax to turn scores into probabilities.\n$$ P(w_j | w_t) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 4: The Multi-Target Loss\nHere is the major difference from CBOW. In CBOW, we had one target (the center). In Skip-Gram, we have $2C$ targets (all the surrounding words). For a sentence \u0026ldquo;The cat sat on mat\u0026rdquo; (Center: \u0026ldquo;sat\u0026rdquo;), the targets are \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;. We want to maximize the probability of all these true context words. The Loss function ($E$) sums the error for each context word:\n$$ E = - \\sum_{w_{ctx} \\in \\text{Window}} \\log P(w_{ctx} | w_t) $$\nStep 5: Backpropagation (Accumulating Gradients)\nSince one input word is responsible for predicting multiple output words, the error signals from all those context words add up.\nAt the Output Layer:\nIf \u0026ldquo;cat\u0026rdquo; was a target, the \u0026ldquo;cat\u0026rdquo; vector in $W_{out}$ gets a signal: \u0026ldquo;Move closer to \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; If \u0026ldquo;apple\u0026rdquo; was NOT a target, the \u0026ldquo;apple\u0026rdquo; vector gets a signal: \u0026ldquo;Move away from \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; At the Hidden Layer ($h$):\nThe center word \u0026ldquo;sat\u0026rdquo; receives feedback from all its neighbors simultaneously. Error signal = (Error from \u0026ldquo;The\u0026rdquo;) + (Error from \u0026ldquo;cat\u0026rdquo;) + (Error from \u0026ldquo;on\u0026rdquo;)\u0026hellip; The vector for \u0026ldquo;sat\u0026rdquo; in $W_{in}$ moves in the average direction of all its neighbors. 3. The \u0026ldquo;Computational Nightmare\u0026rdquo; \u0026amp; Negative Sampling The equations above describe Naive Softmax.\nThe Problem: If $V = 100,000$, computing the denominator $\\sum \\exp(u_k)$ takes 100,000 operations. Doing this for every word in a training set of billions of words is impossibly slow.\nThe Solution: Negative Sampling Instead of updating the entire vocabulary (1 correct word vs 99,999 wrong ones), we approximate the problem.\nPositive Pair: (sat, cat) $\\rightarrow$ Maximize probability (Label 1). Negative Pairs: We pick $K$ (e.g., 5) random words the model didn\u0026rsquo;t see, e.g., (sat, bulldozer), (sat, quantum). $\\rightarrow$ Minimize probability (Label 0). New Equation (Sigmoid instead of Softmax):\n$$ E = - \\log \\sigma({v'_{\\text{pos}}}^T v_{\\text{in}}) - \\sum_{k=1}^K \\log \\sigma(-{v'_{\\text{neg}_{k}}}^T v_{\\text{in}}) $$ Effect: We typically only update 6 vectors per step (1 pos + 5 negs) instead of 100,000.\nLet\u0026rsquo;s see this in code:\nimport torch import torch.nn as nn import torch.optim as optim class SkipGramNegativeSampling(nn.Module): \u0026#34;\u0026#34;\u0026#34; Skip-Gram with Negative Sampling (SGNS) implementation. SGNS approximates the softmax over the entire vocabulary by instead distinguishing between a real context word (positive) and K noise words (negative). \u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embedding_dim): super(SkipGramNegativeSampling, self).__init__() # Input embeddings: used when the word is the center word self.in_embed = nn.Embedding(vocab_size, embedding_dim) # Output embeddings: used when the word is a context or negative sample self.out_embed = nn.Embedding(vocab_size, embedding_dim) # Initialize weights with small values to prevent gradient saturation initrange = 0.5 / embedding_dim self.in_embed.weight.data.uniform_(-initrange, initrange) self.out_embed.weight.data.uniform_(-initrange, initrange) def forward(self, center_words, target_words, negative_words): \u0026#34;\u0026#34;\u0026#34; Computes the negative sampling loss. Args: center_words: (batch_size) target_words: (batch_size) negative_words: (batch_size, K) where K is number of negative samples \u0026#34;\u0026#34;\u0026#34; # Retrieve vectors v_c = self.in_embed(center_words) # (batch_size, embed_dim) u_o = self.out_embed(target_words) # (batch_size, embed_dim) u_n = self.out_embed(negative_words) # (batch_size, K, embed_dim) # 1. Positive Score: log(sigmoid(v_c ¬∑ u_o)) # Compute dot product: (batch, 1, dim) @ (batch, dim, 1) -\u0026gt; (batch, 1) pos_score = torch.bmm(u_o.unsqueeze(1), v_c.unsqueeze(2)).squeeze(2) pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-7) # 2. Negative Score: sum(log(sigmoid(-v_c ¬∑ u_n))) # Compute dot products for all K samples: (batch, K, dim) @ (batch, dim, 1) -\u0026gt; (batch, K) neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2) neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-7), dim=1, keepdim=True) # Total loss is the negative of the objective function loss = -(pos_loss + neg_loss) return torch.mean(loss) # --- Configuration \u0026amp; Mock Data --- VOCAB_SIZE = 100 EMBED_DIM = 10 word_to_ix = {\u0026#39;fox\u0026#39;: 0} # Example vocabulary mapping model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock inputs: 1 center word, 1 target word, 5 negative samples center_id = torch.tensor([0], dtype=torch.long) target_id = torch.tensor([1], dtype=torch.long) negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long) # Training Step model.zero_grad() loss = model(center_id, target_id, negative_ids) loss.backward() optimizer.step() # Output Results print(f\u0026#34;Loss after one step: {loss.item():.6f}\u0026#34;) word_vec = model.in_embed(torch.tensor([word_to_ix[\u0026#39;fox\u0026#39;]])) print(f\u0026#34;Vector for \u0026#39;fox\u0026#39;:\\n{word_vec.detach().numpy()}\u0026#34;) The outputs:\nLoss after one step: 4.158468 Vector for \u0026#39;fox\u0026#39;: [[ 0.04197385 0.02400733 -0.03800093 0.01672485 -0.03872231 -0.0061478 0.0121122 0.04057864 -0.036255 0.03861175]] Interesting Facts About Skip-Gram üí° Makes Rare Words Shine: Skip-Gram is more effective at capturing the meaning of rare words because it focuses on the context of the center word. In CBOW, a rare word\u0026rsquo;s vector is averaged with others, diluting its signal. In Skip-Gram, when the rare word appears as the center, it gets the full, undivided attention of the backpropagation update.\nSlow Training: This method creates more training samples. A window of size 2 creates 4 training pairs per center word, while CBOW creates only 1 training pair per center word.\nSemantic Focus: Skip-Gram puts more emphasis on the semantic relationship between words. It views the context as the primary signal, with the center word as the target. This makes it better at capturing the meaning of words in context. Skip-Gram tends to capture semantic relationships (King/Queen) slightly better, while CBOW captures syntactic ones (walk/walked) slightly better.\n","permalink":"http://localhost:1313/posts/word-embeddings/","summary":"Understanding the mathematics behind Word2Vec, CBOW, and Skip-Gram and how they map language to vector space.","title":"Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n5. Code Examples: We\u0026rsquo;ll use sentencepiece for tokenization and compare results for different models.\nimport sentencepiece as spm import os # Configuration for training the SentencePiece model # SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words. options = { # The source text file used for learning the vocabulary \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # The base name for the generated model (.model) and vocabulary (.vocab) files \u0026#39;model_prefix\u0026#39;: \u0026#39;bpe_model\u0026#39;, # Number of unique tokens in the final vocabulary \u0026#39;vocab_size\u0026#39;: 4000, # \u0026#39;bpe\u0026#39; (Byte Pair Encoding) merges frequent pairs of characters/sequences \u0026#39;model_type\u0026#39;: \u0026#39;bpe\u0026#39;, # Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets \u0026#39;character_coverage\u0026#39;: 0.9995, # When enabled, unknown characters are decomposed into UTF-8 bytes to avoid \u0026#39;unk\u0026#39; tokens \u0026#39;byte_fallback\u0026#39;: True, # Treats digits individually (0-9), preventing large numbers from being treated as single tokens \u0026#39;split_digits\u0026#39;: True, # Prevents adding a whitespace prefix to the first token; useful for fine-grained control \u0026#39;add_dummy_prefix\u0026#39;: False } try: print(\u0026#34;Starting the training process...\u0026#34;) # SentencePieceTrainer.train takes the dictionary of options to build the BPE model spm.SentencePieceTrainer.train(**options) print(\u0026#34;Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created.\u0026#34;) # Initialize the processor and load the newly trained model sp = spm.SentencePieceProcessor() sp.load(\u0026#39;bpe_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Model Metadata:\u0026#34;) # Retrieve the total number of tokens in the vocabulary print(f\u0026#34;Total Vocab Size: {sp.get_piece_size()}\u0026#34;) # Special tokens are used for sequence boundaries and handling unknown characters print(f\u0026#34;BOS (Beginning of Sentence) ID: {sp.bos_id()}\u0026#34;) print(f\u0026#34;EOS (End of Sentence) ID: {sp.eos_id()}\u0026#34;) print(f\u0026#34;UNK (Unknown) ID: {sp.unk_id()}\u0026#34;) print(f\u0026#34;PAD (Padding) ID: {sp.pad_id()}\u0026#34;) # Test the tokenizer on sample strings to see how it breaks down text test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: shows the actual subword units (tokens) print(f\u0026#34;Subword Tokens: {sp.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: shows the numerical mapping for each token print(f\u0026#34;Numerical IDs: {sp.encode_as_ids(text)}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred during training or processing: {e}\u0026#34;) You can see the following output:\nStarting training... Starting the training process... Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created. ------------------------------ Model Metadata: Total Vocab Size: 4000 BOS (Beginning of Sentence) ID: 1 EOS (End of Sentence) ID: 2 UNK (Unknown) ID: 0 PAD (Padding) ID: -1 --- Tokenization Test --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;el\u0026#39;, \u0026#39;lo\u0026#39;, \u0026#39;‚ñÅW\u0026#39;, \u0026#39;orld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 380, 309, 476, 619, 36, 3924, 3964, 3959, 3993, 3978, 3983, 57, 3975, 3976, 60, 3974] --- Tokenization Test --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅmo\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;inst\u0026#39;, \u0026#39;ag\u0026#39;, \u0026#39;ram\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅever\u0026#39;] Numerical IDs: [1894, 304, 3284, 376, 321, 313, 312, 1008, 3313, 1325, 428, 3937, 341, 304, 3284, 1495] The ! is fallen back to UTF bytes. and the word instagrammable is split into multiple subword tokens. Also, original text can be recreated right back by simply concatenating the tokens. And replacing underscore by whitespace. Now we can try Unigram model. and see the same sentences tokenized differently.\nfrom tokenizers import Tokenizer from tokenizers.models import WordPiece from tokenizers.trainers import WordPieceTrainer from tokenizers.pre_tokenizers import Whitespace try: # 1. Initialize the WordPiece Tokenizer # We specify the [UNK] token for handling words not found in the vocabulary. tokenizer = Tokenizer(WordPiece(unk_token=\u0026#34;[UNK]\u0026#34;)) # 2. Configure Pre-tokenization # Before the subword algorithm runs, we need to split the raw text into words. # Whitespace splitting is the standard first step for most English NLP tasks. tokenizer.pre_tokenizer = Whitespace() # 3. Initialize the Trainer # We define our target vocabulary size and the special tokens required for # downstream tasks (like BERT\u0026#39;s [CLS] for classification or [SEP] for separators). trainer = WordPieceTrainer( vocab_size=4000, special_tokens=[\u0026#34;[UNK]\u0026#34;, \u0026#34;[CLS]\u0026#34;, \u0026#34;[SEP]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[MASK]\u0026#34;] ) # 4. Train the Model # The tokenizer scans the training file to build a vocabulary of the most # frequent subword units. tokenizer.train(files=[\u0026#34;train_data.txt\u0026#34;], trainer=trainer) # 5. Persist the Model # Save the configuration and vocabulary to a JSON file for future inference. tokenizer.save(\u0026#34;wordpiece.json\u0026#34;) print(\u0026#34;Training complete. \u0026#39;wordpiece.json\u0026#39; created.\u0026#34;) # 6. Metadata Inspection print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;WordPiece Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {tokenizer.get_vocab_size()}\u0026#34;) # 7. Testing Subword Tokenization # WordPiece shines at handling rare words by breaking them into meaningful chunks. test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (WordPiece) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # Encode converts raw text into a Tokenizer object containing tokens and IDs output = tokenizer.encode(text) # \u0026#39;tokens\u0026#39; shows the subword breakdown (e.g., \u0026#39;un\u0026#39;, \u0026#39;##insta\u0026#39;, etc.) print(f\u0026#34;Subword Tokens: {output.tokens}\u0026#34;) # \u0026#39;ids\u0026#39; are the numerical indices mapped to the vocabulary print(f\u0026#34;Numerical IDs: {output.ids}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred with WordPiece model: {e}\u0026#34;) You can see the following output:\nWordPiece Model Metadata: Total Vocab Size: 2609 --- Tokenization Test (WordPiece) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;H\u0026#39;, \u0026#39;##el\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;##or\u0026#39;, \u0026#39;##ld\u0026#39;, \u0026#39;[UNK]\u0026#39;, \u0026#39;[UNK]\u0026#39;] Numerical IDs: [37, 180, 214, 52, 162, 418, 0, 0] --- Tokenization Test (WordPiece) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;##os\u0026#39;, \u0026#39;##t\u0026#39;, \u0026#39;un\u0026#39;, \u0026#39;##ins\u0026#39;, \u0026#39;##ta\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;##ra\u0026#39;, \u0026#39;##m\u0026#39;, \u0026#39;##ma\u0026#39;, \u0026#39;##ble\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;ever\u0026#39;] Numerical IDs: [691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240] We can see the special character ## in the output. We can also see the breakdowns not having any semantic meaning. They are likelihood based. We can try the Unigram model now.\n# \u0026#39;Unigram\u0026#39; is the default and usually recommended over BPE in SentencePiece. options_unigram = { \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # Path to the raw text file for training \u0026#39;model_prefix\u0026#39;: \u0026#39;unigram_model\u0026#39;, # Prefix for the output .model and .vocab files \u0026#39;vocab_size\u0026#39;: 1200, # Desired size of the final vocabulary \u0026#39;model_type\u0026#39;: \u0026#39;unigram\u0026#39;, # Specifies the Unigram language model algorithm \u0026#39;character_coverage\u0026#39;: 0.9995, # Percentage of characters covered by the model (0.9995 is standard for Latin scripts) \u0026#39;byte_fallback\u0026#39;: True, # Enables mapping unknown characters to UTF-8 bytes to avoid \u0026lt;unk\u0026gt; tokens \u0026#39;split_digits\u0026#39;: True, # Treats each digit as an individual token (useful for numerical data) \u0026#39;add_dummy_prefix\u0026#39;: False # Prevents adding a leading space (SentencePiece default is True) } try: # 1. Train the SentencePiece model using the defined options print(\u0026#34;Starting Unigram training...\u0026#34;) spm.SentencePieceTrainer.train(**options_unigram) print(\u0026#34;Training complete. \u0026#39;unigram_model.model\u0026#39; created.\u0026#34;) # 2. Load the trained model into a processor instance for inference sp_unigram = spm.SentencePieceProcessor() sp_unigram.load(\u0026#39;unigram_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Unigram Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {sp_unigram.get_piece_size()}\u0026#34;) # 3. Define test cases to evaluate how the model handles common and rare words test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] # 4. Iterate through test sentences to visualize subword segmentation for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (Unigram) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: Converts text into subword strings (visual representation) print(f\u0026#34;Subword Tokens: {sp_unigram.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: Converts text into numerical indices for model input print(f\u0026#34;Numerical IDs: {sp_unigram.encode_as_ids(text)}\u0026#34;) except Exception as e: # Handle potential errors during training or loading (e.g., missing input file) print(f\u0026#34;An error occurred with Unigram model: {e}\u0026#34;) The output is as follows:\nUnigram Model Metadata: Total Vocab Size: 1200 --- Tokenization Test (Unigram) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ll\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;ld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311] --- Tokenization Test (Unigram) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;T\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅm\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;sta\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;ra\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ver\u0026#39;] ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"A comprehensive guide to tokenization strategies: BPE, WordPiece, Unigram, and SentencePiece.","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"A brief introduction to Vectors \u0026amp; Verbs and formatting verification.","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"\nFeedforward network (FFN) blocks are the least talked about when in context to LLMs. But that is where more than 70% of the paramters come from. Something as simple and convinient as a couple of linear transforming matrices become a burden when models start scaling in Billions of parameters. So, getting the maximum juice of a lean structure is a challenge. Plus, mostly under represented, because the FFN sub-layer acts as the primary \u0026ldquo;knowledge\u0026rdquo; processing engine, often containing the majority of a model\u0026rsquo;s total parameters.\nStandard Position Wise Feedforward Network This is as simple as it can get, two-layered fully connected network of neurons with a single activation switch, most probably a ReLU variant (for LLMs). $$ FFN(x) = max(0, W_2(W_1(x)) + b_1)W_2 + b_2 $$ Same transformation is applied for each token in parallel, making it efficient for GPU computation. For Example, in BERT model the 512 dimensions of each token are expanded to 2048 to allow for \u0026ldquo;intricate interactions\u0026rdquo; before projecting back to 512 dimensions. This part is important, in every transformer stacking of layers is possible only because the output of every matches it\u0026rsquo;s input dimensions.\nSWiGLU Sigmoid Weighted Gated Linear Unit (SWiGLU) has become the Gold Standard for feed forward networks in LLMs like Llama 3 and PaLM. The primary reason for this choice is the Swish function (as shown in the first image) instead of ReLU or GELU are as following:\nMathematical Linearity: Swish (SILU) is defined as $x\\cdot sigmoid(\\beta x)$, $\\beta$ is a learnable parameter. It is a small monotonic function that allowes small negetive values to pass through, making it mathematically linear. While GELU ($x \\cdot \\phi(x)$) is also smooth and nonmonotonic, Swish\u0026rsquo;s curve and equations have emperically improved the model\u0026rsquo;s performance.\nComputational Efficiency: SILU gradients are simpler to compute. $x/(1+exp(-x))$ compared to the standard GELU, which is an approximation of the tanh function. In the massive scale of modern LLMs these shortcuts save significant training time.\nPerformance: Research in the PaLM and Llama papers found that Swish-based Gated Linear Units consistently outperformed GELU-based versions in terms of perplexity and downstream task accuracy. $$SwiGLU(x, W, V, b, c) = (Swish(xW + b) \\otimes (xV + c))W_2$$ Where $W$ and $V$ are weight matrices for the gate and the data path, and $\\otimes$ is element-wise multiplication.\nMore Complex Filtering: Instead of using ordinary two weight matrices in standard FFN, SWiGLU uses three weight matrices (gate, date, output). This allows it to gather more complex features out of the input without drastically increasing the number of parameters. This performance boost has resulted in modern implementations being low in dimensionality of hidden layers (2/3 of input dimensions) while retaining the performance benefits of gated activations.\nZero-Gradient Avoidance: Unlike ReLU, which has a zero gradient for all negative values (the \u0026ldquo;dying ReLU\u0026rdquo; problem), SwiGLU\u0026rsquo;s smooth curve ensures that the model can still learn from negative inputs, improving training stability in deep stacks.\nMixture of Experts (MoE) This is an entire architecture rather than a single block with activation switch. It comes with it\u0026rsquo;s own challenges. It is a \u0026ldquo;divide and conquer\u0026rdquo; strategy. It became a dominant strategy for scaling insanely large models upto a trillion parameters like DeepSeek-V3 and Mistral. It is because it breaks down the direct and obvious link between model size and its speed. It is because it breaks down the direct and obvious link between model size (information capacity) and its speed (inference cost). Sparse Activation: MoE comes with a system of Manager (called Router in this case) and Experts. The Router is responsible for determining which smaller Experts (FFNs) are best suited for specific tokens. The Experts layer is actually split in different sublayers. For any token only a tiny part of the Experts are used. DeepSeek had 256 Expert layers in a single layer while activating not more than 8 for a single token. Big Scale, Small Compute:* MoE runs with a \u0026ldquo;trillion parameter intelligence\u0026rdquo; that runs at a speed of a much smaller model. DeepSeek-V3 has 671 billion total parameters, which dictates its knowledge base. However, during inference, it only uses 37 billion parameters per token. You get the reasoning performance of a massive model with the latency and cost of a significantly smaller one. Expert Specialization: Because different experts handle different tokens, they naturally specialize. DeepSeek\u0026rsquo;s analysis shows that certain experts become \u0026ldquo;specialists\u0026rdquo; in mathematics or coding, while others handle general language, without human intervention. Training Efficiency: Since you only update a fraction of the weights for each token, MoE models can process many more tokens per second during training compared to a dense model of the same total size. Some points on training: The router gives probabilities for each expert stack to be used for a token. While training, in order to avoid overfitting over specific experts a little amount of noise is added to the router\u0026rsquo;s output. This is just to skew the model from selecting a few experts in small amount of iterations. In some models only one expert is chosen and in others more than are chose and then averaged. This is not as widely used as SwiGLU and for the reasons below.\nThe RAM wall (Memory Capacity):* Even though the model computes only small percentage of parameters at once it should hold all 671B parameters in GPU memory. This is a major challenge for MoE models to run on consumer hardware or single GPUs. The model even running at a fast speed will not work without a big GPU cluster. Training Instability: MoEs struggle from load balancing issues where the router might \u0026ldquo;collapse\u0026rdquo; (sending all tokens to just one expert, ignoring the others). Models often use auxiliary losses to force balance, but this can hurt performance. DeepSeek-V3 introduced an \u0026ldquo;Auxiliary-Loss-Free\u0026rdquo; strategy that balances load dynamically without degrading the model\u0026rsquo;s accuracy. Communication Overhead: In distributed training, experts are often split across different GPUs. If the router decides a token on GPU 1 needs an expert located on GPU 4, that data must travel over the network. The Fix: DeepSeek-V3 employs \u0026ldquo;Node-Limited Routing\u0026rdquo;, which restricts the router to send tokens only to a maximum of 4 nodes, preventing network congestion. Summary: MoE is \u0026ldquo;better\u0026rdquo; if you are a major lab (like DeepSeek, Google, OpenAI) trying to maximize intelligence per dollar of training cost. It is \u0026ldquo;worse\u0026rdquo; if you are a developer with limited VRAM trying to deploy a model locally. You use MoE to push the frontier of intelligence; you use Dense SwiGLU for reliability and ease of deployment.\n","permalink":"http://localhost:1313/posts/feedforward-networks/","summary":"Exploring the hidden layers of trillion-parameter switchboards: Feedforward Neural Networks and Activation Functions.","title":"Anatomy of Trillion-Parameter Switchboards: Understanding Feedforward Blocks"},{"content":"\nWe are about to touch the holy grail of modern AI. The 2017 declaration was simple: Attention Is All You Need. But as we‚Äôve raced from GPT-1 to reasoning models, the definition of \u0026lsquo;Attention\u0026rsquo; has quietly transformed. Is the algorithm ruling the world today actually the same one we started with? Let‚Äôs look under the hood.\nüî• The Core Concept At its heart, the attention mechanism is a form of weighted aggregation. It is often described using a database retrieval analogy. Imagine you have a database of information (context).\nQuery ($Q$): What are you currently looking for? (The token being generated). Key ($K$): The indexing tag associated with each piece of information. Value ($V$): The actual information (The context token). The \u0026ldquo;Attention\u0026rdquo; is the process of matching your Query against all the Keys to determine how much \u0026ldquo;weight\u0026rdquo; to put on each Value. Attention scores are calculated in each transformer layer at every step of the sequence. This is a single unifying factor in all transformer models, from BERT to Gemini.\nüî• Standard Multi-Head Attention (MHA) The original mechanism introduced in the paper \u0026ldquo;Attention Is All You Need\u0026rdquo;.\nHere, instead of performing the Key-Query-Value (KQV) operation on the entire context only once, the attention is split into $N$ parallel heads. Each head learns a different aspect of the language (e.g., word meaning, syntax, semantic relationships, etc.). The equation is a simple culmination of dot products aided by a regularizer like Softmax.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nWhere:\n$QK^T$ computes the similarity scores between the Query and Key. $\\sqrt{d_k}$ is a scaling factor to prevent the dot product from growing too large, mitigating the vanishing/exploding gradient problem. The weighted sum is applied to the values of $V$. The Memory Bandwidth Bottleneck: \u0026gt; During inference, the model must store Key and Value matrices for all previous tokens. This is called the KV Cache. In MHA, every single head has its own unique set of Keys and Values. For large models with massive context windows, loading this massive KV cache from memory (HBM) to the computation chip (SRAM) becomes the primary bottleneck, slowing down generation significantly. Earlier models could often only generate ~8 tokens per second at inference due to this constraint.\nüî• Multi-Query Attention (MQA) This approach addresses the memory bottleneck by sharing the same set of Keys and Values across all heads while keeping separate Query heads.\nMHA: $H$ Query heads, $H$ Key heads, $H$ Value heads MQA: $H$ Query heads, 1 Key head, 1 Value head This drastically reduces the memory requirements of the KV cache by a factor of $H$ (often 8x or 16x). The GPU needs to load far less data, significantly speeding up inference. However, it does not reduce the computation (FLOPs)‚Äîso why is it faster?\nThis can be explained with the Kitchen Analogy: Imagine a kitchen (GPU) with 8 chefs (Heads).\nComputation (Math): The Chef chopping vegetables. Memory Access (Bandwidth): The Assistant running to the fridge to get ingredients. In MHA: Each chef demands unique vegetables from the fridge. The assistant runs back and forth 8 times, fetching 8 different crates. Most of the time is spent waiting for the assistant to return. This results in idle cores waiting for data to load from HBM.\nIn MQA: All chefs agree to use the same vegetables. The assistant runs back and forth only once, fetching one giant crate of potatoes and dumping it. This results in minimized memory transfer and no idle cores.\nThe Technical Reality: During text generation, the model is memory-bound, not compute-bound. The time it takes to move the KV Cache from HBM (High Bandwidth Memory) to the chip\u0026rsquo;s SRAM is the bottleneck. MQA reduces the data volume of that move by a factor of $H$.\nThe Capacity Gap: MQA is a trade-off between nuanced response and memory capacity. The model loses the ability to store different types of information, as a single $V$ matrix must now store all nuances of information flow in the language.\nThe Surgery Trick (Uptraining): Google Research figured out a way to take an existing model with MHA and convert it to MQA by averaging the weight matrices of $H$ heads. This is called Uptraining. You then train this \u0026ldquo;Frankenstein\u0026rdquo; model for ~5% of the original training steps for adjustments. This yields an MQA model that is nearly as good as the original MHA but vastly faster.\nüî• Grouped Query Attention (GQA) A \u0026ldquo;best of both worlds\u0026rdquo; solution used in modern models like Llama 2 and Llama 3.\nHow it works: GQA acts as an interpolation between MHA and MQA. Instead of having just 1 Key head (MQA) or $H$ Key heads (MHA), it divides the query heads into $G$ groups. Each group shares a single Key and Value head.\nEquation modification: If you have 8 query heads, you might create 4 groups (2 queries per group). This reduces the KV cache by 2x instead of 8x, preserving more quality than MQA while still speeding up inference.\nMHA: 8 people (heads) are working on a project. Each person has their own unique filing cabinet (KV cache). It takes forever to move 8 cabinets around. MQA: 8 people share one single filing cabinet. It\u0026rsquo;s very fast to move, but they fight over the organization and lose detail. GQA: The 8 people split into 4 teams of 2. Each team shares a cabinet. It\u0026rsquo;s a balance between speed and organizational detail. üî• Multi-Head Latent Attention (MLA) This is the most advanced mechanism for attention, used in models like DeepSeek-V3. It is a sophisticated design for massive efficiency that takes a radical approach: mathematically compressing the memory into a low-rank \u0026ldquo;latent\u0026rdquo; vector.\nKV Cache Explosion: As mentioned before, for a massive model like DeepSeek-V3 (671B parameters), storing the full Keys and Values for every token in a long conversation requires terabytes of high-speed memory (HBM), and chips spend more time waiting than churning.\nMHA (Standard) stores everything. MQA deletes redundancy but hurts quality. MLA keeps the quality of MHA but compresses the data 4x-6x smaller than even GQA.\nThe Zip File Approach: Low-Rank Joint Compression Let\u0026rsquo;s compare the math of MHA and MLA. In MHA, for every single token, we generate a unique Key ($k$) and Value ($v$) matrix for each attention head.\nThe Memory Problem: The $k$ is stored in the KV Cache. For 128 heads, for example, it calculates a massive vector of $128 \\times 128 = 16,384$ floats per token.\nFor MHA, the attention score looks like: $$\\text{Score} = q^T \\cdot k$$\nBut for MLA, instead of storing the full $k$, it projects the input down into a tiny latent vector $c_{KV}$, which is much smaller (e.g., 512 floats). $$c_{KV} = W_{DKV} \\cdot x$$\nNow here is the catch: Naive compression and decompression would mean constructing a small latent matrix for every token and deconstructing it every single time the information is required, which defeats the purpose of compression.\n$$k_{\\text{reconstructed}} = W_{UK} \\cdot c_{KV}$$ $$\\text{Score} = q^T \\cdot k_{\\text{reconstructed}}$$\nThe Matrix Absorption Trick (Optimization) Instead of reconstructing $k$ for every token, we can absorb the Up-Projection matrix ($W_{UK}$) into the Down-Projection matrix ($W_{DKV}$) and absorb the query $q$ into a new $Q$.\nFrom the original equation: $$\\text{Score} = q^T \\cdot (\\underbrace{W_{UK} \\cdot c_{KV}}_{\\text{This is } k})$$\nWe associate differently:\n$$\\text{Score} = (\\underbrace{q^T \\cdot W_{UK}}_{\\text{Absorbed Query}}) \\cdot c_{KV}$$ We change the order in which the matrix multiplication is performed.\nHow is this allowed? In linear algebra, matrix multiplication is associative:\n$$(A \\cdot B) \\cdot C = A \\cdot (B \\cdot C)$$ We can move the parentheses! Instead of grouping $(W_{UK} \\cdot c_{KV})$ to make the Key, we group $(q^T \\cdot W_{UK})$ to make a new Query.\nNote: The positional embedding information is left untouched. That rotational information from algorithms like RoPE is preserved outside the compression; it requires minimal space anyways.\n","permalink":"http://localhost:1313/posts/attention-mechanisms/","summary":"We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek\u0026rsquo;s MLA, how has the definition of \u0026lsquo;Attention\u0026rsquo; transformed?","title":"Attention Is All You Need, but exactly which one?: MHA, GQA and MLA"},{"content":"\nPositional Embeddings are the \u0026ldquo;voice\u0026rdquo; that tell Transformers where words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.\n\u0026ldquo;The dog chased the cat\u0026rdquo; and \u0026ldquo;cat chased the dog\u0026rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.\nüî• Sinusoidal Embeddings: The Odometer of Language Introduced in the seminal \u0026ldquo;Attention Is All You Need\u0026rdquo; paper, this was the industry standard for many years. This approach adds a vector to each word embedding based on its position. The vector is computed out of wavelike patterns that are periodic and smooth. Mathematically, for a position $pos$ and embedding dimension $i$:\n$$ PE_{(pos, i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $$\nThe model uses sine and cosine waves of different frequencies to capture both short and long-range dependencies. The frequency of the waves decreases as the embedding dimension increases, which helps capture long-range dependencies in the input sequence.\nThe creators wanted the model to calculate distance easily, which is why a linear function was used. The model simply rotates the current position vector using a rotation matrix to derive the next position.\n$$ \\begin{pmatrix} \\cos(k) \u0026 \\sin(k) \\\\ -\\sin(k) \u0026 \\cos(k) \\end{pmatrix} \\cdot PE(pos) \\approx PE(pos + k) $$ Low dimensions act like a fast-ticking clock (high frequency, changing rapidly) and high dimensions act like a slow clock (low frequency, changing slowly).\nThe number 10,000 is an arbitrarily chosen high number to reduce Aliasing. By choosing such a high number, the creators ensured that the wavelength of the slowest dimension is incredibly long. Consequently, the pattern practically never repeats, and each position remains unique.\nThis combination works like a mechanical odometer dial. Each subsequent dimension acts as a slower-moving gear. The model learns to read these \u0026ldquo;dials\u0026rdquo; to understand exactly where a word sits in the sequence.\nThis mitigated the challenge of exploding gradients that previous array-based positional embeddings suffered from (e.g., adding integers 1, 2, 3\u0026hellip;). This model keeps values between -1 and +1, adding stability while ensuring every position gets a distinct code.\nThe Problem: This approach pollutes the word embedding matrix (semantic information). While high-dimensional vectors are almost always orthogonal‚Äîmeaning the noise doesn\u0026rsquo;t fully destroy the word\u0026rsquo;s identity‚Äîit does not strictly guarantee purity.\nThis led to a more mature approach.\nüî• ALiBi: The Fading Streetlight Attention with Linear Biases (ALiBi) is a method where, instead of adding a vector to the embeddings at the start, the model injects positional information linearly, directly into the attention scores. If Sinusoidal Encoding asks, \u0026ldquo;Where am I on the map?\u0026rdquo;, ALiBi asks, \u0026ldquo;How much harder should I squint to see you?\u0026rdquo;\nThe biggest problem ALiBi solved is the \u0026ldquo;Invisible Wall\u0026rdquo;. If you train a Transformer on sequences of length 2048 and then feed it a book with 3000 words, it will crash or output garbage at word 2049. This is a failure of Extrapolation.\nALiBi throws away the idea of adding positional vectors entirely. Instead, it devises a \u0026ldquo;penalty\u0026rdquo; system for attention scores:\n$$ \\text{Score} = (q \\cdot k^\\top) - (m \\cdot \\text{distance}) $$\nDistance: If words A and B are neighbors, the distance is 0, so the penalty is 0. Although the distance is linear, it does not cause exploding gradients because the raw attention scores are normalized by a softmax function. This distance prioritizes nearby words (local context) over distant ones.\nSlope ($m$): If we always penalize distance equally, the model becomes myopic. ALiBi mitigates this by assigning a different slope value to each attention head:\nHead 1 (The Historian): Very low slope. The penalty grows slowly, allowing it to see far back. Head 2 (The Reader): Medium slope. Head 3 (The Myopic): Very high slope (e.g., 0.5); it effectively cannot see beyond 2-3 words. This mimics holding a lantern. Words close to you are bright, while words further away naturally fade into darkness.\nExtrapolation: If you walk 5 miles down the road (a longer sequence), your lantern works exactly the same way. The physics of light decay are constant, allowing the model to handle sequences longer than it was trained on.\nHowever, ALiBi forces a \u0026ldquo;sliding window\u0026rdquo; view where distant words eventually become invisible. This brings us to the modern standard.\nüî• RoPE: The Rotary Revolution Rotary Position Embeddings (RoPE) is the current industry standard (used in Llama 3, Mistral, PaLM) because it mathematically unifies \u0026ldquo;absolute position\u0026rdquo; with \u0026ldquo;relative position.\u0026rdquo; This method rotates the vectors in the Query and Key matrices based on their absolute position.\nIf a vector has 512 dimensions, RoPE breaks it into 256 pairs. Each pair is rotated by a different frequency:\n$$ \\theta_i = 10000^{-2i/d} $$\nThe First pair ($i = 0$) is rotated by the highest frequency. The Last pair ($i = 255$) is rotated by the lowest frequency. This spread ensures the model has \u0026ldquo;high precision\u0026rdquo; (fast rotation) to distinguish immediate neighbors, and \u0026ldquo;low precision\u0026rdquo; (slow rotation) to track global position without the pattern repeating.\nDoesn\u0026rsquo;t rotation change meaning? Yes and No. The vector for \u0026ldquo;Apple\u0026rdquo; at position 1 and \u0026ldquo;Apple\u0026rdquo; at position 256 will look completely different in terms of coordinates. However, rotation does not change the semantic strength (magnitude/norm) of the vector.\nThe Relativity Trick: Transformers compare words using dot products (angles). RoPE relies on the fact that the attention mechanism ignores the absolute rotation and only detects the difference in rotation (relative position). Thus, the \u0026ldquo;relative meaning\u0026rdquo; is encoded purely into the angle between words, leaving the semantic \u0026ldquo;magnitude\u0026rdquo; untouched.\nüî• LongRoPE: The Bifocal Lens While RoPE is powerful, it struggles when stretching context windows from 4k to 2 million tokens. The \u0026ldquo;fast\u0026rdquo; dimensions spin so rapidly over long distances that they become random noise. LongRoPE solves this using an evolutionary search algorithm to find a unique scaling factor ($\\lambda$) for each dimension.\nThe Equations Instead of rotating by the standard $\\theta_i$, LongRoPE rotates by a rescaled frequency:\n$$ \\theta\u0026rsquo;_i = \\lambda_i \\theta_i $$\nTo efficiently find these $\\lambda$ values without searching an infinite space, the algorithm enforces a monotonicity constraint based on NTK theory:\n$$ \\lambda_i \\le \\lambda_{i+1} $$\nThis ensures that low-frequency dimensions (global context) are stretched more than high-frequency dimensions (local context), creating a \u0026ldquo;bifocal\u0026rdquo; effect:\nHigh Frequencies (Local): Kept sharp ($\\lambda \\approx 1$) to maintain grammar. Low Frequencies (Global): Stretched ($\\lambda \u0026gt; 1$) to track massive distances without repeating. Not worth perusing more since the next one is much better at long context positional encoding. üî• HoPE: The Hyperbolic Slide Hyperbolic Rotary Positional Encoding (HoPE) moves from the geometry of a circle to the geometry of a hyperbola (inspired by Lorentz transformations). It was designed to fix a subtle flaw in RoPE: the \u0026ldquo;wobbly\u0026rdquo; or oscillatory nature of attention scores.\nThe Core Problem: RoPE\u0026rsquo;s \u0026ldquo;Wobble\u0026rdquo; In standard RoPE, we rotate vectors around a circle.\nThe Issue: Circles repeat. A dial at $361^\\circ$ looks identical to $1^\\circ$. Even with different frequencies, the dot product fluctuates up and down as distance increases. The Consequence: The model gets confused at long distances, potentially mistaking a word 1,000 tokens away for a neighbor just because the rotation cycles aligned, creating \u0026ldquo;noise.\u0026rdquo; The \u0026ldquo;Hyperbolic\u0026rdquo; Shift HoPE replaces trigonometric functions ($\\sin, \\cos$) with hyperbolic functions ($\\sinh, \\cosh$).\nCircular Rotation: Keeps distance from the center constant. Hyperbolic Rotation: Moves points along a hyperbola. Crucially, hyperbolic functions do not repeat; they grow or decay exponentially. By using hyperbolic rotation, HoPE guarantees that as words get further apart, the attention score decays monotonically (smoothly drops).\nThe Equations HoPE uses a hyperbolic matrix $B(\\theta, m)$ defined using the Lorentz Boost structure:\n$$ B(\\theta, m) = \\begin{pmatrix} \\cosh(m\\theta) \u0026 \\sinh(m\\theta) \\\\ \\sinh(m\\theta) \u0026 \\cosh(m\\theta) \\end{pmatrix} $$ Since hyperbolic functions grow to infinity, HoPE introduces a decay penalty ($e^{-m\\theta'}$). The final attention score simplifies beautifully to a function of relative distance: $$ \\text{Score} \\propto e^{-|m-n|(\\theta\u0026rsquo; - \\theta)} $$\nThis ensures the attention score exponentially decays as distance increases, eliminating the wobbles of standard RoPE. This the latest model that is taking up the space in the industry for extremely long context models.\n","permalink":"http://localhost:1313/posts/positional-embeddings/","summary":"From Sinusoidal to RoPE and HoPE: How Transformers learn to process word order and sequence length.","title":"The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE"},{"content":"Imagine trying to teach a computer the difference between \u0026ldquo;Apple\u0026rdquo; the fruit and \u0026ldquo;Apple\u0026rdquo; the company. To us, the distinction is intuitive. To a machine, it‚Äôs just a string of characters. How do we turn these strings into meaningful math? While early attempts like Word2Vec gave us a great start, they missed the forest for the trees‚Äîor in some cases, the twigs for the branches. Enter GloVe and FastText: two algorithms that revolutionized how machines understand the nuances of human language.\nPrevious static embedding models like Word2Vec successfully captured the local semantics of words, but they failed in addressing contexts between words that might not always appear in the same context window. Furthermore, they treated words as indivisible atomic units in continuous vector spaces, missing out on the internal structure of language.\nThe two simple but effective ideas behind GloVe and FastText are subword units and global co-occurrence. Let\u0026rsquo;s dive into them.\nGloVe: A Global Mapping Strategy Global Vectors for Word Representation (GloVe) was created at Stanford. The core idea was to train a model where word vectors are defined by how often they appear around other words across the entire corpus, rather than just locally.\nTraining Steps: Co-occurrence Matrix X: GloVe scans the entire corpus and creates a giant matrix (spreadsheet) recording the counts of how often words appear near each other. Factorization: It compresses this giant matrix into smaller, dense vectors. Objective: The function tries to equate the dot product of two word vectors with the logarithm of their probability of appearing together. The error between these values is then backpropagated to update the vectors. $$w_i \\cdot w_j + b_i + b_j = \\log(X_{ij})$$\n$w$: The word vector (what we want). $X_{ij}$: The count from our matrix. The Loss Function: We can\u0026rsquo;t satisfy the equation above perfectly for every word pair. So, we use a weighting function to minimize the squared error:\n$$J = \\sum_{i,j=1}^V f(X_{ij}) ({w_i}^T w_j + b_i + b_j - \\log(X_{ij}))^2$$\nIf we do not use the weighting function, the loss would be infinite for rare words (since $\\log(0) = \\infty$) and frequent words (like \u0026rsquo;the\u0026rsquo;) would overpower the model.\n$$ f(x) = \\begin{cases} (x/x_{max})^\\alpha \u0026amp; \\text{if } x \u0026lt; x_{max} \\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$\n(Typically $x_{max}=100$ and $\\alpha=0.75$)\nFastText: The Lego Bricks Strategy FastText was created by Facebook AI Research (FAIR). The idea was to treat words not as solid rocks, but as structures made of \u0026ldquo;Lego bricks\u0026rdquo; (subword units).\nWord2Vec struggled to create embeddings for words it hadn\u0026rsquo;t seen before (Out-Of-Vocabulary or OOV words). FastText solves this by breaking words down.\nGloVe treats a word like \u0026ldquo;Apple\u0026rdquo; as a single, atomic unit. FastText breaks \u0026ldquo;Apple\u0026rdquo; into character n-grams: \u0026lt;ap, app, ppl, ple, le\u0026gt; For a complex or new word like \u0026ldquo;Applesauce\u0026rdquo;, the model might not know the whole word, but it recognizes the Lego brick for \u0026ldquo;Apple\u0026rdquo; and the remaining bits from other subwords.\nFastText takes its inspiration from the SkipGram architecture. The vector for a word is simply the sum of the vectors of its n-gram subwords:\n$$v_w = \\sum_{g \\in G_w} z_g$$\nObjective: The objective is to predict context words given a target word.\n$$P(w_c | w_t) = \\frac{e^{s(v_{w_t} \\cdot v_{w_c})}}{\\sum_{w \\in V} e^{s(v_{w_t} \\cdot v_w)}}$$\nAnd the scoring function is defined as:\n$$s(w_t, w_c) = u_{w_t}^T (\\sum_{g \\in G_w} z_g)$$\n$u_{w_c}$: The vector for the context word (output vector). $z_g$: The vector for the n-gram $g$ (input vector). Modifications to Softmax: Calculating the full softmax for every word in the vocabulary is computationally expensive. FastText uses two methods to approximate this efficiently.\nOption A: Negative Sampling\nWe pick a few random \u0026ldquo;wrong\u0026rdquo; words every time to serve as \u0026ldquo;negative samples\u0026rdquo; and calculate the loss only for the correct word and these noise words.\n$$J = - \\log \\sigma(s(w_t, w_c)) - \\sum\\limits_{i=1}^{N} \\mathbb{E}_{n_i \\sim P_n(w)} [\\log \\sigma(-s(w_t, n_i))]$$\nThe sigmoid function $\\sigma$ squashes the score between 0 and 1. The first term pushes the probability of the correct word towards 1. The second term pushes the probability of the noise words towards 0. Option B: Hierarchical Softmax (Faster for infrequent words)\nInstead of a flat list of words, imagine the vocabulary arranged as a binary tree (Huffman Tree).\nRoot: Top of the tree. Leaves: The actual words. To calculate the probability of a word, we trace a path from the root to that leaf. At every branching node, we calculate the probability (sigmoid) of going left or right. The probability of a word $w$ is the product of the probabilities of the turns taken to reach it. Code Example from gensim.models import FastText from gensim.test.utils import common_texts # Initialize the FastText model with specific hyperparameters model = FastText( vector_size=100, # Dimensionality of the word vectors window=5, # Maximum distance between the current and predicted word within a sentence min_count=1, # Ignores all words with total frequency lower than this sg=1, # Training algorithm: 1 for skip-gram; 0 for CBOW hs=0, # If 0, and negative is non-zero, negative sampling will be used negative=5, # Number of \u0026#34;noise words\u0026#34; to be drawn for negative sampling workers=4, # Number of worker threads to train the model epochs=10, # Number of iterations over the corpus min_n=3, # Minimum length of character n-grams max_n=6 # Maximum length of character n-grams ) # Build the vocabulary from the provided text corpus model.build_vocab(common_texts) # Train the model on the corpus model.train( common_texts, total_examples=model.corpus_count, epochs=model.epochs ) # Calculate and print the cosine similarity between two words in the vocabulary print(f\u0026#34;Similarity between \u0026#39;computer\u0026#39; and \u0026#39;human\u0026#39;: {model.wv.similarity(\u0026#39;computer\u0026#39;, \u0026#39;human\u0026#39;)}\u0026#34;) # Demonstrate FastText\u0026#39;s ability to handle Out-Of-Vocabulary (OOV) words # Even if \u0026#39;computation\u0026#39; wasn\u0026#39;t in the training data, FastText constructs a vector using character n-grams oov_vector = model.wv[\u0026#39;computation\u0026#39;] # Check if a specific word exists in the model\u0026#39;s fixed vocabulary index word = \u0026#34;computer\u0026#34; is_in_vocab = word in model.wv.key_to_index print(f\u0026#34;\\nIs the word \u0026#39;{word}\u0026#39; in the model\u0026#39;s vocabulary? {is_in_vocab}\u0026#34;) ","permalink":"http://localhost:1313/posts/glove-and-fasttext/","summary":"How subword units and global co-occurrence matrices allow GloVe and FastText to capture nuances that Word2Vec missed.","title":"The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText"},{"content":" Before we had Large Language Models writing poetry, we had to teach computers that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; are related not just by spelling, but by meaning. This is the story of that breakthrough. It‚Äôs the moment we stopped counting words and started mapping their souls‚Äîturning raw text into a mathematical landscape where math can solve analogies. Welcome to the world of Word2Vec. üîÆ\nLanguage models require vector representations of words to capture semantic relationships. Before the 2010s, models used word count-based vector representations that captured only the frequency of words (e.g., One-Hot Encoding). The Problems: üöß\nThe Curse of Dimensionality Lack of meaning (Synonyms were treated as mathematically unrelated). In 2011, Mikolov et al. at Google introduced Word2Vec, which used a shallow neural network to learn vector representations of words. The best part? It could create much denser relationships between words than prior models, and it was unsupervised.\nThe training (prediction) is essentially \u0026ldquo;fake\u0026rdquo;‚Äîa pretext task. But the weight matrices we get on the side are a gold mine of fine-grained semantic relationships. ‚õèÔ∏è\nThere are two popular variants of Word2Vec:\nSkipGram Continuous Bag Of Words (CBOW) 1. Continuous Bag of Words (CBOW) üß© CBOW is a simple \u0026lsquo;fill in the blanks\u0026rsquo; machine. It takes the surrounding words as inputs and tries to predict the center word.\n\u0026ldquo;Continuous\u0026rdquo;: It operates in continuous vector space (unlike the discrete space of n-gram models).\n\u0026ldquo;Bag of words\u0026rdquo;: The order of the context does not matter. \u0026ldquo;The cat sat on the mat\u0026rdquo; and \u0026ldquo;The mat sat on the cat\u0026rdquo; are the same for CBOW. They produce exactly the same predictions.\nObjective: Maximize the probability of the target word $w_t$ given its context $w_{t-C}, \\dots, w_{t+C}$.\n2. Training Defining Dimensions:\n$V$: Vocabulary size (number of unique words in the corpus, e.g., 10,000) $N$: Dimension of the embedding space (e.g., 100) $C$: Number of context words (e.g., 2 words on left of target and 2 words on right) Step 1: Input Lookup \u0026amp; One-Hot Encoding\nMathematically, we represent each context word as a one-hot vector $x^{(c)}$. We have an Input Matrix $W_{in}$ of size $V \\times N$. For each context word (e.g., \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;), we select its corresponding row from $W_{in}$.\n$$ v_c = W_{in}^T x^{(c)} $$\nWhere $v_c$ is the $N$-dimensional vector for context word $c$.\nStep 2: Projection (The \u0026ldquo;Mixing\u0026rdquo;)\nWe combine the context vectors. In standard CBOW, we simply average them. This creates the hidden layer vector $h$.\n$$ h = \\frac{1}{2C} \\sum_{c=1}^{2C} v_c $$\n$h$ is a single vector of size $N \\times 1$. Note: This operation is linear and contains no activation function (like ReLU or Sigmoid).\nStep 3: Output Scoring (The \u0026ldquo;Dot Product\u0026rdquo;)\nWe have a second matrix, the Output Matrix $W_{out}$ of size $N \\times V$. We compute the raw score (logit) $u_j$ for every word $j$ in the vocabulary by taking the dot product of the hidden vector $h$ and the output vector $v\u0026rsquo;_j$.\n$$ u_j = {v\u0026rsquo;_j}^T h $$\nOr in matrix form:\n$$ u = W_{out}^T h $$\nResult $u$ is a vector of size $V \\times 1$.\nStep 4: Probability Conversion (Softmax)\nWe convert the raw scores $u$ into probabilities using the Softmax function. This tells us the probability that word $w_j$ is the center word.\n$$ y_j = P(w_j | \\text{context}) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 5: Loss Calculation (Cross-Entropy)\nWe compare the predicted distribution $y$ with the actual ground truth $t$ (a one-hot vector where the index of the true word is 1). The loss function $E$ is:\n$$ E = - \\sum_{j=1}^V t_j \\log(y_j) $$\nSince $t_j$ is 0 for all words except the target word (let\u0026rsquo;s call the target index $j^*$), this simplifies to:\n$$ E = - \\log(y_{j^*}) $$\n3. Backpropagation Details We need to update the weights to minimize $E$. We use the Chain Rule.\nA. Gradient w.r.t. Output Matrix ($W_{out}$)\nWe want to know how the error changes with respect to the raw score $u_j$.\n$$ \\frac{\\partial E}{\\partial u_j} = y_j - t_j $$\nLet\u0026rsquo;s call this error term $e_j$.\nIf $j$ is the target word: $e_j = y_j - 1$ (Result is negative; we boost the score). If $j$ is not target: $e_j = y_j - 0$ (Result is positive; we suppress the score). The update rule for the output vector $v\u0026rsquo;_j$ becomes:\n$$ v'_{j}(\\text{new}) = v'_{j}(\\text{old}) - \\eta \\cdot e_j \\cdot h $$ ($\\eta$ is the learning rate)\nB. Gradient w.r.t. Hidden Layer ($h$)\nWe backpropagate the error from the output layer to the hidden layer.\n$$ EH = \\sum_{j=1}^V e_j \\cdot v\u0026rsquo;_j $$\n$EH$ is an $N$-dimensional vector representing the aggregate error passed back to the projection layer.\nC. Gradient w.r.t. Input Matrix ($W_{in}$)\nSince $h$ was just an average of the input vectors, the error $EH$ is distributed to each context word\u0026rsquo;s input vector. For every word $w_c$ in the context:\n$$ v_{w_c}(\\text{new}) = v_{w_c}(\\text{old}) - \\eta \\cdot \\frac{1}{2C} \\cdot EH $$\nLet\u0026rsquo;s build a single pass through the network for a given context.\nimport torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # Define a simple Continuous Bag of Words (CBOW) style model class CBOWModel(nn.Module): def __init__(self, vocab_size, embedding_dim): super(CBOWModel, self).__init__() # The embedding layer stores the word vectors we want to learn self.embeddings = nn.Embedding(vocab_size, embedding_dim) # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word self.linear = nn.Linear(embedding_dim, vocab_size) def forward(self, inputs): # inputs: tensor of word indices for the context embeds = self.embeddings(inputs) # Aggregate context by calculating the mean of the word embeddings h = torch.mean(embeds, dim=1) # Produce logits (raw scores) for each word in the vocabulary logits = self.linear(h) return logits # Setup vocabulary and mappings word_to_ix = {\u0026#34;the\u0026#34;: 0, \u0026#34;quick\u0026#34;: 1, \u0026#34;brown\u0026#34;: 2, \u0026#34;fox\u0026#34;: 3, \u0026#34;jumps\u0026#34;: 4, \u0026#34;over\u0026#34;: 5, \u0026#34;lazy\u0026#34;: 6, \u0026#34;dog\u0026#34;: 7} ix_to_word = {v: k for k, v in word_to_ix.items()} # Configuration constants EMBEDDING_DIM = 5 VOCAB_SIZE = len(word_to_ix) LEARNING_RATE = 0.01 # Initialize the model, loss function, and optimizer model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM) loss_function = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE) # Prepare dummy training data: context words and the target word # Context: [\u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;over\u0026#34;] -\u0026gt; Target: \u0026#34;fox\u0026#34; context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long) target_idxs = torch.tensor([3], dtype=torch.long) # Perform a single optimization step model.zero_grad() # Forward pass: we reshape context to (1, -1) to simulate a batch of size 1 logits = model(context_idxs.view(1, -1)) # Calculate loss against the target word index loss = loss_function(logits, target_idxs) # Backpropagate and update weights loss.backward() optimizer.step() # Output results for the blog post print(\u0026#34;=== Model Training Snapshot ===\u0026#34;) print(f\u0026#34;Calculated Loss: {loss.item():.6f}\u0026#34;) print(f\u0026#34;\\n=== Learned Vector for \u0026#39;jumps\u0026#39; ===\u0026#34;) word_vec = model.embeddings(torch.tensor([word_to_ix[\u0026#39;jumps\u0026#39;]])) print(word_vec.detach().numpy()) print(\u0026#34;\\n=== Embedding Matrix (Weights) ===\u0026#34;) print(model.embeddings.weight.detach().numpy()) print(\u0026#34;\\n=== Linear Layer Weights ===\u0026#34;) print(model.linear.weight.detach().numpy()) Output shows single pass loss and learned vector for \u0026lsquo;jumps\u0026rsquo; word. This is a single training sample result but after multiple such passes both Embedding and linear weights should be very similar.\n=== Model Training Snapshot === Calculated Loss: 1.989656 === Learned Vector for \u0026#39;jumps\u0026#39; === [[-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775]] === Embedding Matrix (Weights) === [[ 1.66039 -0.11371879 0.6246518 0.35860053 -1.2504417 ] [-2.1847186 -0.77199775 -0.17050214 -0.38411248 -0.03913084] [ 0.11852697 0.90073633 0.8847807 0.7404524 0.900149 ] [ 0.07440972 -0.40259898 2.6246994 -0.08851447 0.02660969] [-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775] [-0.9245572 -0.5545908 0.9083091 -1.0755049 0.84047747] [-1.0237687 0.59466314 0.05621134 -0.6202532 1.3664424 ] [ 0.60998917 -1.0549186 1.6103884 0.8724912 -1.2486908 ]] === Linear Layer Weights === [[ 0.02165058 -0.28883642 0.14545658 -0.3442509 0.32704315] [-0.18731792 0.28583744 0.22635977 0.13245736 0.29019794] [ 0.3158916 -0.15826383 -0.03203773 0.16377363 -0.41457543] [-0.05080034 0.4180087 0.11228557 0.30218413 0.3025514 ] [-0.38419306 0.24475925 -0.39210224 -0.38660625 -0.2673145 ] [-0.32321444 0.12200444 -0.03569533 0.2891424 -0.07345333] [-0.33704326 0.2521956 0.31587374 0.22590035 0.29866052] [ 0.4117266 -0.44231793 0.24064957 -0.29684234 0.333821 ]] Interesting Facts About CBOW üß† The \u0026ldquo;Averaging Problem\u0026rdquo;: This is a common issue in CBOW models. The averaging and training is order-agnostic. All variants of word order in the context window will generate the same output. \u0026ldquo;Dog bit the man\u0026rdquo; and \u0026ldquo;Man bit the dog\u0026rdquo; are the same in CBOW\u0026rsquo;s eyes.\nSmoothing Effect: CBOW models are much faster to train than Skip-Gram but are generally smoother.\nRare Words: CBOW models struggle slightly with rare words because the presence of surrounding common words (like articles and prepositions) can skew the embeddings and dissolve their uniqueness.\nComputational Cost: Softmax requires summing over the entire vocabulary size, which can be computationally expensive if $V$ is very large. Solutions include Hierarchical Softmax and Negative Sampling (approximating the denominator by only checking the target word vs. 5 random noise words).\nVectors as Double Agents: Every word has a different representation in the embedding matrix and the output matrix. Most of the time $W_{out}$ is discarded, but some consider averaging both $W_{in}$ and $W_{out}$ for slightly better performance.\nLinear Relationships: The famous analogy King - Man + Woman = Queen emerges from the model because of the linear relationships between the words.\nInitialization: This is a no-brainer but worth mentioning. Initialization to zero vectors would mean no learning. We must initialize randomly.\n2. Skip-Gram (The \u0026ldquo;Burger Flip\u0026rdquo; of CBOW) üîÑ Skip-Gram flips the CBOW logic. While CBOW tries to predict the center word given the context, Skip-Gram tries to predict the scattered context given the center word.\nCore Concept: If a word like \u0026ldquo;monarch\u0026rdquo; can accurately predict words like \u0026ldquo;king\u0026rdquo;, \u0026ldquo;throne\u0026rdquo;, \u0026ldquo;crown\u0026rdquo;, \u0026ldquo;royalty\u0026rdquo;, and \u0026ldquo;power\u0026rdquo;, then it must effectively capture the concept of royalty.\nTraining Let\u0026rsquo;s define dimensions:\n$V$: Vocabulary size $D$: Embedding dimension $C$: Number of context words Step 1: Input Lookup\nUnlike CBOW, our input is just one word vector (the center word $w_t$). We grab the vector $v_c$ from the Input Matrix $W_{in}$.\n$$ h = v_{w_t} $$\nNote: There is no averaging here. The hidden layer $h$ is simply the raw vector of the center word.\nStep 2: Output Scoring (The Broadcast)\nThe model takes this single vector $h$ and compares it against the Output Matrix $W_{out}$ (the entire vocabulary). It computes a score $u_j$ for every word $j$ in the dictionary.\n$$ u = W_{out}^T h $$\nResult $u$: A vector of size $V \\times 1$ containing raw scores.\nStep 3: Probability Conversion (Softmax)\nWe apply Softmax to turn scores into probabilities.\n$$ P(w_j | w_t) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 4: The Multi-Target Loss\nHere is the major difference from CBOW. In CBOW, we had one target (the center). In Skip-Gram, we have $2C$ targets (all the surrounding words). For a sentence \u0026ldquo;The cat sat on mat\u0026rdquo; (Center: \u0026ldquo;sat\u0026rdquo;), the targets are \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;. We want to maximize the probability of all these true context words. The Loss function ($E$) sums the error for each context word:\n$$ E = - \\sum_{w_{ctx} \\in \\text{Window}} \\log P(w_{ctx} | w_t) $$\nStep 5: Backpropagation (Accumulating Gradients)\nSince one input word is responsible for predicting multiple output words, the error signals from all those context words add up.\nAt the Output Layer:\nIf \u0026ldquo;cat\u0026rdquo; was a target, the \u0026ldquo;cat\u0026rdquo; vector in $W_{out}$ gets a signal: \u0026ldquo;Move closer to \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; If \u0026ldquo;apple\u0026rdquo; was NOT a target, the \u0026ldquo;apple\u0026rdquo; vector gets a signal: \u0026ldquo;Move away from \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; At the Hidden Layer ($h$):\nThe center word \u0026ldquo;sat\u0026rdquo; receives feedback from all its neighbors simultaneously. Error signal = (Error from \u0026ldquo;The\u0026rdquo;) + (Error from \u0026ldquo;cat\u0026rdquo;) + (Error from \u0026ldquo;on\u0026rdquo;)\u0026hellip; The vector for \u0026ldquo;sat\u0026rdquo; in $W_{in}$ moves in the average direction of all its neighbors. 3. The \u0026ldquo;Computational Nightmare\u0026rdquo; \u0026amp; Negative Sampling The equations above describe Naive Softmax.\nThe Problem: If $V = 100,000$, computing the denominator $\\sum \\exp(u_k)$ takes 100,000 operations. Doing this for every word in a training set of billions of words is impossibly slow.\nThe Solution: Negative Sampling Instead of updating the entire vocabulary (1 correct word vs 99,999 wrong ones), we approximate the problem.\nPositive Pair: (sat, cat) $\\rightarrow$ Maximize probability (Label 1). Negative Pairs: We pick $K$ (e.g., 5) random words the model didn\u0026rsquo;t see, e.g., (sat, bulldozer), (sat, quantum). $\\rightarrow$ Minimize probability (Label 0). New Equation (Sigmoid instead of Softmax):\n$$ E = - \\log \\sigma({v'_{\\text{pos}}}^T v_{\\text{in}}) - \\sum_{k=1}^K \\log \\sigma(-{v'_{\\text{neg}_{k}}}^T v_{\\text{in}}) $$ Effect: We typically only update 6 vectors per step (1 pos + 5 negs) instead of 100,000.\nLet\u0026rsquo;s see this in code:\nimport torch import torch.nn as nn import torch.optim as optim class SkipGramNegativeSampling(nn.Module): \u0026#34;\u0026#34;\u0026#34; Skip-Gram with Negative Sampling (SGNS) implementation. SGNS approximates the softmax over the entire vocabulary by instead distinguishing between a real context word (positive) and K noise words (negative). \u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embedding_dim): super(SkipGramNegativeSampling, self).__init__() # Input embeddings: used when the word is the center word self.in_embed = nn.Embedding(vocab_size, embedding_dim) # Output embeddings: used when the word is a context or negative sample self.out_embed = nn.Embedding(vocab_size, embedding_dim) # Initialize weights with small values to prevent gradient saturation initrange = 0.5 / embedding_dim self.in_embed.weight.data.uniform_(-initrange, initrange) self.out_embed.weight.data.uniform_(-initrange, initrange) def forward(self, center_words, target_words, negative_words): \u0026#34;\u0026#34;\u0026#34; Computes the negative sampling loss. Args: center_words: (batch_size) target_words: (batch_size) negative_words: (batch_size, K) where K is number of negative samples \u0026#34;\u0026#34;\u0026#34; # Retrieve vectors v_c = self.in_embed(center_words) # (batch_size, embed_dim) u_o = self.out_embed(target_words) # (batch_size, embed_dim) u_n = self.out_embed(negative_words) # (batch_size, K, embed_dim) # 1. Positive Score: log(sigmoid(v_c ¬∑ u_o)) # Compute dot product: (batch, 1, dim) @ (batch, dim, 1) -\u0026gt; (batch, 1) pos_score = torch.bmm(u_o.unsqueeze(1), v_c.unsqueeze(2)).squeeze(2) pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-7) # 2. Negative Score: sum(log(sigmoid(-v_c ¬∑ u_n))) # Compute dot products for all K samples: (batch, K, dim) @ (batch, dim, 1) -\u0026gt; (batch, K) neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2) neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-7), dim=1, keepdim=True) # Total loss is the negative of the objective function loss = -(pos_loss + neg_loss) return torch.mean(loss) # --- Configuration \u0026amp; Mock Data --- VOCAB_SIZE = 100 EMBED_DIM = 10 word_to_ix = {\u0026#39;fox\u0026#39;: 0} # Example vocabulary mapping model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock inputs: 1 center word, 1 target word, 5 negative samples center_id = torch.tensor([0], dtype=torch.long) target_id = torch.tensor([1], dtype=torch.long) negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long) # Training Step model.zero_grad() loss = model(center_id, target_id, negative_ids) loss.backward() optimizer.step() # Output Results print(f\u0026#34;Loss after one step: {loss.item():.6f}\u0026#34;) word_vec = model.in_embed(torch.tensor([word_to_ix[\u0026#39;fox\u0026#39;]])) print(f\u0026#34;Vector for \u0026#39;fox\u0026#39;:\\n{word_vec.detach().numpy()}\u0026#34;) The outputs:\nLoss after one step: 4.158468 Vector for \u0026#39;fox\u0026#39;: [[ 0.04197385 0.02400733 -0.03800093 0.01672485 -0.03872231 -0.0061478 0.0121122 0.04057864 -0.036255 0.03861175]] Interesting Facts About Skip-Gram üí° Makes Rare Words Shine: Skip-Gram is more effective at capturing the meaning of rare words because it focuses on the context of the center word. In CBOW, a rare word\u0026rsquo;s vector is averaged with others, diluting its signal. In Skip-Gram, when the rare word appears as the center, it gets the full, undivided attention of the backpropagation update.\nSlow Training: This method creates more training samples. A window of size 2 creates 4 training pairs per center word, while CBOW creates only 1 training pair per center word.\nSemantic Focus: Skip-Gram puts more emphasis on the semantic relationship between words. It views the context as the primary signal, with the center word as the target. This makes it better at capturing the meaning of words in context. Skip-Gram tends to capture semantic relationships (King/Queen) slightly better, while CBOW captures syntactic ones (walk/walked) slightly better.\n","permalink":"http://localhost:1313/posts/word-embeddings/","summary":"Understanding the mathematics behind Word2Vec, CBOW, and Skip-Gram and how they map language to vector space.","title":"Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n5. Code Examples: We\u0026rsquo;ll use sentencepiece for tokenization and compare results for different models.\nimport sentencepiece as spm import os # Configuration for training the SentencePiece model # SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words. options = { # The source text file used for learning the vocabulary \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # The base name for the generated model (.model) and vocabulary (.vocab) files \u0026#39;model_prefix\u0026#39;: \u0026#39;bpe_model\u0026#39;, # Number of unique tokens in the final vocabulary \u0026#39;vocab_size\u0026#39;: 4000, # \u0026#39;bpe\u0026#39; (Byte Pair Encoding) merges frequent pairs of characters/sequences \u0026#39;model_type\u0026#39;: \u0026#39;bpe\u0026#39;, # Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets \u0026#39;character_coverage\u0026#39;: 0.9995, # When enabled, unknown characters are decomposed into UTF-8 bytes to avoid \u0026#39;unk\u0026#39; tokens \u0026#39;byte_fallback\u0026#39;: True, # Treats digits individually (0-9), preventing large numbers from being treated as single tokens \u0026#39;split_digits\u0026#39;: True, # Prevents adding a whitespace prefix to the first token; useful for fine-grained control \u0026#39;add_dummy_prefix\u0026#39;: False } try: print(\u0026#34;Starting the training process...\u0026#34;) # SentencePieceTrainer.train takes the dictionary of options to build the BPE model spm.SentencePieceTrainer.train(**options) print(\u0026#34;Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created.\u0026#34;) # Initialize the processor and load the newly trained model sp = spm.SentencePieceProcessor() sp.load(\u0026#39;bpe_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Model Metadata:\u0026#34;) # Retrieve the total number of tokens in the vocabulary print(f\u0026#34;Total Vocab Size: {sp.get_piece_size()}\u0026#34;) # Special tokens are used for sequence boundaries and handling unknown characters print(f\u0026#34;BOS (Beginning of Sentence) ID: {sp.bos_id()}\u0026#34;) print(f\u0026#34;EOS (End of Sentence) ID: {sp.eos_id()}\u0026#34;) print(f\u0026#34;UNK (Unknown) ID: {sp.unk_id()}\u0026#34;) print(f\u0026#34;PAD (Padding) ID: {sp.pad_id()}\u0026#34;) # Test the tokenizer on sample strings to see how it breaks down text test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: shows the actual subword units (tokens) print(f\u0026#34;Subword Tokens: {sp.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: shows the numerical mapping for each token print(f\u0026#34;Numerical IDs: {sp.encode_as_ids(text)}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred during training or processing: {e}\u0026#34;) You can see the following output:\nStarting training... Starting the training process... Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created. ------------------------------ Model Metadata: Total Vocab Size: 4000 BOS (Beginning of Sentence) ID: 1 EOS (End of Sentence) ID: 2 UNK (Unknown) ID: 0 PAD (Padding) ID: -1 --- Tokenization Test --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;el\u0026#39;, \u0026#39;lo\u0026#39;, \u0026#39;‚ñÅW\u0026#39;, \u0026#39;orld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 380, 309, 476, 619, 36, 3924, 3964, 3959, 3993, 3978, 3983, 57, 3975, 3976, 60, 3974] --- Tokenization Test --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅmo\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;inst\u0026#39;, \u0026#39;ag\u0026#39;, \u0026#39;ram\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅever\u0026#39;] Numerical IDs: [1894, 304, 3284, 376, 321, 313, 312, 1008, 3313, 1325, 428, 3937, 341, 304, 3284, 1495] The ! is fallen back to UTF bytes. and the word instagrammable is split into multiple subword tokens. Also, original text can be recreated right back by simply concatenating the tokens. And replacing underscore by whitespace. Now we can try Unigram model. and see the same sentences tokenized differently.\nfrom tokenizers import Tokenizer from tokenizers.models import WordPiece from tokenizers.trainers import WordPieceTrainer from tokenizers.pre_tokenizers import Whitespace try: # 1. Initialize the WordPiece Tokenizer # We specify the [UNK] token for handling words not found in the vocabulary. tokenizer = Tokenizer(WordPiece(unk_token=\u0026#34;[UNK]\u0026#34;)) # 2. Configure Pre-tokenization # Before the subword algorithm runs, we need to split the raw text into words. # Whitespace splitting is the standard first step for most English NLP tasks. tokenizer.pre_tokenizer = Whitespace() # 3. Initialize the Trainer # We define our target vocabulary size and the special tokens required for # downstream tasks (like BERT\u0026#39;s [CLS] for classification or [SEP] for separators). trainer = WordPieceTrainer( vocab_size=4000, special_tokens=[\u0026#34;[UNK]\u0026#34;, \u0026#34;[CLS]\u0026#34;, \u0026#34;[SEP]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[MASK]\u0026#34;] ) # 4. Train the Model # The tokenizer scans the training file to build a vocabulary of the most # frequent subword units. tokenizer.train(files=[\u0026#34;train_data.txt\u0026#34;], trainer=trainer) # 5. Persist the Model # Save the configuration and vocabulary to a JSON file for future inference. tokenizer.save(\u0026#34;wordpiece.json\u0026#34;) print(\u0026#34;Training complete. \u0026#39;wordpiece.json\u0026#39; created.\u0026#34;) # 6. Metadata Inspection print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;WordPiece Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {tokenizer.get_vocab_size()}\u0026#34;) # 7. Testing Subword Tokenization # WordPiece shines at handling rare words by breaking them into meaningful chunks. test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (WordPiece) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # Encode converts raw text into a Tokenizer object containing tokens and IDs output = tokenizer.encode(text) # \u0026#39;tokens\u0026#39; shows the subword breakdown (e.g., \u0026#39;un\u0026#39;, \u0026#39;##insta\u0026#39;, etc.) print(f\u0026#34;Subword Tokens: {output.tokens}\u0026#34;) # \u0026#39;ids\u0026#39; are the numerical indices mapped to the vocabulary print(f\u0026#34;Numerical IDs: {output.ids}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred with WordPiece model: {e}\u0026#34;) You can see the following output:\nWordPiece Model Metadata: Total Vocab Size: 2609 --- Tokenization Test (WordPiece) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;H\u0026#39;, \u0026#39;##el\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;##or\u0026#39;, \u0026#39;##ld\u0026#39;, \u0026#39;[UNK]\u0026#39;, \u0026#39;[UNK]\u0026#39;] Numerical IDs: [37, 180, 214, 52, 162, 418, 0, 0] --- Tokenization Test (WordPiece) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;##os\u0026#39;, \u0026#39;##t\u0026#39;, \u0026#39;un\u0026#39;, \u0026#39;##ins\u0026#39;, \u0026#39;##ta\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;##ra\u0026#39;, \u0026#39;##m\u0026#39;, \u0026#39;##ma\u0026#39;, \u0026#39;##ble\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;ever\u0026#39;] Numerical IDs: [691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240] We can see the special character ## in the output. We can also see the breakdowns not having any semantic meaning. They are likelihood based. We can try the Unigram model now.\n# \u0026#39;Unigram\u0026#39; is the default and usually recommended over BPE in SentencePiece. options_unigram = { \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # Path to the raw text file for training \u0026#39;model_prefix\u0026#39;: \u0026#39;unigram_model\u0026#39;, # Prefix for the output .model and .vocab files \u0026#39;vocab_size\u0026#39;: 1200, # Desired size of the final vocabulary \u0026#39;model_type\u0026#39;: \u0026#39;unigram\u0026#39;, # Specifies the Unigram language model algorithm \u0026#39;character_coverage\u0026#39;: 0.9995, # Percentage of characters covered by the model (0.9995 is standard for Latin scripts) \u0026#39;byte_fallback\u0026#39;: True, # Enables mapping unknown characters to UTF-8 bytes to avoid \u0026lt;unk\u0026gt; tokens \u0026#39;split_digits\u0026#39;: True, # Treats each digit as an individual token (useful for numerical data) \u0026#39;add_dummy_prefix\u0026#39;: False # Prevents adding a leading space (SentencePiece default is True) } try: # 1. Train the SentencePiece model using the defined options print(\u0026#34;Starting Unigram training...\u0026#34;) spm.SentencePieceTrainer.train(**options_unigram) print(\u0026#34;Training complete. \u0026#39;unigram_model.model\u0026#39; created.\u0026#34;) # 2. Load the trained model into a processor instance for inference sp_unigram = spm.SentencePieceProcessor() sp_unigram.load(\u0026#39;unigram_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Unigram Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {sp_unigram.get_piece_size()}\u0026#34;) # 3. Define test cases to evaluate how the model handles common and rare words test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] # 4. Iterate through test sentences to visualize subword segmentation for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (Unigram) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: Converts text into subword strings (visual representation) print(f\u0026#34;Subword Tokens: {sp_unigram.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: Converts text into numerical indices for model input print(f\u0026#34;Numerical IDs: {sp_unigram.encode_as_ids(text)}\u0026#34;) except Exception as e: # Handle potential errors during training or loading (e.g., missing input file) print(f\u0026#34;An error occurred with Unigram model: {e}\u0026#34;) The output is as follows:\nUnigram Model Metadata: Total Vocab Size: 1200 --- Tokenization Test (Unigram) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ll\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;ld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311] --- Tokenization Test (Unigram) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;T\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅm\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;sta\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;ra\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ver\u0026#39;] ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"A comprehensive guide to tokenization strategies: BPE, WordPiece, Unigram, and SentencePiece.","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"A brief introduction to Vectors \u0026amp; Verbs and formatting verification.","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"\nFeedforward network (FFN) blocks are the least talked about elements in the context of LLMs, but they are where more than 70% of the parameters originate. Something as simple and convenient as a couple of linear transformation matrices becomes a burden when models start scaling into billions of parameters. Thus, getting the maximum juice out of a lean structure is a challenge. FFNs are mostly under-represented in discussions because the sub-layer acts as the primary \u0026ldquo;knowledge\u0026rdquo; processing engine, often containing the majority of a model\u0026rsquo;s total parameters.\nStandard Position-Wise Feedforward Network This is as simple as it gets: a two-layered fully connected network of neurons with a single activation switch‚Äîmost probably a ReLU variant (for LLMs).\n$$ FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2 $$\nThe same transformation is applied to each token in parallel, making it efficient for GPU computation. For example, in the BERT model, the 512 dimensions of each token are expanded to 2048 to allow for \u0026ldquo;intricate interactions\u0026rdquo; before projecting back to 512 dimensions. This expansion/contraction is crucial; in every transformer, stacking of layers is possible only because the output of every layer matches its input dimensions.\nSWiGLU Sigmoid Weighted Gated Linear Unit (SWiGLU) has become the gold standard for feedforward networks in LLMs like Llama 3 and PaLM. The primary reasons for choosing the Swish function (shown in the first image) over ReLU or GELU are as follows:\nMathematical Linearity: Swish (SiLU) is defined as $x \\cdot \\text{sigmoid}(\\beta x)$, where $\\beta$ is a learnable parameter. It is a smooth, non-monotonic function that allows small negative values to pass through. While GELU ($x \\cdot \\Phi(x)$) is also smooth, Swish\u0026rsquo;s specific curve has empirically improved model performance. Computational Efficiency: SiLU gradients are simpler to compute ($x / (1 + \\exp(-x))$) compared to standard GELU approximations. At the massive scale of modern LLMs, these shortcuts save significant training time. Performance: Research in the PaLM and Llama papers found that Swish-based Gated Linear Units consistently outperformed GELU-based versions in terms of perplexity and downstream task accuracy. $$SwiGLU(x, W, V, b, c) = (\\text{Swish}(xW + b) \\otimes (xV + c))W_2$$\nWhere $W$ and $V$ are weight matrices for the gate and the data path, and $\\otimes$ is element-wise multiplication.\nMore Complex Filtering: Instead of using two weight matrices as in a standard FFN, SWiGLU uses three (gate, data, and output). This allows it to extract more complex features without a drastic increase in parameters. Modern implementations often lower the dimensionality of hidden layers (to ~2/3 of what they would be in a standard FFN) while retaining the performance benefits. Zero-Gradient Avoidance: Unlike ReLU, which has a zero gradient for all negative values (the \u0026ldquo;dying ReLU\u0026rdquo; problem), SwiGLU‚Äôs smooth curve ensures the model can still learn from negative inputs, improving training stability in deep stacks. Mixture of Experts (MoE) MoE is an entire architecture rather than just a single block. It has become the dominant strategy for scaling models to insane sizes‚Äîup to a trillion parameters (like DeepSeek-V3 and GPT-4)‚Äîbecause it breaks the direct link between model size (information capacity) and speed (inference cost).\nSparse Activation: MoE uses a \u0026ldquo;Router\u0026rdquo; to determine which smaller \u0026ldquo;Experts\u0026rdquo; (specialized FFNs) are best suited for a specific token. Instead of activating the entire network, only a tiny fraction of experts are used per token. For instance, DeepSeek-V3 might have 256 expert layers but only activate 8 for any single token. Big Scale, Small Compute: MoE allows \u0026ldquo;trillion-parameter intelligence\u0026rdquo; to run at the speed of a much smaller model. While DeepSeek-V3 has 671 billion total parameters, it only uses ~37 billion per token during inference. You get the reasoning performance of a massive model with the latency of a smaller one. Expert Specialization: Because different experts handle different tokens, they naturally specialize. Analysis shows certain experts become \u0026ldquo;specialists\u0026rdquo; in mathematics or coding, while others handle general language. Training Efficiency: Since you only update a fraction of the weights for each token, MoE models can process many more tokens per second during training compared to a dense model of the same total size. Training Dynamics: The router assigns probabilities to each expert. To avoid overfitting or \u0026ldquo;expert collapse\u0026rdquo; (where the router sends everything to one expert), noise is often added to the router\u0026rsquo;s output during training. Some models choose one top expert, while others choose multiple and average their outputs.\nThe Challenges of MoE The RAM Wall: Even if only a small percentage of parameters are used for computation, all 671B parameters must still reside in GPU memory. This makes MoE models difficult to run on consumer hardware. Training Instability: MoEs often require \u0026ldquo;auxiliary losses\u0026rdquo; to force the router to balance the load across experts. DeepSeek-V3 introduced an \u0026ldquo;Auxiliary-Loss-Free\u0026rdquo; strategy to balance load dynamically without degrading accuracy. Communication Overhead: In distributed training, experts are often split across different GPUs. If a token on GPU 1 needs an expert on GPU 4, data must travel over the network. DeepSeek-V3 employs \u0026ldquo;Node-Limited Routing\u0026rdquo; to minimize this congestion. Summary: MoE is the better choice for major labs (like DeepSeek, Google, or OpenAI) trying to maximize intelligence per dollar of training cost. Dense SwiGLU remains the standard for reliability, ease of deployment, and local execution.\n","permalink":"http://localhost:1313/posts/feedforward-networks/","summary":"Exploring the hidden layers of trillion-parameter switchboards: Feedforward Neural Networks and Activation Functions.","title":"Anatomy of Trillion-Parameter Switchboards: Understanding Feedforward Blocks"},{"content":"\nWe are about to touch the holy grail of modern AI. The 2017 declaration was simple: Attention Is All You Need. But as we‚Äôve raced from GPT-1 to reasoning models, the definition of \u0026lsquo;Attention\u0026rsquo; has quietly transformed. Is the algorithm ruling the world today actually the same one we started with? Let‚Äôs look under the hood.\nüî• The Core Concept At its heart, the attention mechanism is a form of weighted aggregation. It is often described using a database retrieval analogy. Imagine you have a database of information (context).\nQuery ($Q$): What are you currently looking for? (The token being generated). Key ($K$): The indexing tag associated with each piece of information. Value ($V$): The actual information (The context token). The \u0026ldquo;Attention\u0026rdquo; is the process of matching your Query against all the Keys to determine how much \u0026ldquo;weight\u0026rdquo; to put on each Value. Attention scores are calculated in each transformer layer at every step of the sequence. This is a single unifying factor in all transformer models, from BERT to Gemini.\nüî• Standard Multi-Head Attention (MHA) The original mechanism introduced in the paper \u0026ldquo;Attention Is All You Need\u0026rdquo;.\nHere, instead of performing the Key-Query-Value (KQV) operation on the entire context only once, the attention is split into $N$ parallel heads. Each head learns a different aspect of the language (e.g., word meaning, syntax, semantic relationships, etc.). The equation is a simple culmination of dot products aided by a regularizer like Softmax.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nWhere:\n$QK^T$ computes the similarity scores between the Query and Key. $\\sqrt{d_k}$ is a scaling factor to prevent the dot product from growing too large, mitigating the vanishing/exploding gradient problem. The weighted sum is applied to the values of $V$. The Memory Bandwidth Bottleneck: \u0026gt; During inference, the model must store Key and Value matrices for all previous tokens. This is called the KV Cache. In MHA, every single head has its own unique set of Keys and Values. For large models with massive context windows, loading this massive KV cache from memory (HBM) to the computation chip (SRAM) becomes the primary bottleneck, slowing down generation significantly. Earlier models could often only generate ~8 tokens per second at inference due to this constraint.\nüî• Multi-Query Attention (MQA) This approach addresses the memory bottleneck by sharing the same set of Keys and Values across all heads while keeping separate Query heads.\nMHA: $H$ Query heads, $H$ Key heads, $H$ Value heads MQA: $H$ Query heads, 1 Key head, 1 Value head This drastically reduces the memory requirements of the KV cache by a factor of $H$ (often 8x or 16x). The GPU needs to load far less data, significantly speeding up inference. However, it does not reduce the computation (FLOPs)‚Äîso why is it faster?\nThis can be explained with the Kitchen Analogy: Imagine a kitchen (GPU) with 8 chefs (Heads).\nComputation (Math): The Chef chopping vegetables. Memory Access (Bandwidth): The Assistant running to the fridge to get ingredients. In MHA: Each chef demands unique vegetables from the fridge. The assistant runs back and forth 8 times, fetching 8 different crates. Most of the time is spent waiting for the assistant to return. This results in idle cores waiting for data to load from HBM.\nIn MQA: All chefs agree to use the same vegetables. The assistant runs back and forth only once, fetching one giant crate of potatoes and dumping it. This results in minimized memory transfer and no idle cores.\nThe Technical Reality: During text generation, the model is memory-bound, not compute-bound. The time it takes to move the KV Cache from HBM (High Bandwidth Memory) to the chip\u0026rsquo;s SRAM is the bottleneck. MQA reduces the data volume of that move by a factor of $H$.\nThe Capacity Gap: MQA is a trade-off between nuanced response and memory capacity. The model loses the ability to store different types of information, as a single $V$ matrix must now store all nuances of information flow in the language.\nThe Surgery Trick (Uptraining): Google Research figured out a way to take an existing model with MHA and convert it to MQA by averaging the weight matrices of $H$ heads. This is called Uptraining. You then train this \u0026ldquo;Frankenstein\u0026rdquo; model for ~5% of the original training steps for adjustments. This yields an MQA model that is nearly as good as the original MHA but vastly faster.\nüî• Grouped Query Attention (GQA) A \u0026ldquo;best of both worlds\u0026rdquo; solution used in modern models like Llama 2 and Llama 3.\nHow it works: GQA acts as an interpolation between MHA and MQA. Instead of having just 1 Key head (MQA) or $H$ Key heads (MHA), it divides the query heads into $G$ groups. Each group shares a single Key and Value head.\nEquation modification: If you have 8 query heads, you might create 4 groups (2 queries per group). This reduces the KV cache by 2x instead of 8x, preserving more quality than MQA while still speeding up inference.\nMHA: 8 people (heads) are working on a project. Each person has their own unique filing cabinet (KV cache). It takes forever to move 8 cabinets around. MQA: 8 people share one single filing cabinet. It\u0026rsquo;s very fast to move, but they fight over the organization and lose detail. GQA: The 8 people split into 4 teams of 2. Each team shares a cabinet. It\u0026rsquo;s a balance between speed and organizational detail. üî• Multi-Head Latent Attention (MLA) This is the most advanced mechanism for attention, used in models like DeepSeek-V3. It is a sophisticated design for massive efficiency that takes a radical approach: mathematically compressing the memory into a low-rank \u0026ldquo;latent\u0026rdquo; vector.\nKV Cache Explosion: As mentioned before, for a massive model like DeepSeek-V3 (671B parameters), storing the full Keys and Values for every token in a long conversation requires terabytes of high-speed memory (HBM), and chips spend more time waiting than churning.\nMHA (Standard) stores everything. MQA deletes redundancy but hurts quality. MLA keeps the quality of MHA but compresses the data 4x-6x smaller than even GQA.\nThe Zip File Approach: Low-Rank Joint Compression Let\u0026rsquo;s compare the math of MHA and MLA. In MHA, for every single token, we generate a unique Key ($k$) and Value ($v$) matrix for each attention head.\nThe Memory Problem: The $k$ is stored in the KV Cache. For 128 heads, for example, it calculates a massive vector of $128 \\times 128 = 16,384$ floats per token.\nFor MHA, the attention score looks like: $$\\text{Score} = q^T \\cdot k$$\nBut for MLA, instead of storing the full $k$, it projects the input down into a tiny latent vector $c_{KV}$, which is much smaller (e.g., 512 floats). $$c_{KV} = W_{DKV} \\cdot x$$\nNow here is the catch: Naive compression and decompression would mean constructing a small latent matrix for every token and deconstructing it every single time the information is required, which defeats the purpose of compression.\n$$k_{\\text{reconstructed}} = W_{UK} \\cdot c_{KV}$$ $$\\text{Score} = q^T \\cdot k_{\\text{reconstructed}}$$\nThe Matrix Absorption Trick (Optimization) Instead of reconstructing $k$ for every token, we can absorb the Up-Projection matrix ($W_{UK}$) into the Down-Projection matrix ($W_{DKV}$) and absorb the query $q$ into a new $Q$.\nFrom the original equation: $$\\text{Score} = q^T \\cdot (\\underbrace{W_{UK} \\cdot c_{KV}}_{\\text{This is } k})$$\nWe associate differently:\n$$\\text{Score} = (\\underbrace{q^T \\cdot W_{UK}}_{\\text{Absorbed Query}}) \\cdot c_{KV}$$ We change the order in which the matrix multiplication is performed.\nHow is this allowed? In linear algebra, matrix multiplication is associative:\n$$(A \\cdot B) \\cdot C = A \\cdot (B \\cdot C)$$ We can move the parentheses! Instead of grouping $(W_{UK} \\cdot c_{KV})$ to make the Key, we group $(q^T \\cdot W_{UK})$ to make a new Query.\nNote: The positional embedding information is left untouched. That rotational information from algorithms like RoPE is preserved outside the compression; it requires minimal space anyways.\n","permalink":"http://localhost:1313/posts/attention-mechanisms/","summary":"We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek\u0026rsquo;s MLA, how has the definition of \u0026lsquo;Attention\u0026rsquo; transformed?","title":"Attention Is All You Need, but exactly which one?: MHA, GQA and MLA"},{"content":"\nPositional Embeddings are the \u0026ldquo;voice\u0026rdquo; that tell Transformers where words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.\n\u0026ldquo;The dog chased the cat\u0026rdquo; and \u0026ldquo;cat chased the dog\u0026rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.\nüî• Sinusoidal Embeddings: The Odometer of Language Introduced in the seminal \u0026ldquo;Attention Is All You Need\u0026rdquo; paper, this was the industry standard for many years. This approach adds a vector to each word embedding based on its position. The vector is computed out of wavelike patterns that are periodic and smooth. Mathematically, for a position $pos$ and embedding dimension $i$:\n$$ PE_{(pos, i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $$\nThe model uses sine and cosine waves of different frequencies to capture both short and long-range dependencies. The frequency of the waves decreases as the embedding dimension increases, which helps capture long-range dependencies in the input sequence.\nThe creators wanted the model to calculate distance easily, which is why a linear function was used. The model simply rotates the current position vector using a rotation matrix to derive the next position.\n$$ \\begin{pmatrix} \\cos(k) \u0026 \\sin(k) \\\\ -\\sin(k) \u0026 \\cos(k) \\end{pmatrix} \\cdot PE(pos) \\approx PE(pos + k) $$ Low dimensions act like a fast-ticking clock (high frequency, changing rapidly) and high dimensions act like a slow clock (low frequency, changing slowly).\nThe number 10,000 is an arbitrarily chosen high number to reduce Aliasing. By choosing such a high number, the creators ensured that the wavelength of the slowest dimension is incredibly long. Consequently, the pattern practically never repeats, and each position remains unique.\nThis combination works like a mechanical odometer dial. Each subsequent dimension acts as a slower-moving gear. The model learns to read these \u0026ldquo;dials\u0026rdquo; to understand exactly where a word sits in the sequence.\nThis mitigated the challenge of exploding gradients that previous array-based positional embeddings suffered from (e.g., adding integers 1, 2, 3\u0026hellip;). This model keeps values between -1 and +1, adding stability while ensuring every position gets a distinct code.\nThe Problem: This approach pollutes the word embedding matrix (semantic information). While high-dimensional vectors are almost always orthogonal‚Äîmeaning the noise doesn\u0026rsquo;t fully destroy the word\u0026rsquo;s identity‚Äîit does not strictly guarantee purity.\nThis led to a more mature approach.\nüî• ALiBi: The Fading Streetlight Attention with Linear Biases (ALiBi) is a method where, instead of adding a vector to the embeddings at the start, the model injects positional information linearly, directly into the attention scores. If Sinusoidal Encoding asks, \u0026ldquo;Where am I on the map?\u0026rdquo;, ALiBi asks, \u0026ldquo;How much harder should I squint to see you?\u0026rdquo;\nThe biggest problem ALiBi solved is the \u0026ldquo;Invisible Wall\u0026rdquo;. If you train a Transformer on sequences of length 2048 and then feed it a book with 3000 words, it will crash or output garbage at word 2049. This is a failure of Extrapolation.\nALiBi throws away the idea of adding positional vectors entirely. Instead, it devises a \u0026ldquo;penalty\u0026rdquo; system for attention scores:\n$$ \\text{Score} = (q \\cdot k^\\top) - (m \\cdot \\text{distance}) $$\nDistance: If words A and B are neighbors, the distance is 0, so the penalty is 0. Although the distance is linear, it does not cause exploding gradients because the raw attention scores are normalized by a softmax function. This distance prioritizes nearby words (local context) over distant ones.\nSlope ($m$): If we always penalize distance equally, the model becomes myopic. ALiBi mitigates this by assigning a different slope value to each attention head:\nHead 1 (The Historian): Very low slope. The penalty grows slowly, allowing it to see far back. Head 2 (The Reader): Medium slope. Head 3 (The Myopic): Very high slope (e.g., 0.5); it effectively cannot see beyond 2-3 words. This mimics holding a lantern. Words close to you are bright, while words further away naturally fade into darkness.\nExtrapolation: If you walk 5 miles down the road (a longer sequence), your lantern works exactly the same way. The physics of light decay are constant, allowing the model to handle sequences longer than it was trained on.\nHowever, ALiBi forces a \u0026ldquo;sliding window\u0026rdquo; view where distant words eventually become invisible. This brings us to the modern standard.\nüî• RoPE: The Rotary Revolution Rotary Position Embeddings (RoPE) is the current industry standard (used in Llama 3, Mistral, PaLM) because it mathematically unifies \u0026ldquo;absolute position\u0026rdquo; with \u0026ldquo;relative position.\u0026rdquo; This method rotates the vectors in the Query and Key matrices based on their absolute position.\nIf a vector has 512 dimensions, RoPE breaks it into 256 pairs. Each pair is rotated by a different frequency:\n$$ \\theta_i = 10000^{-2i/d} $$\nThe First pair ($i = 0$) is rotated by the highest frequency. The Last pair ($i = 255$) is rotated by the lowest frequency. This spread ensures the model has \u0026ldquo;high precision\u0026rdquo; (fast rotation) to distinguish immediate neighbors, and \u0026ldquo;low precision\u0026rdquo; (slow rotation) to track global position without the pattern repeating.\nDoesn\u0026rsquo;t rotation change meaning? Yes and No. The vector for \u0026ldquo;Apple\u0026rdquo; at position 1 and \u0026ldquo;Apple\u0026rdquo; at position 256 will look completely different in terms of coordinates. However, rotation does not change the semantic strength (magnitude/norm) of the vector.\nThe Relativity Trick: Transformers compare words using dot products (angles). RoPE relies on the fact that the attention mechanism ignores the absolute rotation and only detects the difference in rotation (relative position). Thus, the \u0026ldquo;relative meaning\u0026rdquo; is encoded purely into the angle between words, leaving the semantic \u0026ldquo;magnitude\u0026rdquo; untouched.\nüî• LongRoPE: The Bifocal Lens While RoPE is powerful, it struggles when stretching context windows from 4k to 2 million tokens. The \u0026ldquo;fast\u0026rdquo; dimensions spin so rapidly over long distances that they become random noise. LongRoPE solves this using an evolutionary search algorithm to find a unique scaling factor ($\\lambda$) for each dimension.\nThe Equations Instead of rotating by the standard $\\theta_i$, LongRoPE rotates by a rescaled frequency:\n$$ \\theta\u0026rsquo;_i = \\lambda_i \\theta_i $$\nTo efficiently find these $\\lambda$ values without searching an infinite space, the algorithm enforces a monotonicity constraint based on NTK theory:\n$$ \\lambda_i \\le \\lambda_{i+1} $$\nThis ensures that low-frequency dimensions (global context) are stretched more than high-frequency dimensions (local context), creating a \u0026ldquo;bifocal\u0026rdquo; effect:\nHigh Frequencies (Local): Kept sharp ($\\lambda \\approx 1$) to maintain grammar. Low Frequencies (Global): Stretched ($\\lambda \u0026gt; 1$) to track massive distances without repeating. Not worth perusing more since the next one is much better at long context positional encoding. üî• HoPE: The Hyperbolic Slide Hyperbolic Rotary Positional Encoding (HoPE) moves from the geometry of a circle to the geometry of a hyperbola (inspired by Lorentz transformations). It was designed to fix a subtle flaw in RoPE: the \u0026ldquo;wobbly\u0026rdquo; or oscillatory nature of attention scores.\nThe Core Problem: RoPE\u0026rsquo;s \u0026ldquo;Wobble\u0026rdquo; In standard RoPE, we rotate vectors around a circle.\nThe Issue: Circles repeat. A dial at $361^\\circ$ looks identical to $1^\\circ$. Even with different frequencies, the dot product fluctuates up and down as distance increases. The Consequence: The model gets confused at long distances, potentially mistaking a word 1,000 tokens away for a neighbor just because the rotation cycles aligned, creating \u0026ldquo;noise.\u0026rdquo; The \u0026ldquo;Hyperbolic\u0026rdquo; Shift HoPE replaces trigonometric functions ($\\sin, \\cos$) with hyperbolic functions ($\\sinh, \\cosh$).\nCircular Rotation: Keeps distance from the center constant. Hyperbolic Rotation: Moves points along a hyperbola. Crucially, hyperbolic functions do not repeat; they grow or decay exponentially. By using hyperbolic rotation, HoPE guarantees that as words get further apart, the attention score decays monotonically (smoothly drops).\nThe Equations HoPE uses a hyperbolic matrix $B(\\theta, m)$ defined using the Lorentz Boost structure:\n$$ B(\\theta, m) = \\begin{pmatrix} \\cosh(m\\theta) \u0026 \\sinh(m\\theta) \\\\ \\sinh(m\\theta) \u0026 \\cosh(m\\theta) \\end{pmatrix} $$ Since hyperbolic functions grow to infinity, HoPE introduces a decay penalty ($e^{-m\\theta'}$). The final attention score simplifies beautifully to a function of relative distance: $$ \\text{Score} \\propto e^{-|m-n|(\\theta\u0026rsquo; - \\theta)} $$\nThis ensures the attention score exponentially decays as distance increases, eliminating the wobbles of standard RoPE. This the latest model that is taking up the space in the industry for extremely long context models.\n","permalink":"http://localhost:1313/posts/positional-embeddings/","summary":"From Sinusoidal to RoPE and HoPE: How Transformers learn to process word order and sequence length.","title":"The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE"},{"content":"Imagine trying to teach a computer the difference between \u0026ldquo;Apple\u0026rdquo; the fruit and \u0026ldquo;Apple\u0026rdquo; the company. To us, the distinction is intuitive. To a machine, it‚Äôs just a string of characters. How do we turn these strings into meaningful math? While early attempts like Word2Vec gave us a great start, they missed the forest for the trees‚Äîor in some cases, the twigs for the branches. Enter GloVe and FastText: two algorithms that revolutionized how machines understand the nuances of human language.\nPrevious static embedding models like Word2Vec successfully captured the local semantics of words, but they failed in addressing contexts between words that might not always appear in the same context window. Furthermore, they treated words as indivisible atomic units in continuous vector spaces, missing out on the internal structure of language.\nThe two simple but effective ideas behind GloVe and FastText are subword units and global co-occurrence. Let\u0026rsquo;s dive into them.\nGloVe: A Global Mapping Strategy Global Vectors for Word Representation (GloVe) was created at Stanford. The core idea was to train a model where word vectors are defined by how often they appear around other words across the entire corpus, rather than just locally.\nTraining Steps: Co-occurrence Matrix X: GloVe scans the entire corpus and creates a giant matrix (spreadsheet) recording the counts of how often words appear near each other. Factorization: It compresses this giant matrix into smaller, dense vectors. Objective: The function tries to equate the dot product of two word vectors with the logarithm of their probability of appearing together. The error between these values is then backpropagated to update the vectors. $$w_i \\cdot w_j + b_i + b_j = \\log(X_{ij})$$\n$w$: The word vector (what we want). $X_{ij}$: The count from our matrix. The Loss Function: We can\u0026rsquo;t satisfy the equation above perfectly for every word pair. So, we use a weighting function to minimize the squared error:\n$$J = \\sum_{i,j=1}^V f(X_{ij}) ({w_i}^T w_j + b_i + b_j - \\log(X_{ij}))^2$$\nIf we do not use the weighting function, the loss would be infinite for rare words (since $\\log(0) = \\infty$) and frequent words (like \u0026rsquo;the\u0026rsquo;) would overpower the model.\n$$ f(x) = \\begin{cases} (x/x_{max})^\\alpha \u0026amp; \\text{if } x \u0026lt; x_{max} \\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$\n(Typically $x_{max}=100$ and $\\alpha=0.75$)\nFastText: The Lego Bricks Strategy FastText was created by Facebook AI Research (FAIR). The idea was to treat words not as solid rocks, but as structures made of \u0026ldquo;Lego bricks\u0026rdquo; (subword units).\nWord2Vec struggled to create embeddings for words it hadn\u0026rsquo;t seen before (Out-Of-Vocabulary or OOV words). FastText solves this by breaking words down.\nGloVe treats a word like \u0026ldquo;Apple\u0026rdquo; as a single, atomic unit. FastText breaks \u0026ldquo;Apple\u0026rdquo; into character n-grams: \u0026lt;ap, app, ppl, ple, le\u0026gt; For a complex or new word like \u0026ldquo;Applesauce\u0026rdquo;, the model might not know the whole word, but it recognizes the Lego brick for \u0026ldquo;Apple\u0026rdquo; and the remaining bits from other subwords.\nFastText takes its inspiration from the SkipGram architecture. The vector for a word is simply the sum of the vectors of its n-gram subwords:\n$$v_w = \\sum_{g \\in G_w} z_g$$\nObjective: The objective is to predict context words given a target word.\n$$P(w_c | w_t) = \\frac{e^{s(v_{w_t} \\cdot v_{w_c})}}{\\sum_{w \\in V} e^{s(v_{w_t} \\cdot v_w)}}$$\nAnd the scoring function is defined as:\n$$s(w_t, w_c) = u_{w_t}^T (\\sum_{g \\in G_w} z_g)$$\n$u_{w_c}$: The vector for the context word (output vector). $z_g$: The vector for the n-gram $g$ (input vector). Modifications to Softmax: Calculating the full softmax for every word in the vocabulary is computationally expensive. FastText uses two methods to approximate this efficiently.\nOption A: Negative Sampling\nWe pick a few random \u0026ldquo;wrong\u0026rdquo; words every time to serve as \u0026ldquo;negative samples\u0026rdquo; and calculate the loss only for the correct word and these noise words.\n$$J = - \\log \\sigma(s(w_t, w_c)) - \\sum\\limits_{i=1}^{N} \\mathbb{E}_{n_i \\sim P_n(w)} [\\log \\sigma(-s(w_t, n_i))]$$\nThe sigmoid function $\\sigma$ squashes the score between 0 and 1. The first term pushes the probability of the correct word towards 1. The second term pushes the probability of the noise words towards 0. Option B: Hierarchical Softmax (Faster for infrequent words)\nInstead of a flat list of words, imagine the vocabulary arranged as a binary tree (Huffman Tree).\nRoot: Top of the tree. Leaves: The actual words. To calculate the probability of a word, we trace a path from the root to that leaf. At every branching node, we calculate the probability (sigmoid) of going left or right. The probability of a word $w$ is the product of the probabilities of the turns taken to reach it. Code Example from gensim.models import FastText from gensim.test.utils import common_texts # Initialize the FastText model with specific hyperparameters model = FastText( vector_size=100, # Dimensionality of the word vectors window=5, # Maximum distance between the current and predicted word within a sentence min_count=1, # Ignores all words with total frequency lower than this sg=1, # Training algorithm: 1 for skip-gram; 0 for CBOW hs=0, # If 0, and negative is non-zero, negative sampling will be used negative=5, # Number of \u0026#34;noise words\u0026#34; to be drawn for negative sampling workers=4, # Number of worker threads to train the model epochs=10, # Number of iterations over the corpus min_n=3, # Minimum length of character n-grams max_n=6 # Maximum length of character n-grams ) # Build the vocabulary from the provided text corpus model.build_vocab(common_texts) # Train the model on the corpus model.train( common_texts, total_examples=model.corpus_count, epochs=model.epochs ) # Calculate and print the cosine similarity between two words in the vocabulary print(f\u0026#34;Similarity between \u0026#39;computer\u0026#39; and \u0026#39;human\u0026#39;: {model.wv.similarity(\u0026#39;computer\u0026#39;, \u0026#39;human\u0026#39;)}\u0026#34;) # Demonstrate FastText\u0026#39;s ability to handle Out-Of-Vocabulary (OOV) words # Even if \u0026#39;computation\u0026#39; wasn\u0026#39;t in the training data, FastText constructs a vector using character n-grams oov_vector = model.wv[\u0026#39;computation\u0026#39;] # Check if a specific word exists in the model\u0026#39;s fixed vocabulary index word = \u0026#34;computer\u0026#34; is_in_vocab = word in model.wv.key_to_index print(f\u0026#34;\\nIs the word \u0026#39;{word}\u0026#39; in the model\u0026#39;s vocabulary? {is_in_vocab}\u0026#34;) ","permalink":"http://localhost:1313/posts/glove-and-fasttext/","summary":"How subword units and global co-occurrence matrices allow GloVe and FastText to capture nuances that Word2Vec missed.","title":"The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText"},{"content":" Before we had Large Language Models writing poetry, we had to teach computers that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; are related not just by spelling, but by meaning. This is the story of that breakthrough. It‚Äôs the moment we stopped counting words and started mapping their souls‚Äîturning raw text into a mathematical landscape where math can solve analogies. Welcome to the world of Word2Vec. üîÆ\nLanguage models require vector representations of words to capture semantic relationships. Before the 2010s, models used word count-based vector representations that captured only the frequency of words (e.g., One-Hot Encoding). The Problems: üöß\nThe Curse of Dimensionality Lack of meaning (Synonyms were treated as mathematically unrelated). In 2011, Mikolov et al. at Google introduced Word2Vec, which used a shallow neural network to learn vector representations of words. The best part? It could create much denser relationships between words than prior models, and it was unsupervised.\nThe training (prediction) is essentially \u0026ldquo;fake\u0026rdquo;‚Äîa pretext task. But the weight matrices we get on the side are a gold mine of fine-grained semantic relationships. ‚õèÔ∏è\nThere are two popular variants of Word2Vec:\nSkipGram Continuous Bag Of Words (CBOW) 1. Continuous Bag of Words (CBOW) üß© CBOW is a simple \u0026lsquo;fill in the blanks\u0026rsquo; machine. It takes the surrounding words as inputs and tries to predict the center word.\n\u0026ldquo;Continuous\u0026rdquo;: It operates in continuous vector space (unlike the discrete space of n-gram models).\n\u0026ldquo;Bag of words\u0026rdquo;: The order of the context does not matter. \u0026ldquo;The cat sat on the mat\u0026rdquo; and \u0026ldquo;The mat sat on the cat\u0026rdquo; are the same for CBOW. They produce exactly the same predictions.\nObjective: Maximize the probability of the target word $w_t$ given its context $w_{t-C}, \\dots, w_{t+C}$.\n2. Training Defining Dimensions:\n$V$: Vocabulary size (number of unique words in the corpus, e.g., 10,000) $N$: Dimension of the embedding space (e.g., 100) $C$: Number of context words (e.g., 2 words on left of target and 2 words on right) Step 1: Input Lookup \u0026amp; One-Hot Encoding\nMathematically, we represent each context word as a one-hot vector $x^{(c)}$. We have an Input Matrix $W_{in}$ of size $V \\times N$. For each context word (e.g., \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;), we select its corresponding row from $W_{in}$.\n$$ v_c = W_{in}^T x^{(c)} $$\nWhere $v_c$ is the $N$-dimensional vector for context word $c$.\nStep 2: Projection (The \u0026ldquo;Mixing\u0026rdquo;)\nWe combine the context vectors. In standard CBOW, we simply average them. This creates the hidden layer vector $h$.\n$$ h = \\frac{1}{2C} \\sum_{c=1}^{2C} v_c $$\n$h$ is a single vector of size $N \\times 1$. Note: This operation is linear and contains no activation function (like ReLU or Sigmoid).\nStep 3: Output Scoring (The \u0026ldquo;Dot Product\u0026rdquo;)\nWe have a second matrix, the Output Matrix $W_{out}$ of size $N \\times V$. We compute the raw score (logit) $u_j$ for every word $j$ in the vocabulary by taking the dot product of the hidden vector $h$ and the output vector $v\u0026rsquo;_j$.\n$$ u_j = {v\u0026rsquo;_j}^T h $$\nOr in matrix form:\n$$ u = W_{out}^T h $$\nResult $u$ is a vector of size $V \\times 1$.\nStep 4: Probability Conversion (Softmax)\nWe convert the raw scores $u$ into probabilities using the Softmax function. This tells us the probability that word $w_j$ is the center word.\n$$ y_j = P(w_j | \\text{context}) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 5: Loss Calculation (Cross-Entropy)\nWe compare the predicted distribution $y$ with the actual ground truth $t$ (a one-hot vector where the index of the true word is 1). The loss function $E$ is:\n$$ E = - \\sum_{j=1}^V t_j \\log(y_j) $$\nSince $t_j$ is 0 for all words except the target word (let\u0026rsquo;s call the target index $j^*$), this simplifies to:\n$$ E = - \\log(y_{j^*}) $$\n3. Backpropagation Details We need to update the weights to minimize $E$. We use the Chain Rule.\nA. Gradient w.r.t. Output Matrix ($W_{out}$)\nWe want to know how the error changes with respect to the raw score $u_j$.\n$$ \\frac{\\partial E}{\\partial u_j} = y_j - t_j $$\nLet\u0026rsquo;s call this error term $e_j$.\nIf $j$ is the target word: $e_j = y_j - 1$ (Result is negative; we boost the score). If $j$ is not target: $e_j = y_j - 0$ (Result is positive; we suppress the score). The update rule for the output vector $v\u0026rsquo;_j$ becomes:\n$$ v'_{j}(\\text{new}) = v'_{j}(\\text{old}) - \\eta \\cdot e_j \\cdot h $$ ($\\eta$ is the learning rate)\nB. Gradient w.r.t. Hidden Layer ($h$)\nWe backpropagate the error from the output layer to the hidden layer.\n$$ EH = \\sum_{j=1}^V e_j \\cdot v\u0026rsquo;_j $$\n$EH$ is an $N$-dimensional vector representing the aggregate error passed back to the projection layer.\nC. Gradient w.r.t. Input Matrix ($W_{in}$)\nSince $h$ was just an average of the input vectors, the error $EH$ is distributed to each context word\u0026rsquo;s input vector. For every word $w_c$ in the context:\n$$ v_{w_c}(\\text{new}) = v_{w_c}(\\text{old}) - \\eta \\cdot \\frac{1}{2C} \\cdot EH $$\nLet\u0026rsquo;s build a single pass through the network for a given context.\nimport torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # Define a simple Continuous Bag of Words (CBOW) style model class CBOWModel(nn.Module): def __init__(self, vocab_size, embedding_dim): super(CBOWModel, self).__init__() # The embedding layer stores the word vectors we want to learn self.embeddings = nn.Embedding(vocab_size, embedding_dim) # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word self.linear = nn.Linear(embedding_dim, vocab_size) def forward(self, inputs): # inputs: tensor of word indices for the context embeds = self.embeddings(inputs) # Aggregate context by calculating the mean of the word embeddings h = torch.mean(embeds, dim=1) # Produce logits (raw scores) for each word in the vocabulary logits = self.linear(h) return logits # Setup vocabulary and mappings word_to_ix = {\u0026#34;the\u0026#34;: 0, \u0026#34;quick\u0026#34;: 1, \u0026#34;brown\u0026#34;: 2, \u0026#34;fox\u0026#34;: 3, \u0026#34;jumps\u0026#34;: 4, \u0026#34;over\u0026#34;: 5, \u0026#34;lazy\u0026#34;: 6, \u0026#34;dog\u0026#34;: 7} ix_to_word = {v: k for k, v in word_to_ix.items()} # Configuration constants EMBEDDING_DIM = 5 VOCAB_SIZE = len(word_to_ix) LEARNING_RATE = 0.01 # Initialize the model, loss function, and optimizer model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM) loss_function = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE) # Prepare dummy training data: context words and the target word # Context: [\u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;over\u0026#34;] -\u0026gt; Target: \u0026#34;fox\u0026#34; context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long) target_idxs = torch.tensor([3], dtype=torch.long) # Perform a single optimization step model.zero_grad() # Forward pass: we reshape context to (1, -1) to simulate a batch of size 1 logits = model(context_idxs.view(1, -1)) # Calculate loss against the target word index loss = loss_function(logits, target_idxs) # Backpropagate and update weights loss.backward() optimizer.step() # Output results for the blog post print(\u0026#34;=== Model Training Snapshot ===\u0026#34;) print(f\u0026#34;Calculated Loss: {loss.item():.6f}\u0026#34;) print(f\u0026#34;\\n=== Learned Vector for \u0026#39;jumps\u0026#39; ===\u0026#34;) word_vec = model.embeddings(torch.tensor([word_to_ix[\u0026#39;jumps\u0026#39;]])) print(word_vec.detach().numpy()) print(\u0026#34;\\n=== Embedding Matrix (Weights) ===\u0026#34;) print(model.embeddings.weight.detach().numpy()) print(\u0026#34;\\n=== Linear Layer Weights ===\u0026#34;) print(model.linear.weight.detach().numpy()) Output shows single pass loss and learned vector for \u0026lsquo;jumps\u0026rsquo; word. This is a single training sample result but after multiple such passes both Embedding and linear weights should be very similar.\n=== Model Training Snapshot === Calculated Loss: 1.989656 === Learned Vector for \u0026#39;jumps\u0026#39; === [[-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775]] === Embedding Matrix (Weights) === [[ 1.66039 -0.11371879 0.6246518 0.35860053 -1.2504417 ] [-2.1847186 -0.77199775 -0.17050214 -0.38411248 -0.03913084] [ 0.11852697 0.90073633 0.8847807 0.7404524 0.900149 ] [ 0.07440972 -0.40259898 2.6246994 -0.08851447 0.02660969] [-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775] [-0.9245572 -0.5545908 0.9083091 -1.0755049 0.84047747] [-1.0237687 0.59466314 0.05621134 -0.6202532 1.3664424 ] [ 0.60998917 -1.0549186 1.6103884 0.8724912 -1.2486908 ]] === Linear Layer Weights === [[ 0.02165058 -0.28883642 0.14545658 -0.3442509 0.32704315] [-0.18731792 0.28583744 0.22635977 0.13245736 0.29019794] [ 0.3158916 -0.15826383 -0.03203773 0.16377363 -0.41457543] [-0.05080034 0.4180087 0.11228557 0.30218413 0.3025514 ] [-0.38419306 0.24475925 -0.39210224 -0.38660625 -0.2673145 ] [-0.32321444 0.12200444 -0.03569533 0.2891424 -0.07345333] [-0.33704326 0.2521956 0.31587374 0.22590035 0.29866052] [ 0.4117266 -0.44231793 0.24064957 -0.29684234 0.333821 ]] Interesting Facts About CBOW üß† The \u0026ldquo;Averaging Problem\u0026rdquo;: This is a common issue in CBOW models. The averaging and training is order-agnostic. All variants of word order in the context window will generate the same output. \u0026ldquo;Dog bit the man\u0026rdquo; and \u0026ldquo;Man bit the dog\u0026rdquo; are the same in CBOW\u0026rsquo;s eyes.\nSmoothing Effect: CBOW models are much faster to train than Skip-Gram but are generally smoother.\nRare Words: CBOW models struggle slightly with rare words because the presence of surrounding common words (like articles and prepositions) can skew the embeddings and dissolve their uniqueness.\nComputational Cost: Softmax requires summing over the entire vocabulary size, which can be computationally expensive if $V$ is very large. Solutions include Hierarchical Softmax and Negative Sampling (approximating the denominator by only checking the target word vs. 5 random noise words).\nVectors as Double Agents: Every word has a different representation in the embedding matrix and the output matrix. Most of the time $W_{out}$ is discarded, but some consider averaging both $W_{in}$ and $W_{out}$ for slightly better performance.\nLinear Relationships: The famous analogy King - Man + Woman = Queen emerges from the model because of the linear relationships between the words.\nInitialization: This is a no-brainer but worth mentioning. Initialization to zero vectors would mean no learning. We must initialize randomly.\n2. Skip-Gram (The \u0026ldquo;Burger Flip\u0026rdquo; of CBOW) üîÑ Skip-Gram flips the CBOW logic. While CBOW tries to predict the center word given the context, Skip-Gram tries to predict the scattered context given the center word.\nCore Concept: If a word like \u0026ldquo;monarch\u0026rdquo; can accurately predict words like \u0026ldquo;king\u0026rdquo;, \u0026ldquo;throne\u0026rdquo;, \u0026ldquo;crown\u0026rdquo;, \u0026ldquo;royalty\u0026rdquo;, and \u0026ldquo;power\u0026rdquo;, then it must effectively capture the concept of royalty.\nTraining Let\u0026rsquo;s define dimensions:\n$V$: Vocabulary size $D$: Embedding dimension $C$: Number of context words Step 1: Input Lookup\nUnlike CBOW, our input is just one word vector (the center word $w_t$). We grab the vector $v_c$ from the Input Matrix $W_{in}$.\n$$ h = v_{w_t} $$\nNote: There is no averaging here. The hidden layer $h$ is simply the raw vector of the center word.\nStep 2: Output Scoring (The Broadcast)\nThe model takes this single vector $h$ and compares it against the Output Matrix $W_{out}$ (the entire vocabulary). It computes a score $u_j$ for every word $j$ in the dictionary.\n$$ u = W_{out}^T h $$\nResult $u$: A vector of size $V \\times 1$ containing raw scores.\nStep 3: Probability Conversion (Softmax)\nWe apply Softmax to turn scores into probabilities.\n$$ P(w_j | w_t) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 4: The Multi-Target Loss\nHere is the major difference from CBOW. In CBOW, we had one target (the center). In Skip-Gram, we have $2C$ targets (all the surrounding words). For a sentence \u0026ldquo;The cat sat on mat\u0026rdquo; (Center: \u0026ldquo;sat\u0026rdquo;), the targets are \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;. We want to maximize the probability of all these true context words. The Loss function ($E$) sums the error for each context word:\n$$ E = - \\sum_{w_{ctx} \\in \\text{Window}} \\log P(w_{ctx} | w_t) $$\nStep 5: Backpropagation (Accumulating Gradients)\nSince one input word is responsible for predicting multiple output words, the error signals from all those context words add up.\nAt the Output Layer:\nIf \u0026ldquo;cat\u0026rdquo; was a target, the \u0026ldquo;cat\u0026rdquo; vector in $W_{out}$ gets a signal: \u0026ldquo;Move closer to \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; If \u0026ldquo;apple\u0026rdquo; was NOT a target, the \u0026ldquo;apple\u0026rdquo; vector gets a signal: \u0026ldquo;Move away from \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; At the Hidden Layer ($h$):\nThe center word \u0026ldquo;sat\u0026rdquo; receives feedback from all its neighbors simultaneously. Error signal = (Error from \u0026ldquo;The\u0026rdquo;) + (Error from \u0026ldquo;cat\u0026rdquo;) + (Error from \u0026ldquo;on\u0026rdquo;)\u0026hellip; The vector for \u0026ldquo;sat\u0026rdquo; in $W_{in}$ moves in the average direction of all its neighbors. 3. The \u0026ldquo;Computational Nightmare\u0026rdquo; \u0026amp; Negative Sampling The equations above describe Naive Softmax.\nThe Problem: If $V = 100,000$, computing the denominator $\\sum \\exp(u_k)$ takes 100,000 operations. Doing this for every word in a training set of billions of words is impossibly slow.\nThe Solution: Negative Sampling Instead of updating the entire vocabulary (1 correct word vs 99,999 wrong ones), we approximate the problem.\nPositive Pair: (sat, cat) $\\rightarrow$ Maximize probability (Label 1). Negative Pairs: We pick $K$ (e.g., 5) random words the model didn\u0026rsquo;t see, e.g., (sat, bulldozer), (sat, quantum). $\\rightarrow$ Minimize probability (Label 0). New Equation (Sigmoid instead of Softmax):\n$$ E = - \\log \\sigma({v'_{\\text{pos}}}^T v_{\\text{in}}) - \\sum_{k=1}^K \\log \\sigma(-{v'_{\\text{neg}_{k}}}^T v_{\\text{in}}) $$ Effect: We typically only update 6 vectors per step (1 pos + 5 negs) instead of 100,000.\nLet\u0026rsquo;s see this in code:\nimport torch import torch.nn as nn import torch.optim as optim class SkipGramNegativeSampling(nn.Module): \u0026#34;\u0026#34;\u0026#34; Skip-Gram with Negative Sampling (SGNS) implementation. SGNS approximates the softmax over the entire vocabulary by instead distinguishing between a real context word (positive) and K noise words (negative). \u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embedding_dim): super(SkipGramNegativeSampling, self).__init__() # Input embeddings: used when the word is the center word self.in_embed = nn.Embedding(vocab_size, embedding_dim) # Output embeddings: used when the word is a context or negative sample self.out_embed = nn.Embedding(vocab_size, embedding_dim) # Initialize weights with small values to prevent gradient saturation initrange = 0.5 / embedding_dim self.in_embed.weight.data.uniform_(-initrange, initrange) self.out_embed.weight.data.uniform_(-initrange, initrange) def forward(self, center_words, target_words, negative_words): \u0026#34;\u0026#34;\u0026#34; Computes the negative sampling loss. Args: center_words: (batch_size) target_words: (batch_size) negative_words: (batch_size, K) where K is number of negative samples \u0026#34;\u0026#34;\u0026#34; # Retrieve vectors v_c = self.in_embed(center_words) # (batch_size, embed_dim) u_o = self.out_embed(target_words) # (batch_size, embed_dim) u_n = self.out_embed(negative_words) # (batch_size, K, embed_dim) # 1. Positive Score: log(sigmoid(v_c ¬∑ u_o)) # Compute dot product: (batch, 1, dim) @ (batch, dim, 1) -\u0026gt; (batch, 1) pos_score = torch.bmm(u_o.unsqueeze(1), v_c.unsqueeze(2)).squeeze(2) pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-7) # 2. Negative Score: sum(log(sigmoid(-v_c ¬∑ u_n))) # Compute dot products for all K samples: (batch, K, dim) @ (batch, dim, 1) -\u0026gt; (batch, K) neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2) neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-7), dim=1, keepdim=True) # Total loss is the negative of the objective function loss = -(pos_loss + neg_loss) return torch.mean(loss) # --- Configuration \u0026amp; Mock Data --- VOCAB_SIZE = 100 EMBED_DIM = 10 word_to_ix = {\u0026#39;fox\u0026#39;: 0} # Example vocabulary mapping model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock inputs: 1 center word, 1 target word, 5 negative samples center_id = torch.tensor([0], dtype=torch.long) target_id = torch.tensor([1], dtype=torch.long) negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long) # Training Step model.zero_grad() loss = model(center_id, target_id, negative_ids) loss.backward() optimizer.step() # Output Results print(f\u0026#34;Loss after one step: {loss.item():.6f}\u0026#34;) word_vec = model.in_embed(torch.tensor([word_to_ix[\u0026#39;fox\u0026#39;]])) print(f\u0026#34;Vector for \u0026#39;fox\u0026#39;:\\n{word_vec.detach().numpy()}\u0026#34;) The outputs:\nLoss after one step: 4.158468 Vector for \u0026#39;fox\u0026#39;: [[ 0.04197385 0.02400733 -0.03800093 0.01672485 -0.03872231 -0.0061478 0.0121122 0.04057864 -0.036255 0.03861175]] Interesting Facts About Skip-Gram üí° Makes Rare Words Shine: Skip-Gram is more effective at capturing the meaning of rare words because it focuses on the context of the center word. In CBOW, a rare word\u0026rsquo;s vector is averaged with others, diluting its signal. In Skip-Gram, when the rare word appears as the center, it gets the full, undivided attention of the backpropagation update.\nSlow Training: This method creates more training samples. A window of size 2 creates 4 training pairs per center word, while CBOW creates only 1 training pair per center word.\nSemantic Focus: Skip-Gram puts more emphasis on the semantic relationship between words. It views the context as the primary signal, with the center word as the target. This makes it better at capturing the meaning of words in context. Skip-Gram tends to capture semantic relationships (King/Queen) slightly better, while CBOW captures syntactic ones (walk/walked) slightly better.\n","permalink":"http://localhost:1313/posts/word-embeddings/","summary":"Understanding the mathematics behind Word2Vec, CBOW, and Skip-Gram and how they map language to vector space.","title":"Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n5. Code Examples: We\u0026rsquo;ll use sentencepiece for tokenization and compare results for different models.\nimport sentencepiece as spm import os # Configuration for training the SentencePiece model # SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words. options = { # The source text file used for learning the vocabulary \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # The base name for the generated model (.model) and vocabulary (.vocab) files \u0026#39;model_prefix\u0026#39;: \u0026#39;bpe_model\u0026#39;, # Number of unique tokens in the final vocabulary \u0026#39;vocab_size\u0026#39;: 4000, # \u0026#39;bpe\u0026#39; (Byte Pair Encoding) merges frequent pairs of characters/sequences \u0026#39;model_type\u0026#39;: \u0026#39;bpe\u0026#39;, # Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets \u0026#39;character_coverage\u0026#39;: 0.9995, # When enabled, unknown characters are decomposed into UTF-8 bytes to avoid \u0026#39;unk\u0026#39; tokens \u0026#39;byte_fallback\u0026#39;: True, # Treats digits individually (0-9), preventing large numbers from being treated as single tokens \u0026#39;split_digits\u0026#39;: True, # Prevents adding a whitespace prefix to the first token; useful for fine-grained control \u0026#39;add_dummy_prefix\u0026#39;: False } try: print(\u0026#34;Starting the training process...\u0026#34;) # SentencePieceTrainer.train takes the dictionary of options to build the BPE model spm.SentencePieceTrainer.train(**options) print(\u0026#34;Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created.\u0026#34;) # Initialize the processor and load the newly trained model sp = spm.SentencePieceProcessor() sp.load(\u0026#39;bpe_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Model Metadata:\u0026#34;) # Retrieve the total number of tokens in the vocabulary print(f\u0026#34;Total Vocab Size: {sp.get_piece_size()}\u0026#34;) # Special tokens are used for sequence boundaries and handling unknown characters print(f\u0026#34;BOS (Beginning of Sentence) ID: {sp.bos_id()}\u0026#34;) print(f\u0026#34;EOS (End of Sentence) ID: {sp.eos_id()}\u0026#34;) print(f\u0026#34;UNK (Unknown) ID: {sp.unk_id()}\u0026#34;) print(f\u0026#34;PAD (Padding) ID: {sp.pad_id()}\u0026#34;) # Test the tokenizer on sample strings to see how it breaks down text test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: shows the actual subword units (tokens) print(f\u0026#34;Subword Tokens: {sp.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: shows the numerical mapping for each token print(f\u0026#34;Numerical IDs: {sp.encode_as_ids(text)}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred during training or processing: {e}\u0026#34;) You can see the following output:\nStarting training... Starting the training process... Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created. ------------------------------ Model Metadata: Total Vocab Size: 4000 BOS (Beginning of Sentence) ID: 1 EOS (End of Sentence) ID: 2 UNK (Unknown) ID: 0 PAD (Padding) ID: -1 --- Tokenization Test --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;el\u0026#39;, \u0026#39;lo\u0026#39;, \u0026#39;‚ñÅW\u0026#39;, \u0026#39;orld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 380, 309, 476, 619, 36, 3924, 3964, 3959, 3993, 3978, 3983, 57, 3975, 3976, 60, 3974] --- Tokenization Test --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅmo\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;inst\u0026#39;, \u0026#39;ag\u0026#39;, \u0026#39;ram\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅever\u0026#39;] Numerical IDs: [1894, 304, 3284, 376, 321, 313, 312, 1008, 3313, 1325, 428, 3937, 341, 304, 3284, 1495] The ! is fallen back to UTF bytes. and the word instagrammable is split into multiple subword tokens. Also, original text can be recreated right back by simply concatenating the tokens. And replacing underscore by whitespace. Now we can try Unigram model. and see the same sentences tokenized differently.\nfrom tokenizers import Tokenizer from tokenizers.models import WordPiece from tokenizers.trainers import WordPieceTrainer from tokenizers.pre_tokenizers import Whitespace try: # 1. Initialize the WordPiece Tokenizer # We specify the [UNK] token for handling words not found in the vocabulary. tokenizer = Tokenizer(WordPiece(unk_token=\u0026#34;[UNK]\u0026#34;)) # 2. Configure Pre-tokenization # Before the subword algorithm runs, we need to split the raw text into words. # Whitespace splitting is the standard first step for most English NLP tasks. tokenizer.pre_tokenizer = Whitespace() # 3. Initialize the Trainer # We define our target vocabulary size and the special tokens required for # downstream tasks (like BERT\u0026#39;s [CLS] for classification or [SEP] for separators). trainer = WordPieceTrainer( vocab_size=4000, special_tokens=[\u0026#34;[UNK]\u0026#34;, \u0026#34;[CLS]\u0026#34;, \u0026#34;[SEP]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[MASK]\u0026#34;] ) # 4. Train the Model # The tokenizer scans the training file to build a vocabulary of the most # frequent subword units. tokenizer.train(files=[\u0026#34;train_data.txt\u0026#34;], trainer=trainer) # 5. Persist the Model # Save the configuration and vocabulary to a JSON file for future inference. tokenizer.save(\u0026#34;wordpiece.json\u0026#34;) print(\u0026#34;Training complete. \u0026#39;wordpiece.json\u0026#39; created.\u0026#34;) # 6. Metadata Inspection print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;WordPiece Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {tokenizer.get_vocab_size()}\u0026#34;) # 7. Testing Subword Tokenization # WordPiece shines at handling rare words by breaking them into meaningful chunks. test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (WordPiece) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # Encode converts raw text into a Tokenizer object containing tokens and IDs output = tokenizer.encode(text) # \u0026#39;tokens\u0026#39; shows the subword breakdown (e.g., \u0026#39;un\u0026#39;, \u0026#39;##insta\u0026#39;, etc.) print(f\u0026#34;Subword Tokens: {output.tokens}\u0026#34;) # \u0026#39;ids\u0026#39; are the numerical indices mapped to the vocabulary print(f\u0026#34;Numerical IDs: {output.ids}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred with WordPiece model: {e}\u0026#34;) You can see the following output:\nWordPiece Model Metadata: Total Vocab Size: 2609 --- Tokenization Test (WordPiece) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;H\u0026#39;, \u0026#39;##el\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;##or\u0026#39;, \u0026#39;##ld\u0026#39;, \u0026#39;[UNK]\u0026#39;, \u0026#39;[UNK]\u0026#39;] Numerical IDs: [37, 180, 214, 52, 162, 418, 0, 0] --- Tokenization Test (WordPiece) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;##os\u0026#39;, \u0026#39;##t\u0026#39;, \u0026#39;un\u0026#39;, \u0026#39;##ins\u0026#39;, \u0026#39;##ta\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;##ra\u0026#39;, \u0026#39;##m\u0026#39;, \u0026#39;##ma\u0026#39;, \u0026#39;##ble\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;ever\u0026#39;] Numerical IDs: [691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240] We can see the special character ## in the output. We can also see the breakdowns not having any semantic meaning. They are likelihood based. We can try the Unigram model now.\n# \u0026#39;Unigram\u0026#39; is the default and usually recommended over BPE in SentencePiece. options_unigram = { \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # Path to the raw text file for training \u0026#39;model_prefix\u0026#39;: \u0026#39;unigram_model\u0026#39;, # Prefix for the output .model and .vocab files \u0026#39;vocab_size\u0026#39;: 1200, # Desired size of the final vocabulary \u0026#39;model_type\u0026#39;: \u0026#39;unigram\u0026#39;, # Specifies the Unigram language model algorithm \u0026#39;character_coverage\u0026#39;: 0.9995, # Percentage of characters covered by the model (0.9995 is standard for Latin scripts) \u0026#39;byte_fallback\u0026#39;: True, # Enables mapping unknown characters to UTF-8 bytes to avoid \u0026lt;unk\u0026gt; tokens \u0026#39;split_digits\u0026#39;: True, # Treats each digit as an individual token (useful for numerical data) \u0026#39;add_dummy_prefix\u0026#39;: False # Prevents adding a leading space (SentencePiece default is True) } try: # 1. Train the SentencePiece model using the defined options print(\u0026#34;Starting Unigram training...\u0026#34;) spm.SentencePieceTrainer.train(**options_unigram) print(\u0026#34;Training complete. \u0026#39;unigram_model.model\u0026#39; created.\u0026#34;) # 2. Load the trained model into a processor instance for inference sp_unigram = spm.SentencePieceProcessor() sp_unigram.load(\u0026#39;unigram_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Unigram Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {sp_unigram.get_piece_size()}\u0026#34;) # 3. Define test cases to evaluate how the model handles common and rare words test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] # 4. Iterate through test sentences to visualize subword segmentation for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (Unigram) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: Converts text into subword strings (visual representation) print(f\u0026#34;Subword Tokens: {sp_unigram.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: Converts text into numerical indices for model input print(f\u0026#34;Numerical IDs: {sp_unigram.encode_as_ids(text)}\u0026#34;) except Exception as e: # Handle potential errors during training or loading (e.g., missing input file) print(f\u0026#34;An error occurred with Unigram model: {e}\u0026#34;) The output is as follows:\nUnigram Model Metadata: Total Vocab Size: 1200 --- Tokenization Test (Unigram) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ll\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;ld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311] --- Tokenization Test (Unigram) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;T\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅm\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;sta\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;ra\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ver\u0026#39;] ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"A comprehensive guide to tokenization strategies: BPE, WordPiece, Unigram, and SentencePiece.","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"A brief introduction to Vectors \u0026amp; Verbs and formatting verification.","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"\nFeedforward network (FFN) blocks are the least talked about elements in the context of LLMs, but they are where more than 70% of the parameters originate. Something as simple and convenient as a couple of linear transformation matrices becomes a burden when models start scaling into billions of parameters. Thus, getting the maximum juice out of a lean structure is a challenge. FFNs are mostly under-represented in discussions because the sub-layer acts as the primary \u0026ldquo;knowledge\u0026rdquo; processing engine, often containing the majority of a model\u0026rsquo;s total parameters.\nüî• Standard Position-Wise Feedforward Network This is as simple as it gets: a two-layered fully connected network of neurons with a single activation switch‚Äîmost probably a ReLU variant (for LLMs).\n$$ FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2 $$\nThe same transformation is applied to each token in parallel, making it efficient for GPU computation. For example, in the BERT model, the 512 dimensions of each token are expanded to 2048 to allow for \u0026ldquo;intricate interactions\u0026rdquo; before projecting back to 512 dimensions. This expansion/contraction is crucial; in every transformer, stacking of layers is possible only because the output of every layer matches its input dimensions.\nüî• SWiGLU Sigmoid Weighted Gated Linear Unit (SWiGLU) has become the gold standard for feedforward networks in LLMs like Llama 3 and PaLM. The primary reasons for choosing the Swish function (shown in the first image) over ReLU or GELU are as follows:\nMathematical Linearity: Swish (SiLU) is defined as $x \\cdot \\text{sigmoid}(\\beta x)$, where $\\beta$ is a learnable parameter. It is a smooth, non-monotonic function that allows small negative values to pass through. While GELU ($x \\cdot \\Phi(x)$) is also smooth, Swish\u0026rsquo;s specific curve has empirically improved model performance. Computational Efficiency: SiLU gradients are simpler to compute ($x / (1 + \\exp(-x))$) compared to standard GELU approximations. At the massive scale of modern LLMs, these shortcuts save significant training time. Performance: Research in the PaLM and Llama papers found that Swish-based Gated Linear Units consistently outperformed GELU-based versions in terms of perplexity and downstream task accuracy. $$SwiGLU(x, W, V, b, c) = (\\text{Swish}(xW + b) \\otimes (xV + c))W_2$$\nWhere $W$ and $V$ are weight matrices for the gate and the data path, and $\\otimes$ is element-wise multiplication.\nMore Complex Filtering: Instead of using two weight matrices as in a standard FFN, SWiGLU uses three (gate, data, and output). This allows it to extract more complex features without a drastic increase in parameters. Modern implementations often lower the dimensionality of hidden layers (to ~2/3 of what they would be in a standard FFN) while retaining the performance benefits. Zero-Gradient Avoidance: Unlike ReLU, which has a zero gradient for all negative values (the \u0026ldquo;dying ReLU\u0026rdquo; problem), SwiGLU‚Äôs smooth curve ensures the model can still learn from negative inputs, improving training stability in deep stacks. üî• Mixture of Experts (MoE) MoE is an entire architecture rather than just a single block. It has become the dominant strategy for scaling models to insane sizes‚Äîup to a trillion parameters (like DeepSeek-V3 and GPT-4)‚Äîbecause it breaks the direct link between model size (information capacity) and speed (inference cost).\nSparse Activation: MoE uses a \u0026ldquo;Router\u0026rdquo; to determine which smaller \u0026ldquo;Experts\u0026rdquo; (specialized FFNs) are best suited for a specific token. Instead of activating the entire network, only a tiny fraction of experts are used per token. For instance, DeepSeek-V3 might have 256 expert layers but only activate 8 for any single token. Big Scale, Small Compute: MoE allows \u0026ldquo;trillion-parameter intelligence\u0026rdquo; to run at the speed of a much smaller model. While DeepSeek-V3 has 671 billion total parameters, it only uses ~37 billion per token during inference. You get the reasoning performance of a massive model with the latency of a smaller one. Expert Specialization: Because different experts handle different tokens, they naturally specialize. Analysis shows certain experts become \u0026ldquo;specialists\u0026rdquo; in mathematics or coding, while others handle general language. Training Efficiency: Since you only update a fraction of the weights for each token, MoE models can process many more tokens per second during training compared to a dense model of the same total size. Training Dynamics: The router assigns probabilities to each expert. To avoid overfitting or \u0026ldquo;expert collapse\u0026rdquo; (where the router sends everything to one expert), noise is often added to the router\u0026rsquo;s output during training. Some models choose one top expert, while others choose multiple and average their outputs.\nThe Challenges of MoE The RAM Wall: Even if only a small percentage of parameters are used for computation, all 671B parameters must still reside in GPU memory. This makes MoE models difficult to run on consumer hardware. Training Instability: MoEs often require \u0026ldquo;auxiliary losses\u0026rdquo; to force the router to balance the load across experts. DeepSeek-V3 introduced an \u0026ldquo;Auxiliary-Loss-Free\u0026rdquo; strategy to balance load dynamically without degrading accuracy. Communication Overhead: In distributed training, experts are often split across different GPUs. If a token on GPU 1 needs an expert on GPU 4, data must travel over the network. DeepSeek-V3 employs \u0026ldquo;Node-Limited Routing\u0026rdquo; to minimize this congestion. Summary: MoE is the better choice for major labs (like DeepSeek, Google, or OpenAI) trying to maximize intelligence per dollar of training cost. Dense SwiGLU remains the standard for reliability, ease of deployment, and local execution.\n","permalink":"http://localhost:1313/posts/feedforward-networks/","summary":"Exploring the hidden layers of trillion-parameter switchboards: Feedforward Neural Networks and Activation Functions.","title":"Anatomy of Trillion-Parameter Switchboards: Understanding Feedforward Blocks"},{"content":"\nWe are about to touch the holy grail of modern AI. The 2017 declaration was simple: Attention Is All You Need. But as we‚Äôve raced from GPT-1 to reasoning models, the definition of \u0026lsquo;Attention\u0026rsquo; has quietly transformed. Is the algorithm ruling the world today actually the same one we started with? Let‚Äôs look under the hood.\nüî• The Core Concept At its heart, the attention mechanism is a form of weighted aggregation. It is often described using a database retrieval analogy. Imagine you have a database of information (context).\nQuery ($Q$): What are you currently looking for? (The token being generated). Key ($K$): The indexing tag associated with each piece of information. Value ($V$): The actual information (The context token). The \u0026ldquo;Attention\u0026rdquo; is the process of matching your Query against all the Keys to determine how much \u0026ldquo;weight\u0026rdquo; to put on each Value. Attention scores are calculated in each transformer layer at every step of the sequence. This is a single unifying factor in all transformer models, from BERT to Gemini.\nüî• Standard Multi-Head Attention (MHA) The original mechanism introduced in the paper \u0026ldquo;Attention Is All You Need\u0026rdquo;.\nHere, instead of performing the Key-Query-Value (KQV) operation on the entire context only once, the attention is split into $N$ parallel heads. Each head learns a different aspect of the language (e.g., word meaning, syntax, semantic relationships, etc.). The equation is a simple culmination of dot products aided by a regularizer like Softmax.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nWhere:\n$QK^T$ computes the similarity scores between the Query and Key. $\\sqrt{d_k}$ is a scaling factor to prevent the dot product from growing too large, mitigating the vanishing/exploding gradient problem. The weighted sum is applied to the values of $V$. The Memory Bandwidth Bottleneck: \u0026gt; During inference, the model must store Key and Value matrices for all previous tokens. This is called the KV Cache. In MHA, every single head has its own unique set of Keys and Values. For large models with massive context windows, loading this massive KV cache from memory (HBM) to the computation chip (SRAM) becomes the primary bottleneck, slowing down generation significantly. Earlier models could often only generate ~8 tokens per second at inference due to this constraint.\nüî• Multi-Query Attention (MQA) This approach addresses the memory bottleneck by sharing the same set of Keys and Values across all heads while keeping separate Query heads.\nMHA: $H$ Query heads, $H$ Key heads, $H$ Value heads MQA: $H$ Query heads, 1 Key head, 1 Value head This drastically reduces the memory requirements of the KV cache by a factor of $H$ (often 8x or 16x). The GPU needs to load far less data, significantly speeding up inference. However, it does not reduce the computation (FLOPs)‚Äîso why is it faster?\nThis can be explained with the Kitchen Analogy: Imagine a kitchen (GPU) with 8 chefs (Heads).\nComputation (Math): The Chef chopping vegetables. Memory Access (Bandwidth): The Assistant running to the fridge to get ingredients. In MHA: Each chef demands unique vegetables from the fridge. The assistant runs back and forth 8 times, fetching 8 different crates. Most of the time is spent waiting for the assistant to return. This results in idle cores waiting for data to load from HBM.\nIn MQA: All chefs agree to use the same vegetables. The assistant runs back and forth only once, fetching one giant crate of potatoes and dumping it. This results in minimized memory transfer and no idle cores.\nThe Technical Reality: During text generation, the model is memory-bound, not compute-bound. The time it takes to move the KV Cache from HBM (High Bandwidth Memory) to the chip\u0026rsquo;s SRAM is the bottleneck. MQA reduces the data volume of that move by a factor of $H$.\nThe Capacity Gap: MQA is a trade-off between nuanced response and memory capacity. The model loses the ability to store different types of information, as a single $V$ matrix must now store all nuances of information flow in the language.\nThe Surgery Trick (Uptraining): Google Research figured out a way to take an existing model with MHA and convert it to MQA by averaging the weight matrices of $H$ heads. This is called Uptraining. You then train this \u0026ldquo;Frankenstein\u0026rdquo; model for ~5% of the original training steps for adjustments. This yields an MQA model that is nearly as good as the original MHA but vastly faster.\nüî• Grouped Query Attention (GQA) A \u0026ldquo;best of both worlds\u0026rdquo; solution used in modern models like Llama 2 and Llama 3.\nHow it works: GQA acts as an interpolation between MHA and MQA. Instead of having just 1 Key head (MQA) or $H$ Key heads (MHA), it divides the query heads into $G$ groups. Each group shares a single Key and Value head.\nEquation modification: If you have 8 query heads, you might create 4 groups (2 queries per group). This reduces the KV cache by 2x instead of 8x, preserving more quality than MQA while still speeding up inference.\nMHA: 8 people (heads) are working on a project. Each person has their own unique filing cabinet (KV cache). It takes forever to move 8 cabinets around. MQA: 8 people share one single filing cabinet. It\u0026rsquo;s very fast to move, but they fight over the organization and lose detail. GQA: The 8 people split into 4 teams of 2. Each team shares a cabinet. It\u0026rsquo;s a balance between speed and organizational detail. üî• Multi-Head Latent Attention (MLA) This is the most advanced mechanism for attention, used in models like DeepSeek-V3. It is a sophisticated design for massive efficiency that takes a radical approach: mathematically compressing the memory into a low-rank \u0026ldquo;latent\u0026rdquo; vector.\nKV Cache Explosion: As mentioned before, for a massive model like DeepSeek-V3 (671B parameters), storing the full Keys and Values for every token in a long conversation requires terabytes of high-speed memory (HBM), and chips spend more time waiting than churning.\nMHA (Standard) stores everything. MQA deletes redundancy but hurts quality. MLA keeps the quality of MHA but compresses the data 4x-6x smaller than even GQA.\nThe Zip File Approach: Low-Rank Joint Compression Let\u0026rsquo;s compare the math of MHA and MLA. In MHA, for every single token, we generate a unique Key ($k$) and Value ($v$) matrix for each attention head.\nThe Memory Problem: The $k$ is stored in the KV Cache. For 128 heads, for example, it calculates a massive vector of $128 \\times 128 = 16,384$ floats per token.\nFor MHA, the attention score looks like: $$\\text{Score} = q^T \\cdot k$$\nBut for MLA, instead of storing the full $k$, it projects the input down into a tiny latent vector $c_{KV}$, which is much smaller (e.g., 512 floats). $$c_{KV} = W_{DKV} \\cdot x$$\nNow here is the catch: Naive compression and decompression would mean constructing a small latent matrix for every token and deconstructing it every single time the information is required, which defeats the purpose of compression.\n$$k_{\\text{reconstructed}} = W_{UK} \\cdot c_{KV}$$ $$\\text{Score} = q^T \\cdot k_{\\text{reconstructed}}$$\nThe Matrix Absorption Trick (Optimization) Instead of reconstructing $k$ for every token, we can absorb the Up-Projection matrix ($W_{UK}$) into the Down-Projection matrix ($W_{DKV}$) and absorb the query $q$ into a new $Q$.\nFrom the original equation: $$\\text{Score} = q^T \\cdot (\\underbrace{W_{UK} \\cdot c_{KV}}_{\\text{This is } k})$$\nWe associate differently:\n$$\\text{Score} = (\\underbrace{q^T \\cdot W_{UK}}_{\\text{Absorbed Query}}) \\cdot c_{KV}$$ We change the order in which the matrix multiplication is performed.\nHow is this allowed? In linear algebra, matrix multiplication is associative:\n$$(A \\cdot B) \\cdot C = A \\cdot (B \\cdot C)$$ We can move the parentheses! Instead of grouping $(W_{UK} \\cdot c_{KV})$ to make the Key, we group $(q^T \\cdot W_{UK})$ to make a new Query.\nNote: The positional embedding information is left untouched. That rotational information from algorithms like RoPE is preserved outside the compression; it requires minimal space anyways.\n","permalink":"http://localhost:1313/posts/attention-mechanisms/","summary":"We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek\u0026rsquo;s MLA, how has the definition of \u0026lsquo;Attention\u0026rsquo; transformed?","title":"Attention Is All You Need, but exactly which one?: MHA, GQA and MLA"},{"content":"\nPositional Embeddings are the \u0026ldquo;voice\u0026rdquo; that tell Transformers where words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.\n\u0026ldquo;The dog chased the cat\u0026rdquo; and \u0026ldquo;cat chased the dog\u0026rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.\nüî• Sinusoidal Embeddings: The Odometer of Language Introduced in the seminal \u0026ldquo;Attention Is All You Need\u0026rdquo; paper, this was the industry standard for many years. This approach adds a vector to each word embedding based on its position. The vector is computed out of wavelike patterns that are periodic and smooth. Mathematically, for a position $pos$ and embedding dimension $i$:\n$$ PE_{(pos, i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $$\nThe model uses sine and cosine waves of different frequencies to capture both short and long-range dependencies. The frequency of the waves decreases as the embedding dimension increases, which helps capture long-range dependencies in the input sequence.\nThe creators wanted the model to calculate distance easily, which is why a linear function was used. The model simply rotates the current position vector using a rotation matrix to derive the next position.\n$$ \\begin{pmatrix} \\cos(k) \u0026 \\sin(k) \\\\ -\\sin(k) \u0026 \\cos(k) \\end{pmatrix} \\cdot PE(pos) \\approx PE(pos + k) $$ Low dimensions act like a fast-ticking clock (high frequency, changing rapidly) and high dimensions act like a slow clock (low frequency, changing slowly).\nThe number 10,000 is an arbitrarily chosen high number to reduce Aliasing. By choosing such a high number, the creators ensured that the wavelength of the slowest dimension is incredibly long. Consequently, the pattern practically never repeats, and each position remains unique.\nThis combination works like a mechanical odometer dial. Each subsequent dimension acts as a slower-moving gear. The model learns to read these \u0026ldquo;dials\u0026rdquo; to understand exactly where a word sits in the sequence.\nThis mitigated the challenge of exploding gradients that previous array-based positional embeddings suffered from (e.g., adding integers 1, 2, 3\u0026hellip;). This model keeps values between -1 and +1, adding stability while ensuring every position gets a distinct code.\nThe Problem: This approach pollutes the word embedding matrix (semantic information). While high-dimensional vectors are almost always orthogonal‚Äîmeaning the noise doesn\u0026rsquo;t fully destroy the word\u0026rsquo;s identity‚Äîit does not strictly guarantee purity.\nThis led to a more mature approach.\nüî• ALiBi: The Fading Streetlight Attention with Linear Biases (ALiBi) is a method where, instead of adding a vector to the embeddings at the start, the model injects positional information linearly, directly into the attention scores. If Sinusoidal Encoding asks, \u0026ldquo;Where am I on the map?\u0026rdquo;, ALiBi asks, \u0026ldquo;How much harder should I squint to see you?\u0026rdquo;\nThe biggest problem ALiBi solved is the \u0026ldquo;Invisible Wall\u0026rdquo;. If you train a Transformer on sequences of length 2048 and then feed it a book with 3000 words, it will crash or output garbage at word 2049. This is a failure of Extrapolation.\nALiBi throws away the idea of adding positional vectors entirely. Instead, it devises a \u0026ldquo;penalty\u0026rdquo; system for attention scores:\n$$ \\text{Score} = (q \\cdot k^\\top) - (m \\cdot \\text{distance}) $$\nDistance: If words A and B are neighbors, the distance is 0, so the penalty is 0. Although the distance is linear, it does not cause exploding gradients because the raw attention scores are normalized by a softmax function. This distance prioritizes nearby words (local context) over distant ones.\nSlope ($m$): If we always penalize distance equally, the model becomes myopic. ALiBi mitigates this by assigning a different slope value to each attention head:\nHead 1 (The Historian): Very low slope. The penalty grows slowly, allowing it to see far back. Head 2 (The Reader): Medium slope. Head 3 (The Myopic): Very high slope (e.g., 0.5); it effectively cannot see beyond 2-3 words. This mimics holding a lantern. Words close to you are bright, while words further away naturally fade into darkness.\nExtrapolation: If you walk 5 miles down the road (a longer sequence), your lantern works exactly the same way. The physics of light decay are constant, allowing the model to handle sequences longer than it was trained on.\nHowever, ALiBi forces a \u0026ldquo;sliding window\u0026rdquo; view where distant words eventually become invisible. This brings us to the modern standard.\nüî• RoPE: The Rotary Revolution Rotary Position Embeddings (RoPE) is the current industry standard (used in Llama 3, Mistral, PaLM) because it mathematically unifies \u0026ldquo;absolute position\u0026rdquo; with \u0026ldquo;relative position.\u0026rdquo; This method rotates the vectors in the Query and Key matrices based on their absolute position.\nIf a vector has 512 dimensions, RoPE breaks it into 256 pairs. Each pair is rotated by a different frequency:\n$$ \\theta_i = 10000^{-2i/d} $$\nThe First pair ($i = 0$) is rotated by the highest frequency. The Last pair ($i = 255$) is rotated by the lowest frequency. This spread ensures the model has \u0026ldquo;high precision\u0026rdquo; (fast rotation) to distinguish immediate neighbors, and \u0026ldquo;low precision\u0026rdquo; (slow rotation) to track global position without the pattern repeating.\nDoesn\u0026rsquo;t rotation change meaning? Yes and No. The vector for \u0026ldquo;Apple\u0026rdquo; at position 1 and \u0026ldquo;Apple\u0026rdquo; at position 256 will look completely different in terms of coordinates. However, rotation does not change the semantic strength (magnitude/norm) of the vector.\nThe Relativity Trick: Transformers compare words using dot products (angles). RoPE relies on the fact that the attention mechanism ignores the absolute rotation and only detects the difference in rotation (relative position). Thus, the \u0026ldquo;relative meaning\u0026rdquo; is encoded purely into the angle between words, leaving the semantic \u0026ldquo;magnitude\u0026rdquo; untouched.\nüî• LongRoPE: The Bifocal Lens While RoPE is powerful, it struggles when stretching context windows from 4k to 2 million tokens. The \u0026ldquo;fast\u0026rdquo; dimensions spin so rapidly over long distances that they become random noise. LongRoPE solves this using an evolutionary search algorithm to find a unique scaling factor ($\\lambda$) for each dimension.\nThe Equations Instead of rotating by the standard $\\theta_i$, LongRoPE rotates by a rescaled frequency:\n$$ \\theta\u0026rsquo;_i = \\lambda_i \\theta_i $$\nTo efficiently find these $\\lambda$ values without searching an infinite space, the algorithm enforces a monotonicity constraint based on NTK theory:\n$$ \\lambda_i \\le \\lambda_{i+1} $$\nThis ensures that low-frequency dimensions (global context) are stretched more than high-frequency dimensions (local context), creating a \u0026ldquo;bifocal\u0026rdquo; effect:\nHigh Frequencies (Local): Kept sharp ($\\lambda \\approx 1$) to maintain grammar. Low Frequencies (Global): Stretched ($\\lambda \u0026gt; 1$) to track massive distances without repeating. Not worth perusing more since the next one is much better at long context positional encoding. üî• HoPE: The Hyperbolic Slide Hyperbolic Rotary Positional Encoding (HoPE) moves from the geometry of a circle to the geometry of a hyperbola (inspired by Lorentz transformations). It was designed to fix a subtle flaw in RoPE: the \u0026ldquo;wobbly\u0026rdquo; or oscillatory nature of attention scores.\nThe Core Problem: RoPE\u0026rsquo;s \u0026ldquo;Wobble\u0026rdquo; In standard RoPE, we rotate vectors around a circle.\nThe Issue: Circles repeat. A dial at $361^\\circ$ looks identical to $1^\\circ$. Even with different frequencies, the dot product fluctuates up and down as distance increases. The Consequence: The model gets confused at long distances, potentially mistaking a word 1,000 tokens away for a neighbor just because the rotation cycles aligned, creating \u0026ldquo;noise.\u0026rdquo; The \u0026ldquo;Hyperbolic\u0026rdquo; Shift HoPE replaces trigonometric functions ($\\sin, \\cos$) with hyperbolic functions ($\\sinh, \\cosh$).\nCircular Rotation: Keeps distance from the center constant. Hyperbolic Rotation: Moves points along a hyperbola. Crucially, hyperbolic functions do not repeat; they grow or decay exponentially. By using hyperbolic rotation, HoPE guarantees that as words get further apart, the attention score decays monotonically (smoothly drops).\nThe Equations HoPE uses a hyperbolic matrix $B(\\theta, m)$ defined using the Lorentz Boost structure:\n$$ B(\\theta, m) = \\begin{pmatrix} \\cosh(m\\theta) \u0026 \\sinh(m\\theta) \\\\ \\sinh(m\\theta) \u0026 \\cosh(m\\theta) \\end{pmatrix} $$ Since hyperbolic functions grow to infinity, HoPE introduces a decay penalty ($e^{-m\\theta'}$). The final attention score simplifies beautifully to a function of relative distance: $$ \\text{Score} \\propto e^{-|m-n|(\\theta\u0026rsquo; - \\theta)} $$\nThis ensures the attention score exponentially decays as distance increases, eliminating the wobbles of standard RoPE. This the latest model that is taking up the space in the industry for extremely long context models.\n","permalink":"http://localhost:1313/posts/positional-embeddings/","summary":"From Sinusoidal to RoPE and HoPE: How Transformers learn to process word order and sequence length.","title":"The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE"},{"content":"Imagine trying to teach a computer the difference between \u0026ldquo;Apple\u0026rdquo; the fruit and \u0026ldquo;Apple\u0026rdquo; the company. To us, the distinction is intuitive. To a machine, it‚Äôs just a string of characters. How do we turn these strings into meaningful math? While early attempts like Word2Vec gave us a great start, they missed the forest for the trees‚Äîor in some cases, the twigs for the branches. Enter GloVe and FastText: two algorithms that revolutionized how machines understand the nuances of human language.\nPrevious static embedding models like Word2Vec successfully captured the local semantics of words, but they failed in addressing contexts between words that might not always appear in the same context window. Furthermore, they treated words as indivisible atomic units in continuous vector spaces, missing out on the internal structure of language.\nThe two simple but effective ideas behind GloVe and FastText are subword units and global co-occurrence. Let\u0026rsquo;s dive into them.\nGloVe: A Global Mapping Strategy Global Vectors for Word Representation (GloVe) was created at Stanford. The core idea was to train a model where word vectors are defined by how often they appear around other words across the entire corpus, rather than just locally.\nTraining Steps: Co-occurrence Matrix X: GloVe scans the entire corpus and creates a giant matrix (spreadsheet) recording the counts of how often words appear near each other. Factorization: It compresses this giant matrix into smaller, dense vectors. Objective: The function tries to equate the dot product of two word vectors with the logarithm of their probability of appearing together. The error between these values is then backpropagated to update the vectors. $$w_i \\cdot w_j + b_i + b_j = \\log(X_{ij})$$\n$w$: The word vector (what we want). $X_{ij}$: The count from our matrix. The Loss Function: We can\u0026rsquo;t satisfy the equation above perfectly for every word pair. So, we use a weighting function to minimize the squared error:\n$$J = \\sum_{i,j=1}^V f(X_{ij}) ({w_i}^T w_j + b_i + b_j - \\log(X_{ij}))^2$$\nIf we do not use the weighting function, the loss would be infinite for rare words (since $\\log(0) = \\infty$) and frequent words (like \u0026rsquo;the\u0026rsquo;) would overpower the model.\n$$ f(x) = \\begin{cases} (x/x_{max})^\\alpha \u0026amp; \\text{if } x \u0026lt; x_{max} \\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$\n(Typically $x_{max}=100$ and $\\alpha=0.75$)\nFastText: The Lego Bricks Strategy FastText was created by Facebook AI Research (FAIR). The idea was to treat words not as solid rocks, but as structures made of \u0026ldquo;Lego bricks\u0026rdquo; (subword units).\nWord2Vec struggled to create embeddings for words it hadn\u0026rsquo;t seen before (Out-Of-Vocabulary or OOV words). FastText solves this by breaking words down.\nGloVe treats a word like \u0026ldquo;Apple\u0026rdquo; as a single, atomic unit. FastText breaks \u0026ldquo;Apple\u0026rdquo; into character n-grams: \u0026lt;ap, app, ppl, ple, le\u0026gt; For a complex or new word like \u0026ldquo;Applesauce\u0026rdquo;, the model might not know the whole word, but it recognizes the Lego brick for \u0026ldquo;Apple\u0026rdquo; and the remaining bits from other subwords.\nFastText takes its inspiration from the SkipGram architecture. The vector for a word is simply the sum of the vectors of its n-gram subwords:\n$$v_w = \\sum_{g \\in G_w} z_g$$\nObjective: The objective is to predict context words given a target word.\n$$P(w_c | w_t) = \\frac{e^{s(v_{w_t} \\cdot v_{w_c})}}{\\sum_{w \\in V} e^{s(v_{w_t} \\cdot v_w)}}$$\nAnd the scoring function is defined as:\n$$s(w_t, w_c) = u_{w_t}^T (\\sum_{g \\in G_w} z_g)$$\n$u_{w_c}$: The vector for the context word (output vector). $z_g$: The vector for the n-gram $g$ (input vector). Modifications to Softmax: Calculating the full softmax for every word in the vocabulary is computationally expensive. FastText uses two methods to approximate this efficiently.\nOption A: Negative Sampling\nWe pick a few random \u0026ldquo;wrong\u0026rdquo; words every time to serve as \u0026ldquo;negative samples\u0026rdquo; and calculate the loss only for the correct word and these noise words.\n$$J = - \\log \\sigma(s(w_t, w_c)) - \\sum\\limits_{i=1}^{N} \\mathbb{E}_{n_i \\sim P_n(w)} [\\log \\sigma(-s(w_t, n_i))]$$\nThe sigmoid function $\\sigma$ squashes the score between 0 and 1. The first term pushes the probability of the correct word towards 1. The second term pushes the probability of the noise words towards 0. Option B: Hierarchical Softmax (Faster for infrequent words)\nInstead of a flat list of words, imagine the vocabulary arranged as a binary tree (Huffman Tree).\nRoot: Top of the tree. Leaves: The actual words. To calculate the probability of a word, we trace a path from the root to that leaf. At every branching node, we calculate the probability (sigmoid) of going left or right. The probability of a word $w$ is the product of the probabilities of the turns taken to reach it. Code Example from gensim.models import FastText from gensim.test.utils import common_texts # Initialize the FastText model with specific hyperparameters model = FastText( vector_size=100, # Dimensionality of the word vectors window=5, # Maximum distance between the current and predicted word within a sentence min_count=1, # Ignores all words with total frequency lower than this sg=1, # Training algorithm: 1 for skip-gram; 0 for CBOW hs=0, # If 0, and negative is non-zero, negative sampling will be used negative=5, # Number of \u0026#34;noise words\u0026#34; to be drawn for negative sampling workers=4, # Number of worker threads to train the model epochs=10, # Number of iterations over the corpus min_n=3, # Minimum length of character n-grams max_n=6 # Maximum length of character n-grams ) # Build the vocabulary from the provided text corpus model.build_vocab(common_texts) # Train the model on the corpus model.train( common_texts, total_examples=model.corpus_count, epochs=model.epochs ) # Calculate and print the cosine similarity between two words in the vocabulary print(f\u0026#34;Similarity between \u0026#39;computer\u0026#39; and \u0026#39;human\u0026#39;: {model.wv.similarity(\u0026#39;computer\u0026#39;, \u0026#39;human\u0026#39;)}\u0026#34;) # Demonstrate FastText\u0026#39;s ability to handle Out-Of-Vocabulary (OOV) words # Even if \u0026#39;computation\u0026#39; wasn\u0026#39;t in the training data, FastText constructs a vector using character n-grams oov_vector = model.wv[\u0026#39;computation\u0026#39;] # Check if a specific word exists in the model\u0026#39;s fixed vocabulary index word = \u0026#34;computer\u0026#34; is_in_vocab = word in model.wv.key_to_index print(f\u0026#34;\\nIs the word \u0026#39;{word}\u0026#39; in the model\u0026#39;s vocabulary? {is_in_vocab}\u0026#34;) ","permalink":"http://localhost:1313/posts/glove-and-fasttext/","summary":"How subword units and global co-occurrence matrices allow GloVe and FastText to capture nuances that Word2Vec missed.","title":"The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText"},{"content":" Before we had Large Language Models writing poetry, we had to teach computers that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; are related not just by spelling, but by meaning. This is the story of that breakthrough. It‚Äôs the moment we stopped counting words and started mapping their souls‚Äîturning raw text into a mathematical landscape where math can solve analogies. Welcome to the world of Word2Vec. üîÆ\nLanguage models require vector representations of words to capture semantic relationships. Before the 2010s, models used word count-based vector representations that captured only the frequency of words (e.g., One-Hot Encoding). The Problems: üöß\nThe Curse of Dimensionality Lack of meaning (Synonyms were treated as mathematically unrelated). In 2011, Mikolov et al. at Google introduced Word2Vec, which used a shallow neural network to learn vector representations of words. The best part? It could create much denser relationships between words than prior models, and it was unsupervised.\nThe training (prediction) is essentially \u0026ldquo;fake\u0026rdquo;‚Äîa pretext task. But the weight matrices we get on the side are a gold mine of fine-grained semantic relationships. ‚õèÔ∏è\nThere are two popular variants of Word2Vec:\nSkipGram Continuous Bag Of Words (CBOW) 1. Continuous Bag of Words (CBOW) üß© CBOW is a simple \u0026lsquo;fill in the blanks\u0026rsquo; machine. It takes the surrounding words as inputs and tries to predict the center word.\n\u0026ldquo;Continuous\u0026rdquo;: It operates in continuous vector space (unlike the discrete space of n-gram models).\n\u0026ldquo;Bag of words\u0026rdquo;: The order of the context does not matter. \u0026ldquo;The cat sat on the mat\u0026rdquo; and \u0026ldquo;The mat sat on the cat\u0026rdquo; are the same for CBOW. They produce exactly the same predictions.\nObjective: Maximize the probability of the target word $w_t$ given its context $w_{t-C}, \\dots, w_{t+C}$.\n2. Training Defining Dimensions:\n$V$: Vocabulary size (number of unique words in the corpus, e.g., 10,000) $N$: Dimension of the embedding space (e.g., 100) $C$: Number of context words (e.g., 2 words on left of target and 2 words on right) Step 1: Input Lookup \u0026amp; One-Hot Encoding\nMathematically, we represent each context word as a one-hot vector $x^{(c)}$. We have an Input Matrix $W_{in}$ of size $V \\times N$. For each context word (e.g., \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;), we select its corresponding row from $W_{in}$.\n$$ v_c = W_{in}^T x^{(c)} $$\nWhere $v_c$ is the $N$-dimensional vector for context word $c$.\nStep 2: Projection (The \u0026ldquo;Mixing\u0026rdquo;)\nWe combine the context vectors. In standard CBOW, we simply average them. This creates the hidden layer vector $h$.\n$$ h = \\frac{1}{2C} \\sum_{c=1}^{2C} v_c $$\n$h$ is a single vector of size $N \\times 1$. Note: This operation is linear and contains no activation function (like ReLU or Sigmoid).\nStep 3: Output Scoring (The \u0026ldquo;Dot Product\u0026rdquo;)\nWe have a second matrix, the Output Matrix $W_{out}$ of size $N \\times V$. We compute the raw score (logit) $u_j$ for every word $j$ in the vocabulary by taking the dot product of the hidden vector $h$ and the output vector $v\u0026rsquo;_j$.\n$$ u_j = {v\u0026rsquo;_j}^T h $$\nOr in matrix form:\n$$ u = W_{out}^T h $$\nResult $u$ is a vector of size $V \\times 1$.\nStep 4: Probability Conversion (Softmax)\nWe convert the raw scores $u$ into probabilities using the Softmax function. This tells us the probability that word $w_j$ is the center word.\n$$ y_j = P(w_j | \\text{context}) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 5: Loss Calculation (Cross-Entropy)\nWe compare the predicted distribution $y$ with the actual ground truth $t$ (a one-hot vector where the index of the true word is 1). The loss function $E$ is:\n$$ E = - \\sum_{j=1}^V t_j \\log(y_j) $$\nSince $t_j$ is 0 for all words except the target word (let\u0026rsquo;s call the target index $j^*$), this simplifies to:\n$$ E = - \\log(y_{j^*}) $$\n3. Backpropagation Details We need to update the weights to minimize $E$. We use the Chain Rule.\nA. Gradient w.r.t. Output Matrix ($W_{out}$)\nWe want to know how the error changes with respect to the raw score $u_j$.\n$$ \\frac{\\partial E}{\\partial u_j} = y_j - t_j $$\nLet\u0026rsquo;s call this error term $e_j$.\nIf $j$ is the target word: $e_j = y_j - 1$ (Result is negative; we boost the score). If $j$ is not target: $e_j = y_j - 0$ (Result is positive; we suppress the score). The update rule for the output vector $v\u0026rsquo;_j$ becomes:\n$$ v'_{j}(\\text{new}) = v'_{j}(\\text{old}) - \\eta \\cdot e_j \\cdot h $$ ($\\eta$ is the learning rate)\nB. Gradient w.r.t. Hidden Layer ($h$)\nWe backpropagate the error from the output layer to the hidden layer.\n$$ EH = \\sum_{j=1}^V e_j \\cdot v\u0026rsquo;_j $$\n$EH$ is an $N$-dimensional vector representing the aggregate error passed back to the projection layer.\nC. Gradient w.r.t. Input Matrix ($W_{in}$)\nSince $h$ was just an average of the input vectors, the error $EH$ is distributed to each context word\u0026rsquo;s input vector. For every word $w_c$ in the context:\n$$ v_{w_c}(\\text{new}) = v_{w_c}(\\text{old}) - \\eta \\cdot \\frac{1}{2C} \\cdot EH $$\nLet\u0026rsquo;s build a single pass through the network for a given context.\nimport torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # Define a simple Continuous Bag of Words (CBOW) style model class CBOWModel(nn.Module): def __init__(self, vocab_size, embedding_dim): super(CBOWModel, self).__init__() # The embedding layer stores the word vectors we want to learn self.embeddings = nn.Embedding(vocab_size, embedding_dim) # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word self.linear = nn.Linear(embedding_dim, vocab_size) def forward(self, inputs): # inputs: tensor of word indices for the context embeds = self.embeddings(inputs) # Aggregate context by calculating the mean of the word embeddings h = torch.mean(embeds, dim=1) # Produce logits (raw scores) for each word in the vocabulary logits = self.linear(h) return logits # Setup vocabulary and mappings word_to_ix = {\u0026#34;the\u0026#34;: 0, \u0026#34;quick\u0026#34;: 1, \u0026#34;brown\u0026#34;: 2, \u0026#34;fox\u0026#34;: 3, \u0026#34;jumps\u0026#34;: 4, \u0026#34;over\u0026#34;: 5, \u0026#34;lazy\u0026#34;: 6, \u0026#34;dog\u0026#34;: 7} ix_to_word = {v: k for k, v in word_to_ix.items()} # Configuration constants EMBEDDING_DIM = 5 VOCAB_SIZE = len(word_to_ix) LEARNING_RATE = 0.01 # Initialize the model, loss function, and optimizer model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM) loss_function = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE) # Prepare dummy training data: context words and the target word # Context: [\u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;over\u0026#34;] -\u0026gt; Target: \u0026#34;fox\u0026#34; context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long) target_idxs = torch.tensor([3], dtype=torch.long) # Perform a single optimization step model.zero_grad() # Forward pass: we reshape context to (1, -1) to simulate a batch of size 1 logits = model(context_idxs.view(1, -1)) # Calculate loss against the target word index loss = loss_function(logits, target_idxs) # Backpropagate and update weights loss.backward() optimizer.step() # Output results for the blog post print(\u0026#34;=== Model Training Snapshot ===\u0026#34;) print(f\u0026#34;Calculated Loss: {loss.item():.6f}\u0026#34;) print(f\u0026#34;\\n=== Learned Vector for \u0026#39;jumps\u0026#39; ===\u0026#34;) word_vec = model.embeddings(torch.tensor([word_to_ix[\u0026#39;jumps\u0026#39;]])) print(word_vec.detach().numpy()) print(\u0026#34;\\n=== Embedding Matrix (Weights) ===\u0026#34;) print(model.embeddings.weight.detach().numpy()) print(\u0026#34;\\n=== Linear Layer Weights ===\u0026#34;) print(model.linear.weight.detach().numpy()) Output shows single pass loss and learned vector for \u0026lsquo;jumps\u0026rsquo; word. This is a single training sample result but after multiple such passes both Embedding and linear weights should be very similar.\n=== Model Training Snapshot === Calculated Loss: 1.989656 === Learned Vector for \u0026#39;jumps\u0026#39; === [[-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775]] === Embedding Matrix (Weights) === [[ 1.66039 -0.11371879 0.6246518 0.35860053 -1.2504417 ] [-2.1847186 -0.77199775 -0.17050214 -0.38411248 -0.03913084] [ 0.11852697 0.90073633 0.8847807 0.7404524 0.900149 ] [ 0.07440972 -0.40259898 2.6246994 -0.08851447 0.02660969] [-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775] [-0.9245572 -0.5545908 0.9083091 -1.0755049 0.84047747] [-1.0237687 0.59466314 0.05621134 -0.6202532 1.3664424 ] [ 0.60998917 -1.0549186 1.6103884 0.8724912 -1.2486908 ]] === Linear Layer Weights === [[ 0.02165058 -0.28883642 0.14545658 -0.3442509 0.32704315] [-0.18731792 0.28583744 0.22635977 0.13245736 0.29019794] [ 0.3158916 -0.15826383 -0.03203773 0.16377363 -0.41457543] [-0.05080034 0.4180087 0.11228557 0.30218413 0.3025514 ] [-0.38419306 0.24475925 -0.39210224 -0.38660625 -0.2673145 ] [-0.32321444 0.12200444 -0.03569533 0.2891424 -0.07345333] [-0.33704326 0.2521956 0.31587374 0.22590035 0.29866052] [ 0.4117266 -0.44231793 0.24064957 -0.29684234 0.333821 ]] Interesting Facts About CBOW üß† The \u0026ldquo;Averaging Problem\u0026rdquo;: This is a common issue in CBOW models. The averaging and training is order-agnostic. All variants of word order in the context window will generate the same output. \u0026ldquo;Dog bit the man\u0026rdquo; and \u0026ldquo;Man bit the dog\u0026rdquo; are the same in CBOW\u0026rsquo;s eyes.\nSmoothing Effect: CBOW models are much faster to train than Skip-Gram but are generally smoother.\nRare Words: CBOW models struggle slightly with rare words because the presence of surrounding common words (like articles and prepositions) can skew the embeddings and dissolve their uniqueness.\nComputational Cost: Softmax requires summing over the entire vocabulary size, which can be computationally expensive if $V$ is very large. Solutions include Hierarchical Softmax and Negative Sampling (approximating the denominator by only checking the target word vs. 5 random noise words).\nVectors as Double Agents: Every word has a different representation in the embedding matrix and the output matrix. Most of the time $W_{out}$ is discarded, but some consider averaging both $W_{in}$ and $W_{out}$ for slightly better performance.\nLinear Relationships: The famous analogy King - Man + Woman = Queen emerges from the model because of the linear relationships between the words.\nInitialization: This is a no-brainer but worth mentioning. Initialization to zero vectors would mean no learning. We must initialize randomly.\n2. Skip-Gram (The \u0026ldquo;Burger Flip\u0026rdquo; of CBOW) üîÑ Skip-Gram flips the CBOW logic. While CBOW tries to predict the center word given the context, Skip-Gram tries to predict the scattered context given the center word.\nCore Concept: If a word like \u0026ldquo;monarch\u0026rdquo; can accurately predict words like \u0026ldquo;king\u0026rdquo;, \u0026ldquo;throne\u0026rdquo;, \u0026ldquo;crown\u0026rdquo;, \u0026ldquo;royalty\u0026rdquo;, and \u0026ldquo;power\u0026rdquo;, then it must effectively capture the concept of royalty.\nTraining Let\u0026rsquo;s define dimensions:\n$V$: Vocabulary size $D$: Embedding dimension $C$: Number of context words Step 1: Input Lookup\nUnlike CBOW, our input is just one word vector (the center word $w_t$). We grab the vector $v_c$ from the Input Matrix $W_{in}$.\n$$ h = v_{w_t} $$\nNote: There is no averaging here. The hidden layer $h$ is simply the raw vector of the center word.\nStep 2: Output Scoring (The Broadcast)\nThe model takes this single vector $h$ and compares it against the Output Matrix $W_{out}$ (the entire vocabulary). It computes a score $u_j$ for every word $j$ in the dictionary.\n$$ u = W_{out}^T h $$\nResult $u$: A vector of size $V \\times 1$ containing raw scores.\nStep 3: Probability Conversion (Softmax)\nWe apply Softmax to turn scores into probabilities.\n$$ P(w_j | w_t) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 4: The Multi-Target Loss\nHere is the major difference from CBOW. In CBOW, we had one target (the center). In Skip-Gram, we have $2C$ targets (all the surrounding words). For a sentence \u0026ldquo;The cat sat on mat\u0026rdquo; (Center: \u0026ldquo;sat\u0026rdquo;), the targets are \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;. We want to maximize the probability of all these true context words. The Loss function ($E$) sums the error for each context word:\n$$ E = - \\sum_{w_{ctx} \\in \\text{Window}} \\log P(w_{ctx} | w_t) $$\nStep 5: Backpropagation (Accumulating Gradients)\nSince one input word is responsible for predicting multiple output words, the error signals from all those context words add up.\nAt the Output Layer:\nIf \u0026ldquo;cat\u0026rdquo; was a target, the \u0026ldquo;cat\u0026rdquo; vector in $W_{out}$ gets a signal: \u0026ldquo;Move closer to \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; If \u0026ldquo;apple\u0026rdquo; was NOT a target, the \u0026ldquo;apple\u0026rdquo; vector gets a signal: \u0026ldquo;Move away from \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; At the Hidden Layer ($h$):\nThe center word \u0026ldquo;sat\u0026rdquo; receives feedback from all its neighbors simultaneously. Error signal = (Error from \u0026ldquo;The\u0026rdquo;) + (Error from \u0026ldquo;cat\u0026rdquo;) + (Error from \u0026ldquo;on\u0026rdquo;)\u0026hellip; The vector for \u0026ldquo;sat\u0026rdquo; in $W_{in}$ moves in the average direction of all its neighbors. 3. The \u0026ldquo;Computational Nightmare\u0026rdquo; \u0026amp; Negative Sampling The equations above describe Naive Softmax.\nThe Problem: If $V = 100,000$, computing the denominator $\\sum \\exp(u_k)$ takes 100,000 operations. Doing this for every word in a training set of billions of words is impossibly slow.\nThe Solution: Negative Sampling Instead of updating the entire vocabulary (1 correct word vs 99,999 wrong ones), we approximate the problem.\nPositive Pair: (sat, cat) $\\rightarrow$ Maximize probability (Label 1). Negative Pairs: We pick $K$ (e.g., 5) random words the model didn\u0026rsquo;t see, e.g., (sat, bulldozer), (sat, quantum). $\\rightarrow$ Minimize probability (Label 0). New Equation (Sigmoid instead of Softmax):\n$$ E = - \\log \\sigma({v'_{\\text{pos}}}^T v_{\\text{in}}) - \\sum_{k=1}^K \\log \\sigma(-{v'_{\\text{neg}_{k}}}^T v_{\\text{in}}) $$ Effect: We typically only update 6 vectors per step (1 pos + 5 negs) instead of 100,000.\nLet\u0026rsquo;s see this in code:\nimport torch import torch.nn as nn import torch.optim as optim class SkipGramNegativeSampling(nn.Module): \u0026#34;\u0026#34;\u0026#34; Skip-Gram with Negative Sampling (SGNS) implementation. SGNS approximates the softmax over the entire vocabulary by instead distinguishing between a real context word (positive) and K noise words (negative). \u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embedding_dim): super(SkipGramNegativeSampling, self).__init__() # Input embeddings: used when the word is the center word self.in_embed = nn.Embedding(vocab_size, embedding_dim) # Output embeddings: used when the word is a context or negative sample self.out_embed = nn.Embedding(vocab_size, embedding_dim) # Initialize weights with small values to prevent gradient saturation initrange = 0.5 / embedding_dim self.in_embed.weight.data.uniform_(-initrange, initrange) self.out_embed.weight.data.uniform_(-initrange, initrange) def forward(self, center_words, target_words, negative_words): \u0026#34;\u0026#34;\u0026#34; Computes the negative sampling loss. Args: center_words: (batch_size) target_words: (batch_size) negative_words: (batch_size, K) where K is number of negative samples \u0026#34;\u0026#34;\u0026#34; # Retrieve vectors v_c = self.in_embed(center_words) # (batch_size, embed_dim) u_o = self.out_embed(target_words) # (batch_size, embed_dim) u_n = self.out_embed(negative_words) # (batch_size, K, embed_dim) # 1. Positive Score: log(sigmoid(v_c ¬∑ u_o)) # Compute dot product: (batch, 1, dim) @ (batch, dim, 1) -\u0026gt; (batch, 1) pos_score = torch.bmm(u_o.unsqueeze(1), v_c.unsqueeze(2)).squeeze(2) pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-7) # 2. Negative Score: sum(log(sigmoid(-v_c ¬∑ u_n))) # Compute dot products for all K samples: (batch, K, dim) @ (batch, dim, 1) -\u0026gt; (batch, K) neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2) neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-7), dim=1, keepdim=True) # Total loss is the negative of the objective function loss = -(pos_loss + neg_loss) return torch.mean(loss) # --- Configuration \u0026amp; Mock Data --- VOCAB_SIZE = 100 EMBED_DIM = 10 word_to_ix = {\u0026#39;fox\u0026#39;: 0} # Example vocabulary mapping model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock inputs: 1 center word, 1 target word, 5 negative samples center_id = torch.tensor([0], dtype=torch.long) target_id = torch.tensor([1], dtype=torch.long) negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long) # Training Step model.zero_grad() loss = model(center_id, target_id, negative_ids) loss.backward() optimizer.step() # Output Results print(f\u0026#34;Loss after one step: {loss.item():.6f}\u0026#34;) word_vec = model.in_embed(torch.tensor([word_to_ix[\u0026#39;fox\u0026#39;]])) print(f\u0026#34;Vector for \u0026#39;fox\u0026#39;:\\n{word_vec.detach().numpy()}\u0026#34;) The outputs:\nLoss after one step: 4.158468 Vector for \u0026#39;fox\u0026#39;: [[ 0.04197385 0.02400733 -0.03800093 0.01672485 -0.03872231 -0.0061478 0.0121122 0.04057864 -0.036255 0.03861175]] Interesting Facts About Skip-Gram üí° Makes Rare Words Shine: Skip-Gram is more effective at capturing the meaning of rare words because it focuses on the context of the center word. In CBOW, a rare word\u0026rsquo;s vector is averaged with others, diluting its signal. In Skip-Gram, when the rare word appears as the center, it gets the full, undivided attention of the backpropagation update.\nSlow Training: This method creates more training samples. A window of size 2 creates 4 training pairs per center word, while CBOW creates only 1 training pair per center word.\nSemantic Focus: Skip-Gram puts more emphasis on the semantic relationship between words. It views the context as the primary signal, with the center word as the target. This makes it better at capturing the meaning of words in context. Skip-Gram tends to capture semantic relationships (King/Queen) slightly better, while CBOW captures syntactic ones (walk/walked) slightly better.\n","permalink":"http://localhost:1313/posts/word-embeddings/","summary":"Understanding the mathematics behind Word2Vec, CBOW, and Skip-Gram and how they map language to vector space.","title":"Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n5. Code Examples: We\u0026rsquo;ll use sentencepiece for tokenization and compare results for different models.\nimport sentencepiece as spm import os # Configuration for training the SentencePiece model # SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words. options = { # The source text file used for learning the vocabulary \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # The base name for the generated model (.model) and vocabulary (.vocab) files \u0026#39;model_prefix\u0026#39;: \u0026#39;bpe_model\u0026#39;, # Number of unique tokens in the final vocabulary \u0026#39;vocab_size\u0026#39;: 4000, # \u0026#39;bpe\u0026#39; (Byte Pair Encoding) merges frequent pairs of characters/sequences \u0026#39;model_type\u0026#39;: \u0026#39;bpe\u0026#39;, # Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets \u0026#39;character_coverage\u0026#39;: 0.9995, # When enabled, unknown characters are decomposed into UTF-8 bytes to avoid \u0026#39;unk\u0026#39; tokens \u0026#39;byte_fallback\u0026#39;: True, # Treats digits individually (0-9), preventing large numbers from being treated as single tokens \u0026#39;split_digits\u0026#39;: True, # Prevents adding a whitespace prefix to the first token; useful for fine-grained control \u0026#39;add_dummy_prefix\u0026#39;: False } try: print(\u0026#34;Starting the training process...\u0026#34;) # SentencePieceTrainer.train takes the dictionary of options to build the BPE model spm.SentencePieceTrainer.train(**options) print(\u0026#34;Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created.\u0026#34;) # Initialize the processor and load the newly trained model sp = spm.SentencePieceProcessor() sp.load(\u0026#39;bpe_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Model Metadata:\u0026#34;) # Retrieve the total number of tokens in the vocabulary print(f\u0026#34;Total Vocab Size: {sp.get_piece_size()}\u0026#34;) # Special tokens are used for sequence boundaries and handling unknown characters print(f\u0026#34;BOS (Beginning of Sentence) ID: {sp.bos_id()}\u0026#34;) print(f\u0026#34;EOS (End of Sentence) ID: {sp.eos_id()}\u0026#34;) print(f\u0026#34;UNK (Unknown) ID: {sp.unk_id()}\u0026#34;) print(f\u0026#34;PAD (Padding) ID: {sp.pad_id()}\u0026#34;) # Test the tokenizer on sample strings to see how it breaks down text test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: shows the actual subword units (tokens) print(f\u0026#34;Subword Tokens: {sp.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: shows the numerical mapping for each token print(f\u0026#34;Numerical IDs: {sp.encode_as_ids(text)}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred during training or processing: {e}\u0026#34;) You can see the following output:\nStarting training... Starting the training process... Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created. ------------------------------ Model Metadata: Total Vocab Size: 4000 BOS (Beginning of Sentence) ID: 1 EOS (End of Sentence) ID: 2 UNK (Unknown) ID: 0 PAD (Padding) ID: -1 --- Tokenization Test --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;el\u0026#39;, \u0026#39;lo\u0026#39;, \u0026#39;‚ñÅW\u0026#39;, \u0026#39;orld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 380, 309, 476, 619, 36, 3924, 3964, 3959, 3993, 3978, 3983, 57, 3975, 3976, 60, 3974] --- Tokenization Test --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅmo\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;inst\u0026#39;, \u0026#39;ag\u0026#39;, \u0026#39;ram\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅever\u0026#39;] Numerical IDs: [1894, 304, 3284, 376, 321, 313, 312, 1008, 3313, 1325, 428, 3937, 341, 304, 3284, 1495] The ! is fallen back to UTF bytes. and the word instagrammable is split into multiple subword tokens. Also, original text can be recreated right back by simply concatenating the tokens. And replacing underscore by whitespace. Now we can try Unigram model. and see the same sentences tokenized differently.\nfrom tokenizers import Tokenizer from tokenizers.models import WordPiece from tokenizers.trainers import WordPieceTrainer from tokenizers.pre_tokenizers import Whitespace try: # 1. Initialize the WordPiece Tokenizer # We specify the [UNK] token for handling words not found in the vocabulary. tokenizer = Tokenizer(WordPiece(unk_token=\u0026#34;[UNK]\u0026#34;)) # 2. Configure Pre-tokenization # Before the subword algorithm runs, we need to split the raw text into words. # Whitespace splitting is the standard first step for most English NLP tasks. tokenizer.pre_tokenizer = Whitespace() # 3. Initialize the Trainer # We define our target vocabulary size and the special tokens required for # downstream tasks (like BERT\u0026#39;s [CLS] for classification or [SEP] for separators). trainer = WordPieceTrainer( vocab_size=4000, special_tokens=[\u0026#34;[UNK]\u0026#34;, \u0026#34;[CLS]\u0026#34;, \u0026#34;[SEP]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[MASK]\u0026#34;] ) # 4. Train the Model # The tokenizer scans the training file to build a vocabulary of the most # frequent subword units. tokenizer.train(files=[\u0026#34;train_data.txt\u0026#34;], trainer=trainer) # 5. Persist the Model # Save the configuration and vocabulary to a JSON file for future inference. tokenizer.save(\u0026#34;wordpiece.json\u0026#34;) print(\u0026#34;Training complete. \u0026#39;wordpiece.json\u0026#39; created.\u0026#34;) # 6. Metadata Inspection print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;WordPiece Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {tokenizer.get_vocab_size()}\u0026#34;) # 7. Testing Subword Tokenization # WordPiece shines at handling rare words by breaking them into meaningful chunks. test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (WordPiece) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # Encode converts raw text into a Tokenizer object containing tokens and IDs output = tokenizer.encode(text) # \u0026#39;tokens\u0026#39; shows the subword breakdown (e.g., \u0026#39;un\u0026#39;, \u0026#39;##insta\u0026#39;, etc.) print(f\u0026#34;Subword Tokens: {output.tokens}\u0026#34;) # \u0026#39;ids\u0026#39; are the numerical indices mapped to the vocabulary print(f\u0026#34;Numerical IDs: {output.ids}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred with WordPiece model: {e}\u0026#34;) You can see the following output:\nWordPiece Model Metadata: Total Vocab Size: 2609 --- Tokenization Test (WordPiece) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;H\u0026#39;, \u0026#39;##el\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;##or\u0026#39;, \u0026#39;##ld\u0026#39;, \u0026#39;[UNK]\u0026#39;, \u0026#39;[UNK]\u0026#39;] Numerical IDs: [37, 180, 214, 52, 162, 418, 0, 0] --- Tokenization Test (WordPiece) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;##os\u0026#39;, \u0026#39;##t\u0026#39;, \u0026#39;un\u0026#39;, \u0026#39;##ins\u0026#39;, \u0026#39;##ta\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;##ra\u0026#39;, \u0026#39;##m\u0026#39;, \u0026#39;##ma\u0026#39;, \u0026#39;##ble\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;ever\u0026#39;] Numerical IDs: [691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240] We can see the special character ## in the output. We can also see the breakdowns not having any semantic meaning. They are likelihood based. We can try the Unigram model now.\n# \u0026#39;Unigram\u0026#39; is the default and usually recommended over BPE in SentencePiece. options_unigram = { \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # Path to the raw text file for training \u0026#39;model_prefix\u0026#39;: \u0026#39;unigram_model\u0026#39;, # Prefix for the output .model and .vocab files \u0026#39;vocab_size\u0026#39;: 1200, # Desired size of the final vocabulary \u0026#39;model_type\u0026#39;: \u0026#39;unigram\u0026#39;, # Specifies the Unigram language model algorithm \u0026#39;character_coverage\u0026#39;: 0.9995, # Percentage of characters covered by the model (0.9995 is standard for Latin scripts) \u0026#39;byte_fallback\u0026#39;: True, # Enables mapping unknown characters to UTF-8 bytes to avoid \u0026lt;unk\u0026gt; tokens \u0026#39;split_digits\u0026#39;: True, # Treats each digit as an individual token (useful for numerical data) \u0026#39;add_dummy_prefix\u0026#39;: False # Prevents adding a leading space (SentencePiece default is True) } try: # 1. Train the SentencePiece model using the defined options print(\u0026#34;Starting Unigram training...\u0026#34;) spm.SentencePieceTrainer.train(**options_unigram) print(\u0026#34;Training complete. \u0026#39;unigram_model.model\u0026#39; created.\u0026#34;) # 2. Load the trained model into a processor instance for inference sp_unigram = spm.SentencePieceProcessor() sp_unigram.load(\u0026#39;unigram_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Unigram Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {sp_unigram.get_piece_size()}\u0026#34;) # 3. Define test cases to evaluate how the model handles common and rare words test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] # 4. Iterate through test sentences to visualize subword segmentation for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (Unigram) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: Converts text into subword strings (visual representation) print(f\u0026#34;Subword Tokens: {sp_unigram.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: Converts text into numerical indices for model input print(f\u0026#34;Numerical IDs: {sp_unigram.encode_as_ids(text)}\u0026#34;) except Exception as e: # Handle potential errors during training or loading (e.g., missing input file) print(f\u0026#34;An error occurred with Unigram model: {e}\u0026#34;) The output is as follows:\nUnigram Model Metadata: Total Vocab Size: 1200 --- Tokenization Test (Unigram) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ll\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;ld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311] --- Tokenization Test (Unigram) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;T\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅm\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;sta\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;ra\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ver\u0026#39;] ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"A comprehensive guide to tokenization strategies: BPE, WordPiece, Unigram, and SentencePiece.","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"A brief introduction to Vectors \u0026amp; Verbs and formatting verification.","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"\nFeedforward network (FFN) blocks are the least talked about elements in the context of LLMs, but they are where more than 70% of the parameters originate. Something as simple and convenient as a couple of linear transformation matrices becomes a burden when models start scaling into billions of parameters. Thus, getting the maximum juice out of a lean structure is a challenge. FFNs are mostly under-represented in discussions even if the sub-layer acts as the primary \u0026ldquo;knowledge\u0026rdquo; processing engine, processing all the meaningful contexts activation layer has provided it.\nüî• Standard Position-Wise Feedforward Network This is as simple as it gets: a two-layered fully connected network of neurons with a single activation switch‚Äîmost probably a ReLU variant (for LLMs).\n$$ FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2 $$\nThe same transformation is applied to each token in parallel, making it efficient for GPU computation. For example, in the BERT model, the 512 dimensions of each token are expanded to 2048 to allow for \u0026ldquo;intricate interactions\u0026rdquo; before projecting back to 512 dimensions. This expansion/contraction is crucial; in every transformer, stacking of layers is possible only because the output of every layer matches its input dimensions.\nüî• SWiGLU Sigmoid Weighted Gated Linear Unit (SWiGLU) has become the gold standard for feedforward networks in LLMs like Llama 3 and PaLM. The primary reasons for choosing the Swish function (shown in the first image) over ReLU or GELU are as follows:\nMathematical Linearity: Swish (SiLU) is defined as $x \\cdot \\text{sigmoid}(\\beta x)$, where $\\beta$ is a learnable parameter. It is a smooth, non-monotonic function that allows small negative values to pass through. While GELU ($x \\cdot \\Phi(x)$) is also smooth, Swish\u0026rsquo;s specific curve has empirically improved model performance. Computational Efficiency: SiLU gradients are simpler to compute ($x / (1 + \\exp(-x))$) compared to standard GELU approximations. At the massive scale of modern LLMs, these shortcuts save significant training time. Performance: Research in the PaLM and Llama papers found that Swish-based Gated Linear Units consistently outperformed GELU-based versions in terms of perplexity and downstream task accuracy. $$SwiGLU(x, W, V, b, c) = (\\text{Swish}(xW + b) \\otimes (xV + c))W_2$$\nWhere $W$ and $V$ are weight matrices for the gate and the data path, and $\\otimes$ is element-wise multiplication.\nMore Complex Filtering: Instead of using two weight matrices as in a standard FFN, SWiGLU uses three (gate, data, and output). This allows it to extract more complex features without a drastic increase in parameters. Modern implementations often lower the dimensionality of hidden layers (to ~2/3 of what they would be in a standard FFN) while retaining the performance benefits. Zero-Gradient Avoidance: Unlike ReLU, which has a zero gradient for all negative values (the \u0026ldquo;dying ReLU\u0026rdquo; problem), SwiGLU‚Äôs smooth curve ensures the model can still learn from negative inputs, improving training stability in deep stacks. üî• Mixture of Experts (MoE) MoE is an entire architecture rather than just a single block. It has become the dominant strategy for scaling models to insane sizes‚Äîup to a trillion parameters (like DeepSeek-V3 and GPT-4)‚Äîbecause it breaks the direct link between model size (information capacity) and speed (inference cost).\nSparse Activation: MoE uses a \u0026ldquo;Router\u0026rdquo; to determine which smaller \u0026ldquo;Experts\u0026rdquo; (specialized FFNs) are best suited for a specific token. Instead of activating the entire network, only a tiny fraction of experts are used per token. For instance, DeepSeek-V3 might have 256 expert layers but only activate 8 for any single token. Big Scale, Small Compute: MoE allows \u0026ldquo;trillion-parameter intelligence\u0026rdquo; to run at the speed of a much smaller model. While DeepSeek-V3 has 671 billion total parameters, it only uses ~37 billion per token during inference. You get the reasoning performance of a massive model with the latency of a smaller one. Expert Specialization: Because different experts handle different tokens, they naturally specialize. Analysis shows certain experts become \u0026ldquo;specialists\u0026rdquo; in mathematics or coding, while others handle general language. Training Efficiency: Since you only update a fraction of the weights for each token, MoE models can process many more tokens per second during training compared to a dense model of the same total size. Training Dynamics: The router assigns probabilities to each expert. To avoid overfitting or \u0026ldquo;expert collapse\u0026rdquo; (where the router sends everything to one expert), noise is often added to the router\u0026rsquo;s output during training. Some models choose one top expert, while others choose multiple and average their outputs.\nThe Challenges of MoE The RAM Wall: Even if only a small percentage of parameters are used for computation, all 671B parameters must still reside in GPU memory. This makes MoE models difficult to run on consumer hardware. Training Instability: MoEs often require \u0026ldquo;auxiliary losses\u0026rdquo; to force the router to balance the load across experts. DeepSeek-V3 introduced an \u0026ldquo;Auxiliary-Loss-Free\u0026rdquo; strategy to balance load dynamically without degrading accuracy. Communication Overhead: In distributed training, experts are often split across different GPUs. If a token on GPU 1 needs an expert on GPU 4, data must travel over the network. DeepSeek-V3 employs \u0026ldquo;Node-Limited Routing\u0026rdquo; to minimize this congestion. Summary: MoE is the better choice for major labs (like DeepSeek, Google, or OpenAI) trying to maximize intelligence per dollar of training cost. Dense SwiGLU remains the standard for reliability, ease of deployment, and local execution.\n","permalink":"http://localhost:1313/posts/feedforward-networks/","summary":"Exploring the hidden layers of trillion-parameter switchboards: Feedforward Neural Networks and Activation Functions.","title":"Anatomy of Trillion-Parameter Switchboards: Understanding Feedforward Blocks"},{"content":"\nWe are about to touch the holy grail of modern AI. The 2017 declaration was simple: Attention Is All You Need. But as we‚Äôve raced from GPT-1 to reasoning models, the definition of \u0026lsquo;Attention\u0026rsquo; has quietly transformed. Is the algorithm ruling the world today actually the same one we started with? Let‚Äôs look under the hood.\nüî• The Core Concept At its heart, the attention mechanism is a form of weighted aggregation. It is often described using a database retrieval analogy. Imagine you have a database of information (context).\nQuery ($Q$): What are you currently looking for? (The token being generated). Key ($K$): The indexing tag associated with each piece of information. Value ($V$): The actual information (The context token). The \u0026ldquo;Attention\u0026rdquo; is the process of matching your Query against all the Keys to determine how much \u0026ldquo;weight\u0026rdquo; to put on each Value. Attention scores are calculated in each transformer layer at every step of the sequence. This is a single unifying factor in all transformer models, from BERT to Gemini.\nüî• Standard Multi-Head Attention (MHA) The original mechanism introduced in the paper \u0026ldquo;Attention Is All You Need\u0026rdquo;.\nHere, instead of performing the Key-Query-Value (KQV) operation on the entire context only once, the attention is split into $N$ parallel heads. Each head learns a different aspect of the language (e.g., word meaning, syntax, semantic relationships, etc.). The equation is a simple culmination of dot products aided by a regularizer like Softmax.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nWhere:\n$QK^T$ computes the similarity scores between the Query and Key. $\\sqrt{d_k}$ is a scaling factor to prevent the dot product from growing too large, mitigating the vanishing/exploding gradient problem. The weighted sum is applied to the values of $V$. The Memory Bandwidth Bottleneck: \u0026gt; During inference, the model must store Key and Value matrices for all previous tokens. This is called the KV Cache. In MHA, every single head has its own unique set of Keys and Values. For large models with massive context windows, loading this massive KV cache from memory (HBM) to the computation chip (SRAM) becomes the primary bottleneck, slowing down generation significantly. Earlier models could often only generate ~8 tokens per second at inference due to this constraint.\nüî• Multi-Query Attention (MQA) This approach addresses the memory bottleneck by sharing the same set of Keys and Values across all heads while keeping separate Query heads.\nMHA: $H$ Query heads, $H$ Key heads, $H$ Value heads MQA: $H$ Query heads, 1 Key head, 1 Value head This drastically reduces the memory requirements of the KV cache by a factor of $H$ (often 8x or 16x). The GPU needs to load far less data, significantly speeding up inference. However, it does not reduce the computation (FLOPs)‚Äîso why is it faster?\nThis can be explained with the Kitchen Analogy: Imagine a kitchen (GPU) with 8 chefs (Heads).\nComputation (Math): The Chef chopping vegetables. Memory Access (Bandwidth): The Assistant running to the fridge to get ingredients. In MHA: Each chef demands unique vegetables from the fridge. The assistant runs back and forth 8 times, fetching 8 different crates. Most of the time is spent waiting for the assistant to return. This results in idle cores waiting for data to load from HBM.\nIn MQA: All chefs agree to use the same vegetables. The assistant runs back and forth only once, fetching one giant crate of potatoes and dumping it. This results in minimized memory transfer and no idle cores.\nThe Technical Reality: During text generation, the model is memory-bound, not compute-bound. The time it takes to move the KV Cache from HBM (High Bandwidth Memory) to the chip\u0026rsquo;s SRAM is the bottleneck. MQA reduces the data volume of that move by a factor of $H$.\nThe Capacity Gap: MQA is a trade-off between nuanced response and memory capacity. The model loses the ability to store different types of information, as a single $V$ matrix must now store all nuances of information flow in the language.\nThe Surgery Trick (Uptraining): Google Research figured out a way to take an existing model with MHA and convert it to MQA by averaging the weight matrices of $H$ heads. This is called Uptraining. You then train this \u0026ldquo;Frankenstein\u0026rdquo; model for ~5% of the original training steps for adjustments. This yields an MQA model that is nearly as good as the original MHA but vastly faster.\nüî• Grouped Query Attention (GQA) A \u0026ldquo;best of both worlds\u0026rdquo; solution used in modern models like Llama 2 and Llama 3.\nHow it works: GQA acts as an interpolation between MHA and MQA. Instead of having just 1 Key head (MQA) or $H$ Key heads (MHA), it divides the query heads into $G$ groups. Each group shares a single Key and Value head.\nEquation modification: If you have 8 query heads, you might create 4 groups (2 queries per group). This reduces the KV cache by 2x instead of 8x, preserving more quality than MQA while still speeding up inference.\nMHA: 8 people (heads) are working on a project. Each person has their own unique filing cabinet (KV cache). It takes forever to move 8 cabinets around. MQA: 8 people share one single filing cabinet. It\u0026rsquo;s very fast to move, but they fight over the organization and lose detail. GQA: The 8 people split into 4 teams of 2. Each team shares a cabinet. It\u0026rsquo;s a balance between speed and organizational detail. üî• Multi-Head Latent Attention (MLA) This is the most advanced mechanism for attention, used in models like DeepSeek-V3. It is a sophisticated design for massive efficiency that takes a radical approach: mathematically compressing the memory into a low-rank \u0026ldquo;latent\u0026rdquo; vector.\nKV Cache Explosion: As mentioned before, for a massive model like DeepSeek-V3 (671B parameters), storing the full Keys and Values for every token in a long conversation requires terabytes of high-speed memory (HBM), and chips spend more time waiting than churning.\nMHA (Standard) stores everything. MQA deletes redundancy but hurts quality. MLA keeps the quality of MHA but compresses the data 4x-6x smaller than even GQA.\nThe Zip File Approach: Low-Rank Joint Compression Let\u0026rsquo;s compare the math of MHA and MLA. In MHA, for every single token, we generate a unique Key ($k$) and Value ($v$) matrix for each attention head.\nThe Memory Problem: The $k$ is stored in the KV Cache. For 128 heads, for example, it calculates a massive vector of $128 \\times 128 = 16,384$ floats per token.\nFor MHA, the attention score looks like: $$\\text{Score} = q^T \\cdot k$$\nBut for MLA, instead of storing the full $k$, it projects the input down into a tiny latent vector $c_{KV}$, which is much smaller (e.g., 512 floats). $$c_{KV} = W_{DKV} \\cdot x$$\nNow here is the catch: Naive compression and decompression would mean constructing a small latent matrix for every token and deconstructing it every single time the information is required, which defeats the purpose of compression.\n$$k_{\\text{reconstructed}} = W_{UK} \\cdot c_{KV}$$ $$\\text{Score} = q^T \\cdot k_{\\text{reconstructed}}$$\nThe Matrix Absorption Trick (Optimization) Instead of reconstructing $k$ for every token, we can absorb the Up-Projection matrix ($W_{UK}$) into the Down-Projection matrix ($W_{DKV}$) and absorb the query $q$ into a new $Q$.\nFrom the original equation: $$\\text{Score} = q^T \\cdot (\\underbrace{W_{UK} \\cdot c_{KV}}_{\\text{This is } k})$$\nWe associate differently:\n$$\\text{Score} = (\\underbrace{q^T \\cdot W_{UK}}_{\\text{Absorbed Query}}) \\cdot c_{KV}$$ We change the order in which the matrix multiplication is performed.\nHow is this allowed? In linear algebra, matrix multiplication is associative:\n$$(A \\cdot B) \\cdot C = A \\cdot (B \\cdot C)$$ We can move the parentheses! Instead of grouping $(W_{UK} \\cdot c_{KV})$ to make the Key, we group $(q^T \\cdot W_{UK})$ to make a new Query.\nNote: The positional embedding information is left untouched. That rotational information from algorithms like RoPE is preserved outside the compression; it requires minimal space anyways.\n","permalink":"http://localhost:1313/posts/attention-mechanisms/","summary":"We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek\u0026rsquo;s MLA, how has the definition of \u0026lsquo;Attention\u0026rsquo; transformed?","title":"Attention Is All You Need, but exactly which one?: MHA, GQA and MLA"},{"content":"\nPositional Embeddings are the \u0026ldquo;voice\u0026rdquo; that tell Transformers where words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.\n\u0026ldquo;The dog chased the cat\u0026rdquo; and \u0026ldquo;cat chased the dog\u0026rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.\nüî• Sinusoidal Embeddings: The Odometer of Language Introduced in the seminal \u0026ldquo;Attention Is All You Need\u0026rdquo; paper, this was the industry standard for many years. This approach adds a vector to each word embedding based on its position. The vector is computed out of wavelike patterns that are periodic and smooth. Mathematically, for a position $pos$ and embedding dimension $i$:\n$$ PE_{(pos, i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $$\nThe model uses sine and cosine waves of different frequencies to capture both short and long-range dependencies. The frequency of the waves decreases as the embedding dimension increases, which helps capture long-range dependencies in the input sequence.\nThe creators wanted the model to calculate distance easily, which is why a linear function was used. The model simply rotates the current position vector using a rotation matrix to derive the next position.\n$$ \\begin{pmatrix} \\cos(k) \u0026 \\sin(k) \\\\ -\\sin(k) \u0026 \\cos(k) \\end{pmatrix} \\cdot PE(pos) \\approx PE(pos + k) $$ Low dimensions act like a fast-ticking clock (high frequency, changing rapidly) and high dimensions act like a slow clock (low frequency, changing slowly).\nThe number 10,000 is an arbitrarily chosen high number to reduce Aliasing. By choosing such a high number, the creators ensured that the wavelength of the slowest dimension is incredibly long. Consequently, the pattern practically never repeats, and each position remains unique.\nThis combination works like a mechanical odometer dial. Each subsequent dimension acts as a slower-moving gear. The model learns to read these \u0026ldquo;dials\u0026rdquo; to understand exactly where a word sits in the sequence.\nThis mitigated the challenge of exploding gradients that previous array-based positional embeddings suffered from (e.g., adding integers 1, 2, 3\u0026hellip;). This model keeps values between -1 and +1, adding stability while ensuring every position gets a distinct code.\nThe Problem: This approach pollutes the word embedding matrix (semantic information). While high-dimensional vectors are almost always orthogonal‚Äîmeaning the noise doesn\u0026rsquo;t fully destroy the word\u0026rsquo;s identity‚Äîit does not strictly guarantee purity.\nThis led to a more mature approach.\nüî• ALiBi: The Fading Streetlight Attention with Linear Biases (ALiBi) is a method where, instead of adding a vector to the embeddings at the start, the model injects positional information linearly, directly into the attention scores. If Sinusoidal Encoding asks, \u0026ldquo;Where am I on the map?\u0026rdquo;, ALiBi asks, \u0026ldquo;How much harder should I squint to see you?\u0026rdquo;\nThe biggest problem ALiBi solved is the \u0026ldquo;Invisible Wall\u0026rdquo;. If you train a Transformer on sequences of length 2048 and then feed it a book with 3000 words, it will crash or output garbage at word 2049. This is a failure of Extrapolation.\nALiBi throws away the idea of adding positional vectors entirely. Instead, it devises a \u0026ldquo;penalty\u0026rdquo; system for attention scores:\n$$ \\text{Score} = (q \\cdot k^\\top) - (m \\cdot \\text{distance}) $$\nDistance: If words A and B are neighbors, the distance is 0, so the penalty is 0. Although the distance is linear, it does not cause exploding gradients because the raw attention scores are normalized by a softmax function. This distance prioritizes nearby words (local context) over distant ones.\nSlope ($m$): If we always penalize distance equally, the model becomes myopic. ALiBi mitigates this by assigning a different slope value to each attention head:\nHead 1 (The Historian): Very low slope. The penalty grows slowly, allowing it to see far back. Head 2 (The Reader): Medium slope. Head 3 (The Myopic): Very high slope (e.g., 0.5); it effectively cannot see beyond 2-3 words. This mimics holding a lantern. Words close to you are bright, while words further away naturally fade into darkness.\nExtrapolation: If you walk 5 miles down the road (a longer sequence), your lantern works exactly the same way. The physics of light decay are constant, allowing the model to handle sequences longer than it was trained on.\nHowever, ALiBi forces a \u0026ldquo;sliding window\u0026rdquo; view where distant words eventually become invisible. This brings us to the modern standard.\nüî• RoPE: The Rotary Revolution Rotary Position Embeddings (RoPE) is the current industry standard (used in Llama 3, Mistral, PaLM) because it mathematically unifies \u0026ldquo;absolute position\u0026rdquo; with \u0026ldquo;relative position.\u0026rdquo; This method rotates the vectors in the Query and Key matrices based on their absolute position.\nIf a vector has 512 dimensions, RoPE breaks it into 256 pairs. Each pair is rotated by a different frequency:\n$$ \\theta_i = 10000^{-2i/d} $$\nThe First pair ($i = 0$) is rotated by the highest frequency. The Last pair ($i = 255$) is rotated by the lowest frequency. This spread ensures the model has \u0026ldquo;high precision\u0026rdquo; (fast rotation) to distinguish immediate neighbors, and \u0026ldquo;low precision\u0026rdquo; (slow rotation) to track global position without the pattern repeating.\nDoesn\u0026rsquo;t rotation change meaning? Yes and No. The vector for \u0026ldquo;Apple\u0026rdquo; at position 1 and \u0026ldquo;Apple\u0026rdquo; at position 256 will look completely different in terms of coordinates. However, rotation does not change the semantic strength (magnitude/norm) of the vector.\nThe Relativity Trick: Transformers compare words using dot products (angles). RoPE relies on the fact that the attention mechanism ignores the absolute rotation and only detects the difference in rotation (relative position). Thus, the \u0026ldquo;relative meaning\u0026rdquo; is encoded purely into the angle between words, leaving the semantic \u0026ldquo;magnitude\u0026rdquo; untouched.\nüî• LongRoPE: The Bifocal Lens While RoPE is powerful, it struggles when stretching context windows from 4k to 2 million tokens. The \u0026ldquo;fast\u0026rdquo; dimensions spin so rapidly over long distances that they become random noise. LongRoPE solves this using an evolutionary search algorithm to find a unique scaling factor ($\\lambda$) for each dimension.\nThe Equations Instead of rotating by the standard $\\theta_i$, LongRoPE rotates by a rescaled frequency:\n$$ \\theta\u0026rsquo;_i = \\lambda_i \\theta_i $$\nTo efficiently find these $\\lambda$ values without searching an infinite space, the algorithm enforces a monotonicity constraint based on NTK theory:\n$$ \\lambda_i \\le \\lambda_{i+1} $$\nThis ensures that low-frequency dimensions (global context) are stretched more than high-frequency dimensions (local context), creating a \u0026ldquo;bifocal\u0026rdquo; effect:\nHigh Frequencies (Local): Kept sharp ($\\lambda \\approx 1$) to maintain grammar. Low Frequencies (Global): Stretched ($\\lambda \u0026gt; 1$) to track massive distances without repeating. Not worth perusing more since the next one is much better at long context positional encoding. üî• HoPE: The Hyperbolic Slide Hyperbolic Rotary Positional Encoding (HoPE) moves from the geometry of a circle to the geometry of a hyperbola (inspired by Lorentz transformations). It was designed to fix a subtle flaw in RoPE: the \u0026ldquo;wobbly\u0026rdquo; or oscillatory nature of attention scores.\nThe Core Problem: RoPE\u0026rsquo;s \u0026ldquo;Wobble\u0026rdquo; In standard RoPE, we rotate vectors around a circle.\nThe Issue: Circles repeat. A dial at $361^\\circ$ looks identical to $1^\\circ$. Even with different frequencies, the dot product fluctuates up and down as distance increases. The Consequence: The model gets confused at long distances, potentially mistaking a word 1,000 tokens away for a neighbor just because the rotation cycles aligned, creating \u0026ldquo;noise.\u0026rdquo; The \u0026ldquo;Hyperbolic\u0026rdquo; Shift HoPE replaces trigonometric functions ($\\sin, \\cos$) with hyperbolic functions ($\\sinh, \\cosh$).\nCircular Rotation: Keeps distance from the center constant. Hyperbolic Rotation: Moves points along a hyperbola. Crucially, hyperbolic functions do not repeat; they grow or decay exponentially. By using hyperbolic rotation, HoPE guarantees that as words get further apart, the attention score decays monotonically (smoothly drops).\nThe Equations HoPE uses a hyperbolic matrix $B(\\theta, m)$ defined using the Lorentz Boost structure:\n$$ B(\\theta, m) = \\begin{pmatrix} \\cosh(m\\theta) \u0026 \\sinh(m\\theta) \\\\ \\sinh(m\\theta) \u0026 \\cosh(m\\theta) \\end{pmatrix} $$ Since hyperbolic functions grow to infinity, HoPE introduces a decay penalty ($e^{-m\\theta'}$). The final attention score simplifies beautifully to a function of relative distance: $$ \\text{Score} \\propto e^{-|m-n|(\\theta\u0026rsquo; - \\theta)} $$\nThis ensures the attention score exponentially decays as distance increases, eliminating the wobbles of standard RoPE. This the latest model that is taking up the space in the industry for extremely long context models.\n","permalink":"http://localhost:1313/posts/positional-embeddings/","summary":"From Sinusoidal to RoPE and HoPE: How Transformers learn to process word order and sequence length.","title":"The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE"},{"content":"Imagine trying to teach a computer the difference between \u0026ldquo;Apple\u0026rdquo; the fruit and \u0026ldquo;Apple\u0026rdquo; the company. To us, the distinction is intuitive. To a machine, it‚Äôs just a string of characters. How do we turn these strings into meaningful math? While early attempts like Word2Vec gave us a great start, they missed the forest for the trees‚Äîor in some cases, the twigs for the branches. Enter GloVe and FastText: two algorithms that revolutionized how machines understand the nuances of human language.\nPrevious static embedding models like Word2Vec successfully captured the local semantics of words, but they failed in addressing contexts between words that might not always appear in the same context window. Furthermore, they treated words as indivisible atomic units in continuous vector spaces, missing out on the internal structure of language.\nThe two simple but effective ideas behind GloVe and FastText are subword units and global co-occurrence. Let\u0026rsquo;s dive into them.\nGloVe: A Global Mapping Strategy Global Vectors for Word Representation (GloVe) was created at Stanford. The core idea was to train a model where word vectors are defined by how often they appear around other words across the entire corpus, rather than just locally.\nTraining Steps: Co-occurrence Matrix X: GloVe scans the entire corpus and creates a giant matrix (spreadsheet) recording the counts of how often words appear near each other. Factorization: It compresses this giant matrix into smaller, dense vectors. Objective: The function tries to equate the dot product of two word vectors with the logarithm of their probability of appearing together. The error between these values is then backpropagated to update the vectors. $$w_i \\cdot w_j + b_i + b_j = \\log(X_{ij})$$\n$w$: The word vector (what we want). $X_{ij}$: The count from our matrix. The Loss Function: We can\u0026rsquo;t satisfy the equation above perfectly for every word pair. So, we use a weighting function to minimize the squared error:\n$$J = \\sum_{i,j=1}^V f(X_{ij}) ({w_i}^T w_j + b_i + b_j - \\log(X_{ij}))^2$$\nIf we do not use the weighting function, the loss would be infinite for rare words (since $\\log(0) = \\infty$) and frequent words (like \u0026rsquo;the\u0026rsquo;) would overpower the model.\n$$ f(x) = \\begin{cases} (x/x_{max})^\\alpha \u0026amp; \\text{if } x \u0026lt; x_{max} \\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$\n(Typically $x_{max}=100$ and $\\alpha=0.75$)\nFastText: The Lego Bricks Strategy FastText was created by Facebook AI Research (FAIR). The idea was to treat words not as solid rocks, but as structures made of \u0026ldquo;Lego bricks\u0026rdquo; (subword units).\nWord2Vec struggled to create embeddings for words it hadn\u0026rsquo;t seen before (Out-Of-Vocabulary or OOV words). FastText solves this by breaking words down.\nGloVe treats a word like \u0026ldquo;Apple\u0026rdquo; as a single, atomic unit. FastText breaks \u0026ldquo;Apple\u0026rdquo; into character n-grams: \u0026lt;ap, app, ppl, ple, le\u0026gt; For a complex or new word like \u0026ldquo;Applesauce\u0026rdquo;, the model might not know the whole word, but it recognizes the Lego brick for \u0026ldquo;Apple\u0026rdquo; and the remaining bits from other subwords.\nFastText takes its inspiration from the SkipGram architecture. The vector for a word is simply the sum of the vectors of its n-gram subwords:\n$$v_w = \\sum_{g \\in G_w} z_g$$\nObjective: The objective is to predict context words given a target word.\n$$P(w_c | w_t) = \\frac{e^{s(v_{w_t} \\cdot v_{w_c})}}{\\sum_{w \\in V} e^{s(v_{w_t} \\cdot v_w)}}$$\nAnd the scoring function is defined as:\n$$s(w_t, w_c) = u_{w_t}^T (\\sum_{g \\in G_w} z_g)$$\n$u_{w_c}$: The vector for the context word (output vector). $z_g$: The vector for the n-gram $g$ (input vector). Modifications to Softmax: Calculating the full softmax for every word in the vocabulary is computationally expensive. FastText uses two methods to approximate this efficiently.\nOption A: Negative Sampling\nWe pick a few random \u0026ldquo;wrong\u0026rdquo; words every time to serve as \u0026ldquo;negative samples\u0026rdquo; and calculate the loss only for the correct word and these noise words.\n$$J = - \\log \\sigma(s(w_t, w_c)) - \\sum\\limits_{i=1}^{N} \\mathbb{E}_{n_i \\sim P_n(w)} [\\log \\sigma(-s(w_t, n_i))]$$\nThe sigmoid function $\\sigma$ squashes the score between 0 and 1. The first term pushes the probability of the correct word towards 1. The second term pushes the probability of the noise words towards 0. Option B: Hierarchical Softmax (Faster for infrequent words)\nInstead of a flat list of words, imagine the vocabulary arranged as a binary tree (Huffman Tree).\nRoot: Top of the tree. Leaves: The actual words. To calculate the probability of a word, we trace a path from the root to that leaf. At every branching node, we calculate the probability (sigmoid) of going left or right. The probability of a word $w$ is the product of the probabilities of the turns taken to reach it. Code Example from gensim.models import FastText from gensim.test.utils import common_texts # Initialize the FastText model with specific hyperparameters model = FastText( vector_size=100, # Dimensionality of the word vectors window=5, # Maximum distance between the current and predicted word within a sentence min_count=1, # Ignores all words with total frequency lower than this sg=1, # Training algorithm: 1 for skip-gram; 0 for CBOW hs=0, # If 0, and negative is non-zero, negative sampling will be used negative=5, # Number of \u0026#34;noise words\u0026#34; to be drawn for negative sampling workers=4, # Number of worker threads to train the model epochs=10, # Number of iterations over the corpus min_n=3, # Minimum length of character n-grams max_n=6 # Maximum length of character n-grams ) # Build the vocabulary from the provided text corpus model.build_vocab(common_texts) # Train the model on the corpus model.train( common_texts, total_examples=model.corpus_count, epochs=model.epochs ) # Calculate and print the cosine similarity between two words in the vocabulary print(f\u0026#34;Similarity between \u0026#39;computer\u0026#39; and \u0026#39;human\u0026#39;: {model.wv.similarity(\u0026#39;computer\u0026#39;, \u0026#39;human\u0026#39;)}\u0026#34;) # Demonstrate FastText\u0026#39;s ability to handle Out-Of-Vocabulary (OOV) words # Even if \u0026#39;computation\u0026#39; wasn\u0026#39;t in the training data, FastText constructs a vector using character n-grams oov_vector = model.wv[\u0026#39;computation\u0026#39;] # Check if a specific word exists in the model\u0026#39;s fixed vocabulary index word = \u0026#34;computer\u0026#34; is_in_vocab = word in model.wv.key_to_index print(f\u0026#34;\\nIs the word \u0026#39;{word}\u0026#39; in the model\u0026#39;s vocabulary? {is_in_vocab}\u0026#34;) ","permalink":"http://localhost:1313/posts/glove-and-fasttext/","summary":"How subword units and global co-occurrence matrices allow GloVe and FastText to capture nuances that Word2Vec missed.","title":"The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText"},{"content":" Before we had Large Language Models writing poetry, we had to teach computers that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; are related not just by spelling, but by meaning. This is the story of that breakthrough. It‚Äôs the moment we stopped counting words and started mapping their souls‚Äîturning raw text into a mathematical landscape where math can solve analogies. Welcome to the world of Word2Vec. üîÆ\nLanguage models require vector representations of words to capture semantic relationships. Before the 2010s, models used word count-based vector representations that captured only the frequency of words (e.g., One-Hot Encoding). The Problems: üöß\nThe Curse of Dimensionality Lack of meaning (Synonyms were treated as mathematically unrelated). In 2011, Mikolov et al. at Google introduced Word2Vec, which used a shallow neural network to learn vector representations of words. The best part? It could create much denser relationships between words than prior models, and it was unsupervised.\nThe training (prediction) is essentially \u0026ldquo;fake\u0026rdquo;‚Äîa pretext task. But the weight matrices we get on the side are a gold mine of fine-grained semantic relationships. ‚õèÔ∏è\nThere are two popular variants of Word2Vec:\nSkipGram Continuous Bag Of Words (CBOW) 1. Continuous Bag of Words (CBOW) üß© CBOW is a simple \u0026lsquo;fill in the blanks\u0026rsquo; machine. It takes the surrounding words as inputs and tries to predict the center word.\n\u0026ldquo;Continuous\u0026rdquo;: It operates in continuous vector space (unlike the discrete space of n-gram models).\n\u0026ldquo;Bag of words\u0026rdquo;: The order of the context does not matter. \u0026ldquo;The cat sat on the mat\u0026rdquo; and \u0026ldquo;The mat sat on the cat\u0026rdquo; are the same for CBOW. They produce exactly the same predictions.\nObjective: Maximize the probability of the target word $w_t$ given its context $w_{t-C}, \\dots, w_{t+C}$.\n2. Training Defining Dimensions:\n$V$: Vocabulary size (number of unique words in the corpus, e.g., 10,000) $N$: Dimension of the embedding space (e.g., 100) $C$: Number of context words (e.g., 2 words on left of target and 2 words on right) Step 1: Input Lookup \u0026amp; One-Hot Encoding\nMathematically, we represent each context word as a one-hot vector $x^{(c)}$. We have an Input Matrix $W_{in}$ of size $V \\times N$. For each context word (e.g., \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;), we select its corresponding row from $W_{in}$.\n$$ v_c = W_{in}^T x^{(c)} $$\nWhere $v_c$ is the $N$-dimensional vector for context word $c$.\nStep 2: Projection (The \u0026ldquo;Mixing\u0026rdquo;)\nWe combine the context vectors. In standard CBOW, we simply average them. This creates the hidden layer vector $h$.\n$$ h = \\frac{1}{2C} \\sum_{c=1}^{2C} v_c $$\n$h$ is a single vector of size $N \\times 1$. Note: This operation is linear and contains no activation function (like ReLU or Sigmoid).\nStep 3: Output Scoring (The \u0026ldquo;Dot Product\u0026rdquo;)\nWe have a second matrix, the Output Matrix $W_{out}$ of size $N \\times V$. We compute the raw score (logit) $u_j$ for every word $j$ in the vocabulary by taking the dot product of the hidden vector $h$ and the output vector $v\u0026rsquo;_j$.\n$$ u_j = {v\u0026rsquo;_j}^T h $$\nOr in matrix form:\n$$ u = W_{out}^T h $$\nResult $u$ is a vector of size $V \\times 1$.\nStep 4: Probability Conversion (Softmax)\nWe convert the raw scores $u$ into probabilities using the Softmax function. This tells us the probability that word $w_j$ is the center word.\n$$ y_j = P(w_j | \\text{context}) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 5: Loss Calculation (Cross-Entropy)\nWe compare the predicted distribution $y$ with the actual ground truth $t$ (a one-hot vector where the index of the true word is 1). The loss function $E$ is:\n$$ E = - \\sum_{j=1}^V t_j \\log(y_j) $$\nSince $t_j$ is 0 for all words except the target word (let\u0026rsquo;s call the target index $j^*$), this simplifies to:\n$$ E = - \\log(y_{j^*}) $$\n3. Backpropagation Details We need to update the weights to minimize $E$. We use the Chain Rule.\nA. Gradient w.r.t. Output Matrix ($W_{out}$)\nWe want to know how the error changes with respect to the raw score $u_j$.\n$$ \\frac{\\partial E}{\\partial u_j} = y_j - t_j $$\nLet\u0026rsquo;s call this error term $e_j$.\nIf $j$ is the target word: $e_j = y_j - 1$ (Result is negative; we boost the score). If $j$ is not target: $e_j = y_j - 0$ (Result is positive; we suppress the score). The update rule for the output vector $v\u0026rsquo;_j$ becomes:\n$$ v'_{j}(\\text{new}) = v'_{j}(\\text{old}) - \\eta \\cdot e_j \\cdot h $$ ($\\eta$ is the learning rate)\nB. Gradient w.r.t. Hidden Layer ($h$)\nWe backpropagate the error from the output layer to the hidden layer.\n$$ EH = \\sum_{j=1}^V e_j \\cdot v\u0026rsquo;_j $$\n$EH$ is an $N$-dimensional vector representing the aggregate error passed back to the projection layer.\nC. Gradient w.r.t. Input Matrix ($W_{in}$)\nSince $h$ was just an average of the input vectors, the error $EH$ is distributed to each context word\u0026rsquo;s input vector. For every word $w_c$ in the context:\n$$ v_{w_c}(\\text{new}) = v_{w_c}(\\text{old}) - \\eta \\cdot \\frac{1}{2C} \\cdot EH $$\nLet\u0026rsquo;s build a single pass through the network for a given context.\nimport torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # Define a simple Continuous Bag of Words (CBOW) style model class CBOWModel(nn.Module): def __init__(self, vocab_size, embedding_dim): super(CBOWModel, self).__init__() # The embedding layer stores the word vectors we want to learn self.embeddings = nn.Embedding(vocab_size, embedding_dim) # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word self.linear = nn.Linear(embedding_dim, vocab_size) def forward(self, inputs): # inputs: tensor of word indices for the context embeds = self.embeddings(inputs) # Aggregate context by calculating the mean of the word embeddings h = torch.mean(embeds, dim=1) # Produce logits (raw scores) for each word in the vocabulary logits = self.linear(h) return logits # Setup vocabulary and mappings word_to_ix = {\u0026#34;the\u0026#34;: 0, \u0026#34;quick\u0026#34;: 1, \u0026#34;brown\u0026#34;: 2, \u0026#34;fox\u0026#34;: 3, \u0026#34;jumps\u0026#34;: 4, \u0026#34;over\u0026#34;: 5, \u0026#34;lazy\u0026#34;: 6, \u0026#34;dog\u0026#34;: 7} ix_to_word = {v: k for k, v in word_to_ix.items()} # Configuration constants EMBEDDING_DIM = 5 VOCAB_SIZE = len(word_to_ix) LEARNING_RATE = 0.01 # Initialize the model, loss function, and optimizer model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM) loss_function = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE) # Prepare dummy training data: context words and the target word # Context: [\u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;over\u0026#34;] -\u0026gt; Target: \u0026#34;fox\u0026#34; context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long) target_idxs = torch.tensor([3], dtype=torch.long) # Perform a single optimization step model.zero_grad() # Forward pass: we reshape context to (1, -1) to simulate a batch of size 1 logits = model(context_idxs.view(1, -1)) # Calculate loss against the target word index loss = loss_function(logits, target_idxs) # Backpropagate and update weights loss.backward() optimizer.step() # Output results for the blog post print(\u0026#34;=== Model Training Snapshot ===\u0026#34;) print(f\u0026#34;Calculated Loss: {loss.item():.6f}\u0026#34;) print(f\u0026#34;\\n=== Learned Vector for \u0026#39;jumps\u0026#39; ===\u0026#34;) word_vec = model.embeddings(torch.tensor([word_to_ix[\u0026#39;jumps\u0026#39;]])) print(word_vec.detach().numpy()) print(\u0026#34;\\n=== Embedding Matrix (Weights) ===\u0026#34;) print(model.embeddings.weight.detach().numpy()) print(\u0026#34;\\n=== Linear Layer Weights ===\u0026#34;) print(model.linear.weight.detach().numpy()) Output shows single pass loss and learned vector for \u0026lsquo;jumps\u0026rsquo; word. This is a single training sample result but after multiple such passes both Embedding and linear weights should be very similar.\n=== Model Training Snapshot === Calculated Loss: 1.989656 === Learned Vector for \u0026#39;jumps\u0026#39; === [[-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775]] === Embedding Matrix (Weights) === [[ 1.66039 -0.11371879 0.6246518 0.35860053 -1.2504417 ] [-2.1847186 -0.77199775 -0.17050214 -0.38411248 -0.03913084] [ 0.11852697 0.90073633 0.8847807 0.7404524 0.900149 ] [ 0.07440972 -0.40259898 2.6246994 -0.08851447 0.02660969] [-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775] [-0.9245572 -0.5545908 0.9083091 -1.0755049 0.84047747] [-1.0237687 0.59466314 0.05621134 -0.6202532 1.3664424 ] [ 0.60998917 -1.0549186 1.6103884 0.8724912 -1.2486908 ]] === Linear Layer Weights === [[ 0.02165058 -0.28883642 0.14545658 -0.3442509 0.32704315] [-0.18731792 0.28583744 0.22635977 0.13245736 0.29019794] [ 0.3158916 -0.15826383 -0.03203773 0.16377363 -0.41457543] [-0.05080034 0.4180087 0.11228557 0.30218413 0.3025514 ] [-0.38419306 0.24475925 -0.39210224 -0.38660625 -0.2673145 ] [-0.32321444 0.12200444 -0.03569533 0.2891424 -0.07345333] [-0.33704326 0.2521956 0.31587374 0.22590035 0.29866052] [ 0.4117266 -0.44231793 0.24064957 -0.29684234 0.333821 ]] Interesting Facts About CBOW üß† The \u0026ldquo;Averaging Problem\u0026rdquo;: This is a common issue in CBOW models. The averaging and training is order-agnostic. All variants of word order in the context window will generate the same output. \u0026ldquo;Dog bit the man\u0026rdquo; and \u0026ldquo;Man bit the dog\u0026rdquo; are the same in CBOW\u0026rsquo;s eyes.\nSmoothing Effect: CBOW models are much faster to train than Skip-Gram but are generally smoother.\nRare Words: CBOW models struggle slightly with rare words because the presence of surrounding common words (like articles and prepositions) can skew the embeddings and dissolve their uniqueness.\nComputational Cost: Softmax requires summing over the entire vocabulary size, which can be computationally expensive if $V$ is very large. Solutions include Hierarchical Softmax and Negative Sampling (approximating the denominator by only checking the target word vs. 5 random noise words).\nVectors as Double Agents: Every word has a different representation in the embedding matrix and the output matrix. Most of the time $W_{out}$ is discarded, but some consider averaging both $W_{in}$ and $W_{out}$ for slightly better performance.\nLinear Relationships: The famous analogy King - Man + Woman = Queen emerges from the model because of the linear relationships between the words.\nInitialization: This is a no-brainer but worth mentioning. Initialization to zero vectors would mean no learning. We must initialize randomly.\n2. Skip-Gram (The \u0026ldquo;Burger Flip\u0026rdquo; of CBOW) üîÑ Skip-Gram flips the CBOW logic. While CBOW tries to predict the center word given the context, Skip-Gram tries to predict the scattered context given the center word.\nCore Concept: If a word like \u0026ldquo;monarch\u0026rdquo; can accurately predict words like \u0026ldquo;king\u0026rdquo;, \u0026ldquo;throne\u0026rdquo;, \u0026ldquo;crown\u0026rdquo;, \u0026ldquo;royalty\u0026rdquo;, and \u0026ldquo;power\u0026rdquo;, then it must effectively capture the concept of royalty.\nTraining Let\u0026rsquo;s define dimensions:\n$V$: Vocabulary size $D$: Embedding dimension $C$: Number of context words Step 1: Input Lookup\nUnlike CBOW, our input is just one word vector (the center word $w_t$). We grab the vector $v_c$ from the Input Matrix $W_{in}$.\n$$ h = v_{w_t} $$\nNote: There is no averaging here. The hidden layer $h$ is simply the raw vector of the center word.\nStep 2: Output Scoring (The Broadcast)\nThe model takes this single vector $h$ and compares it against the Output Matrix $W_{out}$ (the entire vocabulary). It computes a score $u_j$ for every word $j$ in the dictionary.\n$$ u = W_{out}^T h $$\nResult $u$: A vector of size $V \\times 1$ containing raw scores.\nStep 3: Probability Conversion (Softmax)\nWe apply Softmax to turn scores into probabilities.\n$$ P(w_j | w_t) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 4: The Multi-Target Loss\nHere is the major difference from CBOW. In CBOW, we had one target (the center). In Skip-Gram, we have $2C$ targets (all the surrounding words). For a sentence \u0026ldquo;The cat sat on mat\u0026rdquo; (Center: \u0026ldquo;sat\u0026rdquo;), the targets are \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;. We want to maximize the probability of all these true context words. The Loss function ($E$) sums the error for each context word:\n$$ E = - \\sum_{w_{ctx} \\in \\text{Window}} \\log P(w_{ctx} | w_t) $$\nStep 5: Backpropagation (Accumulating Gradients)\nSince one input word is responsible for predicting multiple output words, the error signals from all those context words add up.\nAt the Output Layer:\nIf \u0026ldquo;cat\u0026rdquo; was a target, the \u0026ldquo;cat\u0026rdquo; vector in $W_{out}$ gets a signal: \u0026ldquo;Move closer to \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; If \u0026ldquo;apple\u0026rdquo; was NOT a target, the \u0026ldquo;apple\u0026rdquo; vector gets a signal: \u0026ldquo;Move away from \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; At the Hidden Layer ($h$):\nThe center word \u0026ldquo;sat\u0026rdquo; receives feedback from all its neighbors simultaneously. Error signal = (Error from \u0026ldquo;The\u0026rdquo;) + (Error from \u0026ldquo;cat\u0026rdquo;) + (Error from \u0026ldquo;on\u0026rdquo;)\u0026hellip; The vector for \u0026ldquo;sat\u0026rdquo; in $W_{in}$ moves in the average direction of all its neighbors. 3. The \u0026ldquo;Computational Nightmare\u0026rdquo; \u0026amp; Negative Sampling The equations above describe Naive Softmax.\nThe Problem: If $V = 100,000$, computing the denominator $\\sum \\exp(u_k)$ takes 100,000 operations. Doing this for every word in a training set of billions of words is impossibly slow.\nThe Solution: Negative Sampling Instead of updating the entire vocabulary (1 correct word vs 99,999 wrong ones), we approximate the problem.\nPositive Pair: (sat, cat) $\\rightarrow$ Maximize probability (Label 1). Negative Pairs: We pick $K$ (e.g., 5) random words the model didn\u0026rsquo;t see, e.g., (sat, bulldozer), (sat, quantum). $\\rightarrow$ Minimize probability (Label 0). New Equation (Sigmoid instead of Softmax):\n$$ E = - \\log \\sigma({v'_{\\text{pos}}}^T v_{\\text{in}}) - \\sum_{k=1}^K \\log \\sigma(-{v'_{\\text{neg}_{k}}}^T v_{\\text{in}}) $$ Effect: We typically only update 6 vectors per step (1 pos + 5 negs) instead of 100,000.\nLet\u0026rsquo;s see this in code:\nimport torch import torch.nn as nn import torch.optim as optim class SkipGramNegativeSampling(nn.Module): \u0026#34;\u0026#34;\u0026#34; Skip-Gram with Negative Sampling (SGNS) implementation. SGNS approximates the softmax over the entire vocabulary by instead distinguishing between a real context word (positive) and K noise words (negative). \u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embedding_dim): super(SkipGramNegativeSampling, self).__init__() # Input embeddings: used when the word is the center word self.in_embed = nn.Embedding(vocab_size, embedding_dim) # Output embeddings: used when the word is a context or negative sample self.out_embed = nn.Embedding(vocab_size, embedding_dim) # Initialize weights with small values to prevent gradient saturation initrange = 0.5 / embedding_dim self.in_embed.weight.data.uniform_(-initrange, initrange) self.out_embed.weight.data.uniform_(-initrange, initrange) def forward(self, center_words, target_words, negative_words): \u0026#34;\u0026#34;\u0026#34; Computes the negative sampling loss. Args: center_words: (batch_size) target_words: (batch_size) negative_words: (batch_size, K) where K is number of negative samples \u0026#34;\u0026#34;\u0026#34; # Retrieve vectors v_c = self.in_embed(center_words) # (batch_size, embed_dim) u_o = self.out_embed(target_words) # (batch_size, embed_dim) u_n = self.out_embed(negative_words) # (batch_size, K, embed_dim) # 1. Positive Score: log(sigmoid(v_c ¬∑ u_o)) # Compute dot product: (batch, 1, dim) @ (batch, dim, 1) -\u0026gt; (batch, 1) pos_score = torch.bmm(u_o.unsqueeze(1), v_c.unsqueeze(2)).squeeze(2) pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-7) # 2. Negative Score: sum(log(sigmoid(-v_c ¬∑ u_n))) # Compute dot products for all K samples: (batch, K, dim) @ (batch, dim, 1) -\u0026gt; (batch, K) neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2) neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-7), dim=1, keepdim=True) # Total loss is the negative of the objective function loss = -(pos_loss + neg_loss) return torch.mean(loss) # --- Configuration \u0026amp; Mock Data --- VOCAB_SIZE = 100 EMBED_DIM = 10 word_to_ix = {\u0026#39;fox\u0026#39;: 0} # Example vocabulary mapping model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock inputs: 1 center word, 1 target word, 5 negative samples center_id = torch.tensor([0], dtype=torch.long) target_id = torch.tensor([1], dtype=torch.long) negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long) # Training Step model.zero_grad() loss = model(center_id, target_id, negative_ids) loss.backward() optimizer.step() # Output Results print(f\u0026#34;Loss after one step: {loss.item():.6f}\u0026#34;) word_vec = model.in_embed(torch.tensor([word_to_ix[\u0026#39;fox\u0026#39;]])) print(f\u0026#34;Vector for \u0026#39;fox\u0026#39;:\\n{word_vec.detach().numpy()}\u0026#34;) The outputs:\nLoss after one step: 4.158468 Vector for \u0026#39;fox\u0026#39;: [[ 0.04197385 0.02400733 -0.03800093 0.01672485 -0.03872231 -0.0061478 0.0121122 0.04057864 -0.036255 0.03861175]] Interesting Facts About Skip-Gram üí° Makes Rare Words Shine: Skip-Gram is more effective at capturing the meaning of rare words because it focuses on the context of the center word. In CBOW, a rare word\u0026rsquo;s vector is averaged with others, diluting its signal. In Skip-Gram, when the rare word appears as the center, it gets the full, undivided attention of the backpropagation update.\nSlow Training: This method creates more training samples. A window of size 2 creates 4 training pairs per center word, while CBOW creates only 1 training pair per center word.\nSemantic Focus: Skip-Gram puts more emphasis on the semantic relationship between words. It views the context as the primary signal, with the center word as the target. This makes it better at capturing the meaning of words in context. Skip-Gram tends to capture semantic relationships (King/Queen) slightly better, while CBOW captures syntactic ones (walk/walked) slightly better.\n","permalink":"http://localhost:1313/posts/word-embeddings/","summary":"Understanding the mathematics behind Word2Vec, CBOW, and Skip-Gram and how they map language to vector space.","title":"Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n5. Code Examples: We\u0026rsquo;ll use sentencepiece for tokenization and compare results for different models.\nimport sentencepiece as spm import os # Configuration for training the SentencePiece model # SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words. options = { # The source text file used for learning the vocabulary \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # The base name for the generated model (.model) and vocabulary (.vocab) files \u0026#39;model_prefix\u0026#39;: \u0026#39;bpe_model\u0026#39;, # Number of unique tokens in the final vocabulary \u0026#39;vocab_size\u0026#39;: 4000, # \u0026#39;bpe\u0026#39; (Byte Pair Encoding) merges frequent pairs of characters/sequences \u0026#39;model_type\u0026#39;: \u0026#39;bpe\u0026#39;, # Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets \u0026#39;character_coverage\u0026#39;: 0.9995, # When enabled, unknown characters are decomposed into UTF-8 bytes to avoid \u0026#39;unk\u0026#39; tokens \u0026#39;byte_fallback\u0026#39;: True, # Treats digits individually (0-9), preventing large numbers from being treated as single tokens \u0026#39;split_digits\u0026#39;: True, # Prevents adding a whitespace prefix to the first token; useful for fine-grained control \u0026#39;add_dummy_prefix\u0026#39;: False } try: print(\u0026#34;Starting the training process...\u0026#34;) # SentencePieceTrainer.train takes the dictionary of options to build the BPE model spm.SentencePieceTrainer.train(**options) print(\u0026#34;Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created.\u0026#34;) # Initialize the processor and load the newly trained model sp = spm.SentencePieceProcessor() sp.load(\u0026#39;bpe_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Model Metadata:\u0026#34;) # Retrieve the total number of tokens in the vocabulary print(f\u0026#34;Total Vocab Size: {sp.get_piece_size()}\u0026#34;) # Special tokens are used for sequence boundaries and handling unknown characters print(f\u0026#34;BOS (Beginning of Sentence) ID: {sp.bos_id()}\u0026#34;) print(f\u0026#34;EOS (End of Sentence) ID: {sp.eos_id()}\u0026#34;) print(f\u0026#34;UNK (Unknown) ID: {sp.unk_id()}\u0026#34;) print(f\u0026#34;PAD (Padding) ID: {sp.pad_id()}\u0026#34;) # Test the tokenizer on sample strings to see how it breaks down text test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: shows the actual subword units (tokens) print(f\u0026#34;Subword Tokens: {sp.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: shows the numerical mapping for each token print(f\u0026#34;Numerical IDs: {sp.encode_as_ids(text)}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred during training or processing: {e}\u0026#34;) You can see the following output:\nStarting training... Starting the training process... Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created. ------------------------------ Model Metadata: Total Vocab Size: 4000 BOS (Beginning of Sentence) ID: 1 EOS (End of Sentence) ID: 2 UNK (Unknown) ID: 0 PAD (Padding) ID: -1 --- Tokenization Test --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;el\u0026#39;, \u0026#39;lo\u0026#39;, \u0026#39;‚ñÅW\u0026#39;, \u0026#39;orld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 380, 309, 476, 619, 36, 3924, 3964, 3959, 3993, 3978, 3983, 57, 3975, 3976, 60, 3974] --- Tokenization Test --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅmo\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;inst\u0026#39;, \u0026#39;ag\u0026#39;, \u0026#39;ram\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅever\u0026#39;] Numerical IDs: [1894, 304, 3284, 376, 321, 313, 312, 1008, 3313, 1325, 428, 3937, 341, 304, 3284, 1495] The ! is fallen back to UTF bytes. and the word instagrammable is split into multiple subword tokens. Also, original text can be recreated right back by simply concatenating the tokens. And replacing underscore by whitespace. Now we can try Unigram model. and see the same sentences tokenized differently.\nfrom tokenizers import Tokenizer from tokenizers.models import WordPiece from tokenizers.trainers import WordPieceTrainer from tokenizers.pre_tokenizers import Whitespace try: # 1. Initialize the WordPiece Tokenizer # We specify the [UNK] token for handling words not found in the vocabulary. tokenizer = Tokenizer(WordPiece(unk_token=\u0026#34;[UNK]\u0026#34;)) # 2. Configure Pre-tokenization # Before the subword algorithm runs, we need to split the raw text into words. # Whitespace splitting is the standard first step for most English NLP tasks. tokenizer.pre_tokenizer = Whitespace() # 3. Initialize the Trainer # We define our target vocabulary size and the special tokens required for # downstream tasks (like BERT\u0026#39;s [CLS] for classification or [SEP] for separators). trainer = WordPieceTrainer( vocab_size=4000, special_tokens=[\u0026#34;[UNK]\u0026#34;, \u0026#34;[CLS]\u0026#34;, \u0026#34;[SEP]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[MASK]\u0026#34;] ) # 4. Train the Model # The tokenizer scans the training file to build a vocabulary of the most # frequent subword units. tokenizer.train(files=[\u0026#34;train_data.txt\u0026#34;], trainer=trainer) # 5. Persist the Model # Save the configuration and vocabulary to a JSON file for future inference. tokenizer.save(\u0026#34;wordpiece.json\u0026#34;) print(\u0026#34;Training complete. \u0026#39;wordpiece.json\u0026#39; created.\u0026#34;) # 6. Metadata Inspection print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;WordPiece Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {tokenizer.get_vocab_size()}\u0026#34;) # 7. Testing Subword Tokenization # WordPiece shines at handling rare words by breaking them into meaningful chunks. test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (WordPiece) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # Encode converts raw text into a Tokenizer object containing tokens and IDs output = tokenizer.encode(text) # \u0026#39;tokens\u0026#39; shows the subword breakdown (e.g., \u0026#39;un\u0026#39;, \u0026#39;##insta\u0026#39;, etc.) print(f\u0026#34;Subword Tokens: {output.tokens}\u0026#34;) # \u0026#39;ids\u0026#39; are the numerical indices mapped to the vocabulary print(f\u0026#34;Numerical IDs: {output.ids}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred with WordPiece model: {e}\u0026#34;) You can see the following output:\nWordPiece Model Metadata: Total Vocab Size: 2609 --- Tokenization Test (WordPiece) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;H\u0026#39;, \u0026#39;##el\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;##or\u0026#39;, \u0026#39;##ld\u0026#39;, \u0026#39;[UNK]\u0026#39;, \u0026#39;[UNK]\u0026#39;] Numerical IDs: [37, 180, 214, 52, 162, 418, 0, 0] --- Tokenization Test (WordPiece) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;##os\u0026#39;, \u0026#39;##t\u0026#39;, \u0026#39;un\u0026#39;, \u0026#39;##ins\u0026#39;, \u0026#39;##ta\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;##ra\u0026#39;, \u0026#39;##m\u0026#39;, \u0026#39;##ma\u0026#39;, \u0026#39;##ble\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;ever\u0026#39;] Numerical IDs: [691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240] We can see the special character ## in the output. We can also see the breakdowns not having any semantic meaning. They are likelihood based. We can try the Unigram model now.\n# \u0026#39;Unigram\u0026#39; is the default and usually recommended over BPE in SentencePiece. options_unigram = { \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # Path to the raw text file for training \u0026#39;model_prefix\u0026#39;: \u0026#39;unigram_model\u0026#39;, # Prefix for the output .model and .vocab files \u0026#39;vocab_size\u0026#39;: 1200, # Desired size of the final vocabulary \u0026#39;model_type\u0026#39;: \u0026#39;unigram\u0026#39;, # Specifies the Unigram language model algorithm \u0026#39;character_coverage\u0026#39;: 0.9995, # Percentage of characters covered by the model (0.9995 is standard for Latin scripts) \u0026#39;byte_fallback\u0026#39;: True, # Enables mapping unknown characters to UTF-8 bytes to avoid \u0026lt;unk\u0026gt; tokens \u0026#39;split_digits\u0026#39;: True, # Treats each digit as an individual token (useful for numerical data) \u0026#39;add_dummy_prefix\u0026#39;: False # Prevents adding a leading space (SentencePiece default is True) } try: # 1. Train the SentencePiece model using the defined options print(\u0026#34;Starting Unigram training...\u0026#34;) spm.SentencePieceTrainer.train(**options_unigram) print(\u0026#34;Training complete. \u0026#39;unigram_model.model\u0026#39; created.\u0026#34;) # 2. Load the trained model into a processor instance for inference sp_unigram = spm.SentencePieceProcessor() sp_unigram.load(\u0026#39;unigram_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Unigram Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {sp_unigram.get_piece_size()}\u0026#34;) # 3. Define test cases to evaluate how the model handles common and rare words test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] # 4. Iterate through test sentences to visualize subword segmentation for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (Unigram) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: Converts text into subword strings (visual representation) print(f\u0026#34;Subword Tokens: {sp_unigram.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: Converts text into numerical indices for model input print(f\u0026#34;Numerical IDs: {sp_unigram.encode_as_ids(text)}\u0026#34;) except Exception as e: # Handle potential errors during training or loading (e.g., missing input file) print(f\u0026#34;An error occurred with Unigram model: {e}\u0026#34;) The output is as follows:\nUnigram Model Metadata: Total Vocab Size: 1200 --- Tokenization Test (Unigram) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ll\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;ld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311] --- Tokenization Test (Unigram) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;T\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅm\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;sta\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;ra\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ver\u0026#39;] ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"A comprehensive guide to tokenization strategies: BPE, WordPiece, Unigram, and SentencePiece.","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"A brief introduction to Vectors \u0026amp; Verbs and formatting verification.","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"\nFeedforward network (FFN) blocks are the least talked about elements in the context of LLMs, but they are where more than 70% of the parameters originate. Something as simple and convenient as a couple of linear transformation matrices becomes a burden when models start scaling into billions of parameters. Thus, getting the maximum juice out of a lean structure is a challenge. FFNs are mostly under-represented in discussions even if the sub-layer acts as the primary \u0026ldquo;knowledge\u0026rdquo; processing engine, processing all the meaningful contexts activation layer has provided it.\nüî• Standard Position-Wise Feedforward Network This is as simple as it gets: a two-layered fully connected network of neurons with a single activation switch‚Äîmost probably a ReLU variant (for LLMs).\n$$ FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2 $$\nThe same transformation is applied to each token in parallel, making it efficient for GPU computation. For example, in the BERT model, the 512 dimensions of each token are expanded to 2048 to allow for \u0026ldquo;intricate interactions\u0026rdquo; before projecting back to 512 dimensions. This expansion/contraction is crucial; in every transformer, stacking of layers is possible only because the output of every layer matches its input dimensions.\nüî• SWiGLU Sigmoid Weighted Gated Linear Unit (SWiGLU) has become the gold standard for feedforward networks in LLMs like Llama 3 and PaLM. The primary reasons for choosing the Swish function (shown in the first image) over ReLU or GELU are as follows:\nMathematical Linearity: Swish (SiLU) is defined as $x \\cdot \\text{sigmoid}(\\beta x)$, where $\\beta$ is a learnable parameter. It is a smooth, non-monotonic function that allows small negative values to pass through. While GELU ($x \\cdot \\Phi(x)$) is also smooth, Swish\u0026rsquo;s specific curve has empirically improved model performance. Computational Efficiency: SiLU gradients are simpler to compute ($x / (1 + \\exp(-x))$) compared to standard GELU approximations. At the massive scale of modern LLMs, these shortcuts save significant training time. Performance: Research in the PaLM and Llama papers found that Swish-based Gated Linear Units consistently outperformed GELU-based versions in terms of perplexity and downstream task accuracy. $$SwiGLU(x, W, V, b, c) = (\\text{Swish}(xW + b) \\otimes (xV + c))W_2$$\nWhere $W$ and $V$ are weight matrices for the gate and the data path, and $\\otimes$ is element-wise multiplication.\nMore Complex Filtering: Instead of using two weight matrices as in a standard FFN, SWiGLU uses three (gate, data, and output). This allows it to extract more complex features without a drastic increase in parameters. Modern implementations often lower the dimensionality of hidden layers (to ~2/3 of what they would be in a standard FFN) while retaining the performance benefits. Zero-Gradient Avoidance: Unlike ReLU, which has a zero gradient for all negative values (the \u0026ldquo;dying ReLU\u0026rdquo; problem), SwiGLU‚Äôs smooth curve ensures the model can still learn from negative inputs, improving training stability in deep stacks. üî• Mixture of Experts (MoE) MoE is an entire architecture rather than just a single block. It has become the dominant strategy for scaling models to insane sizes‚Äîup to a trillion parameters (like DeepSeek-V3 and GPT-4)‚Äîbecause it breaks the direct link between model size (information capacity) and speed (inference cost).\nSparse Activation: MoE uses a \u0026ldquo;Router\u0026rdquo; to determine which smaller \u0026ldquo;Experts\u0026rdquo; (specialized FFNs) are best suited for a specific token. Instead of activating the entire network, only a tiny fraction of experts are used per token. For instance, DeepSeek-V3 might have 256 expert layers but only activate 8 for any single token. Big Scale, Small Compute: MoE allows \u0026ldquo;trillion-parameter intelligence\u0026rdquo; to run at the speed of a much smaller model. While DeepSeek-V3 has 671 billion total parameters, it only uses ~37 billion per token during inference. You get the reasoning performance of a massive model with the latency of a smaller one. Expert Specialization: Because different experts handle different tokens, they naturally specialize. Analysis shows certain experts become \u0026ldquo;specialists\u0026rdquo; in mathematics or coding, while others handle general language. Training Efficiency: Since you only update a fraction of the weights for each token, MoE models can process many more tokens per second during training compared to a dense model of the same total size. Training Dynamics and instability: The router assigns probabilities to each expert. To avoid overfitting or \u0026ldquo;expert collapse\u0026rdquo; (where the router sends everything to one expert), noise is often added to the router\u0026rsquo;s output during training. Some models choose one top expert, while others choose multiple and average their outputs. This is called the \u0026ldquo;auxilliary loss free\u0026rdquo; strategy to controll training biases.\nThe Challenges of MoE The RAM Wall: Even if only a small percentage of parameters are used for computation, all 671B parameters must still reside in GPU memory. This makes MoE models difficult to run on consumer hardware. Communication Overhead: In distributed training, experts are often split across different GPUs. If a token on GPU 1 needs an expert on GPU 4, data must travel over the network. DeepSeek-V3 employs \u0026ldquo;Node-Limited Routing\u0026rdquo; to minimize this congestion. Summary: MoE is the better choice for major labs (like DeepSeek, Google, or OpenAI) trying to maximize intelligence per dollar of training cost. Dense SwiGLU remains the standard for reliability, ease of deployment, and local execution.\n","permalink":"http://localhost:1313/posts/feedforward-networks/","summary":"Exploring the hidden layers of trillion-parameter switchboards: Feedforward Neural Networks and Activation Functions.","title":"Anatomy of Trillion-Parameter Switchboards: Understanding Feedforward Blocks"},{"content":"\nWe are about to touch the holy grail of modern AI. The 2017 declaration was simple: Attention Is All You Need. But as we‚Äôve raced from GPT-1 to reasoning models, the definition of \u0026lsquo;Attention\u0026rsquo; has quietly transformed. Is the algorithm ruling the world today actually the same one we started with? Let‚Äôs look under the hood.\nüî• The Core Concept At its heart, the attention mechanism is a form of weighted aggregation. It is often described using a database retrieval analogy. Imagine you have a database of information (context).\nQuery ($Q$): What are you currently looking for? (The token being generated). Key ($K$): The indexing tag associated with each piece of information. Value ($V$): The actual information (The context token). The \u0026ldquo;Attention\u0026rdquo; is the process of matching your Query against all the Keys to determine how much \u0026ldquo;weight\u0026rdquo; to put on each Value. Attention scores are calculated in each transformer layer at every step of the sequence. This is a single unifying factor in all transformer models, from BERT to Gemini.\nüî• Standard Multi-Head Attention (MHA) The original mechanism introduced in the paper \u0026ldquo;Attention Is All You Need\u0026rdquo;.\nHere, instead of performing the Key-Query-Value (KQV) operation on the entire context only once, the attention is split into $N$ parallel heads. Each head learns a different aspect of the language (e.g., word meaning, syntax, semantic relationships, etc.). The equation is a simple culmination of dot products aided by a regularizer like Softmax.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nWhere:\n$QK^T$ computes the similarity scores between the Query and Key. $\\sqrt{d_k}$ is a scaling factor to prevent the dot product from growing too large, mitigating the vanishing/exploding gradient problem. The weighted sum is applied to the values of $V$. The Memory Bandwidth Bottleneck: \u0026gt; During inference, the model must store Key and Value matrices for all previous tokens. This is called the KV Cache. In MHA, every single head has its own unique set of Keys and Values. For large models with massive context windows, loading this massive KV cache from memory (HBM) to the computation chip (SRAM) becomes the primary bottleneck, slowing down generation significantly. Earlier models could often only generate ~8 tokens per second at inference due to this constraint.\nüî• Multi-Query Attention (MQA) This approach addresses the memory bottleneck by sharing the same set of Keys and Values across all heads while keeping separate Query heads.\nMHA: $H$ Query heads, $H$ Key heads, $H$ Value heads MQA: $H$ Query heads, 1 Key head, 1 Value head This drastically reduces the memory requirements of the KV cache by a factor of $H$ (often 8x or 16x). The GPU needs to load far less data, significantly speeding up inference. However, it does not reduce the computation (FLOPs)‚Äîso why is it faster?\nThis can be explained with the Kitchen Analogy: Imagine a kitchen (GPU) with 8 chefs (Heads).\nComputation (Math): The Chef chopping vegetables. Memory Access (Bandwidth): The Assistant running to the fridge to get ingredients. In MHA: Each chef demands unique vegetables from the fridge. The assistant runs back and forth 8 times, fetching 8 different crates. Most of the time is spent waiting for the assistant to return. This results in idle cores waiting for data to load from HBM.\nIn MQA: All chefs agree to use the same vegetables. The assistant runs back and forth only once, fetching one giant crate of potatoes and dumping it. This results in minimized memory transfer and no idle cores.\nThe Technical Reality: During text generation, the model is memory-bound, not compute-bound. The time it takes to move the KV Cache from HBM (High Bandwidth Memory) to the chip\u0026rsquo;s SRAM is the bottleneck. MQA reduces the data volume of that move by a factor of $H$.\nThe Capacity Gap: MQA is a trade-off between nuanced response and memory capacity. The model loses the ability to store different types of information, as a single $V$ matrix must now store all nuances of information flow in the language.\nThe Surgery Trick (Uptraining): Google Research figured out a way to take an existing model with MHA and convert it to MQA by averaging the weight matrices of $H$ heads. This is called Uptraining. You then train this \u0026ldquo;Frankenstein\u0026rdquo; model for ~5% of the original training steps for adjustments. This yields an MQA model that is nearly as good as the original MHA but vastly faster.\nüî• Grouped Query Attention (GQA) A \u0026ldquo;best of both worlds\u0026rdquo; solution used in modern models like Llama 2 and Llama 3.\nHow it works: GQA acts as an interpolation between MHA and MQA. Instead of having just 1 Key head (MQA) or $H$ Key heads (MHA), it divides the query heads into $G$ groups. Each group shares a single Key and Value head.\nEquation modification: If you have 8 query heads, you might create 4 groups (2 queries per group). This reduces the KV cache by 2x instead of 8x, preserving more quality than MQA while still speeding up inference.\nMHA: 8 people (heads) are working on a project. Each person has their own unique filing cabinet (KV cache). It takes forever to move 8 cabinets around. MQA: 8 people share one single filing cabinet. It\u0026rsquo;s very fast to move, but they fight over the organization and lose detail. GQA: The 8 people split into 4 teams of 2. Each team shares a cabinet. It\u0026rsquo;s a balance between speed and organizational detail. üî• Multi-Head Latent Attention (MLA) This is the most advanced mechanism for attention, used in models like DeepSeek-V3. It is a sophisticated design for massive efficiency that takes a radical approach: mathematically compressing the memory into a low-rank \u0026ldquo;latent\u0026rdquo; vector.\nKV Cache Explosion: As mentioned before, for a massive model like DeepSeek-V3 (671B parameters), storing the full Keys and Values for every token in a long conversation requires terabytes of high-speed memory (HBM), and chips spend more time waiting than churning.\nMHA (Standard) stores everything. MQA deletes redundancy but hurts quality. MLA keeps the quality of MHA but compresses the data 4x-6x smaller than even GQA.\nThe Zip File Approach: Low-Rank Joint Compression Let\u0026rsquo;s compare the math of MHA and MLA. In MHA, for every single token, we generate a unique Key ($k$) and Value ($v$) matrix for each attention head.\nThe Memory Problem: The $k$ is stored in the KV Cache. For 128 heads, for example, it calculates a massive vector of $128 \\times 128 = 16,384$ floats per token.\nFor MHA, the attention score looks like: $$\\text{Score} = q^T \\cdot k$$\nBut for MLA, instead of storing the full $k$, it projects the input down into a tiny latent vector $c_{KV}$, which is much smaller (e.g., 512 floats). $$c_{KV} = W_{DKV} \\cdot x$$\nNow here is the catch: Naive compression and decompression would mean constructing a small latent matrix for every token and deconstructing it every single time the information is required, which defeats the purpose of compression.\n$$k_{\\text{reconstructed}} = W_{UK} \\cdot c_{KV}$$ $$\\text{Score} = q^T \\cdot k_{\\text{reconstructed}}$$\nThe Matrix Absorption Trick (Optimization) Instead of reconstructing $k$ for every token, we can absorb the Up-Projection matrix ($W_{UK}$) into the Down-Projection matrix ($W_{DKV}$) and absorb the query $q$ into a new $Q$.\nFrom the original equation: $$\\text{Score} = q^T \\cdot (\\underbrace{W_{UK} \\cdot c_{KV}}_{\\text{This is } k})$$\nWe associate differently:\n$$\\text{Score} = (\\underbrace{q^T \\cdot W_{UK}}_{\\text{Absorbed Query}}) \\cdot c_{KV}$$ We change the order in which the matrix multiplication is performed.\nHow is this allowed? In linear algebra, matrix multiplication is associative:\n$$(A \\cdot B) \\cdot C = A \\cdot (B \\cdot C)$$ We can move the parentheses! Instead of grouping $(W_{UK} \\cdot c_{KV})$ to make the Key, we group $(q^T \\cdot W_{UK})$ to make a new Query.\nNote: The positional embedding information is left untouched. That rotational information from algorithms like RoPE is preserved outside the compression; it requires minimal space anyways.\n","permalink":"http://localhost:1313/posts/attention-mechanisms/","summary":"We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek\u0026rsquo;s MLA, how has the definition of \u0026lsquo;Attention\u0026rsquo; transformed?","title":"Attention Is All You Need, but exactly which one?: MHA, GQA and MLA"},{"content":"\nPositional Embeddings are the \u0026ldquo;voice\u0026rdquo; that tell Transformers where words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.\n\u0026ldquo;The dog chased the cat\u0026rdquo; and \u0026ldquo;cat chased the dog\u0026rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.\nüî• Sinusoidal Embeddings: The Odometer of Language Introduced in the seminal \u0026ldquo;Attention Is All You Need\u0026rdquo; paper, this was the industry standard for many years. This approach adds a vector to each word embedding based on its position. The vector is computed out of wavelike patterns that are periodic and smooth. Mathematically, for a position $pos$ and embedding dimension $i$:\n$$ PE_{(pos, i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $$\nThe model uses sine and cosine waves of different frequencies to capture both short and long-range dependencies. The frequency of the waves decreases as the embedding dimension increases, which helps capture long-range dependencies in the input sequence.\nThe creators wanted the model to calculate distance easily, which is why a linear function was used. The model simply rotates the current position vector using a rotation matrix to derive the next position.\n$$ \\begin{pmatrix} \\cos(k) \u0026 \\sin(k) \\\\ -\\sin(k) \u0026 \\cos(k) \\end{pmatrix} \\cdot PE(pos) \\approx PE(pos + k) $$ Low dimensions act like a fast-ticking clock (high frequency, changing rapidly) and high dimensions act like a slow clock (low frequency, changing slowly).\nThe number 10,000 is an arbitrarily chosen high number to reduce Aliasing. By choosing such a high number, the creators ensured that the wavelength of the slowest dimension is incredibly long. Consequently, the pattern practically never repeats, and each position remains unique.\nThis combination works like a mechanical odometer dial. Each subsequent dimension acts as a slower-moving gear. The model learns to read these \u0026ldquo;dials\u0026rdquo; to understand exactly where a word sits in the sequence.\nThis mitigated the challenge of exploding gradients that previous array-based positional embeddings suffered from (e.g., adding integers 1, 2, 3\u0026hellip;). This model keeps values between -1 and +1, adding stability while ensuring every position gets a distinct code.\nThe Problem: This approach pollutes the word embedding matrix (semantic information). While high-dimensional vectors are almost always orthogonal‚Äîmeaning the noise doesn\u0026rsquo;t fully destroy the word\u0026rsquo;s identity‚Äîit does not strictly guarantee purity.\nThis led to a more mature approach.\nüî• ALiBi: The Fading Streetlight Attention with Linear Biases (ALiBi) is a method where, instead of adding a vector to the embeddings at the start, the model injects positional information linearly, directly into the attention scores. If Sinusoidal Encoding asks, \u0026ldquo;Where am I on the map?\u0026rdquo;, ALiBi asks, \u0026ldquo;How much harder should I squint to see you?\u0026rdquo;\nThe biggest problem ALiBi solved is the \u0026ldquo;Invisible Wall\u0026rdquo;. If you train a Transformer on sequences of length 2048 and then feed it a book with 3000 words, it will crash or output garbage at word 2049. This is a failure of Extrapolation.\nALiBi throws away the idea of adding positional vectors entirely. Instead, it devises a \u0026ldquo;penalty\u0026rdquo; system for attention scores:\n$$ \\text{Score} = (q \\cdot k^\\top) - (m \\cdot \\text{distance}) $$\nDistance: If words A and B are neighbors, the distance is 0, so the penalty is 0. Although the distance is linear, it does not cause exploding gradients because the raw attention scores are normalized by a softmax function. This distance prioritizes nearby words (local context) over distant ones.\nSlope ($m$): If we always penalize distance equally, the model becomes myopic. ALiBi mitigates this by assigning a different slope value to each attention head:\nHead 1 (The Historian): Very low slope. The penalty grows slowly, allowing it to see far back. Head 2 (The Reader): Medium slope. Head 3 (The Myopic): Very high slope (e.g., 0.5); it effectively cannot see beyond 2-3 words. This mimics holding a lantern. Words close to you are bright, while words further away naturally fade into darkness.\nExtrapolation: If you walk 5 miles down the road (a longer sequence), your lantern works exactly the same way. The physics of light decay are constant, allowing the model to handle sequences longer than it was trained on.\nHowever, ALiBi forces a \u0026ldquo;sliding window\u0026rdquo; view where distant words eventually become invisible. This brings us to the modern standard.\nüî• RoPE: The Rotary Revolution Rotary Position Embeddings (RoPE) is the current industry standard (used in Llama 3, Mistral, PaLM) because it mathematically unifies \u0026ldquo;absolute position\u0026rdquo; with \u0026ldquo;relative position.\u0026rdquo; This method rotates the vectors in the Query and Key matrices based on their absolute position.\nIf a vector has 512 dimensions, RoPE breaks it into 256 pairs. Each pair is rotated by a different frequency:\n$$ \\theta_i = 10000^{-2i/d} $$\nThe First pair ($i = 0$) is rotated by the highest frequency. The Last pair ($i = 255$) is rotated by the lowest frequency. This spread ensures the model has \u0026ldquo;high precision\u0026rdquo; (fast rotation) to distinguish immediate neighbors, and \u0026ldquo;low precision\u0026rdquo; (slow rotation) to track global position without the pattern repeating.\nDoesn\u0026rsquo;t rotation change meaning? Yes and No. The vector for \u0026ldquo;Apple\u0026rdquo; at position 1 and \u0026ldquo;Apple\u0026rdquo; at position 256 will look completely different in terms of coordinates. However, rotation does not change the semantic strength (magnitude/norm) of the vector.\nThe Relativity Trick: Transformers compare words using dot products (angles). RoPE relies on the fact that the attention mechanism ignores the absolute rotation and only detects the difference in rotation (relative position). Thus, the \u0026ldquo;relative meaning\u0026rdquo; is encoded purely into the angle between words, leaving the semantic \u0026ldquo;magnitude\u0026rdquo; untouched.\nüî• LongRoPE: The Bifocal Lens While RoPE is powerful, it struggles when stretching context windows from 4k to 2 million tokens. The \u0026ldquo;fast\u0026rdquo; dimensions spin so rapidly over long distances that they become random noise. LongRoPE solves this using an evolutionary search algorithm to find a unique scaling factor ($\\lambda$) for each dimension.\nThe Equations Instead of rotating by the standard $\\theta_i$, LongRoPE rotates by a rescaled frequency:\n$$ \\theta\u0026rsquo;_i = \\lambda_i \\theta_i $$\nTo efficiently find these $\\lambda$ values without searching an infinite space, the algorithm enforces a monotonicity constraint based on NTK theory:\n$$ \\lambda_i \\le \\lambda_{i+1} $$\nThis ensures that low-frequency dimensions (global context) are stretched more than high-frequency dimensions (local context), creating a \u0026ldquo;bifocal\u0026rdquo; effect:\nHigh Frequencies (Local): Kept sharp ($\\lambda \\approx 1$) to maintain grammar. Low Frequencies (Global): Stretched ($\\lambda \u0026gt; 1$) to track massive distances without repeating. Not worth perusing more since the next one is much better at long context positional encoding. üî• HoPE: The Hyperbolic Slide Hyperbolic Rotary Positional Encoding (HoPE) moves from the geometry of a circle to the geometry of a hyperbola (inspired by Lorentz transformations). It was designed to fix a subtle flaw in RoPE: the \u0026ldquo;wobbly\u0026rdquo; or oscillatory nature of attention scores.\nThe Core Problem: RoPE\u0026rsquo;s \u0026ldquo;Wobble\u0026rdquo; In standard RoPE, we rotate vectors around a circle.\nThe Issue: Circles repeat. A dial at $361^\\circ$ looks identical to $1^\\circ$. Even with different frequencies, the dot product fluctuates up and down as distance increases. The Consequence: The model gets confused at long distances, potentially mistaking a word 1,000 tokens away for a neighbor just because the rotation cycles aligned, creating \u0026ldquo;noise.\u0026rdquo; The \u0026ldquo;Hyperbolic\u0026rdquo; Shift HoPE replaces trigonometric functions ($\\sin, \\cos$) with hyperbolic functions ($\\sinh, \\cosh$).\nCircular Rotation: Keeps distance from the center constant. Hyperbolic Rotation: Moves points along a hyperbola. Crucially, hyperbolic functions do not repeat; they grow or decay exponentially. By using hyperbolic rotation, HoPE guarantees that as words get further apart, the attention score decays monotonically (smoothly drops).\nThe Equations HoPE uses a hyperbolic matrix $B(\\theta, m)$ defined using the Lorentz Boost structure:\n$$ B(\\theta, m) = \\begin{pmatrix} \\cosh(m\\theta) \u0026 \\sinh(m\\theta) \\\\ \\sinh(m\\theta) \u0026 \\cosh(m\\theta) \\end{pmatrix} $$ Since hyperbolic functions grow to infinity, HoPE introduces a decay penalty ($e^{-m\\theta'}$). The final attention score simplifies beautifully to a function of relative distance: $$ \\text{Score} \\propto e^{-|m-n|(\\theta\u0026rsquo; - \\theta)} $$\nThis ensures the attention score exponentially decays as distance increases, eliminating the wobbles of standard RoPE. This the latest model that is taking up the space in the industry for extremely long context models.\n","permalink":"http://localhost:1313/posts/positional-embeddings/","summary":"From Sinusoidal to RoPE and HoPE: How Transformers learn to process word order and sequence length.","title":"The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE"},{"content":"Imagine trying to teach a computer the difference between \u0026ldquo;Apple\u0026rdquo; the fruit and \u0026ldquo;Apple\u0026rdquo; the company. To us, the distinction is intuitive. To a machine, it‚Äôs just a string of characters. How do we turn these strings into meaningful math? While early attempts like Word2Vec gave us a great start, they missed the forest for the trees‚Äîor in some cases, the twigs for the branches. Enter GloVe and FastText: two algorithms that revolutionized how machines understand the nuances of human language.\nPrevious static embedding models like Word2Vec successfully captured the local semantics of words, but they failed in addressing contexts between words that might not always appear in the same context window. Furthermore, they treated words as indivisible atomic units in continuous vector spaces, missing out on the internal structure of language.\nThe two simple but effective ideas behind GloVe and FastText are subword units and global co-occurrence. Let\u0026rsquo;s dive into them.\nGloVe: A Global Mapping Strategy Global Vectors for Word Representation (GloVe) was created at Stanford. The core idea was to train a model where word vectors are defined by how often they appear around other words across the entire corpus, rather than just locally.\nTraining Steps: Co-occurrence Matrix X: GloVe scans the entire corpus and creates a giant matrix (spreadsheet) recording the counts of how often words appear near each other. Factorization: It compresses this giant matrix into smaller, dense vectors. Objective: The function tries to equate the dot product of two word vectors with the logarithm of their probability of appearing together. The error between these values is then backpropagated to update the vectors. $$w_i \\cdot w_j + b_i + b_j = \\log(X_{ij})$$\n$w$: The word vector (what we want). $X_{ij}$: The count from our matrix. The Loss Function: We can\u0026rsquo;t satisfy the equation above perfectly for every word pair. So, we use a weighting function to minimize the squared error:\n$$J = \\sum_{i,j=1}^V f(X_{ij}) ({w_i}^T w_j + b_i + b_j - \\log(X_{ij}))^2$$\nIf we do not use the weighting function, the loss would be infinite for rare words (since $\\log(0) = \\infty$) and frequent words (like \u0026rsquo;the\u0026rsquo;) would overpower the model.\n$$ f(x) = \\begin{cases} (x/x_{max})^\\alpha \u0026amp; \\text{if } x \u0026lt; x_{max} \\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$\n(Typically $x_{max}=100$ and $\\alpha=0.75$)\nFastText: The Lego Bricks Strategy FastText was created by Facebook AI Research (FAIR). The idea was to treat words not as solid rocks, but as structures made of \u0026ldquo;Lego bricks\u0026rdquo; (subword units).\nWord2Vec struggled to create embeddings for words it hadn\u0026rsquo;t seen before (Out-Of-Vocabulary or OOV words). FastText solves this by breaking words down.\nGloVe treats a word like \u0026ldquo;Apple\u0026rdquo; as a single, atomic unit. FastText breaks \u0026ldquo;Apple\u0026rdquo; into character n-grams: \u0026lt;ap, app, ppl, ple, le\u0026gt; For a complex or new word like \u0026ldquo;Applesauce\u0026rdquo;, the model might not know the whole word, but it recognizes the Lego brick for \u0026ldquo;Apple\u0026rdquo; and the remaining bits from other subwords.\nFastText takes its inspiration from the SkipGram architecture. The vector for a word is simply the sum of the vectors of its n-gram subwords:\n$$v_w = \\sum_{g \\in G_w} z_g$$\nObjective: The objective is to predict context words given a target word.\n$$P(w_c | w_t) = \\frac{e^{s(v_{w_t} \\cdot v_{w_c})}}{\\sum_{w \\in V} e^{s(v_{w_t} \\cdot v_w)}}$$\nAnd the scoring function is defined as:\n$$s(w_t, w_c) = u_{w_t}^T (\\sum_{g \\in G_w} z_g)$$\n$u_{w_c}$: The vector for the context word (output vector). $z_g$: The vector for the n-gram $g$ (input vector). Modifications to Softmax: Calculating the full softmax for every word in the vocabulary is computationally expensive. FastText uses two methods to approximate this efficiently.\nOption A: Negative Sampling\nWe pick a few random \u0026ldquo;wrong\u0026rdquo; words every time to serve as \u0026ldquo;negative samples\u0026rdquo; and calculate the loss only for the correct word and these noise words.\n$$J = - \\log \\sigma(s(w_t, w_c)) - \\sum\\limits_{i=1}^{N} \\mathbb{E}_{n_i \\sim P_n(w)} [\\log \\sigma(-s(w_t, n_i))]$$\nThe sigmoid function $\\sigma$ squashes the score between 0 and 1. The first term pushes the probability of the correct word towards 1. The second term pushes the probability of the noise words towards 0. Option B: Hierarchical Softmax (Faster for infrequent words)\nInstead of a flat list of words, imagine the vocabulary arranged as a binary tree (Huffman Tree).\nRoot: Top of the tree. Leaves: The actual words. To calculate the probability of a word, we trace a path from the root to that leaf. At every branching node, we calculate the probability (sigmoid) of going left or right. The probability of a word $w$ is the product of the probabilities of the turns taken to reach it. Code Example from gensim.models import FastText from gensim.test.utils import common_texts # Initialize the FastText model with specific hyperparameters model = FastText( vector_size=100, # Dimensionality of the word vectors window=5, # Maximum distance between the current and predicted word within a sentence min_count=1, # Ignores all words with total frequency lower than this sg=1, # Training algorithm: 1 for skip-gram; 0 for CBOW hs=0, # If 0, and negative is non-zero, negative sampling will be used negative=5, # Number of \u0026#34;noise words\u0026#34; to be drawn for negative sampling workers=4, # Number of worker threads to train the model epochs=10, # Number of iterations over the corpus min_n=3, # Minimum length of character n-grams max_n=6 # Maximum length of character n-grams ) # Build the vocabulary from the provided text corpus model.build_vocab(common_texts) # Train the model on the corpus model.train( common_texts, total_examples=model.corpus_count, epochs=model.epochs ) # Calculate and print the cosine similarity between two words in the vocabulary print(f\u0026#34;Similarity between \u0026#39;computer\u0026#39; and \u0026#39;human\u0026#39;: {model.wv.similarity(\u0026#39;computer\u0026#39;, \u0026#39;human\u0026#39;)}\u0026#34;) # Demonstrate FastText\u0026#39;s ability to handle Out-Of-Vocabulary (OOV) words # Even if \u0026#39;computation\u0026#39; wasn\u0026#39;t in the training data, FastText constructs a vector using character n-grams oov_vector = model.wv[\u0026#39;computation\u0026#39;] # Check if a specific word exists in the model\u0026#39;s fixed vocabulary index word = \u0026#34;computer\u0026#34; is_in_vocab = word in model.wv.key_to_index print(f\u0026#34;\\nIs the word \u0026#39;{word}\u0026#39; in the model\u0026#39;s vocabulary? {is_in_vocab}\u0026#34;) ","permalink":"http://localhost:1313/posts/glove-and-fasttext/","summary":"How subword units and global co-occurrence matrices allow GloVe and FastText to capture nuances that Word2Vec missed.","title":"The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText"},{"content":" Before we had Large Language Models writing poetry, we had to teach computers that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; are related not just by spelling, but by meaning. This is the story of that breakthrough. It‚Äôs the moment we stopped counting words and started mapping their souls‚Äîturning raw text into a mathematical landscape where math can solve analogies. Welcome to the world of Word2Vec. üîÆ\nLanguage models require vector representations of words to capture semantic relationships. Before the 2010s, models used word count-based vector representations that captured only the frequency of words (e.g., One-Hot Encoding). The Problems: üöß\nThe Curse of Dimensionality Lack of meaning (Synonyms were treated as mathematically unrelated). In 2011, Mikolov et al. at Google introduced Word2Vec, which used a shallow neural network to learn vector representations of words. The best part? It could create much denser relationships between words than prior models, and it was unsupervised.\nThe training (prediction) is essentially \u0026ldquo;fake\u0026rdquo;‚Äîa pretext task. But the weight matrices we get on the side are a gold mine of fine-grained semantic relationships. ‚õèÔ∏è\nThere are two popular variants of Word2Vec:\nSkipGram Continuous Bag Of Words (CBOW) 1. Continuous Bag of Words (CBOW) üß© CBOW is a simple \u0026lsquo;fill in the blanks\u0026rsquo; machine. It takes the surrounding words as inputs and tries to predict the center word.\n\u0026ldquo;Continuous\u0026rdquo;: It operates in continuous vector space (unlike the discrete space of n-gram models).\n\u0026ldquo;Bag of words\u0026rdquo;: The order of the context does not matter. \u0026ldquo;The cat sat on the mat\u0026rdquo; and \u0026ldquo;The mat sat on the cat\u0026rdquo; are the same for CBOW. They produce exactly the same predictions.\nObjective: Maximize the probability of the target word $w_t$ given its context $w_{t-C}, \\dots, w_{t+C}$.\n2. Training Defining Dimensions:\n$V$: Vocabulary size (number of unique words in the corpus, e.g., 10,000) $N$: Dimension of the embedding space (e.g., 100) $C$: Number of context words (e.g., 2 words on left of target and 2 words on right) Step 1: Input Lookup \u0026amp; One-Hot Encoding\nMathematically, we represent each context word as a one-hot vector $x^{(c)}$. We have an Input Matrix $W_{in}$ of size $V \\times N$. For each context word (e.g., \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;), we select its corresponding row from $W_{in}$.\n$$ v_c = W_{in}^T x^{(c)} $$\nWhere $v_c$ is the $N$-dimensional vector for context word $c$.\nStep 2: Projection (The \u0026ldquo;Mixing\u0026rdquo;)\nWe combine the context vectors. In standard CBOW, we simply average them. This creates the hidden layer vector $h$.\n$$ h = \\frac{1}{2C} \\sum_{c=1}^{2C} v_c $$\n$h$ is a single vector of size $N \\times 1$. Note: This operation is linear and contains no activation function (like ReLU or Sigmoid).\nStep 3: Output Scoring (The \u0026ldquo;Dot Product\u0026rdquo;)\nWe have a second matrix, the Output Matrix $W_{out}$ of size $N \\times V$. We compute the raw score (logit) $u_j$ for every word $j$ in the vocabulary by taking the dot product of the hidden vector $h$ and the output vector $v\u0026rsquo;_j$.\n$$ u_j = {v\u0026rsquo;_j}^T h $$\nOr in matrix form:\n$$ u = W_{out}^T h $$\nResult $u$ is a vector of size $V \\times 1$.\nStep 4: Probability Conversion (Softmax)\nWe convert the raw scores $u$ into probabilities using the Softmax function. This tells us the probability that word $w_j$ is the center word.\n$$ y_j = P(w_j | \\text{context}) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 5: Loss Calculation (Cross-Entropy)\nWe compare the predicted distribution $y$ with the actual ground truth $t$ (a one-hot vector where the index of the true word is 1). The loss function $E$ is:\n$$ E = - \\sum_{j=1}^V t_j \\log(y_j) $$\nSince $t_j$ is 0 for all words except the target word (let\u0026rsquo;s call the target index $j^*$), this simplifies to:\n$$ E = - \\log(y_{j^*}) $$\n3. Backpropagation Details We need to update the weights to minimize $E$. We use the Chain Rule.\nA. Gradient w.r.t. Output Matrix ($W_{out}$)\nWe want to know how the error changes with respect to the raw score $u_j$.\n$$ \\frac{\\partial E}{\\partial u_j} = y_j - t_j $$\nLet\u0026rsquo;s call this error term $e_j$.\nIf $j$ is the target word: $e_j = y_j - 1$ (Result is negative; we boost the score). If $j$ is not target: $e_j = y_j - 0$ (Result is positive; we suppress the score). The update rule for the output vector $v\u0026rsquo;_j$ becomes:\n$$ v'_{j}(\\text{new}) = v'_{j}(\\text{old}) - \\eta \\cdot e_j \\cdot h $$ ($\\eta$ is the learning rate)\nB. Gradient w.r.t. Hidden Layer ($h$)\nWe backpropagate the error from the output layer to the hidden layer.\n$$ EH = \\sum_{j=1}^V e_j \\cdot v\u0026rsquo;_j $$\n$EH$ is an $N$-dimensional vector representing the aggregate error passed back to the projection layer.\nC. Gradient w.r.t. Input Matrix ($W_{in}$)\nSince $h$ was just an average of the input vectors, the error $EH$ is distributed to each context word\u0026rsquo;s input vector. For every word $w_c$ in the context:\n$$ v_{w_c}(\\text{new}) = v_{w_c}(\\text{old}) - \\eta \\cdot \\frac{1}{2C} \\cdot EH $$\nLet\u0026rsquo;s build a single pass through the network for a given context.\nimport torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # Define a simple Continuous Bag of Words (CBOW) style model class CBOWModel(nn.Module): def __init__(self, vocab_size, embedding_dim): super(CBOWModel, self).__init__() # The embedding layer stores the word vectors we want to learn self.embeddings = nn.Embedding(vocab_size, embedding_dim) # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word self.linear = nn.Linear(embedding_dim, vocab_size) def forward(self, inputs): # inputs: tensor of word indices for the context embeds = self.embeddings(inputs) # Aggregate context by calculating the mean of the word embeddings h = torch.mean(embeds, dim=1) # Produce logits (raw scores) for each word in the vocabulary logits = self.linear(h) return logits # Setup vocabulary and mappings word_to_ix = {\u0026#34;the\u0026#34;: 0, \u0026#34;quick\u0026#34;: 1, \u0026#34;brown\u0026#34;: 2, \u0026#34;fox\u0026#34;: 3, \u0026#34;jumps\u0026#34;: 4, \u0026#34;over\u0026#34;: 5, \u0026#34;lazy\u0026#34;: 6, \u0026#34;dog\u0026#34;: 7} ix_to_word = {v: k for k, v in word_to_ix.items()} # Configuration constants EMBEDDING_DIM = 5 VOCAB_SIZE = len(word_to_ix) LEARNING_RATE = 0.01 # Initialize the model, loss function, and optimizer model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM) loss_function = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE) # Prepare dummy training data: context words and the target word # Context: [\u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;over\u0026#34;] -\u0026gt; Target: \u0026#34;fox\u0026#34; context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long) target_idxs = torch.tensor([3], dtype=torch.long) # Perform a single optimization step model.zero_grad() # Forward pass: we reshape context to (1, -1) to simulate a batch of size 1 logits = model(context_idxs.view(1, -1)) # Calculate loss against the target word index loss = loss_function(logits, target_idxs) # Backpropagate and update weights loss.backward() optimizer.step() # Output results for the blog post print(\u0026#34;=== Model Training Snapshot ===\u0026#34;) print(f\u0026#34;Calculated Loss: {loss.item():.6f}\u0026#34;) print(f\u0026#34;\\n=== Learned Vector for \u0026#39;jumps\u0026#39; ===\u0026#34;) word_vec = model.embeddings(torch.tensor([word_to_ix[\u0026#39;jumps\u0026#39;]])) print(word_vec.detach().numpy()) print(\u0026#34;\\n=== Embedding Matrix (Weights) ===\u0026#34;) print(model.embeddings.weight.detach().numpy()) print(\u0026#34;\\n=== Linear Layer Weights ===\u0026#34;) print(model.linear.weight.detach().numpy()) Output shows single pass loss and learned vector for \u0026lsquo;jumps\u0026rsquo; word. This is a single training sample result but after multiple such passes both Embedding and linear weights should be very similar.\n=== Model Training Snapshot === Calculated Loss: 1.989656 === Learned Vector for \u0026#39;jumps\u0026#39; === [[-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775]] === Embedding Matrix (Weights) === [[ 1.66039 -0.11371879 0.6246518 0.35860053 -1.2504417 ] [-2.1847186 -0.77199775 -0.17050214 -0.38411248 -0.03913084] [ 0.11852697 0.90073633 0.8847807 0.7404524 0.900149 ] [ 0.07440972 -0.40259898 2.6246994 -0.08851447 0.02660969] [-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775] [-0.9245572 -0.5545908 0.9083091 -1.0755049 0.84047747] [-1.0237687 0.59466314 0.05621134 -0.6202532 1.3664424 ] [ 0.60998917 -1.0549186 1.6103884 0.8724912 -1.2486908 ]] === Linear Layer Weights === [[ 0.02165058 -0.28883642 0.14545658 -0.3442509 0.32704315] [-0.18731792 0.28583744 0.22635977 0.13245736 0.29019794] [ 0.3158916 -0.15826383 -0.03203773 0.16377363 -0.41457543] [-0.05080034 0.4180087 0.11228557 0.30218413 0.3025514 ] [-0.38419306 0.24475925 -0.39210224 -0.38660625 -0.2673145 ] [-0.32321444 0.12200444 -0.03569533 0.2891424 -0.07345333] [-0.33704326 0.2521956 0.31587374 0.22590035 0.29866052] [ 0.4117266 -0.44231793 0.24064957 -0.29684234 0.333821 ]] Interesting Facts About CBOW üß† The \u0026ldquo;Averaging Problem\u0026rdquo;: This is a common issue in CBOW models. The averaging and training is order-agnostic. All variants of word order in the context window will generate the same output. \u0026ldquo;Dog bit the man\u0026rdquo; and \u0026ldquo;Man bit the dog\u0026rdquo; are the same in CBOW\u0026rsquo;s eyes.\nSmoothing Effect: CBOW models are much faster to train than Skip-Gram but are generally smoother.\nRare Words: CBOW models struggle slightly with rare words because the presence of surrounding common words (like articles and prepositions) can skew the embeddings and dissolve their uniqueness.\nComputational Cost: Softmax requires summing over the entire vocabulary size, which can be computationally expensive if $V$ is very large. Solutions include Hierarchical Softmax and Negative Sampling (approximating the denominator by only checking the target word vs. 5 random noise words).\nVectors as Double Agents: Every word has a different representation in the embedding matrix and the output matrix. Most of the time $W_{out}$ is discarded, but some consider averaging both $W_{in}$ and $W_{out}$ for slightly better performance.\nLinear Relationships: The famous analogy King - Man + Woman = Queen emerges from the model because of the linear relationships between the words.\nInitialization: This is a no-brainer but worth mentioning. Initialization to zero vectors would mean no learning. We must initialize randomly.\n2. Skip-Gram (The \u0026ldquo;Burger Flip\u0026rdquo; of CBOW) üîÑ Skip-Gram flips the CBOW logic. While CBOW tries to predict the center word given the context, Skip-Gram tries to predict the scattered context given the center word.\nCore Concept: If a word like \u0026ldquo;monarch\u0026rdquo; can accurately predict words like \u0026ldquo;king\u0026rdquo;, \u0026ldquo;throne\u0026rdquo;, \u0026ldquo;crown\u0026rdquo;, \u0026ldquo;royalty\u0026rdquo;, and \u0026ldquo;power\u0026rdquo;, then it must effectively capture the concept of royalty.\nTraining Let\u0026rsquo;s define dimensions:\n$V$: Vocabulary size $D$: Embedding dimension $C$: Number of context words Step 1: Input Lookup\nUnlike CBOW, our input is just one word vector (the center word $w_t$). We grab the vector $v_c$ from the Input Matrix $W_{in}$.\n$$ h = v_{w_t} $$\nNote: There is no averaging here. The hidden layer $h$ is simply the raw vector of the center word.\nStep 2: Output Scoring (The Broadcast)\nThe model takes this single vector $h$ and compares it against the Output Matrix $W_{out}$ (the entire vocabulary). It computes a score $u_j$ for every word $j$ in the dictionary.\n$$ u = W_{out}^T h $$\nResult $u$: A vector of size $V \\times 1$ containing raw scores.\nStep 3: Probability Conversion (Softmax)\nWe apply Softmax to turn scores into probabilities.\n$$ P(w_j | w_t) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 4: The Multi-Target Loss\nHere is the major difference from CBOW. In CBOW, we had one target (the center). In Skip-Gram, we have $2C$ targets (all the surrounding words). For a sentence \u0026ldquo;The cat sat on mat\u0026rdquo; (Center: \u0026ldquo;sat\u0026rdquo;), the targets are \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;. We want to maximize the probability of all these true context words. The Loss function ($E$) sums the error for each context word:\n$$ E = - \\sum_{w_{ctx} \\in \\text{Window}} \\log P(w_{ctx} | w_t) $$\nStep 5: Backpropagation (Accumulating Gradients)\nSince one input word is responsible for predicting multiple output words, the error signals from all those context words add up.\nAt the Output Layer:\nIf \u0026ldquo;cat\u0026rdquo; was a target, the \u0026ldquo;cat\u0026rdquo; vector in $W_{out}$ gets a signal: \u0026ldquo;Move closer to \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; If \u0026ldquo;apple\u0026rdquo; was NOT a target, the \u0026ldquo;apple\u0026rdquo; vector gets a signal: \u0026ldquo;Move away from \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; At the Hidden Layer ($h$):\nThe center word \u0026ldquo;sat\u0026rdquo; receives feedback from all its neighbors simultaneously. Error signal = (Error from \u0026ldquo;The\u0026rdquo;) + (Error from \u0026ldquo;cat\u0026rdquo;) + (Error from \u0026ldquo;on\u0026rdquo;)\u0026hellip; The vector for \u0026ldquo;sat\u0026rdquo; in $W_{in}$ moves in the average direction of all its neighbors. 3. The \u0026ldquo;Computational Nightmare\u0026rdquo; \u0026amp; Negative Sampling The equations above describe Naive Softmax.\nThe Problem: If $V = 100,000$, computing the denominator $\\sum \\exp(u_k)$ takes 100,000 operations. Doing this for every word in a training set of billions of words is impossibly slow.\nThe Solution: Negative Sampling Instead of updating the entire vocabulary (1 correct word vs 99,999 wrong ones), we approximate the problem.\nPositive Pair: (sat, cat) $\\rightarrow$ Maximize probability (Label 1). Negative Pairs: We pick $K$ (e.g., 5) random words the model didn\u0026rsquo;t see, e.g., (sat, bulldozer), (sat, quantum). $\\rightarrow$ Minimize probability (Label 0). New Equation (Sigmoid instead of Softmax):\n$$ E = - \\log \\sigma({v'_{\\text{pos}}}^T v_{\\text{in}}) - \\sum_{k=1}^K \\log \\sigma(-{v'_{\\text{neg}_{k}}}^T v_{\\text{in}}) $$ Effect: We typically only update 6 vectors per step (1 pos + 5 negs) instead of 100,000.\nLet\u0026rsquo;s see this in code:\nimport torch import torch.nn as nn import torch.optim as optim class SkipGramNegativeSampling(nn.Module): \u0026#34;\u0026#34;\u0026#34; Skip-Gram with Negative Sampling (SGNS) implementation. SGNS approximates the softmax over the entire vocabulary by instead distinguishing between a real context word (positive) and K noise words (negative). \u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embedding_dim): super(SkipGramNegativeSampling, self).__init__() # Input embeddings: used when the word is the center word self.in_embed = nn.Embedding(vocab_size, embedding_dim) # Output embeddings: used when the word is a context or negative sample self.out_embed = nn.Embedding(vocab_size, embedding_dim) # Initialize weights with small values to prevent gradient saturation initrange = 0.5 / embedding_dim self.in_embed.weight.data.uniform_(-initrange, initrange) self.out_embed.weight.data.uniform_(-initrange, initrange) def forward(self, center_words, target_words, negative_words): \u0026#34;\u0026#34;\u0026#34; Computes the negative sampling loss. Args: center_words: (batch_size) target_words: (batch_size) negative_words: (batch_size, K) where K is number of negative samples \u0026#34;\u0026#34;\u0026#34; # Retrieve vectors v_c = self.in_embed(center_words) # (batch_size, embed_dim) u_o = self.out_embed(target_words) # (batch_size, embed_dim) u_n = self.out_embed(negative_words) # (batch_size, K, embed_dim) # 1. Positive Score: log(sigmoid(v_c ¬∑ u_o)) # Compute dot product: (batch, 1, dim) @ (batch, dim, 1) -\u0026gt; (batch, 1) pos_score = torch.bmm(u_o.unsqueeze(1), v_c.unsqueeze(2)).squeeze(2) pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-7) # 2. Negative Score: sum(log(sigmoid(-v_c ¬∑ u_n))) # Compute dot products for all K samples: (batch, K, dim) @ (batch, dim, 1) -\u0026gt; (batch, K) neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2) neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-7), dim=1, keepdim=True) # Total loss is the negative of the objective function loss = -(pos_loss + neg_loss) return torch.mean(loss) # --- Configuration \u0026amp; Mock Data --- VOCAB_SIZE = 100 EMBED_DIM = 10 word_to_ix = {\u0026#39;fox\u0026#39;: 0} # Example vocabulary mapping model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock inputs: 1 center word, 1 target word, 5 negative samples center_id = torch.tensor([0], dtype=torch.long) target_id = torch.tensor([1], dtype=torch.long) negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long) # Training Step model.zero_grad() loss = model(center_id, target_id, negative_ids) loss.backward() optimizer.step() # Output Results print(f\u0026#34;Loss after one step: {loss.item():.6f}\u0026#34;) word_vec = model.in_embed(torch.tensor([word_to_ix[\u0026#39;fox\u0026#39;]])) print(f\u0026#34;Vector for \u0026#39;fox\u0026#39;:\\n{word_vec.detach().numpy()}\u0026#34;) The outputs:\nLoss after one step: 4.158468 Vector for \u0026#39;fox\u0026#39;: [[ 0.04197385 0.02400733 -0.03800093 0.01672485 -0.03872231 -0.0061478 0.0121122 0.04057864 -0.036255 0.03861175]] Interesting Facts About Skip-Gram üí° Makes Rare Words Shine: Skip-Gram is more effective at capturing the meaning of rare words because it focuses on the context of the center word. In CBOW, a rare word\u0026rsquo;s vector is averaged with others, diluting its signal. In Skip-Gram, when the rare word appears as the center, it gets the full, undivided attention of the backpropagation update.\nSlow Training: This method creates more training samples. A window of size 2 creates 4 training pairs per center word, while CBOW creates only 1 training pair per center word.\nSemantic Focus: Skip-Gram puts more emphasis on the semantic relationship between words. It views the context as the primary signal, with the center word as the target. This makes it better at capturing the meaning of words in context. Skip-Gram tends to capture semantic relationships (King/Queen) slightly better, while CBOW captures syntactic ones (walk/walked) slightly better.\n","permalink":"http://localhost:1313/posts/word-embeddings/","summary":"Understanding the mathematics behind Word2Vec, CBOW, and Skip-Gram and how they map language to vector space.","title":"Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n5. Code Examples: We\u0026rsquo;ll use sentencepiece for tokenization and compare results for different models.\nimport sentencepiece as spm import os # Configuration for training the SentencePiece model # SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words. options = { # The source text file used for learning the vocabulary \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # The base name for the generated model (.model) and vocabulary (.vocab) files \u0026#39;model_prefix\u0026#39;: \u0026#39;bpe_model\u0026#39;, # Number of unique tokens in the final vocabulary \u0026#39;vocab_size\u0026#39;: 4000, # \u0026#39;bpe\u0026#39; (Byte Pair Encoding) merges frequent pairs of characters/sequences \u0026#39;model_type\u0026#39;: \u0026#39;bpe\u0026#39;, # Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets \u0026#39;character_coverage\u0026#39;: 0.9995, # When enabled, unknown characters are decomposed into UTF-8 bytes to avoid \u0026#39;unk\u0026#39; tokens \u0026#39;byte_fallback\u0026#39;: True, # Treats digits individually (0-9), preventing large numbers from being treated as single tokens \u0026#39;split_digits\u0026#39;: True, # Prevents adding a whitespace prefix to the first token; useful for fine-grained control \u0026#39;add_dummy_prefix\u0026#39;: False } try: print(\u0026#34;Starting the training process...\u0026#34;) # SentencePieceTrainer.train takes the dictionary of options to build the BPE model spm.SentencePieceTrainer.train(**options) print(\u0026#34;Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created.\u0026#34;) # Initialize the processor and load the newly trained model sp = spm.SentencePieceProcessor() sp.load(\u0026#39;bpe_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Model Metadata:\u0026#34;) # Retrieve the total number of tokens in the vocabulary print(f\u0026#34;Total Vocab Size: {sp.get_piece_size()}\u0026#34;) # Special tokens are used for sequence boundaries and handling unknown characters print(f\u0026#34;BOS (Beginning of Sentence) ID: {sp.bos_id()}\u0026#34;) print(f\u0026#34;EOS (End of Sentence) ID: {sp.eos_id()}\u0026#34;) print(f\u0026#34;UNK (Unknown) ID: {sp.unk_id()}\u0026#34;) print(f\u0026#34;PAD (Padding) ID: {sp.pad_id()}\u0026#34;) # Test the tokenizer on sample strings to see how it breaks down text test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: shows the actual subword units (tokens) print(f\u0026#34;Subword Tokens: {sp.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: shows the numerical mapping for each token print(f\u0026#34;Numerical IDs: {sp.encode_as_ids(text)}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred during training or processing: {e}\u0026#34;) You can see the following output:\nStarting training... Starting the training process... Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created. ------------------------------ Model Metadata: Total Vocab Size: 4000 BOS (Beginning of Sentence) ID: 1 EOS (End of Sentence) ID: 2 UNK (Unknown) ID: 0 PAD (Padding) ID: -1 --- Tokenization Test --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;el\u0026#39;, \u0026#39;lo\u0026#39;, \u0026#39;‚ñÅW\u0026#39;, \u0026#39;orld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 380, 309, 476, 619, 36, 3924, 3964, 3959, 3993, 3978, 3983, 57, 3975, 3976, 60, 3974] --- Tokenization Test --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅmo\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;inst\u0026#39;, \u0026#39;ag\u0026#39;, \u0026#39;ram\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅever\u0026#39;] Numerical IDs: [1894, 304, 3284, 376, 321, 313, 312, 1008, 3313, 1325, 428, 3937, 341, 304, 3284, 1495] The ! is fallen back to UTF bytes. and the word instagrammable is split into multiple subword tokens. Also, original text can be recreated right back by simply concatenating the tokens. And replacing underscore by whitespace. Now we can try Unigram model. and see the same sentences tokenized differently.\nfrom tokenizers import Tokenizer from tokenizers.models import WordPiece from tokenizers.trainers import WordPieceTrainer from tokenizers.pre_tokenizers import Whitespace try: # 1. Initialize the WordPiece Tokenizer # We specify the [UNK] token for handling words not found in the vocabulary. tokenizer = Tokenizer(WordPiece(unk_token=\u0026#34;[UNK]\u0026#34;)) # 2. Configure Pre-tokenization # Before the subword algorithm runs, we need to split the raw text into words. # Whitespace splitting is the standard first step for most English NLP tasks. tokenizer.pre_tokenizer = Whitespace() # 3. Initialize the Trainer # We define our target vocabulary size and the special tokens required for # downstream tasks (like BERT\u0026#39;s [CLS] for classification or [SEP] for separators). trainer = WordPieceTrainer( vocab_size=4000, special_tokens=[\u0026#34;[UNK]\u0026#34;, \u0026#34;[CLS]\u0026#34;, \u0026#34;[SEP]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[MASK]\u0026#34;] ) # 4. Train the Model # The tokenizer scans the training file to build a vocabulary of the most # frequent subword units. tokenizer.train(files=[\u0026#34;train_data.txt\u0026#34;], trainer=trainer) # 5. Persist the Model # Save the configuration and vocabulary to a JSON file for future inference. tokenizer.save(\u0026#34;wordpiece.json\u0026#34;) print(\u0026#34;Training complete. \u0026#39;wordpiece.json\u0026#39; created.\u0026#34;) # 6. Metadata Inspection print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;WordPiece Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {tokenizer.get_vocab_size()}\u0026#34;) # 7. Testing Subword Tokenization # WordPiece shines at handling rare words by breaking them into meaningful chunks. test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (WordPiece) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # Encode converts raw text into a Tokenizer object containing tokens and IDs output = tokenizer.encode(text) # \u0026#39;tokens\u0026#39; shows the subword breakdown (e.g., \u0026#39;un\u0026#39;, \u0026#39;##insta\u0026#39;, etc.) print(f\u0026#34;Subword Tokens: {output.tokens}\u0026#34;) # \u0026#39;ids\u0026#39; are the numerical indices mapped to the vocabulary print(f\u0026#34;Numerical IDs: {output.ids}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred with WordPiece model: {e}\u0026#34;) You can see the following output:\nWordPiece Model Metadata: Total Vocab Size: 2609 --- Tokenization Test (WordPiece) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;H\u0026#39;, \u0026#39;##el\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;##or\u0026#39;, \u0026#39;##ld\u0026#39;, \u0026#39;[UNK]\u0026#39;, \u0026#39;[UNK]\u0026#39;] Numerical IDs: [37, 180, 214, 52, 162, 418, 0, 0] --- Tokenization Test (WordPiece) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;##os\u0026#39;, \u0026#39;##t\u0026#39;, \u0026#39;un\u0026#39;, \u0026#39;##ins\u0026#39;, \u0026#39;##ta\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;##ra\u0026#39;, \u0026#39;##m\u0026#39;, \u0026#39;##ma\u0026#39;, \u0026#39;##ble\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;ever\u0026#39;] Numerical IDs: [691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240] We can see the special character ## in the output. We can also see the breakdowns not having any semantic meaning. They are likelihood based. We can try the Unigram model now.\n# \u0026#39;Unigram\u0026#39; is the default and usually recommended over BPE in SentencePiece. options_unigram = { \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # Path to the raw text file for training \u0026#39;model_prefix\u0026#39;: \u0026#39;unigram_model\u0026#39;, # Prefix for the output .model and .vocab files \u0026#39;vocab_size\u0026#39;: 1200, # Desired size of the final vocabulary \u0026#39;model_type\u0026#39;: \u0026#39;unigram\u0026#39;, # Specifies the Unigram language model algorithm \u0026#39;character_coverage\u0026#39;: 0.9995, # Percentage of characters covered by the model (0.9995 is standard for Latin scripts) \u0026#39;byte_fallback\u0026#39;: True, # Enables mapping unknown characters to UTF-8 bytes to avoid \u0026lt;unk\u0026gt; tokens \u0026#39;split_digits\u0026#39;: True, # Treats each digit as an individual token (useful for numerical data) \u0026#39;add_dummy_prefix\u0026#39;: False # Prevents adding a leading space (SentencePiece default is True) } try: # 1. Train the SentencePiece model using the defined options print(\u0026#34;Starting Unigram training...\u0026#34;) spm.SentencePieceTrainer.train(**options_unigram) print(\u0026#34;Training complete. \u0026#39;unigram_model.model\u0026#39; created.\u0026#34;) # 2. Load the trained model into a processor instance for inference sp_unigram = spm.SentencePieceProcessor() sp_unigram.load(\u0026#39;unigram_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Unigram Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {sp_unigram.get_piece_size()}\u0026#34;) # 3. Define test cases to evaluate how the model handles common and rare words test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] # 4. Iterate through test sentences to visualize subword segmentation for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (Unigram) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: Converts text into subword strings (visual representation) print(f\u0026#34;Subword Tokens: {sp_unigram.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: Converts text into numerical indices for model input print(f\u0026#34;Numerical IDs: {sp_unigram.encode_as_ids(text)}\u0026#34;) except Exception as e: # Handle potential errors during training or loading (e.g., missing input file) print(f\u0026#34;An error occurred with Unigram model: {e}\u0026#34;) The output is as follows:\nUnigram Model Metadata: Total Vocab Size: 1200 --- Tokenization Test (Unigram) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ll\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;ld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311] --- Tokenization Test (Unigram) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;T\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅm\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;sta\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;ra\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ver\u0026#39;] ","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"A comprehensive guide to tokenization strategies: BPE, WordPiece, Unigram, and SentencePiece.","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"A brief introduction to Vectors \u0026amp; Verbs and formatting verification.","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"}]