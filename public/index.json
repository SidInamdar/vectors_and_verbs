[{"content":"The DNA of Language: A Deep Dive into popular LLM Tokenization concepts Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\nByte Pair Encoding (BPE) It is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most important, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces.\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003ch2 id=\"the-dna-of-language-a-deep-dive-into-popular-llm-tokenization-concepts\"\u003eThe DNA of Language: A Deep Dive into popular LLM Tokenization concepts\u003c/h2\u003e\n\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated—they’re crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\nByte Pair Encoding (BPE) It is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most important, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces.\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated—they’re crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\nByte Pair Encoding (BPE) It is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most important, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces.\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated—they’re crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\nByte Pair Encoding (BPE) It is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most important, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces.\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated—they’re crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\nByte Pair Encoding (BPE) It is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most important, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces.\nWordPiece To be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated—they’re crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\nByte Pair Encoding (BPE) It is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most important, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces.\nWordPiece To be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated—they’re crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) It is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most important, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces.\nWordPiece To be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated—they’re crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"},{"content":"Imagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\nThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\n1. Byte Pair Encoding (BPE) It is the most popular tokenization strategy used in LLMs mostly because it made the engineers worry less about confronting the problem of missing tokens. It recieved massive popularity after the release of GPT-2, GPT-3 and Llama and most important, it is a simple bottoms up frequency based strategy.\nHow does it work? BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree like subword joining structure. By favouring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words and phrases are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nThe Byte level BPE goes even further to start the process with the UTF-8 byte as a character. This ensures that the tokenization process is language agnostic and can handle any language, even those languages that use non-latin scripts or cannot be split by spaces.\n2. WordPiece To be continued\u0026hellip;\n","permalink":"http://localhost:1313/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house only with massive boulders as walls (too big) or one cannot build a house only with tiny pebbles (too small). We need exactly the right sized bricks. Same analogy applies for linguistics. We need to figure out strategies to break down petabytes of data into usable atomic chunks.\u003c/p\u003e\n\u003cp\u003eThe the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to view a sizable amount of fluid language data into discrete mathematical language which machines can process. It is the invisible filter in the heart of LLMs where every prompt is passed and every response is born. Let\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"http://localhost:1313/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated—they’re crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"http://localhost:1313/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"}]