[{"content":"","permalink":"https://vectorsandverbs.com/posts/glove-and-fasttext/","summary":"","title":"GloVe and FastText"},{"content":" Before we had Large Language Models writing poetry, we had to teach computers that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; are related not just by spelling, but by meaning. This is the story of that breakthrough. It‚Äôs the moment we stopped counting words and started mapping their souls‚Äîturning raw text into a mathematical landscape where math can solve analogies. Welcome to the world of Word2Vec. üîÆ\nLanguage models require vector representations of words to capture semantic relationships. Before the 2010s, models used word count-based vector representations that captured only the frequency of words (e.g., One-Hot Encoding). The Problems: üöß\nThe Curse of Dimensionality Lack of meaning (Synonyms were treated as mathematically unrelated). In 2011, Mikolov et al. at Google introduced Word2Vec, which used a shallow neural network to learn vector representations of words. The best part? It could create much denser relationships between words than prior models, and it was unsupervised.\nThe training (prediction) is essentially \u0026ldquo;fake\u0026rdquo;‚Äîa pretext task. But the weight matrices we get on the side are a gold mine of fine-grained semantic relationships. ‚õèÔ∏è\nThere are two popular variants of Word2Vec:\nSkipGram Continuous Bag Of Words (CBOW) 1. Continuous Bag of Words (CBOW) üß© CBOW is a simple \u0026lsquo;fill in the blanks\u0026rsquo; machine. It takes the surrounding words as inputs and tries to predict the center word.\n\u0026ldquo;Continuous\u0026rdquo;: It operates in continuous vector space (unlike the discrete space of n-gram models).\n\u0026ldquo;Bag of words\u0026rdquo;: The order of the context does not matter. \u0026ldquo;The cat sat on the mat\u0026rdquo; and \u0026ldquo;The mat sat on the cat\u0026rdquo; are the same for CBOW. They produce exactly the same predictions.\nObjective: Maximize the probability of the target word $w_t$ given its context $w_{t-C}, \\dots, w_{t+C}$.\n2. Training Defining Dimensions:\n$V$: Vocabulary size (number of unique words in the corpus, e.g., 10,000) $N$: Dimension of the embedding space (e.g., 100) $C$: Number of context words (e.g., 2 words on left of target and 2 words on right) Step 1: Input Lookup \u0026amp; One-Hot Encoding\nMathematically, we represent each context word as a one-hot vector $x^{(c)}$. We have an Input Matrix $W_{in}$ of size $V \\times N$. For each context word (e.g., \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;), we select its corresponding row from $W_{in}$.\n$$ v_c = W_{in}^T x^{(c)} $$\nWhere $v_c$ is the $N$-dimensional vector for context word $c$.\nStep 2: Projection (The \u0026ldquo;Mixing\u0026rdquo;)\nWe combine the context vectors. In standard CBOW, we simply average them. This creates the hidden layer vector $h$.\n$$ h = \\frac{1}{2C} \\sum_{c=1}^{2C} v_c $$\n$h$ is a single vector of size $N \\times 1$. Note: This operation is linear and contains no activation function (like ReLU or Sigmoid).\nStep 3: Output Scoring (The \u0026ldquo;Dot Product\u0026rdquo;)\nWe have a second matrix, the Output Matrix $W_{out}$ of size $N \\times V$. We compute the raw score (logit) $u_j$ for every word $j$ in the vocabulary by taking the dot product of the hidden vector $h$ and the output vector $v\u0026rsquo;_j$.\n$$ u_j = {v\u0026rsquo;_j}^T h $$\nOr in matrix form:\n$$ u = W_{out}^T h $$\nResult $u$ is a vector of size $V \\times 1$.\nStep 4: Probability Conversion (Softmax)\nWe convert the raw scores $u$ into probabilities using the Softmax function. This tells us the probability that word $w_j$ is the center word.\n$$ y_j = P(w_j | \\text{context}) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 5: Loss Calculation (Cross-Entropy)\nWe compare the predicted distribution $y$ with the actual ground truth $t$ (a one-hot vector where the index of the true word is 1). The loss function $E$ is:\n$$ E = - \\sum_{j=1}^V t_j \\log(y_j) $$\nSince $t_j$ is 0 for all words except the target word (let\u0026rsquo;s call the target index $j^*$), this simplifies to:\n$$ E = - \\log(y_{j^*}) $$\n3. Backpropagation Details We need to update the weights to minimize $E$. We use the Chain Rule.\nA. Gradient w.r.t. Output Matrix ($W_{out}$)\nWe want to know how the error changes with respect to the raw score $u_j$.\n$$ \\frac{\\partial E}{\\partial u_j} = y_j - t_j $$\nLet\u0026rsquo;s call this error term $e_j$.\nIf $j$ is the target word: $e_j = y_j - 1$ (Result is negative; we boost the score). If $j$ is not target: $e_j = y_j - 0$ (Result is positive; we suppress the score). The update rule for the output vector $v\u0026rsquo;_j$ becomes:\n$$ v'_{j}(\\text{new}) = v'_{j}(\\text{old}) - \\eta \\cdot e_j \\cdot h $$ ($\\eta$ is the learning rate)\nB. Gradient w.r.t. Hidden Layer ($h$)\nWe backpropagate the error from the output layer to the hidden layer.\n$$ EH = \\sum_{j=1}^V e_j \\cdot v\u0026rsquo;_j $$\n$EH$ is an $N$-dimensional vector representing the aggregate error passed back to the projection layer.\nC. Gradient w.r.t. Input Matrix ($W_{in}$)\nSince $h$ was just an average of the input vectors, the error $EH$ is distributed to each context word\u0026rsquo;s input vector. For every word $w_c$ in the context:\n$$ v_{w_c}(\\text{new}) = v_{w_c}(\\text{old}) - \\eta \\cdot \\frac{1}{2C} \\cdot EH $$\nLet\u0026rsquo;s build a single pass through the network for a given context.\nimport torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # Define a simple Continuous Bag of Words (CBOW) style model class CBOWModel(nn.Module): def __init__(self, vocab_size, embedding_dim): super(CBOWModel, self).__init__() # The embedding layer stores the word vectors we want to learn self.embeddings = nn.Embedding(vocab_size, embedding_dim) # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word self.linear = nn.Linear(embedding_dim, vocab_size) def forward(self, inputs): # inputs: tensor of word indices for the context embeds = self.embeddings(inputs) # Aggregate context by calculating the mean of the word embeddings h = torch.mean(embeds, dim=1) # Produce logits (raw scores) for each word in the vocabulary logits = self.linear(h) return logits # Setup vocabulary and mappings word_to_ix = {\u0026#34;the\u0026#34;: 0, \u0026#34;quick\u0026#34;: 1, \u0026#34;brown\u0026#34;: 2, \u0026#34;fox\u0026#34;: 3, \u0026#34;jumps\u0026#34;: 4, \u0026#34;over\u0026#34;: 5, \u0026#34;lazy\u0026#34;: 6, \u0026#34;dog\u0026#34;: 7} ix_to_word = {v: k for k, v in word_to_ix.items()} # Configuration constants EMBEDDING_DIM = 5 VOCAB_SIZE = len(word_to_ix) LEARNING_RATE = 0.01 # Initialize the model, loss function, and optimizer model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM) loss_function = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE) # Prepare dummy training data: context words and the target word # Context: [\u0026#34;quick\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;over\u0026#34;] -\u0026gt; Target: \u0026#34;fox\u0026#34; context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long) target_idxs = torch.tensor([3], dtype=torch.long) # Perform a single optimization step model.zero_grad() # Forward pass: we reshape context to (1, -1) to simulate a batch of size 1 logits = model(context_idxs.view(1, -1)) # Calculate loss against the target word index loss = loss_function(logits, target_idxs) # Backpropagate and update weights loss.backward() optimizer.step() # Output results for the blog post print(\u0026#34;=== Model Training Snapshot ===\u0026#34;) print(f\u0026#34;Calculated Loss: {loss.item():.6f}\u0026#34;) print(f\u0026#34;\\n=== Learned Vector for \u0026#39;jumps\u0026#39; ===\u0026#34;) word_vec = model.embeddings(torch.tensor([word_to_ix[\u0026#39;jumps\u0026#39;]])) print(word_vec.detach().numpy()) print(\u0026#34;\\n=== Embedding Matrix (Weights) ===\u0026#34;) print(model.embeddings.weight.detach().numpy()) print(\u0026#34;\\n=== Linear Layer Weights ===\u0026#34;) print(model.linear.weight.detach().numpy()) Output shows single pass loss and learned vector for \u0026lsquo;jumps\u0026rsquo; word. This is a single training sample result but after multiple such passes both Embedding and linear weights should be very similar.\n=== Model Training Snapshot === Calculated Loss: 1.989656 === Learned Vector for \u0026#39;jumps\u0026#39; === [[-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775]] === Embedding Matrix (Weights) === [[ 1.66039 -0.11371879 0.6246518 0.35860053 -1.2504417 ] [-2.1847186 -0.77199775 -0.17050214 -0.38411248 -0.03913084] [ 0.11852697 0.90073633 0.8847807 0.7404524 0.900149 ] [ 0.07440972 -0.40259898 2.6246994 -0.08851447 0.02660969] [-1.5040519 -0.5602162 -0.11328011 -0.67929274 -0.84375775] [-0.9245572 -0.5545908 0.9083091 -1.0755049 0.84047747] [-1.0237687 0.59466314 0.05621134 -0.6202532 1.3664424 ] [ 0.60998917 -1.0549186 1.6103884 0.8724912 -1.2486908 ]] === Linear Layer Weights === [[ 0.02165058 -0.28883642 0.14545658 -0.3442509 0.32704315] [-0.18731792 0.28583744 0.22635977 0.13245736 0.29019794] [ 0.3158916 -0.15826383 -0.03203773 0.16377363 -0.41457543] [-0.05080034 0.4180087 0.11228557 0.30218413 0.3025514 ] [-0.38419306 0.24475925 -0.39210224 -0.38660625 -0.2673145 ] [-0.32321444 0.12200444 -0.03569533 0.2891424 -0.07345333] [-0.33704326 0.2521956 0.31587374 0.22590035 0.29866052] [ 0.4117266 -0.44231793 0.24064957 -0.29684234 0.333821 ]] Interesting Facts About CBOW üß† The \u0026ldquo;Averaging Problem\u0026rdquo;: This is a common issue in CBOW models. The averaging and training is order-agnostic. All variants of word order in the context window will generate the same output. \u0026ldquo;Dog bit the man\u0026rdquo; and \u0026ldquo;Man bit the dog\u0026rdquo; are the same in CBOW\u0026rsquo;s eyes.\nSmoothing Effect: CBOW models are much faster to train than Skip-Gram but are generally smoother.\nRare Words: CBOW models struggle slightly with rare words because the presence of surrounding common words (like articles and prepositions) can skew the embeddings and dissolve their uniqueness.\nComputational Cost: Softmax requires summing over the entire vocabulary size, which can be computationally expensive if $V$ is very large. Solutions include Hierarchical Softmax and Negative Sampling (approximating the denominator by only checking the target word vs. 5 random noise words).\nVectors as Double Agents: Every word has a different representation in the embedding matrix and the output matrix. Most of the time $W_{out}$ is discarded, but some consider averaging both $W_{in}$ and $W_{out}$ for slightly better performance.\nLinear Relationships: The famous analogy King - Man + Woman = Queen emerges from the model because of the linear relationships between the words.\nInitialization: This is a no-brainer but worth mentioning. Initialization to zero vectors would mean no learning. We must initialize randomly.\n2. Skip-Gram (The \u0026ldquo;Burger Flip\u0026rdquo; of CBOW) üîÑ Skip-Gram flips the CBOW logic. While CBOW tries to predict the center word given the context, Skip-Gram tries to predict the scattered context given the center word.\nCore Concept: If a word like \u0026ldquo;monarch\u0026rdquo; can accurately predict words like \u0026ldquo;king\u0026rdquo;, \u0026ldquo;throne\u0026rdquo;, \u0026ldquo;crown\u0026rdquo;, \u0026ldquo;royalty\u0026rdquo;, and \u0026ldquo;power\u0026rdquo;, then it must effectively capture the concept of royalty.\nTraining Let\u0026rsquo;s define dimensions:\n$V$: Vocabulary size $D$: Embedding dimension $C$: Number of context words Step 1: Input Lookup\nUnlike CBOW, our input is just one word vector (the center word $w_t$). We grab the vector $v_c$ from the Input Matrix $W_{in}$.\n$$ h = v_{w_t} $$\nNote: There is no averaging here. The hidden layer $h$ is simply the raw vector of the center word.\nStep 2: Output Scoring (The Broadcast)\nThe model takes this single vector $h$ and compares it against the Output Matrix $W_{out}$ (the entire vocabulary). It computes a score $u_j$ for every word $j$ in the dictionary.\n$$ u = W_{out}^T h $$\nResult $u$: A vector of size $V \\times 1$ containing raw scores.\nStep 3: Probability Conversion (Softmax)\nWe apply Softmax to turn scores into probabilities.\n$$ P(w_j | w_t) = \\frac{\\exp(u_j)}{\\sum_{k=1}^V \\exp(u_k)} $$\nStep 4: The Multi-Target Loss\nHere is the major difference from CBOW. In CBOW, we had one target (the center). In Skip-Gram, we have $2C$ targets (all the surrounding words). For a sentence \u0026ldquo;The cat sat on mat\u0026rdquo; (Center: \u0026ldquo;sat\u0026rdquo;), the targets are \u0026ldquo;The\u0026rdquo;, \u0026ldquo;cat\u0026rdquo;, \u0026ldquo;on\u0026rdquo;, \u0026ldquo;mat\u0026rdquo;. We want to maximize the probability of all these true context words. The Loss function ($E$) sums the error for each context word:\n$$ E = - \\sum_{w_{ctx} \\in \\text{Window}} \\log P(w_{ctx} | w_t) $$\nStep 5: Backpropagation (Accumulating Gradients)\nSince one input word is responsible for predicting multiple output words, the error signals from all those context words add up.\nAt the Output Layer:\nIf \u0026ldquo;cat\u0026rdquo; was a target, the \u0026ldquo;cat\u0026rdquo; vector in $W_{out}$ gets a signal: \u0026ldquo;Move closer to \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; If \u0026ldquo;apple\u0026rdquo; was NOT a target, the \u0026ldquo;apple\u0026rdquo; vector gets a signal: \u0026ldquo;Move away from \u0026lsquo;sat\u0026rsquo;.\u0026rdquo; At the Hidden Layer ($h$):\nThe center word \u0026ldquo;sat\u0026rdquo; receives feedback from all its neighbors simultaneously. Error signal = (Error from \u0026ldquo;The\u0026rdquo;) + (Error from \u0026ldquo;cat\u0026rdquo;) + (Error from \u0026ldquo;on\u0026rdquo;)\u0026hellip; The vector for \u0026ldquo;sat\u0026rdquo; in $W_{in}$ moves in the average direction of all its neighbors. 3. The \u0026ldquo;Computational Nightmare\u0026rdquo; \u0026amp; Negative Sampling The equations above describe Naive Softmax.\nThe Problem: If $V = 100,000$, computing the denominator $\\sum \\exp(u_k)$ takes 100,000 operations. Doing this for every word in a training set of billions of words is impossibly slow.\nThe Solution: Negative Sampling Instead of updating the entire vocabulary (1 correct word vs 99,999 wrong ones), we approximate the problem.\nPositive Pair: (sat, cat) $\\rightarrow$ Maximize probability (Label 1). Negative Pairs: We pick $K$ (e.g., 5) random words the model didn\u0026rsquo;t see, e.g., (sat, bulldozer), (sat, quantum). $\\rightarrow$ Minimize probability (Label 0). New Equation (Sigmoid instead of Softmax):\n$$ E = - \\log \\sigma({v'_{\\text{pos}}}^T v_{\\text{in}}) - \\sum_{k=1}^K \\log \\sigma(-{v'_{\\text{neg}_{k}}}^T v_{\\text{in}}) $$ Effect: We typically only update 6 vectors per step (1 pos + 5 negs) instead of 100,000.\nLet\u0026rsquo;s see this in code:\nimport torch import torch.nn as nn import torch.optim as optim class SkipGramNegativeSampling(nn.Module): \u0026#34;\u0026#34;\u0026#34; Skip-Gram with Negative Sampling (SGNS) implementation. SGNS approximates the softmax over the entire vocabulary by instead distinguishing between a real context word (positive) and K noise words (negative). \u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embedding_dim): super(SkipGramNegativeSampling, self).__init__() # Input embeddings: used when the word is the center word self.in_embed = nn.Embedding(vocab_size, embedding_dim) # Output embeddings: used when the word is a context or negative sample self.out_embed = nn.Embedding(vocab_size, embedding_dim) # Initialize weights with small values to prevent gradient saturation initrange = 0.5 / embedding_dim self.in_embed.weight.data.uniform_(-initrange, initrange) self.out_embed.weight.data.uniform_(-initrange, initrange) def forward(self, center_words, target_words, negative_words): \u0026#34;\u0026#34;\u0026#34; Computes the negative sampling loss. Args: center_words: (batch_size) target_words: (batch_size) negative_words: (batch_size, K) where K is number of negative samples \u0026#34;\u0026#34;\u0026#34; # Retrieve vectors v_c = self.in_embed(center_words) # (batch_size, embed_dim) u_o = self.out_embed(target_words) # (batch_size, embed_dim) u_n = self.out_embed(negative_words) # (batch_size, K, embed_dim) # 1. Positive Score: log(sigmoid(v_c ¬∑ u_o)) # Compute dot product: (batch, 1, dim) @ (batch, dim, 1) -\u0026gt; (batch, 1) pos_score = torch.bmm(u_o.unsqueeze(1), v_c.unsqueeze(2)).squeeze(2) pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-7) # 2. Negative Score: sum(log(sigmoid(-v_c ¬∑ u_n))) # Compute dot products for all K samples: (batch, K, dim) @ (batch, dim, 1) -\u0026gt; (batch, K) neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2) neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-7), dim=1, keepdim=True) # Total loss is the negative of the objective function loss = -(pos_loss + neg_loss) return torch.mean(loss) # --- Configuration \u0026amp; Mock Data --- VOCAB_SIZE = 100 EMBED_DIM = 10 word_to_ix = {\u0026#39;fox\u0026#39;: 0} # Example vocabulary mapping model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM) optimizer = optim.SGD(model.parameters(), lr=0.01) # Mock inputs: 1 center word, 1 target word, 5 negative samples center_id = torch.tensor([0], dtype=torch.long) target_id = torch.tensor([1], dtype=torch.long) negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long) # Training Step model.zero_grad() loss = model(center_id, target_id, negative_ids) loss.backward() optimizer.step() # Output Results print(f\u0026#34;Loss after one step: {loss.item():.6f}\u0026#34;) word_vec = model.in_embed(torch.tensor([word_to_ix[\u0026#39;fox\u0026#39;]])) print(f\u0026#34;Vector for \u0026#39;fox\u0026#39;:\\n{word_vec.detach().numpy()}\u0026#34;) The outputs:\nLoss after one step: 4.158468 Vector for \u0026#39;fox\u0026#39;: [[ 0.04197385 0.02400733 -0.03800093 0.01672485 -0.03872231 -0.0061478 0.0121122 0.04057864 -0.036255 0.03861175]] Interesting Facts About Skip-Gram üí° Makes Rare Words Shine: Skip-Gram is more effective at capturing the meaning of rare words because it focuses on the context of the center word. In CBOW, a rare word\u0026rsquo;s vector is averaged with others, diluting its signal. In Skip-Gram, when the rare word appears as the center, it gets the full, undivided attention of the backpropagation update.\nSlow Training: This method creates more training samples. A window of size 2 creates 4 training pairs per center word, while CBOW creates only 1 training pair per center word.\nSemantic Focus: Skip-Gram puts more emphasis on the semantic relationship between words. It views the context as the primary signal, with the center word as the target. This makes it better at capturing the meaning of words in context. Skip-Gram tends to capture semantic relationships (King/Queen) slightly better, while CBOW captures syntactic ones (walk/walked) slightly better.\n","permalink":"https://vectorsandverbs.com/posts/word-embeddings/","summary":"\u003cp\u003e\u003cimg alt=\"SkipGram and CBOW Process Diagram\" loading=\"lazy\" src=\"/images/embedding-concept-process-flow.png\"\u003e\nBefore we had Large Language Models writing poetry, we had to teach computers that \u0026ldquo;king\u0026rdquo; and \u0026ldquo;queen\u0026rdquo; are related not just by spelling, but by \u003cem\u003emeaning\u003c/em\u003e. This is the story of that breakthrough. It‚Äôs the moment we stopped counting words and started mapping their souls‚Äîturning raw text into a mathematical landscape where math can solve analogies. Welcome to the world of \u003cstrong\u003eWord2Vec\u003c/strong\u003e. üîÆ\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLanguage models\u003c/strong\u003e require vector representations of words to capture semantic relationships.\u003c/li\u003e\n\u003cli\u003eBefore the 2010s, models used word count-based vector representations that captured only the frequency of words (e.g., One-Hot Encoding).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eThe Problems:\u003c/strong\u003e üöß\u003c/p\u003e","title":"Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram"},{"content":"Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\nThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\nLet\u0026rsquo;s dive into the most popular tokenization strategies used in LLMs today.\n1. Byte Pair Encoding (BPE) Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an \u0026ldquo;out of vocabulary\u0026rdquo; token (\u0026lt;UNK\u0026gt;). Rare words like \u0026ldquo;uninstagrammable\u0026rdquo; were the usual victims of this process.\nBPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.\nBPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.\nModern Byte-level BPE goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.\nThe tokenization operates in 4 steps:\nInitialization: The process starts with a vocabulary of single characters. Pair Counting: The process counts the frequency of each pair of characters. Merge: The process merges the most frequent pairs of characters. Iteration: The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000). The \u0026ldquo;SolidGoldMagikarp\u0026rdquo; üêü phenomenon: This is a famous instance of \u0026ldquo;glitch tokens\u0026rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of \u0026ldquo;glitch tokens.\u0026rdquo; In GPT-2 and GPT-3, specific strings like \u0026ldquo;SolidGoldMagikarp\u0026rdquo; (a Reddit username) or \u0026ldquo;StreamerBot\u0026rdquo; cause the model to hallucinate or break down.\nGPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don\u0026rsquo;t want to merge a word with the punctuation following it (e.g., \u0026ldquo;dog\u0026rdquo; and \u0026ldquo;.\u0026rdquo; becoming \u0026ldquo;dog.\u0026rdquo;).\n2. WordPiece WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.\nSuperficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is: $$ \\text{Score} = \\frac{\\text{Frequency}(AB)}{\\text{Frequency}(A) \\times \\text{Frequency}(B)} $$ This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently. The algorithm wrorks in two phases:\nTraining: The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix \u0026lsquo;##\u0026rsquo; so that the parts which start the word are distinct from those which trail in a word.\nInference: Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a \u0026ldquo;greedy longest-match-first\u0026rdquo; strategy). Given a word like \u0026ldquo;hugs\u0026rdquo;, it checks if the full word is in the vocabulary.\n‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., \u0026ldquo;hug\u0026rdquo;).\n‚Ä¢ It then attempts to tokenize the remainder (\u0026ldquo;s\u0026rdquo;) using the ## prefix (e.g., \u0026ldquo;##s\u0026rdquo;).\n‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.\nThe model has certain pitfalls. The \u0026lt;UNK\u0026gt; token acts as a \u0026ldquo;hard cliff,\u0026rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, \u0026ldquo;advice\u0026rdquo; and \u0026ldquo;advises\u0026rdquo; are tokenized entirely differently.\n3. Unigram: Chipping away unwanted tokens Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: \u0026lsquo;Which breakdown of the text will maximizes the likelihood of the data?\u0026rsquo;\nA single word can be tokenized in multiple ways. For example hugs can be tokenized as:\n[hug, s] [h, ug, s] [h, u, g, s] Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:\n1. Initialization: The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.\n2. Expectation (calculating Loss): The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can \u0026ldquo;explain\u0026rdquo; the training text.\n3. Maximization (pruning): For every possible token in the vocabulary, the model calculates: \u0026lsquo;How much will the overall loss change if we remove this token?\u0026rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.\n4. Selection: The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.\nBecause Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the Viterbi algorithm during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.\nUnigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the \u0026lsquo;best\u0026rsquo; tokenization always but sometimes a \u0026lsquo;good enough\u0026rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.\nRecent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram\u0026rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.\n4. SentencePiece: The Universal Adapter Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: \u0026lsquo;Hello World\u0026rsquo; becomes _Hello _World.\nLossless Reconstruction: The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.\nIt is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram. Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream. ‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE\u0026rsquo;s merge-based efficiency while retaining SentencePiece\u0026rsquo;s language-agnostic handling of Unicode.\nHandling unknowns: The model provides an option called byte fallback, so whenever the model identifies a token as unknown, instead of adding it as \u0026lt;UNK\u0026gt;, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.\nSentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.\n5. Code Examples: We\u0026rsquo;ll use sentencepiece for tokenization and compare results for different models.\nimport sentencepiece as spm import os # Configuration for training the SentencePiece model # SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words. options = { # The source text file used for learning the vocabulary \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # The base name for the generated model (.model) and vocabulary (.vocab) files \u0026#39;model_prefix\u0026#39;: \u0026#39;bpe_model\u0026#39;, # Number of unique tokens in the final vocabulary \u0026#39;vocab_size\u0026#39;: 4000, # \u0026#39;bpe\u0026#39; (Byte Pair Encoding) merges frequent pairs of characters/sequences \u0026#39;model_type\u0026#39;: \u0026#39;bpe\u0026#39;, # Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets \u0026#39;character_coverage\u0026#39;: 0.9995, # When enabled, unknown characters are decomposed into UTF-8 bytes to avoid \u0026#39;unk\u0026#39; tokens \u0026#39;byte_fallback\u0026#39;: True, # Treats digits individually (0-9), preventing large numbers from being treated as single tokens \u0026#39;split_digits\u0026#39;: True, # Prevents adding a whitespace prefix to the first token; useful for fine-grained control \u0026#39;add_dummy_prefix\u0026#39;: False } try: print(\u0026#34;Starting the training process...\u0026#34;) # SentencePieceTrainer.train takes the dictionary of options to build the BPE model spm.SentencePieceTrainer.train(**options) print(\u0026#34;Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created.\u0026#34;) # Initialize the processor and load the newly trained model sp = spm.SentencePieceProcessor() sp.load(\u0026#39;bpe_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Model Metadata:\u0026#34;) # Retrieve the total number of tokens in the vocabulary print(f\u0026#34;Total Vocab Size: {sp.get_piece_size()}\u0026#34;) # Special tokens are used for sequence boundaries and handling unknown characters print(f\u0026#34;BOS (Beginning of Sentence) ID: {sp.bos_id()}\u0026#34;) print(f\u0026#34;EOS (End of Sentence) ID: {sp.eos_id()}\u0026#34;) print(f\u0026#34;UNK (Unknown) ID: {sp.unk_id()}\u0026#34;) print(f\u0026#34;PAD (Padding) ID: {sp.pad_id()}\u0026#34;) # Test the tokenizer on sample strings to see how it breaks down text test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: shows the actual subword units (tokens) print(f\u0026#34;Subword Tokens: {sp.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: shows the numerical mapping for each token print(f\u0026#34;Numerical IDs: {sp.encode_as_ids(text)}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred during training or processing: {e}\u0026#34;) You can see the following output:\nStarting training... Starting the training process... Training complete. \u0026#39;bpe_model.model\u0026#39; and \u0026#39;bpe_model.vocab\u0026#39; have been created. ------------------------------ Model Metadata: Total Vocab Size: 4000 BOS (Beginning of Sentence) ID: 1 EOS (End of Sentence) ID: 2 UNK (Unknown) ID: 0 PAD (Padding) ID: -1 --- Tokenization Test --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;el\u0026#39;, \u0026#39;lo\u0026#39;, \u0026#39;‚ñÅW\u0026#39;, \u0026#39;orld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 380, 309, 476, 619, 36, 3924, 3964, 3959, 3993, 3978, 3983, 57, 3975, 3976, 60, 3974] --- Tokenization Test --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅmo\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;inst\u0026#39;, \u0026#39;ag\u0026#39;, \u0026#39;ram\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;log\u0026#39;, \u0026#39;‚ñÅever\u0026#39;] Numerical IDs: [1894, 304, 3284, 376, 321, 313, 312, 1008, 3313, 1325, 428, 3937, 341, 304, 3284, 1495] The ! is fallen back to UTF bytes. and the word instagrammable is split into multiple subword tokens. Also, original text can be recreated right back by simply concatenating the tokens. And replacing underscore by whitespace. Now we can try Unigram model. and see the same sentences tokenized differently.\nfrom tokenizers import Tokenizer from tokenizers.models import WordPiece from tokenizers.trainers import WordPieceTrainer from tokenizers.pre_tokenizers import Whitespace try: # 1. Initialize the WordPiece Tokenizer # We specify the [UNK] token for handling words not found in the vocabulary. tokenizer = Tokenizer(WordPiece(unk_token=\u0026#34;[UNK]\u0026#34;)) # 2. Configure Pre-tokenization # Before the subword algorithm runs, we need to split the raw text into words. # Whitespace splitting is the standard first step for most English NLP tasks. tokenizer.pre_tokenizer = Whitespace() # 3. Initialize the Trainer # We define our target vocabulary size and the special tokens required for # downstream tasks (like BERT\u0026#39;s [CLS] for classification or [SEP] for separators). trainer = WordPieceTrainer( vocab_size=4000, special_tokens=[\u0026#34;[UNK]\u0026#34;, \u0026#34;[CLS]\u0026#34;, \u0026#34;[SEP]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[MASK]\u0026#34;] ) # 4. Train the Model # The tokenizer scans the training file to build a vocabulary of the most # frequent subword units. tokenizer.train(files=[\u0026#34;train_data.txt\u0026#34;], trainer=trainer) # 5. Persist the Model # Save the configuration and vocabulary to a JSON file for future inference. tokenizer.save(\u0026#34;wordpiece.json\u0026#34;) print(\u0026#34;Training complete. \u0026#39;wordpiece.json\u0026#39; created.\u0026#34;) # 6. Metadata Inspection print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;WordPiece Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {tokenizer.get_vocab_size()}\u0026#34;) # 7. Testing Subword Tokenization # WordPiece shines at handling rare words by breaking them into meaningful chunks. test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (WordPiece) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # Encode converts raw text into a Tokenizer object containing tokens and IDs output = tokenizer.encode(text) # \u0026#39;tokens\u0026#39; shows the subword breakdown (e.g., \u0026#39;un\u0026#39;, \u0026#39;##insta\u0026#39;, etc.) print(f\u0026#34;Subword Tokens: {output.tokens}\u0026#34;) # \u0026#39;ids\u0026#39; are the numerical indices mapped to the vocabulary print(f\u0026#34;Numerical IDs: {output.ids}\u0026#34;) except Exception as e: print(f\u0026#34;An error occurred with WordPiece model: {e}\u0026#34;) You can see the following output:\nWordPiece Model Metadata: Total Vocab Size: 2609 --- Tokenization Test (WordPiece) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;H\u0026#39;, \u0026#39;##el\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;##or\u0026#39;, \u0026#39;##ld\u0026#39;, \u0026#39;[UNK]\u0026#39;, \u0026#39;[UNK]\u0026#39;] Numerical IDs: [37, 180, 214, 52, 162, 418, 0, 0] --- Tokenization Test (WordPiece) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;This\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;##os\u0026#39;, \u0026#39;##t\u0026#39;, \u0026#39;un\u0026#39;, \u0026#39;##ins\u0026#39;, \u0026#39;##ta\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;##ra\u0026#39;, \u0026#39;##m\u0026#39;, \u0026#39;##ma\u0026#39;, \u0026#39;##ble\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;##lo\u0026#39;, \u0026#39;##g\u0026#39;, \u0026#39;ever\u0026#39;] Numerical IDs: [691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240] We can see the special character ## in the output. We can also see the breakdowns not having any semantic meaning. They are likelihood based. We can try the Unigram model now.\n# \u0026#39;Unigram\u0026#39; is the default and usually recommended over BPE in SentencePiece. options_unigram = { \u0026#39;input\u0026#39;: \u0026#39;train_data.txt\u0026#39;, # Path to the raw text file for training \u0026#39;model_prefix\u0026#39;: \u0026#39;unigram_model\u0026#39;, # Prefix for the output .model and .vocab files \u0026#39;vocab_size\u0026#39;: 1200, # Desired size of the final vocabulary \u0026#39;model_type\u0026#39;: \u0026#39;unigram\u0026#39;, # Specifies the Unigram language model algorithm \u0026#39;character_coverage\u0026#39;: 0.9995, # Percentage of characters covered by the model (0.9995 is standard for Latin scripts) \u0026#39;byte_fallback\u0026#39;: True, # Enables mapping unknown characters to UTF-8 bytes to avoid \u0026lt;unk\u0026gt; tokens \u0026#39;split_digits\u0026#39;: True, # Treats each digit as an individual token (useful for numerical data) \u0026#39;add_dummy_prefix\u0026#39;: False # Prevents adding a leading space (SentencePiece default is True) } try: # 1. Train the SentencePiece model using the defined options print(\u0026#34;Starting Unigram training...\u0026#34;) spm.SentencePieceTrainer.train(**options_unigram) print(\u0026#34;Training complete. \u0026#39;unigram_model.model\u0026#39; created.\u0026#34;) # 2. Load the trained model into a processor instance for inference sp_unigram = spm.SentencePieceProcessor() sp_unigram.load(\u0026#39;unigram_model.model\u0026#39;) print(\u0026#34;-\u0026#34; * 30) print(\u0026#34;Unigram Model Metadata:\u0026#34;) print(f\u0026#34;Total Vocab Size: {sp_unigram.get_piece_size()}\u0026#34;) # 3. Define test cases to evaluate how the model handles common and rare words test_sentences = [ \u0026#39;Hello World! 1234567890\u0026#39;, \u0026#39;This blog is the most uninstagrammable blog ever\u0026#39; ] # 4. Iterate through test sentences to visualize subword segmentation for text in test_sentences: print(\u0026#34;\\n--- Tokenization Test (Unigram) ---\u0026#34;) print(f\u0026#34;Original Text: {text}\u0026#34;) # encode_as_pieces: Converts text into subword strings (visual representation) print(f\u0026#34;Subword Tokens: {sp_unigram.encode_as_pieces(text)}\u0026#34;) # encode_as_ids: Converts text into numerical indices for model input print(f\u0026#34;Numerical IDs: {sp_unigram.encode_as_ids(text)}\u0026#34;) except Exception as e: # Handle potential errors during training or loading (e.g., missing input file) print(f\u0026#34;An error occurred with Unigram model: {e}\u0026#34;) The output is as follows:\nUnigram Model Metadata: Total Vocab Size: 1200 --- Tokenization Test (Unigram) --- Original Text: Hello World! 1234567890 Subword Tokens: [\u0026#39;\u0026lt;0x48\u0026gt;\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ll\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;ld\u0026#39;, \u0026#39;\u0026lt;0x21\u0026gt;\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;\u0026lt;0x36\u0026gt;\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;\u0026lt;0x39\u0026gt;\u0026#39;, \u0026#39;0\u0026#39;] Numerical IDs: [75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311] --- Tokenization Test (Unigram) --- Original Text: This blog is the most uninstagrammable blog ever Subword Tokens: [\u0026#39;T\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅis\u0026#39;, \u0026#39;‚ñÅthe\u0026#39;, \u0026#39;‚ñÅm\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;st\u0026#39;, \u0026#39;‚ñÅun\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;sta\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;ra\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;able\u0026#39;, \u0026#39;‚ñÅb\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;‚ñÅ\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;ver\u0026#39;] ","permalink":"https://vectorsandverbs.com/posts/the-dna-of-language/","summary":"\u003cp\u003eImagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.\u003c/p\u003e\n\u003cp\u003eThe same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.\u003c/p\u003e","title":"The DNA of Language: A Deep Dive into LLM Tokenization concepts"},{"content":"Welcome to Vectors \u0026amp; Verbs This is a demo post to verify the PaperMod theme setup.\nFeatures of this theme: Clean and minimal design Dark mode support Fast loading speed def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) Stay tuned for more updates!\n","permalink":"https://vectorsandverbs.com/posts/first-post/","summary":"\u003ch2 id=\"welcome-to-vectors--verbs\"\u003eWelcome to Vectors \u0026amp; Verbs\u003c/h2\u003e\n\u003cp\u003eThis is a demo post to verify the PaperMod theme setup.\u003c/p\u003e\n\u003ch3 id=\"features-of-this-theme\"\u003eFeatures of this theme:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eClean and minimal design\u003c/li\u003e\n\u003cli\u003eDark mode support\u003c/li\u003e\n\u003cli\u003eFast loading speed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003ehello_world\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;Hello, Hugo!\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eStay tuned for more updates!\u003c/p\u003e","title":"My First Post"},{"content":"Siddhesh Inamdar Lead Data Scientist | M.Tech, IIT Madras | BITS Goa\nI believe the most powerful data-driven solutions aren\u0026rsquo;t just calculated‚Äîthey‚Äôre crafted. To me, the magic happens at the intersection of algorithmic precision and creative intuition. I dedicate my time to deconstructing frameworks and exploring models, not just for their logic, but for the unique ingenuity they bring to the table. This is where I share my perspective on the technologies that are truly redefining our field.\nExpertise \u0026amp; Domains Optimization \u0026amp; Predictive Maintenance: Developed Monte Carlo Markov Chain (MCMC) optimizers and discrete programming workflows (CP-SAT) that have delivered tens of millions of dollars in annual value. Deep Learning \u0026amp; NLP: Experienced in deploying Large Language Model (LLM) applications using RAG (Retrieval-Augmented Generation), fine-tuned Llama models, and Langchain to automate complex business queries. Time-Series \u0026amp; Forecasting: Leveraged Temporal Fusion Transformers and Variational Autoencoders for high-stakes uplift prediction and sensor anomaly detection across thousands of assets. MLOps \u0026amp; Engineering: Specialized in building agile, production-ready stacks including Docker, AWS Lambda, Snowflake, and Azure Databricks. Professional Journey Team Leadership \u0026amp; Strategy Currently, I lead a specialized data science team within the industrial machinery and energy sector. My focus is on defining deployment strategies and building agile-focused development stacks.\nAdvanced Technology Development Previously, I spent several years developing high-impact AI solutions for global energy leaders. My work involved:\nBuilding autonomous recommendation tools using One-Class Support Vector Machines to reduce human dependency in asset optimization. Designing Generative AI chatbots that allow field specialists to interact with complex optimization logs via natural language. Implementing Computer Vision and Sensor Analytics to improve production efficiency by filtering noise from thousands of industrial flowline sensors. Technical Toolkit Category Tools \u0026amp; Technologies Languages Python (NumPy, Pandas, PyTorch), SQL, MATLAB AI/ML Pattern Recognition, Constrained Optimization, NLP, LLMs Engineering Docker, AWS (Lambda, ECR), Apache Airflow, Snowflake Frameworks Langchain, LangGraph, SIMULINK, COMSOL Let\u0026rsquo;s Connect I am always open to discussing the future of AI, autonomous agents, and scalable data architecture. So do contact me on my social handles!\n","permalink":"https://vectorsandverbs.com/about/","summary":"Lead Data Scientist specializing in AI-driven optimization, anomaly detection, and cloud-scale machine learning engineering.","title":"About Me"}]