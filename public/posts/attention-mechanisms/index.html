<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Attention Is All You Need, but exactly which one?: MHA, GQA and MLA | Vectors &amp; Verbs</title>
<meta name="keywords" content="artificial-intelligence, large-language-models, attention, transformer">
<meta name="description" content="We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek&rsquo;s MLA, how has the definition of &lsquo;Attention&rsquo; transformed?">
<meta name="author" content="">
<link rel="canonical" href="https://vectorsandverbs.com/posts/attention-mechanisms/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.af84f51093f576d67527cfa457c9742fd73a0c82d9399c6bc9c935e115ac4e68.css" integrity="sha256-r4T1EJP1dtZ1J8&#43;kV8l0L9c6DILZOZxryck14RWsTmg=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://vectorsandverbs.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://vectorsandverbs.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://vectorsandverbs.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://vectorsandverbs.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://vectorsandverbs.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://vectorsandverbs.com/posts/attention-mechanisms/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

<meta property="og:url" content="https://vectorsandverbs.com/posts/attention-mechanisms/">
  <meta property="og:site_name" content="Vectors & Verbs">
  <meta property="og:title" content="Attention Is All You Need, but exactly which one?: MHA, GQA and MLA">
  <meta property="og:description" content="We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek‚Äôs MLA, how has the definition of ‚ÄòAttention‚Äô transformed?">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2026-01-10T01:43:00+05:30">
    <meta property="article:modified_time" content="2026-01-10T01:43:00+05:30">
    <meta property="article:tag" content="Artificial-Intelligence">
    <meta property="article:tag" content="Large-Language-Models">
    <meta property="article:tag" content="Attention">
    <meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Attention Is All You Need, but exactly which one?: MHA, GQA and MLA">
<meta name="twitter:description" content="We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek&rsquo;s MLA, how has the definition of &lsquo;Attention&rsquo; transformed?">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://vectorsandverbs.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Attention Is All You Need, but exactly which one?: MHA, GQA and MLA",
      "item": "https://vectorsandverbs.com/posts/attention-mechanisms/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Attention Is All You Need, but exactly which one?: MHA, GQA and MLA",
  "name": "Attention Is All You Need, but exactly which one?: MHA, GQA and MLA",
  "description": "We are about to touch the holy grail of modern AI. From the original 2017 paper to DeepSeek\u0026rsquo;s MLA, how has the definition of \u0026lsquo;Attention\u0026rsquo; transformed?",
  "keywords": [
    "artificial-intelligence", "large-language-models", "attention", "transformer"
  ],
  "articleBody": "\nWe are about to touch the holy grail of modern AI. The 2017 declaration was simple: Attention Is All You Need. But as we‚Äôve raced from GPT-1 to reasoning models, the definition of ‚ÄòAttention‚Äô has quietly transformed. Is the algorithm ruling the world today actually the same one we started with? Let‚Äôs look under the hood.\nüî• The Core Concept At its heart, the attention mechanism is a form of weighted aggregation. It is often described using a database retrieval analogy. Imagine you have a database of information (context).\nQuery ($Q$): What are you currently looking for? (The token being generated). Key ($K$): The indexing tag associated with each piece of information. Value ($V$): The actual information (The context token). The ‚ÄúAttention‚Äù is the process of matching your Query against all the Keys to determine how much ‚Äúweight‚Äù to put on each Value. Attention scores are calculated in each transformer layer at every step of the sequence. This is a single unifying factor in all transformer models, from BERT to Gemini.\nüî• Standard Multi-Head Attention (MHA) The original mechanism introduced in the paper ‚ÄúAttention Is All You Need‚Äù.\nHere, instead of performing the Key-Query-Value (KQV) operation on the entire context only once, the attention is split into $N$ parallel heads. Each head learns a different aspect of the language (e.g., word meaning, syntax, semantic relationships, etc.). The equation is a simple culmination of dot products aided by a regularizer like Softmax.\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nWhere:\n$QK^T$ computes the similarity scores between the Query and Key. $\\sqrt{d_k}$ is a scaling factor to prevent the dot product from growing too large, mitigating the vanishing/exploding gradient problem. The weighted sum is applied to the values of $V$. The Memory Bandwidth Bottleneck: \u003e During inference, the model must store Key and Value matrices for all previous tokens. This is called the KV Cache. In MHA, every single head has its own unique set of Keys and Values. For large models with massive context windows, loading this massive KV cache from memory (HBM) to the computation chip (SRAM) becomes the primary bottleneck, slowing down generation significantly. Earlier models could often only generate ~8 tokens per second at inference due to this constraint.\nüî• Multi-Query Attention (MQA) This approach addresses the memory bottleneck by sharing the same set of Keys and Values across all heads while keeping separate Query heads.\nMHA: $H$ Query heads, $H$ Key heads, $H$ Value heads MQA: $H$ Query heads, 1 Key head, 1 Value head This drastically reduces the memory requirements of the KV cache by a factor of $H$ (often 8x or 16x). The GPU needs to load far less data, significantly speeding up inference. However, it does not reduce the computation (FLOPs)‚Äîso why is it faster?\nThis can be explained with the Kitchen Analogy: Imagine a kitchen (GPU) with 8 chefs (Heads).\nComputation (Math): The Chef chopping vegetables. Memory Access (Bandwidth): The Assistant running to the fridge to get ingredients. In MHA: Each chef demands unique vegetables from the fridge. The assistant runs back and forth 8 times, fetching 8 different crates. Most of the time is spent waiting for the assistant to return. This results in idle cores waiting for data to load from HBM.\nIn MQA: All chefs agree to use the same vegetables. The assistant runs back and forth only once, fetching one giant crate of potatoes and dumping it. This results in minimized memory transfer and no idle cores.\nThe Technical Reality: During text generation, the model is memory-bound, not compute-bound. The time it takes to move the KV Cache from HBM (High Bandwidth Memory) to the chip‚Äôs SRAM is the bottleneck. MQA reduces the data volume of that move by a factor of $H$.\nThe Capacity Gap: MQA is a trade-off between nuanced response and memory capacity. The model loses the ability to store different types of information, as a single $V$ matrix must now store all nuances of information flow in the language.\nThe Surgery Trick (Uptraining): Google Research figured out a way to take an existing model with MHA and convert it to MQA by averaging the weight matrices of $H$ heads. This is called Uptraining. You then train this ‚ÄúFrankenstein‚Äù model for ~5% of the original training steps for adjustments. This yields an MQA model that is nearly as good as the original MHA but vastly faster.\nüî• Grouped Query Attention (GQA) A ‚Äúbest of both worlds‚Äù solution used in modern models like Llama 2 and Llama 3.\nHow it works: GQA acts as an interpolation between MHA and MQA. Instead of having just 1 Key head (MQA) or $H$ Key heads (MHA), it divides the query heads into $G$ groups. Each group shares a single Key and Value head.\nEquation modification: If you have 8 query heads, you might create 4 groups (2 queries per group). This reduces the KV cache by 2x instead of 8x, preserving more quality than MQA while still speeding up inference.\nMHA: 8 people (heads) are working on a project. Each person has their own unique filing cabinet (KV cache). It takes forever to move 8 cabinets around. MQA: 8 people share one single filing cabinet. It‚Äôs very fast to move, but they fight over the organization and lose detail. GQA: The 8 people split into 4 teams of 2. Each team shares a cabinet. It‚Äôs a balance between speed and organizational detail. üî• Multi-Head Latent Attention (MLA) This is the most advanced mechanism for attention, used in models like DeepSeek-V3. It is a sophisticated design for massive efficiency that takes a radical approach: mathematically compressing the memory into a low-rank ‚Äúlatent‚Äù vector.\nKV Cache Explosion: As mentioned before, for a massive model like DeepSeek-V3 (671B parameters), storing the full Keys and Values for every token in a long conversation requires terabytes of high-speed memory (HBM), and chips spend more time waiting than churning.\nMHA (Standard) stores everything. MQA deletes redundancy but hurts quality. MLA keeps the quality of MHA but compresses the data 4x-6x smaller than even GQA.\nThe Zip File Approach: Low-Rank Joint Compression Let‚Äôs compare the math of MHA and MLA. In MHA, for every single token, we generate a unique Key ($k$) and Value ($v$) matrix for each attention head.\nThe Memory Problem: The $k$ is stored in the KV Cache. For 128 heads, for example, it calculates a massive vector of $128 \\times 128 = 16,384$ floats per token.\nFor MHA, the attention score looks like: $$\\text{Score} = q^T \\cdot k$$\nBut for MLA, instead of storing the full $k$, it projects the input down into a tiny latent vector $c_{KV}$, which is much smaller (e.g., 512 floats). $$c_{KV} = W_{DKV} \\cdot x$$\nNow here is the catch: Naive compression and decompression would mean constructing a small latent matrix for every token and deconstructing it every single time the information is required, which defeats the purpose of compression.\n$$k_{\\text{reconstructed}} = W_{UK} \\cdot c_{KV}$$ $$\\text{Score} = q^T \\cdot k_{\\text{reconstructed}}$$\nThe Matrix Absorption Trick (Optimization) Instead of reconstructing $k$ for every token, we can absorb the Up-Projection matrix ($W_{UK}$) into the Down-Projection matrix ($W_{DKV}$) and absorb the query $q$ into a new $Q$.\nFrom the original equation: $$\\text{Score} = q^T \\cdot (\\underbrace{W_{UK} \\cdot c_{KV}}_{\\text{This is } k})$$\nWe associate differently:\n$$\\text{Score} = (\\underbrace{q^T \\cdot W_{UK}}_{\\text{Absorbed Query}}) \\cdot c_{KV}$$ We change the order in which the matrix multiplication is performed.\nHow is this allowed? In linear algebra, matrix multiplication is associative:\n$$(A \\cdot B) \\cdot C = A \\cdot (B \\cdot C)$$ We can move the parentheses! Instead of grouping $(W_{UK} \\cdot c_{KV})$ to make the Key, we group $(q^T \\cdot W_{UK})$ to make a new Query.\nNote: The positional embedding information is left untouched. That rotational information from algorithms like RoPE is preserved outside the compression; it requires minimal space anyways.\n",
  "wordCount" : "1303",
  "inLanguage": "en",
  "datePublished": "2026-01-10T01:43:00+05:30",
  "dateModified": "2026-01-10T01:43:00+05:30",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://vectorsandverbs.com/posts/attention-mechanisms/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Vectors \u0026 Verbs",
    "logo": {
      "@type": "ImageObject",
      "url": "https://vectorsandverbs.com/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://vectorsandverbs.com/" accesskey="h" title="Vectors &amp; Verbs (Alt + H)">Vectors &amp; Verbs</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://vectorsandverbs.com/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://vectorsandverbs.com/about/" title="About Me">
                    <span>About Me</span>
                </a>
            </li>
            <li>
                <a href="https://vectorsandverbs.com/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://vectorsandverbs.com/">Home</a>&nbsp;¬ª&nbsp;<a href="https://vectorsandverbs.com/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Attention Is All You Need, but exactly which one?: MHA, GQA and MLA
    </h1>
    <div class="post-meta"><span title='2026-01-10 01:43:00 +0530 IST'>January 10, 2026</span>&nbsp;¬∑&nbsp;<span>7 min</span>

</div>
  </header> 
  <div class="post-content"><p><img alt="Attention Mechanism Overview" loading="lazy" src="/images/attention-header.png"></p>
<p>We are about to touch the holy grail of modern AI. The 2017 declaration was simple: <em>Attention Is All You Need</em>. But as we‚Äôve raced from GPT-1 to reasoning models, the definition of &lsquo;Attention&rsquo; has quietly transformed. Is the algorithm ruling the world today actually the same one we started with? Let‚Äôs look under the hood.</p>
<h3 id="-the-core-concept">üî• The Core Concept<a hidden class="anchor" aria-hidden="true" href="#-the-core-concept">#</a></h3>
<p>At its heart, the attention mechanism is a form of weighted aggregation. It is often described using a database retrieval analogy. Imagine you have a database of information (context).</p>
<ul>
<li><strong>Query ($Q$):</strong> What are you currently looking for? (The token being generated).</li>
<li><strong>Key ($K$):</strong> The indexing tag associated with each piece of information.</li>
<li><strong>Value ($V$):</strong> The actual information (The context token).</li>
</ul>
<p>The &ldquo;Attention&rdquo; is the process of matching your <strong>Query</strong> against all the <strong>Keys</strong> to determine how much &ldquo;weight&rdquo; to put on each <strong>Value</strong>. Attention scores are calculated in each transformer layer at every step of the sequence. This is a single unifying factor in all transformer models, from BERT to Gemini.</p>
<hr>
<h3 id="-standard-multi-head-attention-mha">üî• Standard Multi-Head Attention (MHA)<a hidden class="anchor" aria-hidden="true" href="#-standard-multi-head-attention-mha">#</a></h3>
<p>The original mechanism introduced in the paper <em>&ldquo;Attention Is All You Need&rdquo;</em>.</p>
<p>Here, instead of performing the Key-Query-Value (KQV) operation on the entire context only once, the attention is split into $N$ parallel heads. Each head learns a different aspect of the language (e.g., word meaning, syntax, semantic relationships, etc.). The equation is a simple culmination of dot products aided by a regularizer like Softmax.</p>
<p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$</p>
<p>Where:</p>
<ul>
<li>$QK^T$ computes the similarity scores between the Query and Key.</li>
<li>$\sqrt{d_k}$ is a scaling factor to prevent the dot product from growing too large, mitigating the vanishing/exploding gradient problem.</li>
<li>The weighted sum is applied to the values of $V$.</li>
</ul>
<blockquote>
<p><strong>The Memory Bandwidth Bottleneck:</strong> &gt; During inference, the model must store Key and Value matrices for all previous tokens. This is called the <strong>KV Cache</strong>. In MHA, every single head has its own unique set of Keys and Values. For large models with massive context windows, loading this massive KV cache from memory (HBM) to the computation chip (SRAM) becomes the primary bottleneck, slowing down generation significantly. Earlier models could often only generate ~8 tokens per second at inference due to this constraint.</p>
</blockquote>
<hr>
<h3 id="-multi-query-attention-mqa">üî• Multi-Query Attention (MQA)<a hidden class="anchor" aria-hidden="true" href="#-multi-query-attention-mqa">#</a></h3>
<p>This approach addresses the memory bottleneck by sharing the same set of Keys and Values across all heads while keeping separate Query heads.</p>
<ul>
<li><strong>MHA:</strong> $H$ Query heads, $H$ Key heads, $H$ Value heads</li>
<li><strong>MQA:</strong> $H$ Query heads, <strong>1</strong> Key head, <strong>1</strong> Value head</li>
</ul>
<p>This drastically reduces the memory requirements of the KV cache by a factor of $H$ (often 8x or 16x). The GPU needs to load far less data, significantly speeding up inference. However, it does not reduce the computation (FLOPs)‚Äîso why is it faster?</p>
<p>This can be explained with the <strong>Kitchen Analogy</strong>: Imagine a kitchen (GPU) with 8 chefs (Heads).</p>
<ul>
<li><strong>Computation (Math):</strong> The Chef chopping vegetables.</li>
<li><strong>Memory Access (Bandwidth):</strong> The Assistant running to the fridge to get ingredients.</li>
</ul>
<blockquote>
<p><strong>In MHA:</strong> Each chef demands unique vegetables from the fridge. The assistant runs back and forth 8 times, fetching 8 different crates. Most of the time is spent waiting for the assistant to return. This results in idle cores waiting for data to load from HBM.</p>
<p><strong>In MQA:</strong> All chefs agree to use the same vegetables. The assistant runs back and forth only once, fetching one giant crate of potatoes and dumping it. This results in minimized memory transfer and no idle cores.</p>
</blockquote>
<p><strong>The Technical Reality:</strong> During text generation, the model is memory-bound, not compute-bound. The time it takes to move the KV Cache from HBM (High Bandwidth Memory) to the chip&rsquo;s SRAM is the bottleneck. MQA reduces the data volume of that move by a factor of $H$.</p>
<p><strong>The Capacity Gap:</strong> MQA is a trade-off between nuanced response and memory capacity. The model loses the ability to store different types of information, as a single $V$ matrix must now store all nuances of information flow in the language.</p>
<p><strong>The Surgery Trick (Uptraining):</strong> Google Research figured out a way to take an existing model with MHA and convert it to MQA by averaging the weight matrices of $H$ heads. This is called <em>Uptraining</em>. You then train this &ldquo;Frankenstein&rdquo; model for ~5% of the original training steps for adjustments. This yields an MQA model that is nearly as good as the original MHA but vastly faster.</p>
<hr>
<h3 id="-grouped-query-attention-gqa">üî• Grouped Query Attention (GQA)<a hidden class="anchor" aria-hidden="true" href="#-grouped-query-attention-gqa">#</a></h3>
<p>A &ldquo;best of both worlds&rdquo; solution used in modern models like Llama 2 and Llama 3.</p>
<p><strong>How it works:</strong> GQA acts as an interpolation between MHA and MQA. Instead of having just 1 Key head (MQA) or $H$ Key heads (MHA), it divides the query heads into $G$ groups. Each group shares a single Key and Value head.</p>
<p><strong>Equation modification:</strong> If you have 8 query heads, you might create 4 groups (2 queries per group). This reduces the KV cache by 2x instead of 8x, preserving more quality than MQA while still speeding up inference.</p>
<blockquote>
<ul>
<li><strong>MHA:</strong> 8 people (heads) are working on a project. Each person has their own unique filing cabinet (KV cache). It takes forever to move 8 cabinets around.</li>
<li><strong>MQA:</strong> 8 people share one single filing cabinet. It&rsquo;s very fast to move, but they fight over the organization and lose detail.</li>
<li><strong>GQA:</strong> The 8 people split into 4 teams of 2. Each team shares a cabinet. It&rsquo;s a balance between speed and organizational detail.</li>
</ul>
</blockquote>
<hr>
<h3 id="-multi-head-latent-attention-mla">üî• Multi-Head Latent Attention (MLA)<a hidden class="anchor" aria-hidden="true" href="#-multi-head-latent-attention-mla">#</a></h3>
<p>This is the most advanced mechanism for attention, used in models like DeepSeek-V3. It is a sophisticated design for massive efficiency that takes a radical approach: mathematically compressing the memory into a low-rank &ldquo;latent&rdquo; vector.</p>
<p><strong>KV Cache Explosion:</strong> As mentioned before, for a massive model like DeepSeek-V3 (671B parameters), storing the full Keys and Values for every token in a long conversation requires terabytes of high-speed memory (HBM), and chips spend more time waiting than churning.</p>
<p>MHA (Standard) stores everything. MQA deletes redundancy but hurts quality. MLA keeps the quality of MHA but compresses the data 4x-6x smaller than even GQA.</p>
<h4 id="the-zip-file-approach-low-rank-joint-compression">The Zip File Approach: Low-Rank Joint Compression<a hidden class="anchor" aria-hidden="true" href="#the-zip-file-approach-low-rank-joint-compression">#</a></h4>
<p>Let&rsquo;s compare the math of MHA and MLA. In MHA, for every single token, we generate a unique Key ($k$) and Value ($v$) matrix for each attention head.</p>
<p><strong>The Memory Problem:</strong> The $k$ is stored in the KV Cache. For 128 heads, for example, it calculates a massive vector of $128 \times 128 = 16,384$ floats <em>per token</em>.</p>
<p>For MHA, the attention score looks like:
$$\text{Score} = q^T \cdot k$$</p>
<p>But for MLA, instead of storing the full $k$, it projects the input down into a tiny latent vector $c_{KV}$, which is much smaller (e.g., 512 floats).
$$c_{KV} = W_{DKV} \cdot x$$</p>
<p>Now here is the catch: Naive compression and decompression would mean constructing a small latent matrix for every token and deconstructing it every single time the information is required, which defeats the purpose of compression.</p>
<p>$$k_{\text{reconstructed}} = W_{UK} \cdot c_{KV}$$
$$\text{Score} = q^T \cdot k_{\text{reconstructed}}$$</p>
<h4 id="the-matrix-absorption-trick-optimization">The Matrix Absorption Trick (Optimization)<a hidden class="anchor" aria-hidden="true" href="#the-matrix-absorption-trick-optimization">#</a></h4>
<p>Instead of reconstructing $k$ for every token, we can absorb the Up-Projection matrix ($W_{UK}$) into the Down-Projection matrix ($W_{DKV}$) and absorb the query $q$ into a new $Q$.</p>
<p>From the original equation:
$$\text{Score} = q^T \cdot (\underbrace{W_{UK} \cdot c_{KV}}_{\text{This is } k})$$</p>
<p>We associate differently:</p>
<div>
$$\text{Score} = (\underbrace{q^T \cdot W_{UK}}_{\text{Absorbed Query}}) \cdot c_{KV}$$
</div>
<p>We change the order in which the matrix multiplication is performed.</p>
<p><strong>How is this allowed?</strong> In linear algebra, matrix multiplication is associative:<br>
$$(A \cdot B) \cdot C = A \cdot (B \cdot C)$$
We can move the parentheses! Instead of grouping $(W_{UK} \cdot c_{KV})$ to make the Key, we group $(q^T \cdot W_{UK})$ to make a new Query.</p>
<p><strong>Note:</strong> The <a href="https://vectorsandverbs.com/posts/positional-embeddings/">positional embedding</a> information is left untouched. That rotational information from algorithms like RoPE is preserved outside the compression; it requires minimal space anyways.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://vectorsandverbs.com/tags/artificial-intelligence/">Artificial-Intelligence</a></li>
      <li><a href="https://vectorsandverbs.com/tags/large-language-models/">Large-Language-Models</a></li>
      <li><a href="https://vectorsandverbs.com/tags/attention/">Attention</a></li>
      <li><a href="https://vectorsandverbs.com/tags/transformer/">Transformer</a></li>
    </ul>
<nav class="paginav">
    <a class="prev" href="https://vectorsandverbs.com/posts/positional-embeddings/">
        <span class="title">¬´ Prev</span>
        <br>
        <span>The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE</span>
    </a>
</nav><div class="social-icons" align="left">
    <a href="https://x.com/siddheshinamdar" target="_blank" rel="noopener noreferrer me"
        title="X">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor">
    <path
        d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z">
    </path>
</svg>
    </a>
    <a href="https://www.linkedin.com/in/siddhesh-inamdar-24634694/" target="_blank" rel="noopener noreferrer me"
        title="Linkedin">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path>
    <rect x="2" y="9" width="4" height="12"></rect>
    <circle cx="4" cy="4" r="2"></circle>
</svg>
    </a>
    <a href="https://medium.com/@siddheshnmdr" target="_blank" rel="noopener noreferrer me"
        title="Medium">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 76.000000 76.000000" fill="currentColor" stroke-width="2"
    preserveAspectRatio="xMidYMid meet">
    <g transform="translate(0.000000,76.000000) scale(0.100000,-0.100000)">
        <path
            d="M0 380 l0 -380 380 0 380 0 0 380 0 380 -380 0 -380 0 0 -380z m334 85 c30 -63 57 -115 59 -115 2 0 16 30 31 68 15 37 37 88 49 115 l20 47 76 0 76 -1 -27 -20 -28 -21 0 -151 c0 -150 0 -151 27 -179 l27 -28 -109 0 -109 0 27 28 c26 27 27 32 26 143 0 131 3 134 -71 -58 -24 -62 -48 -113 -53 -113 -6 0 -17 16 -24 35 -7 19 -36 83 -64 142 l-52 108 -3 -98 c-3 -97 -2 -99 28 -133 16 -19 30 -39 30 -44 0 -6 -31 -10 -70 -10 -45 0 -70 4 -70 11 0 6 14 27 30 46 30 33 30 35 30 151 0 116 0 118 -31 155 l-30 37 75 0 76 0 54 -115z" />
    </g>
</svg>
    </a>
    <a href="https://github.com/SidInamdar" target="_blank" rel="noopener noreferrer me"
        title="Github">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path
        d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
    </path>
</svg>
    </a>
</div>
</footer>

  <div class="newsletter-form" style="width: 100%; margin-top: 10px;">
    <script async src="https://subscribe-forms.beehiiv.com/embed.js"></script><iframe
        src="https://subscribe-forms.beehiiv.com/cb315fa7-29d0-46fa-ae1f-a34a59e9d3b0" class="beehiiv-embed"
        data-test-id="beehiiv-embed" frameborder="0" scrolling="no"
        style="width: 100%; height: 32px; margin: 0; border-radius: 0px 0px 0px 0px !important; background-color: transparent; box-shadow: 0 0 #0000; max-width: 100%;"></iframe>
</div>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://vectorsandverbs.com/">Vectors &amp; Verbs</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
