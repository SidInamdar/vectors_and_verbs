<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText | Vectors &amp; Verbs</title>
<meta name="keywords" content="artificial-intelligence, word-embeddings, GloVe, FastText">
<meta name="description" content="How subword units and global co-occurrence matrices allow GloVe and FastText to capture nuances that Word2Vec missed.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/glove-and-fasttext/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.af84f51093f576d67527cfa457c9742fd73a0c82d9399c6bc9c935e115ac4e68.css" integrity="sha256-r4T1EJP1dtZ1J8&#43;kV8l0L9c6DILZOZxryck14RWsTmg=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/glove-and-fasttext/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>


</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Vectors &amp; Verbs (Alt + H)">Vectors &amp; Verbs</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about/" title="About Me">
                    <span>About Me</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText
    </h1>
    <div class="post-meta"><span title='2026-01-08 00:10:00 +0530 IST'>January 8, 2026</span>&nbsp;·&nbsp;<span>5 min</span>

</div>
  </header> 
  <div class="post-content"><p><em>Imagine trying to teach a computer the difference between &ldquo;Apple&rdquo; the fruit and &ldquo;Apple&rdquo; the company. To us, the distinction is intuitive. To a machine, it’s just a string of characters. How do we turn these strings into meaningful math? While early attempts like Word2Vec gave us a great start, they missed the forest for the trees—or in some cases, the twigs for the branches. Enter GloVe and FastText: two algorithms that revolutionized how machines understand the nuances of human language.</em></p>
<hr>
<p>Previous static embedding models like Word2Vec successfully captured the local semantics of words, but they failed in addressing contexts between words that might not always appear in the same context window. Furthermore, they treated words as indivisible atomic units in continuous vector spaces, missing out on the internal structure of language.</p>
<p>The two simple but effective ideas behind GloVe and FastText are <strong>subword units</strong> and <strong>global co-occurrence</strong>. Let&rsquo;s dive into them.</p>
<h3 id="glove-a-global-mapping-strategy">GloVe: A Global Mapping Strategy<a hidden class="anchor" aria-hidden="true" href="#glove-a-global-mapping-strategy">#</a></h3>
<p><strong>Global Vectors for Word Representation (GloVe)</strong> was created at Stanford. The core idea was to train a model where word vectors are defined by how often they appear around other words across the <em>entire</em> corpus, rather than just locally.</p>
<h4 id="training-steps">Training Steps:<a hidden class="anchor" aria-hidden="true" href="#training-steps">#</a></h4>
<ol>
<li><strong>Co-occurrence Matrix <code>X</code>:</strong> GloVe scans the entire corpus and creates a giant matrix (spreadsheet) recording the counts of how often words appear near each other.</li>
<li><strong>Factorization:</strong> It compresses this giant matrix into smaller, dense vectors.</li>
<li><strong>Objective:</strong> The function tries to equate the dot product of two word vectors with the logarithm of their probability of appearing together. The error between these values is then backpropagated to update the vectors.</li>
</ol>
<p>$$w_i \cdot w_j + b_i + b_j = \log(X_{ij})$$</p>
<ul>
<li>$w$: The word vector (what we want).</li>
<li>$X_{ij}$: The count from our matrix.</li>
</ul>
<h4 id="the-loss-function">The Loss Function:<a hidden class="anchor" aria-hidden="true" href="#the-loss-function">#</a></h4>
<p>We can&rsquo;t satisfy the equation above perfectly for every word pair. So, we use a weighting function to minimize the squared error:</p>
<p>$$J = \sum_{i,j=1}^V f(X_{ij}) ({w_i}^T w_j + b_i + b_j - \log(X_{ij}))^2$$</p>
<p>If we do not use the weighting function, the loss would be infinite for rare words (since $\log(0) = \infty$) and frequent words (like &rsquo;the&rsquo;) would overpower the model.</p>
<p>$$
f(x) =
\begin{cases}
(x/x_{max})^\alpha &amp; \text{if } x &lt; x_{max} \
1 &amp; \text{otherwise}
\end{cases}
$$</p>
<p><em>(Typically $x_{max}=100$ and $\alpha=0.75$)</em></p>
<hr>
<h3 id="fasttext-the-lego-bricks-strategy">FastText: The Lego Bricks Strategy<a hidden class="anchor" aria-hidden="true" href="#fasttext-the-lego-bricks-strategy">#</a></h3>
<p><strong>FastText</strong> was created by Facebook AI Research (FAIR). The idea was to treat words not as solid rocks, but as structures made of &ldquo;Lego bricks&rdquo; (subword units).</p>
<p>Word2Vec struggled to create embeddings for words it hadn&rsquo;t seen before (Out-Of-Vocabulary or OOV words). FastText solves this by breaking words down.</p>
<ul>
<li>GloVe treats a word like &ldquo;Apple&rdquo; as a single, atomic unit.</li>
<li>FastText breaks &ldquo;Apple&rdquo; into character n-grams: <code>&lt;ap, app, ppl, ple, le&gt;</code></li>
</ul>
<p>For a complex or new word like &ldquo;<strong>Applesauce</strong>&rdquo;, the model might not know the whole word, but it recognizes the Lego brick for &ldquo;Apple&rdquo; and the remaining bits from other subwords.</p>
<p>FastText takes its inspiration from the SkipGram architecture. The vector for a word is simply the sum of the vectors of its n-gram subwords:</p>
<p>$$v_w = \sum_{g \in G_w} z_g$$</p>
<h4 id="objective">Objective:<a hidden class="anchor" aria-hidden="true" href="#objective">#</a></h4>
<p>The objective is to predict context words given a target word.</p>
<p>$$P(w_c | w_t) = \frac{e^{s(v_{w_t} \cdot v_{w_c})}}{\sum_{w \in V} e^{s(v_{w_t} \cdot v_w)}}$$</p>
<p>And the scoring function is defined as:</p>
<p>$$s(w_t, w_c) = u_{w_t}^T (\sum_{g \in G_w} z_g)$$</p>
<ul>
<li>$u_{w_c}$: The vector for the context word (output vector).</li>
<li>$z_g$: The vector for the n-gram $g$ (input vector).</li>
</ul>
<h4 id="modifications-to-softmax">Modifications to Softmax:<a hidden class="anchor" aria-hidden="true" href="#modifications-to-softmax">#</a></h4>
<p>Calculating the full softmax for every word in the vocabulary is computationally expensive. FastText uses two methods to approximate this efficiently.</p>
<p><strong>Option A: Negative Sampling</strong></p>
<p>We pick a few random &ldquo;wrong&rdquo; words every time to serve as &ldquo;negative samples&rdquo; and calculate the loss only for the correct word and these noise words.</p>
<p>$$J = - \log \sigma(s(w_t, w_c)) - \sum\limits_{i=1}^{N} \mathbb{E}_{n_i \sim P_n(w)} [\log \sigma(-s(w_t, n_i))]$$</p>
<ul>
<li>The sigmoid function $\sigma$ squashes the score between 0 and 1.</li>
<li>The first term pushes the probability of the <strong>correct</strong> word towards 1.</li>
<li>The second term pushes the probability of the <strong>noise</strong> words towards 0.</li>
</ul>
<p><strong>Option B: Hierarchical Softmax (Faster for infrequent words)</strong></p>
<p>Instead of a flat list of words, imagine the vocabulary arranged as a binary tree (Huffman Tree).</p>
<ul>
<li><strong>Root:</strong> Top of the tree.</li>
<li><strong>Leaves:</strong> The actual words.</li>
<li>To calculate the probability of a word, we trace a path from the root to that leaf. At every branching node, we calculate the probability (sigmoid) of going left or right.</li>
<li>The probability of a word $w$ is the product of the probabilities of the turns taken to reach it.</li>
</ul>
<h3 id="code-example">Code Example<a hidden class="anchor" aria-hidden="true" href="#code-example">#</a></h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">FastText</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">gensim.test.utils</span> <span class="kn">import</span> <span class="n">common_texts</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Initialize the FastText model with specific hyperparameters</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">FastText</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>    <span class="c1"># Dimensionality of the word vectors</span>
</span></span><span class="line"><span class="cl">    <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>           <span class="c1"># Maximum distance between the current and predicted word within a sentence</span>
</span></span><span class="line"><span class="cl">    <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>        <span class="c1"># Ignores all words with total frequency lower than this</span>
</span></span><span class="line"><span class="cl">    <span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>               <span class="c1"># Training algorithm: 1 for skip-gram; 0 for CBOW</span>
</span></span><span class="line"><span class="cl">    <span class="n">hs</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>               <span class="c1"># If 0, and negative is non-zero, negative sampling will be used</span>
</span></span><span class="line"><span class="cl">    <span class="n">negative</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>         <span class="c1"># Number of &#34;noise words&#34; to be drawn for negative sampling</span>
</span></span><span class="line"><span class="cl">    <span class="n">workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>          <span class="c1"># Number of worker threads to train the model</span>
</span></span><span class="line"><span class="cl">    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>          <span class="c1"># Number of iterations over the corpus</span>
</span></span><span class="line"><span class="cl">    <span class="n">min_n</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>            <span class="c1"># Minimum length of character n-grams</span>
</span></span><span class="line"><span class="cl">    <span class="n">max_n</span><span class="o">=</span><span class="mi">6</span>             <span class="c1"># Maximum length of character n-grams</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Build the vocabulary from the provided text corpus</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">common_texts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Train the model on the corpus</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">common_texts</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="n">total_examples</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">corpus_count</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">    <span class="n">epochs</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">epochs</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Calculate and print the cosine similarity between two words in the vocabulary</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Similarity between &#39;computer&#39; and &#39;human&#39;: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s1">&#39;computer&#39;</span><span class="p">,</span> <span class="s1">&#39;human&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Demonstrate FastText&#39;s ability to handle Out-Of-Vocabulary (OOV) words</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Even if &#39;computation&#39; wasn&#39;t in the training data, FastText constructs a vector using character n-grams</span>
</span></span><span class="line"><span class="cl"><span class="n">oov_vector</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s1">&#39;computation&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Check if a specific word exists in the model&#39;s fixed vocabulary index</span>
</span></span><span class="line"><span class="cl"><span class="n">word</span> <span class="o">=</span> <span class="s2">&#34;computer&#34;</span>
</span></span><span class="line"><span class="cl"><span class="n">is_in_vocab</span> <span class="o">=</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">key_to_index</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">Is the word &#39;</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2">&#39; in the model&#39;s vocabulary? </span><span class="si">{</span><span class="n">is_in_vocab</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/artificial-intelligence/">Artificial-Intelligence</a></li>
      <li><a href="http://localhost:1313/tags/word-embeddings/">Word-Embeddings</a></li>
      <li><a href="http://localhost:1313/tags/glove/">GloVe</a></li>
      <li><a href="http://localhost:1313/tags/fasttext/">FastText</a></li>
    </ul>
<nav class="paginav">
    <a class="prev" href="http://localhost:1313/posts/word-embeddings/">
        <span class="title">« Prev</span>
        <br>
        <span>Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram</span>
    </a>
    <a class="next" href="http://localhost:1313/posts/positional-embeddings/">
        <span class="title">Next »</span>
        <br>
        <span>The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE</span>
    </a>
</nav><div class="social-icons" align="left">
    <a href="https://x.com/siddheshinamdar" target="_blank" rel="noopener noreferrer me"
        title="X">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor">
    <path
        d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z">
    </path>
</svg>
    </a>
    <a href="https://www.linkedin.com/in/siddhesh-inamdar-24634694/" target="_blank" rel="noopener noreferrer me"
        title="Linkedin">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path>
    <rect x="2" y="9" width="4" height="12"></rect>
    <circle cx="4" cy="4" r="2"></circle>
</svg>
    </a>
    <a href="https://medium.com/@siddheshnmdr" target="_blank" rel="noopener noreferrer me"
        title="Medium">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 76.000000 76.000000" fill="currentColor" stroke-width="2"
    preserveAspectRatio="xMidYMid meet">
    <g transform="translate(0.000000,76.000000) scale(0.100000,-0.100000)">
        <path
            d="M0 380 l0 -380 380 0 380 0 0 380 0 380 -380 0 -380 0 0 -380z m334 85 c30 -63 57 -115 59 -115 2 0 16 30 31 68 15 37 37 88 49 115 l20 47 76 0 76 -1 -27 -20 -28 -21 0 -151 c0 -150 0 -151 27 -179 l27 -28 -109 0 -109 0 27 28 c26 27 27 32 26 143 0 131 3 134 -71 -58 -24 -62 -48 -113 -53 -113 -6 0 -17 16 -24 35 -7 19 -36 83 -64 142 l-52 108 -3 -98 c-3 -97 -2 -99 28 -133 16 -19 30 -39 30 -44 0 -6 -31 -10 -70 -10 -45 0 -70 4 -70 11 0 6 14 27 30 46 30 33 30 35 30 151 0 116 0 118 -31 155 l-30 37 75 0 76 0 54 -115z" />
    </g>
</svg>
    </a>
    <a href="https://github.com/SidInamdar" target="_blank" rel="noopener noreferrer me"
        title="Github">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path
        d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
    </path>
</svg>
    </a>
</div>
</footer>

  <div class="newsletter-form" style="width: 100%; margin-top: 10px;">
    <script async src="https://subscribe-forms.beehiiv.com/embed.js"></script><iframe
        src="https://subscribe-forms.beehiiv.com/cb315fa7-29d0-46fa-ae1f-a34a59e9d3b0" class="beehiiv-embed"
        data-test-id="beehiiv-embed" frameborder="0" scrolling="no"
        style="width: 100%; height: 32px; margin: 0; border-radius: 0px 0px 0px 0px !important; background-color: transparent; box-shadow: 0 0 #0000; max-width: 100%;"></iframe>
</div>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="http://localhost:1313/">Vectors &amp; Verbs</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
