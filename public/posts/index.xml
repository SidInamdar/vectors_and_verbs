<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Vectors &amp; Verbs</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on Vectors &amp; Verbs</description>
    <generator>Hugo -- 0.154.2</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 07 Jan 2026 00:23:31 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Semantic Alchemists: SkipGram and Continuous Bag Of Words (CBOW)</title>
      <link>http://localhost:1313/posts/word-embeddings/</link>
      <pubDate>Wed, 07 Jan 2026 00:23:31 +0530</pubDate>
      <guid>http://localhost:1313/posts/word-embeddings/</guid>
      <description>&lt;p&gt;&lt;img alt=&#34;SkipGram and CBOW Process Diagram&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/images/embedding-concept-process-flow.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lnaguage models require vector representations of words to capture semantic relationships.&lt;/li&gt;
&lt;li&gt;Earlier than 2010s, models used word count based vector representations that captured only the frequency of words. For Example, one-hot encoding.
THe problems:&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;The Curse of Dimensionality&lt;/li&gt;
&lt;li&gt;Lack of meaning&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In 2011, At Google, Mikolov et al. introduced Word2Vec, which used a shallow neural network to learn vector representations of words. The best part was that it could create much denser relationships between words than prior models and it was unsupervised. The training (prediction) is fake but the weight matrices we get on the side are gold mine of fine-grained semantic relationships.
There are two popular variants of Word2Vec:&lt;/p&gt;</description>
    </item>
    <item>
      <title>The DNA of Language: A Deep Dive into LLM Tokenization concepts</title>
      <link>http://localhost:1313/posts/the-dna-of-language/</link>
      <pubDate>Mon, 05 Jan 2026 00:23:31 +0530</pubDate>
      <guid>http://localhost:1313/posts/the-dna-of-language/</guid>
      <description>&lt;p&gt;Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.&lt;/p&gt;
&lt;p&gt;The same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.&lt;/p&gt;</description>
    </item>
    <item>
      <title>My First Post</title>
      <link>http://localhost:1313/posts/first-post/</link>
      <pubDate>Sun, 04 Jan 2026 09:00:00 +0530</pubDate>
      <guid>http://localhost:1313/posts/first-post/</guid>
      <description>&lt;h2 id=&#34;welcome-to-vectors--verbs&#34;&gt;Welcome to Vectors &amp;amp; Verbs&lt;/h2&gt;
&lt;p&gt;This is a demo post to verify the PaperMod theme setup.&lt;/p&gt;
&lt;h3 id=&#34;features-of-this-theme&#34;&gt;Features of this theme:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Clean and minimal design&lt;/li&gt;
&lt;li&gt;Dark mode support&lt;/li&gt;
&lt;li&gt;Fast loading speed&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;hello_world&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;():&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Hello, Hugo!&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Stay tuned for more updates!&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
