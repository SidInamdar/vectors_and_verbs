<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>The DNA of Language: A Deep Dive into LLM Tokenization concepts | Vectors &amp; Verbs</title>
<meta name="keywords" content="artificial-intelligence, large-language-models, tokenization, sentencepiece, byte-pair-encoding, wordpiece, unigram">
<meta name="description" content="Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.
The same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/the-dna-of-language/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.af84f51093f576d67527cfa457c9742fd73a0c82d9399c6bc9c935e115ac4e68.css" integrity="sha256-r4T1EJP1dtZ1J8&#43;kV8l0L9c6DILZOZxryck14RWsTmg=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/the-dna-of-language/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>


</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Vectors &amp; Verbs (Alt + H)">Vectors &amp; Verbs</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about/" title="About Me">
                    <span>About Me</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;¬ª&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      The DNA of Language: A Deep Dive into LLM Tokenization concepts
    </h1>
    <div class="post-meta"><span title='2026-01-05 00:23:31 +0530 IST'>January 5, 2026</span>&nbsp;¬∑&nbsp;<span>14 min</span>

</div>
  </header> 
  <div class="post-content"><p>Imagine you have to build a house. You cannot build a stable house using only massive boulders as walls (too big), nor can you build one using only tiny pebbles (too small). You need exactly the right-sized bricks.</p>
<p>The same analogy applies to linguistics. We need to find strategies to break down petabytes of language data into usable, atomic chunks. In the context of Large Language Models (LLMs), these bricks are called tokens. Tokens enable us to transform a sizable amount of fluid language data into a discrete mathematical language that machines can process. It is the invisible filter at the heart of LLMs through which every prompt is passed and every response is born.</p>
<p>Let&rsquo;s dive into the most popular tokenization strategies used in LLMs today.</p>
<h3 id="1-byte-pair-encoding-bpe">1. Byte Pair Encoding (BPE)<a hidden class="anchor" aria-hidden="true" href="#1-byte-pair-encoding-bpe">#</a></h3>
<p>Early Natural Language Processing (NLP) models used to split text into words to form a corpus. During inference (i.e., when we want to generate text), any unknown word would be assigned an &ldquo;out of vocabulary&rdquo; token (<code>&lt;UNK&gt;</code>). Rare words like &ldquo;uninstagrammable&rdquo; were the usual victims of this process.</p>
<p>BPE is the most popular tokenization strategy used in LLMs, largely because it allows engineers to worry less about the problem of missing tokens. It received massive popularity after the release of GPT-2, GPT-3, and Llama. Most importantly, it is a simple, bottom-up, frequency-based strategy.</p>
<p>BPE starts with a vocabulary of single characters and then iteratively merges the most frequent pairs of characters until it reaches the desired vocabulary size. This creates a tree-like subword joining structure. By favoring frequent sequences, BPE ensures that the most common words and phrases are represented by a single token, while rarer words are represented by a combination of tokens. This makes BPE a powerful tool for representing natural language data in a compact and efficient way.</p>
<p>Modern <strong>Byte-level BPE</strong> goes even further by starting the process with the UTF-8 byte as a character. This ensures that the tokenization process is language-agnostic and can handle any language‚Äîeven those that use non-Latin scripts or cannot be split by spaces, such as Japanese or Korean. While Unicode contains over 150,000 characters, using UTF bytes allows the vocabulary to start with only 256 tokens. The caveat is that the more fragmented the tokens, the higher the number of tokens required to represent a word, which increases the computational cost of tokenization.</p>
<p><img alt="BPE Merge Process Diagram" loading="lazy" src="/images/bpe-merge-process.png"></p>
<p>The tokenization operates in 4 steps:</p>
<ol>
<li><strong>Initialization:</strong> The process starts with a vocabulary of single characters.</li>
<li><strong>Pair Counting:</strong> The process counts the frequency of each pair of characters.</li>
<li><strong>Merge:</strong> The process merges the most frequent pairs of characters.</li>
<li><strong>Iteration:</strong> The process repeats thousands of times until the desired vocabulary size is reached (roughly 100000).</li>
</ol>
<p><strong>The &ldquo;SolidGoldMagikarp&rdquo; üêü phenomenon:</strong>
This is a famous instance of &ldquo;glitch tokens&rdquo; in BPE. Since BPE is a frequency-based heuristic, One of the most bizarre side effects of BPE is the existence of &ldquo;glitch tokens.&rdquo; In GPT-2 and GPT-3, specific strings like &ldquo;SolidGoldMagikarp&rdquo; (a Reddit username) or &ldquo;StreamerBot&rdquo; cause the model to hallucinate or break down.</p>
<p>GPT 4 improved the BPE tokenization by adding Regex updates. For example, you generally don&rsquo;t want to merge a word with the punctuation following it (e.g., &ldquo;dog&rdquo; and &ldquo;.&rdquo; becoming &ldquo;dog.&rdquo;).</p>
<h3 id="2-wordpiece">2. WordPiece<a hidden class="anchor" aria-hidden="true" href="#2-wordpiece">#</a></h3>
<p>WordPiece was developed by Google and gained widespread attention for its use in the BERT model and its derivatives. The WordPiece strategy addresses some of the limitations of BPE. Rather than relying solely on the raw frequency of subword tokens, it uses a maximum likelihood strategy to identify a meaningful vocabulary.</p>
<p>Superficially, WordPiece resembles BPE: it starts with a small vocabulary of single characters and iteratively merges them. However, WordPiece uses a statistical approach where scores are assigned to individual tokens in the training data. The score for a potential merge of two terms, $A$ and $B$, is:
$$
\text{Score} = \frac{\text{Frequency}(AB)}{\text{Frequency}(A) \times \text{Frequency}(B)}
$$
This formula ensures that common individual parts are penalized if they do not frequently appear together. This semantic cohesion guarantees that words appearing together are given more importance (a higher score) than parts that occur independently.
The algorithm wrorks in two phases:</p>
<p><strong>Training:</strong> The algorithm starts with base characters and then starts the merge process. For the part of the word that is in continuation it adds a special prefix &lsquo;##&rsquo; so that the parts which start the word are distinct from those which trail in a word.<br>
<strong>Inference:</strong> Unlike BPE, which memorizes merge rules, WordPiece operates by finding the longest possible subword match (a &ldquo;greedy longest-match-first&rdquo; strategy).
Given a word like &ldquo;hugs&rdquo;, it checks if the full word is in the vocabulary.<br>
‚Ä¢ If not, it looks for the longest prefix that is in the vocabulary (e.g., &ldquo;hug&rdquo;).<br>
‚Ä¢ It then attempts to tokenize the remainder (&ldquo;s&rdquo;) using the ## prefix (e.g., &ldquo;##s&rdquo;).<br>
‚Ä¢ If a subword cannot be found in the vocabulary, the entire word is replaced with the [UNK] (Unknown) token.</p>
<p>The model has certain pitfalls. The  <code>&lt;UNK&gt;</code> token acts as a &ldquo;hard cliff,&rdquo; making it impossible to resolve very rare words. Recent research indicates that most tokens in WordPiece are start tokens (~70%), while only ~30% are continuation tokens. Furthermore, the model does not capture semantic links between words; for example, &ldquo;advice&rdquo; and &ldquo;advises&rdquo; are tokenized entirely differently.</p>
<h3 id="3-unigram-chipping-away-unwanted-tokens">3. Unigram: Chipping away unwanted tokens<a hidden class="anchor" aria-hidden="true" href="#3-unigram-chipping-away-unwanted-tokens">#</a></h3>
<p>Unigram follows the reverse strategy of learning tokens from data than BPE or WordPiece. It starts with a large vocabulary and then removes the least frequent tokens and it does this by selecting tokens based on the fundamental question: <strong>&lsquo;Which breakdown of the text will maximizes the likelihood of the data?&rsquo;</strong></p>
<p>A single word can be tokenized in multiple ways. For example <code>hugs</code> can be tokenized as:</p>
<ul>
<li><code>[hug, s]</code></li>
<li><code>[h, ug, s]</code></li>
<li><code>[h, u, g, s]</code></li>
</ul>
<p>Each of these tokenizations will be assigned a probability score and the one with the maximum is selected as the likely winner. The mechanism uses Expectation - Maximization algorithm as follows:</p>
<p><strong>1. Initialization:</strong> The model starts with the entire vocabulary and all the possible substrings in the corpus. Initial size is much bigger than the desired vocabulary.<br>
<strong>2. Expectation (calculating Loss):</strong> The model calculates the loss for each tokenization by using the negative log-likelihood of the data given the tokenization. Essentially, it measures how well the current tokens can &ldquo;explain&rdquo; the training text.<br>
<strong>3. Maximization (pruning):</strong> For every possible token in the vocabulary, the model calculates: &lsquo;How much will the overall loss change if we remove this token?&rsquo;. If the loss spikes on token removal (means that the token was important in compressing the vocabulary), it is kept. If the loss does not spike (token contributed very little to compression), it is removed.<br>
<strong>4. Selection:</strong> The model discards bottom 10 to 20 % of the original vocabulary and the cycle repeats till desired vocabulary size is reached.</p>
<p>Because Unigram allows multiple segmentations for the same text, it relies on a dynamic programming method called the <strong>Viterbi algorithm</strong> during inference. When tokenizing a word, the algorithm builds a graph where nodes are characters and edges are possible subwords, then unrolls the path with the highest score. This ensures the tokenization is mathematically optimal rather than just a result of greedy merging.</p>
<p>Unigram is the default tokenization strategy used in SentencePiece. The model can use subword regularization during training to create a more robust tokenization. It means not picking the &lsquo;best&rsquo; tokenization always but sometimes a &lsquo;good enough&rsquo; tokenization. Also the model can use sampled segmentation which gives different optimal paths to tokenization of the same word, something chatbot applications really like. Unigram also gravitates toward tokens that compress the text most efficiently.</p>
<p>Recent comparative studies have shown that tokenization strategies affect languages differently. While BPE tends to perform better for Germanic languages (like English and German), Unigram (via SentencePiece) has been shown to be more effective for Romance languages (like Spanish) and Indic languages (like Hindi). This suggests that Unigram&rsquo;s probabilistic approach may better capture the morphological nuances of certain language families.</p>
<h3 id="4-sentencepiece-the-universal-adapter">4. SentencePiece: The Universal Adapter<a hidden class="anchor" aria-hidden="true" href="#4-sentencepiece-the-universal-adapter">#</a></h3>
<p>Bold claim of sentences is that words are not separated by spaces, which is central logic to split token for many traditional tokenization strategies including BERT Tokenizers. The model treats whitespaces as part of the word. For example: &lsquo;Hello World&rsquo; becomes _Hello _World.</p>
<p><strong>Lossless Reconstruction:</strong> The model is lossless, meaning it can perfectly reconstruct the original text from the tokenized version. This is because it treats whitespaces as part of the word. The original text can be recreated by simply concatenating the words.</p>
<p>It is a common misconception that SentencePiece is a tokenization algorithm in the same vein as BPE. In reality, SentencePiece is a library and a strategy that can implement different segmentation algorithms, most notably BPE and Unigram.
Unigram Integration: Models like ALBRET, T5, mBART, and XLNet utilize SentencePiece configured with the Unigram algorithm. This approach starts with a massive vocabulary and probabilistically trims it down, optimizing for the best segmentation of the raw input stream.
‚Ä¢ BPE Integration: Conversely, models like Llama 2 utilize SentencePiece configured with Byte-Pair Encoding (BPE). This allows them to benefit from BPE&rsquo;s merge-based efficiency while retaining SentencePiece&rsquo;s language-agnostic handling of Unicode.</p>
<p><strong>Handling unknowns:</strong> The model provides an option called <code>byte fallback</code>, so whenever the model identifies a token as unknown, instead of adding it as <code>&lt;UNK&gt;</code>, the model can split the word into UTF-8 bytes and represent them as individual tokens. This is a more efficient way of handling unknown tokens and is used in models like Llama 2. It is a more efficient way of tokenization and is lossless.</p>
<p>SentencePiece standardized the NLP pipelines in ways that models consume text, by treating text as a continuous stream of characters and modelling whitespaces it can enable multilingual models like T5 and mBART.</p>
<h3 id="5-code-examples">5. Code Examples:<a hidden class="anchor" aria-hidden="true" href="#5-code-examples">#</a></h3>
<p>We&rsquo;ll use sentencepiece for tokenization and compare results for different models.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">sentencepiece</span> <span class="k">as</span> <span class="nn">spm</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Configuration for training the SentencePiece model</span>
</span></span><span class="line"><span class="cl"><span class="c1"># SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words.</span>
</span></span><span class="line"><span class="cl"><span class="n">options</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># The source text file used for learning the vocabulary</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;input&#39;</span><span class="p">:</span> <span class="s1">&#39;train_data.txt&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># The base name for the generated model (.model) and vocabulary (.vocab) files</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;model_prefix&#39;</span><span class="p">:</span> <span class="s1">&#39;bpe_model&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Number of unique tokens in the final vocabulary</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;vocab_size&#39;</span><span class="p">:</span> <span class="mi">4000</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># &#39;bpe&#39; (Byte Pair Encoding) merges frequent pairs of characters/sequences</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;model_type&#39;</span><span class="p">:</span> <span class="s1">&#39;bpe&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;character_coverage&#39;</span><span class="p">:</span> <span class="mf">0.9995</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># When enabled, unknown characters are decomposed into UTF-8 bytes to avoid &#39;unk&#39; tokens</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;byte_fallback&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Treats digits individually (0-9), preventing large numbers from being treated as single tokens</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;split_digits&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Prevents adding a whitespace prefix to the first token; useful for fine-grained control</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;add_dummy_prefix&#39;</span><span class="p">:</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Starting the training process...&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># SentencePieceTrainer.train takes the dictionary of options to build the BPE model</span>
</span></span><span class="line"><span class="cl">    <span class="n">spm</span><span class="o">.</span><span class="n">SentencePieceTrainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">options</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Training complete. &#39;bpe_model.model&#39; and &#39;bpe_model.vocab&#39; have been created.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># Initialize the processor and load the newly trained model</span>
</span></span><span class="line"><span class="cl">    <span class="n">sp</span> <span class="o">=</span> <span class="n">spm</span><span class="o">.</span><span class="n">SentencePieceProcessor</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">sp</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;bpe_model.model&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;-&#34;</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Model Metadata:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Retrieve the total number of tokens in the vocabulary</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Total Vocab Size: </span><span class="si">{</span><span class="n">sp</span><span class="o">.</span><span class="n">get_piece_size</span><span class="p">()</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Special tokens are used for sequence boundaries and handling unknown characters</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;BOS (Beginning of Sentence) ID: </span><span class="si">{</span><span class="n">sp</span><span class="o">.</span><span class="n">bos_id</span><span class="p">()</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;EOS (End of Sentence) ID:       </span><span class="si">{</span><span class="n">sp</span><span class="o">.</span><span class="n">eos_id</span><span class="p">()</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;UNK (Unknown) ID:              </span><span class="si">{</span><span class="n">sp</span><span class="o">.</span><span class="n">unk_id</span><span class="p">()</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;PAD (Padding) ID:              </span><span class="si">{</span><span class="n">sp</span><span class="o">.</span><span class="n">pad_id</span><span class="p">()</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Test the tokenizer on sample strings to see how it breaks down text</span>
</span></span><span class="line"><span class="cl">    <span class="n">test_sentences</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;Hello World! 1234567890&#39;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;This blog is the most uninstagrammable blog ever&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">test_sentences</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">--- Tokenization Test ---&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Original Text: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># encode_as_pieces: shows the actual subword units (tokens)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Subword Tokens: </span><span class="si">{</span><span class="n">sp</span><span class="o">.</span><span class="n">encode_as_pieces</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># encode_as_ids: shows the numerical mapping for each token</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Numerical IDs:  </span><span class="si">{</span><span class="n">sp</span><span class="o">.</span><span class="n">encode_as_ids</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;An error occurred during training or processing: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p>You can see the following output:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">Starting training...
</span></span><span class="line"><span class="cl">Starting the training process...
</span></span><span class="line"><span class="cl">Training complete. <span class="s1">&#39;bpe_model.model&#39;</span> and <span class="s1">&#39;bpe_model.vocab&#39;</span> have been created.
</span></span><span class="line"><span class="cl">------------------------------
</span></span><span class="line"><span class="cl">Model Metadata:
</span></span><span class="line"><span class="cl">Total Vocab Size: <span class="m">4000</span>
</span></span><span class="line"><span class="cl">BOS <span class="o">(</span>Beginning of Sentence<span class="o">)</span> ID: <span class="m">1</span>
</span></span><span class="line"><span class="cl">EOS <span class="o">(</span>End of Sentence<span class="o">)</span> ID:       <span class="m">2</span>
</span></span><span class="line"><span class="cl">UNK <span class="o">(</span>Unknown<span class="o">)</span> ID:              <span class="m">0</span>
</span></span><span class="line"><span class="cl">PAD <span class="o">(</span>Padding<span class="o">)</span> ID:              -1
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">--- Tokenization Test ---
</span></span><span class="line"><span class="cl">Original Text: Hello World! <span class="m">1234567890</span>
</span></span><span class="line"><span class="cl">Subword Tokens: <span class="o">[</span><span class="s1">&#39;&lt;0x48&gt;&#39;</span>, <span class="s1">&#39;el&#39;</span>, <span class="s1">&#39;lo&#39;</span>, <span class="s1">&#39;‚ñÅW&#39;</span>, <span class="s1">&#39;orld&#39;</span>, <span class="s1">&#39;&lt;0x21&gt;&#39;</span>, <span class="s1">&#39;‚ñÅ&#39;</span>, <span class="s1">&#39;1&#39;</span>, <span class="s1">&#39;2&#39;</span>, <span class="s1">&#39;3&#39;</span>, <span class="s1">&#39;4&#39;</span>, <span class="s1">&#39;5&#39;</span>, <span class="s1">&#39;&lt;0x36&gt;&#39;</span>, <span class="s1">&#39;7&#39;</span>, <span class="s1">&#39;8&#39;</span>, <span class="s1">&#39;&lt;0x39&gt;&#39;</span>, <span class="s1">&#39;0&#39;</span><span class="o">]</span>
</span></span><span class="line"><span class="cl">Numerical IDs:  <span class="o">[</span>75, 380, 309, 476, 619, 36, 3924, 3964, 3959, 3993, 3978, 3983, 57, 3975, 3976, 60, 3974<span class="o">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">--- Tokenization Test ---
</span></span><span class="line"><span class="cl">Original Text: This blog is the most uninstagrammable blog ever
</span></span><span class="line"><span class="cl">Subword Tokens: <span class="o">[</span><span class="s1">&#39;This&#39;</span>, <span class="s1">&#39;‚ñÅb&#39;</span>, <span class="s1">&#39;log&#39;</span>, <span class="s1">&#39;‚ñÅis&#39;</span>, <span class="s1">&#39;‚ñÅthe&#39;</span>, <span class="s1">&#39;‚ñÅmo&#39;</span>, <span class="s1">&#39;st&#39;</span>, <span class="s1">&#39;‚ñÅun&#39;</span>, <span class="s1">&#39;inst&#39;</span>, <span class="s1">&#39;ag&#39;</span>, <span class="s1">&#39;ram&#39;</span>, <span class="s1">&#39;m&#39;</span>, <span class="s1">&#39;able&#39;</span>, <span class="s1">&#39;‚ñÅb&#39;</span>, <span class="s1">&#39;log&#39;</span>, <span class="s1">&#39;‚ñÅever&#39;</span><span class="o">]</span>
</span></span><span class="line"><span class="cl">Numerical IDs:  <span class="o">[</span>1894, 304, 3284, 376, 321, 313, 312, 1008, 3313, 1325, 428, 3937, 341, 304, 3284, 1495<span class="o">]</span>
</span></span></code></pre></div><p>The <code>!</code> is fallen back to UTF bytes. and the word <code>instagrammable</code> is split into multiple subword tokens. Also, original text can be recreated right back by simply concatenating the tokens. And replacing underscore by whitespace.
Now we can try Unigram model. and see the same sentences tokenized differently.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">tokenizers.models</span> <span class="kn">import</span> <span class="n">WordPiece</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">tokenizers.trainers</span> <span class="kn">import</span> <span class="n">WordPieceTrainer</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">tokenizers.pre_tokenizers</span> <span class="kn">import</span> <span class="n">Whitespace</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 1. Initialize the WordPiece Tokenizer</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># We specify the [UNK] token for handling words not found in the vocabulary.</span>
</span></span><span class="line"><span class="cl">    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">WordPiece</span><span class="p">(</span><span class="n">unk_token</span><span class="o">=</span><span class="s2">&#34;[UNK]&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 2. Configure Pre-tokenization</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Before the subword algorithm runs, we need to split the raw text into words.</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Whitespace splitting is the standard first step for most English NLP tasks.</span>
</span></span><span class="line"><span class="cl">    <span class="n">tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">Whitespace</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 3. Initialize the Trainer</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># We define our target vocabulary size and the special tokens required for </span>
</span></span><span class="line"><span class="cl">    <span class="c1"># downstream tasks (like BERT&#39;s [CLS] for classification or [SEP] for separators).</span>
</span></span><span class="line"><span class="cl">    <span class="n">trainer</span> <span class="o">=</span> <span class="n">WordPieceTrainer</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">vocab_size</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;[UNK]&#34;</span><span class="p">,</span> <span class="s2">&#34;[CLS]&#34;</span><span class="p">,</span> <span class="s2">&#34;[SEP]&#34;</span><span class="p">,</span> <span class="s2">&#34;[PAD]&#34;</span><span class="p">,</span> <span class="s2">&#34;[MASK]&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 4. Train the Model</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># The tokenizer scans the training file to build a vocabulary of the most </span>
</span></span><span class="line"><span class="cl">    <span class="c1"># frequent subword units.</span>
</span></span><span class="line"><span class="cl">    <span class="n">tokenizer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">files</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;train_data.txt&#34;</span><span class="p">],</span> <span class="n">trainer</span><span class="o">=</span><span class="n">trainer</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 5. Persist the Model</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Save the configuration and vocabulary to a JSON file for future inference.</span>
</span></span><span class="line"><span class="cl">    <span class="n">tokenizer</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&#34;wordpiece.json&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Training complete. &#39;wordpiece.json&#39; created.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 6. Metadata Inspection</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;-&#34;</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;WordPiece Model Metadata:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Total Vocab Size: </span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">get_vocab_size</span><span class="p">()</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 7. Testing Subword Tokenization</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># WordPiece shines at handling rare words by breaking them into meaningful chunks.</span>
</span></span><span class="line"><span class="cl">    <span class="n">test_sentences</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;Hello World! 1234567890&#39;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;This blog is the most uninstagrammable blog ever&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">test_sentences</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">--- Tokenization Test (WordPiece) ---&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Original Text: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Encode converts raw text into a Tokenizer object containing tokens and IDs</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># &#39;tokens&#39; shows the subword breakdown (e.g., &#39;un&#39;, &#39;##insta&#39;, etc.)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Subword Tokens: </span><span class="si">{</span><span class="n">output</span><span class="o">.</span><span class="n">tokens</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># &#39;ids&#39; are the numerical indices mapped to the vocabulary</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Numerical IDs:  </span><span class="si">{</span><span class="n">output</span><span class="o">.</span><span class="n">ids</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;An error occurred with WordPiece model: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p>You can see the following output:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">WordPiece Model Metadata:
</span></span><span class="line"><span class="cl">Total Vocab Size: <span class="m">2609</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">--- Tokenization Test <span class="o">(</span>WordPiece<span class="o">)</span> ---
</span></span><span class="line"><span class="cl">Original Text: Hello World! <span class="m">1234567890</span>
</span></span><span class="line"><span class="cl">Subword Tokens: <span class="o">[</span><span class="s1">&#39;H&#39;</span>, <span class="s1">&#39;##el&#39;</span>, <span class="s1">&#39;##lo&#39;</span>, <span class="s1">&#39;W&#39;</span>, <span class="s1">&#39;##or&#39;</span>, <span class="s1">&#39;##ld&#39;</span>, <span class="s1">&#39;[UNK]&#39;</span>, <span class="s1">&#39;[UNK]&#39;</span><span class="o">]</span>
</span></span><span class="line"><span class="cl">Numerical IDs:  <span class="o">[</span>37, 180, 214, 52, 162, 418, 0, 0<span class="o">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">--- Tokenization Test <span class="o">(</span>WordPiece<span class="o">)</span> ---
</span></span><span class="line"><span class="cl">Original Text: This blog is the most uninstagrammable blog ever
</span></span><span class="line"><span class="cl">Subword Tokens: <span class="o">[</span><span class="s1">&#39;This&#39;</span>, <span class="s1">&#39;b&#39;</span>, <span class="s1">&#39;##lo&#39;</span>, <span class="s1">&#39;##g&#39;</span>, <span class="s1">&#39;is&#39;</span>, <span class="s1">&#39;the&#39;</span>, <span class="s1">&#39;m&#39;</span>, <span class="s1">&#39;##os&#39;</span>, <span class="s1">&#39;##t&#39;</span>, <span class="s1">&#39;un&#39;</span>, <span class="s1">&#39;##ins&#39;</span>, <span class="s1">&#39;##ta&#39;</span>, <span class="s1">&#39;##g&#39;</span>, <span class="s1">&#39;##ra&#39;</span>, <span class="s1">&#39;##m&#39;</span>, <span class="s1">&#39;##ma&#39;</span>, <span class="s1">&#39;##ble&#39;</span>, <span class="s1">&#39;b&#39;</span>, <span class="s1">&#39;##lo&#39;</span>, <span class="s1">&#39;##g&#39;</span>, <span class="s1">&#39;ever&#39;</span><span class="o">]</span>
</span></span><span class="line"><span class="cl">Numerical IDs:  <span class="o">[</span>691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240<span class="o">]</span>
</span></span></code></pre></div><p>We can see the special character <code>##</code> in the output. We can also see the breakdowns not having any semantic meaning. They are likelihood based.
We can try the Unigram model now.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># &#39;Unigram&#39; is the default and usually recommended over BPE in SentencePiece.</span>
</span></span><span class="line"><span class="cl"><span class="n">options_unigram</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;input&#39;</span><span class="p">:</span> <span class="s1">&#39;train_data.txt&#39;</span><span class="p">,</span>        <span class="c1"># Path to the raw text file for training</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;model_prefix&#39;</span><span class="p">:</span> <span class="s1">&#39;unigram_model&#39;</span><span class="p">,</span>  <span class="c1"># Prefix for the output .model and .vocab files</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;vocab_size&#39;</span><span class="p">:</span> <span class="mi">1200</span><span class="p">,</span>               <span class="c1"># Desired size of the final vocabulary</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;model_type&#39;</span><span class="p">:</span> <span class="s1">&#39;unigram&#39;</span><span class="p">,</span>          <span class="c1"># Specifies the Unigram language model algorithm</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;character_coverage&#39;</span><span class="p">:</span> <span class="mf">0.9995</span><span class="p">,</span>     <span class="c1"># Percentage of characters covered by the model (0.9995 is standard for Latin scripts)</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;byte_fallback&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>            <span class="c1"># Enables mapping unknown characters to UTF-8 bytes to avoid &lt;unk&gt; tokens</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;split_digits&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>             <span class="c1"># Treats each digit as an individual token (useful for numerical data)</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;add_dummy_prefix&#39;</span><span class="p">:</span> <span class="kc">False</span>         <span class="c1"># Prevents adding a leading space (SentencePiece default is True)</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 1. Train the SentencePiece model using the defined options</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Starting Unigram training...&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">spm</span><span class="o">.</span><span class="n">SentencePieceTrainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="o">**</span><span class="n">options_unigram</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Training complete. &#39;unigram_model.model&#39; created.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 2. Load the trained model into a processor instance for inference</span>
</span></span><span class="line"><span class="cl">    <span class="n">sp_unigram</span> <span class="o">=</span> <span class="n">spm</span><span class="o">.</span><span class="n">SentencePieceProcessor</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">sp_unigram</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;unigram_model.model&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;-&#34;</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Unigram Model Metadata:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Total Vocab Size: </span><span class="si">{</span><span class="n">sp_unigram</span><span class="o">.</span><span class="n">get_piece_size</span><span class="p">()</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 3. Define test cases to evaluate how the model handles common and rare words</span>
</span></span><span class="line"><span class="cl">    <span class="n">test_sentences</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;Hello World! 1234567890&#39;</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;This blog is the most uninstagrammable blog ever&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 4. Iterate through test sentences to visualize subword segmentation</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">test_sentences</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">--- Tokenization Test (Unigram) ---&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Original Text: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># encode_as_pieces: Converts text into subword strings (visual representation)</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Subword Tokens: </span><span class="si">{</span><span class="n">sp_unigram</span><span class="o">.</span><span class="n">encode_as_pieces</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># encode_as_ids: Converts text into numerical indices for model input</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Numerical IDs:  </span><span class="si">{</span><span class="n">sp_unigram</span><span class="o">.</span><span class="n">encode_as_ids</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Handle potential errors during training or loading (e.g., missing input file)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;An error occurred with Unigram model: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div><p>The output is as follows:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">Unigram Model Metadata:
</span></span><span class="line"><span class="cl">Total Vocab Size: <span class="m">1200</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">--- Tokenization Test <span class="o">(</span>Unigram<span class="o">)</span> ---
</span></span><span class="line"><span class="cl">Original Text: Hello World! <span class="m">1234567890</span>
</span></span><span class="line"><span class="cl">Subword Tokens: <span class="o">[</span><span class="s1">&#39;&lt;0x48&gt;&#39;</span>, <span class="s1">&#39;e&#39;</span>, <span class="s1">&#39;ll&#39;</span>, <span class="s1">&#39;o&#39;</span>, <span class="s1">&#39;‚ñÅ&#39;</span>, <span class="s1">&#39;W&#39;</span>, <span class="s1">&#39;or&#39;</span>, <span class="s1">&#39;ld&#39;</span>, <span class="s1">&#39;&lt;0x21&gt;&#39;</span>, <span class="s1">&#39;‚ñÅ&#39;</span>, <span class="s1">&#39;1&#39;</span>, <span class="s1">&#39;2&#39;</span>, <span class="s1">&#39;3&#39;</span>, <span class="s1">&#39;4&#39;</span>, <span class="s1">&#39;5&#39;</span>, <span class="s1">&#39;&lt;0x36&gt;&#39;</span>, <span class="s1">&#39;7&#39;</span>, <span class="s1">&#39;8&#39;</span>, <span class="s1">&#39;&lt;0x39&gt;&#39;</span>, <span class="s1">&#39;0&#39;</span><span class="o">]</span>
</span></span><span class="line"><span class="cl">Numerical IDs:  <span class="o">[</span>75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311<span class="o">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">--- Tokenization Test <span class="o">(</span>Unigram<span class="o">)</span> ---
</span></span><span class="line"><span class="cl">Original Text: This blog is the most uninstagrammable blog ever
</span></span><span class="line"><span class="cl">Subword Tokens: <span class="o">[</span><span class="s1">&#39;T&#39;</span>, <span class="s1">&#39;his&#39;</span>, <span class="s1">&#39;‚ñÅb&#39;</span>, <span class="s1">&#39;l&#39;</span>, <span class="s1">&#39;o&#39;</span>, <span class="s1">&#39;g&#39;</span>, <span class="s1">&#39;‚ñÅis&#39;</span>, <span class="s1">&#39;‚ñÅthe&#39;</span>, <span class="s1">&#39;‚ñÅm&#39;</span>, <span class="s1">&#39;o&#39;</span>, <span class="s1">&#39;st&#39;</span>, <span class="s1">&#39;‚ñÅun&#39;</span>, <span class="s1">&#39;in&#39;</span>, <span class="s1">&#39;sta&#39;</span>, <span class="s1">&#39;g&#39;</span>, <span class="s1">&#39;ra&#39;</span>, <span class="s1">&#39;m&#39;</span>, <span class="s1">&#39;m&#39;</span>, <span class="s1">&#39;able&#39;</span>, <span class="s1">&#39;‚ñÅb&#39;</span>, <span class="s1">&#39;l&#39;</span>, <span class="s1">&#39;o&#39;</span>, <span class="s1">&#39;g&#39;</span>, <span class="s1">&#39;‚ñÅ&#39;</span>, <span class="s1">&#39;e&#39;</span>, <span class="s1">&#39;ver&#39;</span><span class="o">]</span>
</span></span></code></pre></div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/artificial-intelligence/">Artificial-Intelligence</a></li>
      <li><a href="http://localhost:1313/tags/large-language-models/">Large-Language-Models</a></li>
      <li><a href="http://localhost:1313/tags/tokenization/">Tokenization</a></li>
      <li><a href="http://localhost:1313/tags/sentencepiece/">Sentencepiece</a></li>
      <li><a href="http://localhost:1313/tags/byte-pair-encoding/">Byte-Pair-Encoding</a></li>
      <li><a href="http://localhost:1313/tags/wordpiece/">Wordpiece</a></li>
      <li><a href="http://localhost:1313/tags/unigram/">Unigram</a></li>
    </ul>
<nav class="paginav">
    <a class="prev" href="http://localhost:1313/posts/first-post/">
        <span class="title">¬´ Prev</span>
        <br>
        <span>My First Post</span>
    </a>
    <a class="next" href="http://localhost:1313/posts/word-embeddings/">
        <span class="title">Next ¬ª</span>
        <br>
        <span>Semantic Alchemy: Cracking Word2Vec with CBOW and Skip-Gram</span>
    </a>
</nav><div class="social-icons" align="left">
    <a href="https://x.com/siddheshinamdar" target="_blank" rel="noopener noreferrer me"
        title="X">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor">
    <path
        d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z">
    </path>
</svg>
    </a>
    <a href="https://www.linkedin.com/in/siddhesh-inamdar-24634694/" target="_blank" rel="noopener noreferrer me"
        title="Linkedin">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path>
    <rect x="2" y="9" width="4" height="12"></rect>
    <circle cx="4" cy="4" r="2"></circle>
</svg>
    </a>
    <a href="https://medium.com/@siddheshnmdr" target="_blank" rel="noopener noreferrer me"
        title="Medium">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 76.000000 76.000000" fill="currentColor" stroke-width="2"
    preserveAspectRatio="xMidYMid meet">
    <g transform="translate(0.000000,76.000000) scale(0.100000,-0.100000)">
        <path
            d="M0 380 l0 -380 380 0 380 0 0 380 0 380 -380 0 -380 0 0 -380z m334 85 c30 -63 57 -115 59 -115 2 0 16 30 31 68 15 37 37 88 49 115 l20 47 76 0 76 -1 -27 -20 -28 -21 0 -151 c0 -150 0 -151 27 -179 l27 -28 -109 0 -109 0 27 28 c26 27 27 32 26 143 0 131 3 134 -71 -58 -24 -62 -48 -113 -53 -113 -6 0 -17 16 -24 35 -7 19 -36 83 -64 142 l-52 108 -3 -98 c-3 -97 -2 -99 28 -133 16 -19 30 -39 30 -44 0 -6 -31 -10 -70 -10 -45 0 -70 4 -70 11 0 6 14 27 30 46 30 33 30 35 30 151 0 116 0 118 -31 155 l-30 37 75 0 76 0 54 -115z" />
    </g>
</svg>
    </a>
    <a href="https://github.com/SidInamdar" target="_blank" rel="noopener noreferrer me"
        title="Github">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path
        d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
    </path>
</svg>
    </a>
</div>
</footer>

  <div class="newsletter-form" style="width: 100%; margin-top: 10px;">
    <script async src="https://subscribe-forms.beehiiv.com/embed.js"></script><iframe
        src="https://subscribe-forms.beehiiv.com/cb315fa7-29d0-46fa-ae1f-a34a59e9d3b0" class="beehiiv-embed"
        data-test-id="beehiiv-embed" frameborder="0" scrolling="no"
        style="width: 100%; height: 32px; margin: 0; border-radius: 0px 0px 0px 0px !important; background-color: transparent; box-shadow: 0 0 #0000; max-width: 100%;"></iframe>
</div>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="http://localhost:1313/">Vectors &amp; Verbs</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
