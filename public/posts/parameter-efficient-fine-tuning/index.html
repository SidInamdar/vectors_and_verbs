<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Crafty Patchwork: Parameter-Efficient Fine-Tuning | Vectors &amp; Verbs</title>
<meta name="keywords" content="artificial-intelligence, large-language-models, fine-tuning, peft, lora, qlora">
<meta name="description" content="An introduction to Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA, QLoRA, and more.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/parameter-efficient-fine-tuning/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.af84f51093f576d67527cfa457c9742fd73a0c82d9399c6bc9c935e115ac4e68.css" integrity="sha256-r4T1EJP1dtZ1J8&#43;kV8l0L9c6DILZOZxryck14RWsTmg=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/parameter-efficient-fine-tuning/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>


</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Vectors &amp; Verbs (Alt + H)">Vectors &amp; Verbs</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about/" title="About Me">
                    <span>About Me</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Crafty Patchwork: Parameter-Efficient Fine-Tuning
    </h1>
    <div class="post-meta"><span title='2026-01-12 10:52:56 +0530 IST'>January 12, 2026</span>&nbsp;·&nbsp;<span>7 min</span>

</div>
  </header> 
  <div class="post-content"><p>The typical modern trajectory of creating intelligent models involves adding scores of GPUs and terabytes of data. This high barrier to entry is a significant limitation, creating a &ldquo;wall&rdquo; between common engineers and the democratization of AI.</p>
<p>Models that require billions of parameters to function for even a single training pass often cannot fit on consumer hardware. Although full training may be out of reach, we can still nudge these models in desired directions—a process known as <strong>Fine-Tuning</strong>.</p>
<p>In the traditional sense, <strong>Full Fine-Tuning (FFT)</strong> was performed on earlier models where every parameter was updated during backpropagation. However, for a model like Llama 3 with 70B parameters, FFT is computationally prohibitive. Beyond the parameters themselves, the model must store gradients, optimizer states, and activation caches, which can dwarf the memory footprint of the original weights. This is where <strong>Parameter-Efficient Fine-Tuning (PEFT)</strong> techniques come in.</p>
<h3 id="the-geometry-of-adaptation">The Geometry of Adaptation<a hidden class="anchor" aria-hidden="true" href="#the-geometry-of-adaptation">#</a></h3>
<p>After navigating the loss landscapes of model parameters, researchers discovered that LLMs are often over-parameterized. The solutions to specific tasks frequently lie in a much lower-dimensional manifold or &ldquo;subspace&rdquo; of the parameter space. This is referred to as the <strong>Intrinsic Dimension</strong> of the objective function—the minimum number of degrees of freedom required to reach a satisfactory solution.</p>
<p>During initial training, a model learns vast, general representations of language, requiring high dimensionality to capture the nuances of world knowledge. However, when fine-tuning for a specific task (such as adapting a model to write SQL code), the &ldquo;delta&rdquo; or required change in weights is not random. The model only needs to adjust a few dimensions of its subspace to master the new task.</p>
<p>This realization is the key behind the success of PEFT. In fact, the larger the model, the less &ldquo;steering&rdquo; effort is needed during fine-tuning. Mathematically, traditional updates look like this:</p>
<p>$$h = W_0x$$
$$W_{\text{tuned}} = W_0 + \Delta W$$
$$h = W_0x + \Delta Wx$$</p>
<p>Here, $W_0$ and $\Delta W$ share the same dimensions. However, according to the intrinsic dimension hypothesis, $\Delta W$ is <strong>rank-deficient</strong>, meaning it contains far fewer unique dimensions than its total size would suggest. This concept—that $\Delta W$ can be represented more compactly—is the cornerstone of LoRA.</p>
<h3 id="low-rank-adaptation-lora">Low-Rank Adaptation (LoRA)<a hidden class="anchor" aria-hidden="true" href="#low-rank-adaptation-lora">#</a></h3>
<p>LoRA converts the theoretical concept of intrinsic dimension into a practical architecture. Instead of training $\Delta W$ directly, we decompose it into two smaller, trainable matrices, $A$ and $B$:</p>
<p>$$\Delta W = BA$$</p>
<ul>
<li><strong>Matrix B</strong>: A trainable matrix with dimensions $d \times r$.</li>
<li><strong>Matrix A</strong>: A trainable matrix with dimensions $r \times k$.</li>
</ul>
<p>The <strong>rank $r$</strong> is a hyperparameter that determines the dimensionality of the update. For minor refinements (like improving linguistic nuance), a rank of $r=4$ is often sufficient. For more complex tasks (like learning a new topic), $r=8$ or higher might be used.</p>
<p>Because $r \ll \min(d, k)$, the number of parameters in $A$ and $B$ is vastly smaller than in $\Delta W$.</p>
<h4 id="example-memory-savings">Example: Memory Savings<a hidden class="anchor" aria-hidden="true" href="#example-memory-savings">#</a></h4>
<p>Consider a weight matrix $W$ with dimensions $4096 \times 4096$:</p>
<ul>
<li><strong>Full $\Delta W$</strong>: $4096 \times 4096 = 16,777,216$ parameters.</li>
<li><strong>LoRA ($r = 8$)</strong>:
<ul>
<li>Matrix $A$: $8 \times 4096 = 32,768$</li>
<li>Matrix $B$: $4096 \times 8 = 32,768$</li>
<li><strong>Total Parameters</strong>: $65,536$</li>
</ul>
</li>
</ul>
<p><strong>Reduction Percentage</strong>:
$$\frac{65,536}{16,777,216} \times 100 \approx 0.39%$$</p>
<p>The number of trainable parameters is reduced by over 99%.</p>
<h4 id="the-alpha-scaling-factor">The Alpha Scaling Factor<a hidden class="anchor" aria-hidden="true" href="#the-alpha-scaling-factor">#</a></h4>
<p>The LoRA update is typically scaled:
$$h = W_0 x + \frac{\alpha}{r} (B A x)$$</p>
<p>$\alpha$ (Alpha) is a scaling constant, similar to a learning rate, that amplifies the signal of the adapter. Dividing by $r$ ensures that if you change the rank during experimentation, you don&rsquo;t need to drastically re-tune $\alpha$. It keeps the update magnitude roughly constant—much like steering a giant ship using a small but precisely controlled boat.</p>
<h4 id="initialization-strategy">Initialization Strategy<a hidden class="anchor" aria-hidden="true" href="#initialization-strategy">#</a></h4>
<p>If the added parameters ($A$ and $B$) are initialized with random values, the initial gradients would be chaotic, harming performance. Instead:</p>
<ul>
<li><strong>Matrix A</strong> is initialized with a random Gaussian distribution (small random numbers).</li>
<li><strong>Matrix B</strong> is initialized to zero.</li>
</ul>
<p>At the start of training, $\Delta W$ (which is $B \times A$) is exactly zero. This ensures the model starts with its original performance and gradually adapts.</p>
<h4 id="vram-and-latency">VRAM and Latency<a hidden class="anchor" aria-hidden="true" href="#vram-and-latency">#</a></h4>
<p>$$VRAM \approx \text{Base Model Size} + \text{Optimizer States for LoRA Parameters}$$</p>
<p>For a 70B model, FFT might require hundreds of gigabytes of VRAM. With LoRA, the memory overhead is significantly reduced.</p>
<p>Furthermore, LoRA offers <strong>Zero Latency Inference</strong>. Unlike &ldquo;Adapter Layers&rdquo; which insert new physical layers into the network, LoRA weights can be mathematically merged into the base weights ($W_{\text{new}} = W_0 + BA$) after training. This means no extra operations are required during inference.</p>
<hr>
<h3 id="qlora-breaking-the-memory-wall-with-quantization">QLoRA: Breaking the Memory Wall with Quantization<a hidden class="anchor" aria-hidden="true" href="#qlora-breaking-the-memory-wall-with-quantization">#</a></h3>
<p>Quantized LoRA (QLoRA) addresses the model weight bottleneck. It reduces the floating-point precision of the original model weights via quantization while keeping the LoRA parameters in full precision.</p>
<p>Quantization is the process of mapping high-precision floating-point numbers to lower-precision integers. While standard 4-bit quantization (Int4) evenly spaces values, neural network weights typically follow a Normal distribution, clustering near zero. QLoRA solves this with <strong>NF4 (NormalFloat 4)</strong>: it uses quantile quantization where each bin holds an equal probability mass of the distribution.</p>
<p>This innovation allows a 70B model to fit into roughly 35–40 GB of VRAM, making it possible to fine-tune on a single 48GB workstation card (like an RTX 6000 Ada) or two consumer 24GB cards.</p>
<h4 id="double-quantization">Double Quantization<a hidden class="anchor" aria-hidden="true" href="#double-quantization">#</a></h4>
<p>In quantization, we use &ldquo;quantization constants&rdquo; (scaling factors) to map small integers back to their approximate real values. To maintain accuracy, weights are quantized in blocks (e.g., 64 weights per block), each with its own 32-bit constant. QLoRA introduces <strong>Double Quantization</strong>, which quantizes the constants themselves.</p>
<ol>
<li><strong>First Pass</strong>: Quantize weights to 4-bit, producing 32-bit constants.</li>
<li><strong>Second Pass</strong>: Quantize those 32-bit constants into 8-bit floats.</li>
</ol>
<p>This &ldquo;inception-style&rdquo; approach reduces the memory overhead of the constants from ~4GB to ~0.1GB. In the tight confines of GPU memory, every gigabyte counts.</p>
<h4 id="the-compute-memory-trade-off">The Compute-Memory Trade-off<a hidden class="anchor" aria-hidden="true" href="#the-compute-memory-trade-off">#</a></h4>
<p>QLoRA is a marvel of efficiency, but it introduces a trade-off in compute latency. Standard hardware cannot perform computation directly on 4-bit tensors, so QLoRA uses <strong>On-The-Fly (OTF) Dequantization</strong>. Weights are dequantized to 16-bit for the calculation and then discarded. This makes QLoRA roughly 30% to 50% slower than standard LoRA.</p>
<hr>
<h3 id="dora-weight-decomposed-low-rank-adaptation">DoRA: Weight-Decomposed Low-Rank Adaptation<a hidden class="anchor" aria-hidden="true" href="#dora-weight-decomposed-low-rank-adaptation">#</a></h3>
<p><strong>DoRA</strong> is a recent advancement that addresses mathematical caveats in standard LoRA. Researchers found that LoRA doesn&rsquo;t always match the &ldquo;learning style&rdquo; of full fine-tuning because magnitude and direction updates are coupled.</p>
<p>In vanilla LoRA, changing the direction of the weights often accidentally changes their magnitude as well. It’s like driving a car where turning the steering wheel also forces you to accelerate. DoRA decouples these two components.</p>
<h4 id="the-mathematics-of-dora">The Mathematics of DoRA<a hidden class="anchor" aria-hidden="true" href="#the-mathematics-of-dora">#</a></h4>
<p>DoRA re-parametrizes weight updates by separating the magnitude ($m$) and the direction ($V$):</p>
<p>$$W = m \frac{V + \Delta V}{||V + \Delta V||}$$</p>
<h4 id="example-the-dora-advantage">Example: The DoRA Advantage<a hidden class="anchor" aria-hidden="true" href="#example-the-dora-advantage">#</a></h4>
<p>Imagine a weight vector with values $[3, 4]$.</p>
<ol>
<li>
<p><strong>The LoRA Way (Coupled)</strong>: You add an update $\Delta W = [1, 1]$.
$$W_{\text{new}} = [3, 4] + [1, 1] = [4, 5]$$
The ratio changed (from 3:4 to 4:5), but the length (magnitude) jumped from 5 to $\approx 6.4$. If the model only wanted to change the &ldquo;concept&rdquo; (direction) without becoming &ldquo;louder&rdquo; (magnitude), it couldn&rsquo;t.</p>
</li>
<li>
<p><strong>The DoRA Way (Decomposed)</strong>: DoRA treats magnitude ($m = 5$) and direction ($V = [3, 4]$) separately.</p>
<ul>
<li><strong>Update Direction</strong>: Add the LoRA update to $V$, getting $V&rsquo; = [4, 5]$.</li>
<li><strong>Normalize</strong>: &ldquo;Re-normalize&rdquo; $V&rsquo;$ so its length is 1 again ($\approx [0.62, 0.78]$).</li>
<li><strong>Apply Magnitude</strong>: Multiply that pure direction by the magnitude $m$ (which might stay 5).
$$W_{\text{final}} = 5 \times [0.62, 0.78] = [3.1, 3.9]$$</li>
</ul>
</li>
</ol>
<p>By giving magnitude and direction their own degrees of freedom, DoRA mimics Full Fine-Tuning more closely, often achieving superior results with the same number of parameters.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/artificial-intelligence/">Artificial-Intelligence</a></li>
      <li><a href="http://localhost:1313/tags/large-language-models/">Large-Language-Models</a></li>
      <li><a href="http://localhost:1313/tags/fine-tuning/">Fine-Tuning</a></li>
      <li><a href="http://localhost:1313/tags/peft/">Peft</a></li>
      <li><a href="http://localhost:1313/tags/lora/">Lora</a></li>
      <li><a href="http://localhost:1313/tags/qlora/">Qlora</a></li>
    </ul>
<nav class="paginav">
    <a class="prev" href="http://localhost:1313/posts/llm-hardware-optimization/">
        <span class="title">« Prev</span>
        <br>
        <span>Mission Impossible: Fitting Trillion-Parameter Giants into 80GB GPUs</span>
    </a>
</nav><div class="social-icons" align="left">
    <a href="https://x.com/siddheshinamdar" target="_blank" rel="noopener noreferrer me"
        title="X">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor">
    <path
        d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z">
    </path>
</svg>
    </a>
    <a href="https://www.linkedin.com/in/siddhesh-inamdar-24634694/" target="_blank" rel="noopener noreferrer me"
        title="Linkedin">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path>
    <rect x="2" y="9" width="4" height="12"></rect>
    <circle cx="4" cy="4" r="2"></circle>
</svg>
    </a>
    <a href="https://medium.com/@siddheshnmdr" target="_blank" rel="noopener noreferrer me"
        title="Medium">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 76.000000 76.000000" fill="currentColor" stroke-width="2"
    preserveAspectRatio="xMidYMid meet">
    <g transform="translate(0.000000,76.000000) scale(0.100000,-0.100000)">
        <path
            d="M0 380 l0 -380 380 0 380 0 0 380 0 380 -380 0 -380 0 0 -380z m334 85 c30 -63 57 -115 59 -115 2 0 16 30 31 68 15 37 37 88 49 115 l20 47 76 0 76 -1 -27 -20 -28 -21 0 -151 c0 -150 0 -151 27 -179 l27 -28 -109 0 -109 0 27 28 c26 27 27 32 26 143 0 131 3 134 -71 -58 -24 -62 -48 -113 -53 -113 -6 0 -17 16 -24 35 -7 19 -36 83 -64 142 l-52 108 -3 -98 c-3 -97 -2 -99 28 -133 16 -19 30 -39 30 -44 0 -6 -31 -10 -70 -10 -45 0 -70 4 -70 11 0 6 14 27 30 46 30 33 30 35 30 151 0 116 0 118 -31 155 l-30 37 75 0 76 0 54 -115z" />
    </g>
</svg>
    </a>
    <a href="https://github.com/SidInamdar" target="_blank" rel="noopener noreferrer me"
        title="Github">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path
        d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
    </path>
</svg>
    </a>
</div>
</footer>

  <div class="newsletter-form" style="width: 100%; margin-top: 10px;">
    <script async src="https://subscribe-forms.beehiiv.com/embed.js"></script><iframe
        src="https://subscribe-forms.beehiiv.com/cb315fa7-29d0-46fa-ae1f-a34a59e9d3b0" class="beehiiv-embed"
        data-test-id="beehiiv-embed" frameborder="0" scrolling="no"
        style="width: 100%; height: 32px; margin: 0; border-radius: 0px 0px 0px 0px !important; background-color: transparent; box-shadow: 0 0 #0000; max-width: 100%;"></iframe>
</div>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="http://localhost:1313/">Vectors &amp; Verbs</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
