<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE | Vectors &amp; Verbs</title>
<meta name="keywords" content="nlp, transformers, deep-learning, math">
<meta name="description" content="
Positional Embeddings are the &ldquo;voice&rdquo; that tell Transformers where words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.
&ldquo;The dog chased the cat&rdquo; and &ldquo;cat chased the dog&rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/positional-embeddings/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.02c0341269d3105359b7e3ab511a3586fbb52380b509e60780c94c41e1d8bdf9.css" integrity="sha256-AsA0EmnTEFNZt&#43;OrURo1hvu1I4C1CeYHgMlMQeHYvfk=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/positional-embeddings/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>


</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Vectors &amp; Verbs (Alt + H)">Vectors &amp; Verbs</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about/" title="About Me">
                    <span>About Me</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;Â»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE
    </h1>
    <div class="post-meta"><span title='2026-01-09 21:45:00 +0530 IST'>January 9, 2026</span>&nbsp;Â·&nbsp;<span>7 min</span>

</div>
  </header> 
  <div class="post-content"><p><img alt="Positional Embeddings Map" loading="lazy" src="/images/positional-embeddings-header.png"></p>
<p>Positional Embeddings are the &ldquo;voice&rdquo; that tell Transformers <em>where</em> words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.</p>
<p>&ldquo;The dog chased the cat&rdquo; and &ldquo;cat chased the dog&rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.</p>
<h3 id="-sinusoidal-embeddings-the-odometer-of-language">ðŸ”¥ Sinusoidal Embeddings: The Odometer of Language<a hidden class="anchor" aria-hidden="true" href="#-sinusoidal-embeddings-the-odometer-of-language">#</a></h3>
<p>Introduced in the seminal &ldquo;Attention Is All You Need&rdquo; paper, this was the industry standard for many years. This approach adds a vector to each word embedding based on its position. The vector is computed out of wavelike patterns that are periodic and smooth. Mathematically, for a position $pos$ and embedding dimension $i$:</p>
<p>$$
PE_{(pos, i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$</p>
<p>The model uses sine and cosine waves of different frequencies to capture both short and long-range dependencies. The frequency of the waves decreases as the embedding dimension increases, which helps capture long-range dependencies in the input sequence.</p>
<p>The creators wanted the model to calculate distance easily, which is why a linear function was used. The model simply rotates the current position vector using a rotation matrix to derive the next position.</p>
<div>
$$
\begin{pmatrix}
\cos(k) & \sin(k) \\
-\sin(k) & \cos(k)
\end{pmatrix} \cdot PE(pos) \approx PE(pos + k)
$$
</div>
<p><strong>Low dimensions</strong> act like a fast-ticking clock (high frequency, changing rapidly) and <strong>high dimensions</strong> act like a slow clock (low frequency, changing slowly).</p>
<p>The number <strong>10,000</strong> is an arbitrarily chosen high number to reduce <strong>Aliasing</strong>. By choosing such a high number, the creators ensured that the wavelength of the slowest dimension is incredibly long. Consequently, the pattern practically never repeats, and each position remains unique.</p>
<p>This combination works like a <strong>mechanical odometer dial</strong>. Each subsequent dimension acts as a slower-moving gear. The model learns to read these &ldquo;dials&rdquo; to understand exactly where a word sits in the sequence.</p>
<p>This mitigated the challenge of exploding gradients that previous array-based positional embeddings suffered from (e.g., adding integers 1, 2, 3&hellip;). This model keeps values between -1 and +1, adding stability while ensuring every position gets a distinct code.</p>
<blockquote>
<p><strong>The Problem:</strong> This approach pollutes the word embedding matrix (semantic information). While high-dimensional vectors are almost always orthogonalâ€”meaning the noise doesn&rsquo;t fully destroy the word&rsquo;s identityâ€”it does not strictly <em>guarantee</em> purity.</p>
</blockquote>
<p>This led to a more mature approach.</p>
<h3 id="-alibi-the-fading-streetlight">ðŸ”¥ ALiBi: The Fading Streetlight<a hidden class="anchor" aria-hidden="true" href="#-alibi-the-fading-streetlight">#</a></h3>
<p><strong>Attention with Linear Biases (ALiBi)</strong> is a method where, instead of adding a vector to the embeddings at the start, the model injects positional information linearly, directly into the attention scores. If Sinusoidal Encoding asks, &ldquo;Where am I on the map?&rdquo;, ALiBi asks, &ldquo;How much harder should I squint to see you?&rdquo;</p>
<p>The biggest problem ALiBi solved is the <strong>&ldquo;Invisible Wall&rdquo;</strong>. If you train a Transformer on sequences of length 2048 and then feed it a book with 3000 words, it will crash or output garbage at word 2049. This is a failure of <strong>Extrapolation</strong>.</p>
<p>ALiBi throws away the idea of adding positional vectors entirely. Instead, it devises a &ldquo;penalty&rdquo; system for attention scores:</p>
<p>$$
\text{Score} = (q \cdot k^\top) - (m \cdot \text{distance})
$$</p>
<p><strong>Distance:</strong> If words A and B are neighbors, the distance is 0, so the penalty is 0. Although the distance is linear, it does not cause exploding gradients because the raw attention scores are normalized by a <strong>softmax</strong> function. This distance prioritizes nearby words (local context) over distant ones.</p>
<p><strong>Slope ($m$):</strong> If we always penalize distance equally, the model becomes myopic. ALiBi mitigates this by assigning a different slope value to each attention head:</p>
<ul>
<li><strong>Head 1 (The Historian):</strong> Very low slope. The penalty grows slowly, allowing it to see far back.</li>
<li><strong>Head 2 (The Reader):</strong> Medium slope.</li>
<li><strong>Head 3 (The Myopic):</strong> Very high slope (e.g., 0.5); it effectively cannot see beyond 2-3 words.</li>
</ul>
<p>This mimics holding a <strong>lantern</strong>. Words close to you are bright, while words further away naturally fade into darkness.</p>
<p><strong>Extrapolation:</strong> If you walk 5 miles down the road (a longer sequence), your lantern works exactly the same way. The physics of light decay are constant, allowing the model to handle sequences longer than it was trained on.</p>
<p>However, ALiBi forces a &ldquo;sliding window&rdquo; view where distant words eventually become invisible. This brings us to the modern standard.</p>
<h3 id="-rope-the-rotary-revolution">ðŸ”¥ RoPE: The Rotary Revolution<a hidden class="anchor" aria-hidden="true" href="#-rope-the-rotary-revolution">#</a></h3>
<p><strong>Rotary Position Embeddings (RoPE)</strong> is the current industry standard (used in Llama 3, Mistral, PaLM) because it mathematically unifies &ldquo;absolute position&rdquo; with &ldquo;relative position.&rdquo; This method <strong>rotates</strong> the vectors in the Query and Key matrices based on their absolute position.</p>
<p>If a vector has 512 dimensions, RoPE breaks it into 256 pairs. Each pair is rotated by a different frequency:</p>
<p>$$
\theta_i = 10000^{-2i/d}
$$</p>
<p>The <strong>First pair ($i = 0$)</strong> is rotated by the highest frequency. The <strong>Last pair ($i = 255$)</strong> is rotated by the lowest frequency. This spread ensures the model has &ldquo;high precision&rdquo; (fast rotation) to distinguish immediate neighbors, and &ldquo;low precision&rdquo; (slow rotation) to track global position without the pattern repeating.</p>
<p><strong>Doesn&rsquo;t rotation change meaning?</strong> Yes and No. The vector for &ldquo;Apple&rdquo; at position 1 and &ldquo;Apple&rdquo; at position 256 will look completely different in terms of coordinates. However, rotation does <strong>not</strong> change the <em>semantic strength</em> (magnitude/norm) of the vector.</p>
<p><strong>The Relativity Trick:</strong> Transformers compare words using dot products (angles). RoPE relies on the fact that the attention mechanism ignores the absolute rotation and only detects the <strong>difference</strong> in rotation (relative position). Thus, the &ldquo;relative meaning&rdquo; is encoded purely into the angle between words, leaving the semantic &ldquo;magnitude&rdquo; untouched.</p>
<h3 id="-longrope-the-bifocal-lens">ðŸ”¥ LongRoPE: The Bifocal Lens<a hidden class="anchor" aria-hidden="true" href="#-longrope-the-bifocal-lens">#</a></h3>
<p>While RoPE is powerful, it struggles when stretching context windows from 4k to 2 million tokens. The &ldquo;fast&rdquo; dimensions spin so rapidly over long distances that they become random noise. <strong>LongRoPE</strong> solves this using an evolutionary search algorithm to find a unique scaling factor ($\lambda$) for each dimension.</p>
<h3 id="the-equations">The Equations<a hidden class="anchor" aria-hidden="true" href="#the-equations">#</a></h3>
<p>Instead of rotating by the standard $\theta_i$, LongRoPE rotates by a rescaled frequency:</p>
<p>$$
\theta&rsquo;_i = \lambda_i \theta_i
$$</p>
<p>To efficiently find these $\lambda$ values without searching an infinite space, the algorithm enforces a <strong>monotonicity constraint</strong> based on NTK theory:</p>
<p>$$
\lambda_i \le \lambda_{i+1}
$$</p>
<p>This ensures that low-frequency dimensions (global context) are stretched more than high-frequency dimensions (local context), creating a &ldquo;bifocal&rdquo; effect:</p>
<ol>
<li><strong>High Frequencies (Local):</strong> Kept sharp ($\lambda \approx 1$) to maintain grammar.</li>
<li><strong>Low Frequencies (Global):</strong> Stretched ($\lambda &gt; 1$) to track massive distances without repeating.
Not worth perusing more since the next one is much better at long context positional encoding.</li>
</ol>
<h3 id="-hope-the-hyperbolic-slide">ðŸ”¥ HoPE: The Hyperbolic Slide<a hidden class="anchor" aria-hidden="true" href="#-hope-the-hyperbolic-slide">#</a></h3>
<p><strong>Hyperbolic Rotary Positional Encoding (HoPE)</strong> moves from the geometry of a circle to the geometry of a hyperbola (inspired by Lorentz transformations). It was designed to fix a subtle flaw in RoPE: the &ldquo;wobbly&rdquo; or oscillatory nature of attention scores.</p>
<h4 id="the-core-problem-ropes-wobble">The Core Problem: RoPE&rsquo;s &ldquo;Wobble&rdquo;<a hidden class="anchor" aria-hidden="true" href="#the-core-problem-ropes-wobble">#</a></h4>
<p>In standard RoPE, we rotate vectors around a circle.</p>
<ul>
<li><strong>The Issue:</strong> Circles repeat. A dial at $361^\circ$ looks identical to $1^\circ$. Even with different frequencies, the dot product fluctuates up and down as distance increases.</li>
<li><strong>The Consequence:</strong> The model gets confused at long distances, potentially mistaking a word 1,000 tokens away for a neighbor just because the rotation cycles aligned, creating &ldquo;noise.&rdquo;</li>
</ul>
<h4 id="the-hyperbolic-shift">The &ldquo;Hyperbolic&rdquo; Shift<a hidden class="anchor" aria-hidden="true" href="#the-hyperbolic-shift">#</a></h4>
<p>HoPE replaces trigonometric functions ($\sin, \cos$) with hyperbolic functions ($\sinh, \cosh$).</p>
<ul>
<li><strong>Circular Rotation:</strong> Keeps distance from the center constant.</li>
<li><strong>Hyperbolic Rotation:</strong> Moves points along a hyperbola. Crucially, <strong>hyperbolic functions do not repeat</strong>; they grow or decay exponentially.</li>
</ul>
<p>By using hyperbolic rotation, HoPE guarantees that as words get further apart, the attention score <strong>decays monotonically</strong> (smoothly drops).</p>
<h4 id="the-equations-1">The Equations<a hidden class="anchor" aria-hidden="true" href="#the-equations-1">#</a></h4>
<p>HoPE uses a hyperbolic matrix $B(\theta, m)$ defined using the Lorentz Boost structure:</p>
<div>
$$
B(\theta, m) = \begin{pmatrix} \cosh(m\theta) & \sinh(m\theta) \\ \sinh(m\theta) & \cosh(m\theta) \end{pmatrix}
$$
</div>
Since hyperbolic functions grow to infinity, HoPE introduces a decay penalty ($e^{-m\theta'}$). The final attention score simplifies beautifully to a function of relative distance:
<p>$$
\text{Score} \propto e^{-|m-n|(\theta&rsquo; - \theta)}
$$</p>
<p>This ensures the attention score <strong>exponentially decays</strong> as distance increases, eliminating the wobbles of standard RoPE. This the latest model that is taking up the space in the industry for extremely long context models.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/nlp/">Nlp</a></li>
      <li><a href="http://localhost:1313/tags/transformers/">Transformers</a></li>
      <li><a href="http://localhost:1313/tags/deep-learning/">Deep-Learning</a></li>
      <li><a href="http://localhost:1313/tags/math/">Math</a></li>
    </ul>
<nav class="paginav">
    <a class="prev" href="http://localhost:1313/posts/glove-and-fasttext/">
        <span class="title">Â« Prev</span>
        <br>
        <span>The Global Accountant and the Subword Surgeon: Decoding GloVe and FastText</span>
    </a>
</nav><div class="social-icons" align="left">
    <a href="https://x.com/siddheshinamdar" target="_blank" rel="noopener noreferrer me"
        title="X">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor">
    <path
        d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z">
    </path>
</svg>
    </a>
    <a href="https://www.linkedin.com/in/siddhesh-inamdar-24634694/" target="_blank" rel="noopener noreferrer me"
        title="Linkedin">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path>
    <rect x="2" y="9" width="4" height="12"></rect>
    <circle cx="4" cy="4" r="2"></circle>
</svg>
    </a>
    <a href="https://medium.com/@siddheshnmdr" target="_blank" rel="noopener noreferrer me"
        title="Medium">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 76.000000 76.000000" fill="currentColor" stroke-width="2"
    preserveAspectRatio="xMidYMid meet">
    <g transform="translate(0.000000,76.000000) scale(0.100000,-0.100000)">
        <path
            d="M0 380 l0 -380 380 0 380 0 0 380 0 380 -380 0 -380 0 0 -380z m334 85 c30 -63 57 -115 59 -115 2 0 16 30 31 68 15 37 37 88 49 115 l20 47 76 0 76 -1 -27 -20 -28 -21 0 -151 c0 -150 0 -151 27 -179 l27 -28 -109 0 -109 0 27 28 c26 27 27 32 26 143 0 131 3 134 -71 -58 -24 -62 -48 -113 -53 -113 -6 0 -17 16 -24 35 -7 19 -36 83 -64 142 l-52 108 -3 -98 c-3 -97 -2 -99 28 -133 16 -19 30 -39 30 -44 0 -6 -31 -10 -70 -10 -45 0 -70 4 -70 11 0 6 14 27 30 46 30 33 30 35 30 151 0 116 0 118 -31 155 l-30 37 75 0 76 0 54 -115z" />
    </g>
</svg>
    </a>
</div>
</footer>

  <div class="newsletter-form" style="width: 100%; margin-top: 10px;">
    <script async src="https://subscribe-forms.beehiiv.com/embed.js"></script><iframe
        src="https://subscribe-forms.beehiiv.com/cb315fa7-29d0-46fa-ae1f-a34a59e9d3b0" class="beehiiv-embed"
        data-test-id="beehiiv-embed" frameborder="0" scrolling="no"
        style="width: 100%; height: 32px; margin: 0; border-radius: 0px 0px 0px 0px !important; background-color: transparent; box-shadow: 0 0 #0000; max-width: 100%;"></iframe>
</div>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="http://localhost:1313/">Vectors &amp; Verbs</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
