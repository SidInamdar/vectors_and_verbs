<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Math on Vectors &amp; Verbs</title>
    <link>http://localhost:1313/tags/math/</link>
    <description>Recent content in Math on Vectors &amp; Verbs</description>
    <generator>Hugo -- 0.154.2</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 Jan 2026 21:45:00 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/math/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The Geometry of Meaning: Sine, ALiBi, RoPE, and HoPE</title>
      <link>http://localhost:1313/posts/positional-embeddings/</link>
      <pubDate>Fri, 09 Jan 2026 21:45:00 +0530</pubDate>
      <guid>http://localhost:1313/posts/positional-embeddings/</guid>
      <description>&lt;p&gt;&lt;img alt=&#34;Positional Embeddings Map&#34; loading=&#34;lazy&#34; src=&#34;http://localhost:1313/images/positional-embeddings-header.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Positional Embeddings are the &amp;ldquo;voice&amp;rdquo; that tell Transformers &lt;em&gt;where&lt;/em&gt; words are in a sentence. Without them, there are no mathematical principles by which LLMs can differentiate sentences with alternate arrangements of words.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The dog chased the cat&amp;rdquo; and &amp;ldquo;cat chased the dog&amp;rdquo; are mathematically indistinguishable to a Transformer without positional embeddings, as self-attention treats them as a chaotic bag of marbles. Here is an in-depth breakdown of the major positional encoders that solved this problem. The key ingredient in these algorithms is the delicate balance of adding information to word embeddings without polluting the semantic vectors.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
