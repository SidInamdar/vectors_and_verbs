Summary
Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose \textbf{Youtu-Agent}, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a \textbf{Workflow} mode for standard tasks and a \textbf{Meta-Agent} mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an \textbf{Agent Practice} module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an \textbf{Agent RL} module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\%) and GAIA (72.8\%) using open-weight models. Our automated generation pipeline achieves over 81\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\% and +5.4\% respectively. Moreover, our Agent RL training achieves 40\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\% and 21\% on Maths and general/multi-hop QA benchmarks.

Listen to Summary
AI-generated audio breakdown


0:00

4:18


1x
Paper Details
Paper ID: 2512.24615
Date Published: 2025-12-31
Date Updated: 2025-12-31
Categories:
cs.AI
View on arXiv
Download PDF
Executive Summary
Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization
The problem this work tackles is twofold: building high-quality LLM-based agents is labor-intensive due to heavy manual tool integration and prompt engineering, and deploying agents that can adapt to dynamic environments without costly retraining remains challenging. Traditional approaches rely on a craftsman-like workflow that demands domain expertise, extensive engineering, and frequent fine-tuning, which creates a high barrier to entry and limits scalability.

The core breakthrough of Youtu-Agent is a modular, end-to-end framework that automates both the construction and continual improvement of LLM agents. It introduces a YAML-based, layered architecture that decouples the execution environment, toolkits, and the agent’s planning logic. It also presents two generation paradigms—Workflow mode for routine tasks and Meta-Agent mode for complex, evolving requirements—along with a hybrid policy optimization system. The Practice module enables training-free, in-context improvement, while the RL module delivers scalable, end-to-end reinforcement learning in production-grade workflows.

At a high level, Youtu-Agent organizes agent development around three hierarchical layers: Environment, Tools, and Agent. This separation enables automated synthesis of complete agent configurations, including executable tool code and optimized prompts. The Workflow mode provides a deterministic four-stage pipeline to generate agents from task descriptions, whereas the Meta-Agent mode uses an Architect Agent to dynamically plan generation, search the library for existing tools, synthesize missing Python tools with tests, and assemble the final YAML configuration. The approach is validated with open-source models, achieving strong performance on benchmarks (WebWalkerQA 71.47% pass@1; GAIA 72.8% pass@1) and an automated tool-synthesis success rate over 81%. The framework also demonstrates practical-scale RL improvements, including a 40% speedup in training iteration time and stable scaling to 128 GPUs.

Why this matters is straightforward: it lowers the cost of building capable, adaptable agents and enables continuous, low-risk improvement without gradient-based retraining. The automated generation of tools and configurations reduces manual engineering bottlenecks, while the Practice and RL modules provide complementary pathways for both quick, inference-time improvements and long-horizon performance gains. In short, Youtu-Agent offers a practical blueprint for evolvable, open-source agents that can keep pace with evolving tasks and environments.

Bottom line: Youtu-Agent demonstrates that automated generation plus hybrid optimization can produce robust, scalable, and evolvable LLM agents using open-source models, with clear performance benefits and a pathway toward broader adoption in real-world settings.

Detailed Breakdown
Background
The landscape of LLM-based agents has progressed rapidly, but practical deployment is hampered by high configuration costs and static capabilities. Creating a robust agent typically requires careful tool selection, prompt engineering, and custom tool implementations, all of which are labor-intensive. Moreover, once deployed, agents tend to be static and struggle to adapt to changing environments without extensive fine-tuning, which is costly and sometimes infeasible.

To address these gaps, Youtu-Agent proposes a modular, YAML-driven framework that cleanly separates Environment (execution context), Tools (atomic operations), and Agent (LLM-driven planning). This separation supports reuse, interchangeability, and automated synthesis, forming the foundation for automated agent generation and continuous optimization.

Problem / Research Question
The central questions are: (1) Can we automate the generation of complete agent configurations, including executable tool code and prompts, to reduce manual effort and error-prone engineering? (2) Can we provide a dual-path progression—from low-cost, inference-time improvements to scalable RL—that yields stable, real-world performance gains for LLM agents?

Innovation / Contribution
Youtu-Agent contributes several novel elements: a YAML-based, layered architecture that decouples environments, tools, and agent logic; Workflow and Meta-Agent generation paradigms enabling both routine and complex, dynamic agent construction; Agent Practice (Training-free GRPO) for cost-effective in-context improvement; and Agent RL for end-to-end reinforcement learning at scale, including infrastructure and algorithm optimizations to tackle entropy explosion and concurrency.

A distinctive innovation is the automatic synthesis of tool code (Python) and the end-to-end assembly of a complete YAML agent configuration via a meta-architect agent, enabling agents to be created with minimal human intervention while remaining adaptable.

Methodology / Approach
The framework is built around three hierarchical layers: Environment, Tools, and Agent. The Environment Layer provides the execution substrate (e.g., Playwright-based browsers, shells, sandboxed code execution). The Tools Layer wraps atomic and composite operations across environment-related actions, utilities, and MCP tools that integrate external Model Context Protocol services. The Agent Layer houses the LLM-driven planner/executor that executes a perceive-reason-act loop.

The YAML-based configuration declares environment specifications, toolkits, agent prompts, and context management settings. These configurations are the target of automated generation in both Workflow and Meta-Agent modes. The Workflow mode uses a deterministic four-stage pipeline: analyze user task, search the toolkit library, synthesize executable tooling, and generate optimized system prompts before producing a complete YAML config. The Meta-Agent mode uses an Architect Agent with capabilities such as search_tool, create_tool, ask_user, and create_agent_config to plan and execute the generation process across ambiguous or complex tasks.

For optimization, the Agent Practice module implements Training-free Group Relative Policy Optimization (GRPO). It runs multiple rollouts on small datasets, builds a contextual memory of experiential knowledge, and injects distilled semantic guidance as a textual LoRA into the agent’s context during online testing, without gradient updates. The Agent RL module connects to distributed RL frameworks (via Agent-Lightning and VeRL) to enable scalable end-to-end training, with infrastructure improvements including RESTful wrapping, Ray-based concurrency, and hierarchical timeouts to maintain stability at large scale (up to 128 GPUs).

Experiments / Evaluation
The authors validate the framework on several benchmarks. Open-source models form the backbone, avoiding proprietary APIs. In static evaluation, Youtu-Agent achieves 71.47% pass@1 on WebWalkerQA and 72.8% pass@1 on GAIA (text-only subset), demonstrating solid tool-use and reasoning capabilities. For automated generation, the tool synthesis success rate exceeds 81%. In the Agent Practice setting, Training-free GRPO yields consistent gains of +2.7% on AIME 2024 and +5.4% on AIME 2025 with only 100 samples and modest cost. In the RL pathway, infrastructural optimizations deliver about 40% faster training iteration times and enable stable scaling to 128 GPUs, with notable performance improvements on AIME tasks (e.g., Qwen2.5-7B accuracy from 10% to 45% on AIME 2024) and multi-domain benchmarks (e.g., 17–21% gains across various datasets in tabled results).

The evaluation also includes qualitative demonstrations of automated tool synthesis and end-to-end agent generation via Meta-Agent mode, which achieves 98.75% validity in AgentGen-80 tasks, with occasional end-to-end failures when final tool configuration is malformed.

Key Results
Key quantitative takeaways include state-of-the-art-ish open baseline performance on WebWalkerQA and GAIA among open-weight models, an automation success rate over 81%, and meaningful gains from both the Practice and RL modules. The RL improvements emphasize stability and scalability, addressing entropy explosion and convergence concerns common in long-horizon tasks. The combination of Workflow and Meta-Agent paradigms yields robust agent generation across simple and complex use cases.

Practical Applications
Beyond academic benchmarks, Youtu-Agent aims to lower barriers to deploying capable agents in real-world settings. The approach works entirely with open-source models, enabling researchers and practitioners to build evolvable agents without vendor lock-in. The Tip on-device desktop assistant illustrates a practical consumer-facing application, where an on-device, multimodal assistant can load existing agent configurations, automate desktop tasks, and replay reusable GUI skills locally and safely.

Limitations & Considerations
While promising, the approach relies on high-quality toolkit libraries and robust tool synthesis. The Meta-Agent mode, though highly effective, occasionally yields ill-formed final configurations, indicating a need for improved validation and error handling. The evaluation centers on web search, code execution, and standard QA benchmarks; extending to richer multimodal tasks and long-running real-world deployments remains to be explored. Dependence on open-source model quality and tool libraries raises questions about cross-domain generalization, latency overhead of automated generation, and reproducibility across hardware and software stacks. Finally, while Training-free GRPO reduces data and compute needs, a deeper understanding of when in-context optimization reliably translates to lasting improvements across diverse tasks is warranted.

Overall, the work presents a compelling blueprint for evolvable agents, but future work should broaden task domains, scrutinize failure modes in generation, and quantify real-world deployment constraints such as latency, reliability, and safety in automated tool use.

Conceptual Simplification
Youtu-Agent tackles a core bottleneck in AI assistance: making powerful LLM-driven agents easier to build and continually improve. The authors propose a modular, YAML-driven framework that cleanly separates the agent’s world (Environment), the actions it can take (Tools), and its mind (Agent). This decoupling lets developers mix and match execution backends, utility functions, and memory/context strategies without reengineering the whole system. On top of this, Youtu-Agent offers two automated generation pathways: a Workflow mode that deterministically assembles agents for routine tasks, and a Meta-Agent mode that uses a higher-level Architect Agent to dynamically plan and generate new tools and configurations for more complex, evolving tasks. This automated generation slashes the manual labor of prompt design and tool integration, enabling faster prototyping and broader reuse of components.

Beyond generation, Youtu-Agent embeds a two-pronged approach to improvement. The Agent Practice module implements training-free improvements by accumulating experience from a handful of examples and distilling a semantic, group-level learning signal that guides reasoning during inference. This avoids costly gradient updates and is especially valuable when access to the base model’s weights is restricted. For scenarios demanding lasting performance gains, the Agent RL module provides an end-to-end reinforcement learning pipeline that scales across distributed infrastructure while maintaining stability through infrastructure-level (RESTful wrapping, Ray-based concurrency, timeout controls) and algorithmic (entropy explosion mitigation, biased advantage correction) innovations. Put together, these components shift agent development from manual curation toward automated construction and continuous self-improvement.

The empirical message is clear: even with open-source models, a well-designed framework like Youtu-Agent can achieve competitive results on real-world benchmarks, enable substantial tool synthesis success, and deliver scalable RL workflows. The framework’s emphasis on openness, reusability, and automated synthesis suggests a practical path toward evolvable AI agents that can adapt to new tasks, tools, and environments with minimal hand-tuning. As agent applications broaden—from web interaction and code execution to desktop automation and autonomous RL—Youtu-Agent provides a concrete blueprint for building, deploying, and continually improving general-purpose agents in a collaborative, open-source ecosystem.

Explain it Like I'm 14
Imagine you want a super-smart helper robot that can do online research, write little programs, and grab stuff from the web all on its own. But making that helper is really hard: you have to pick the right tools, write instructions for the robot, and sometimes teach it new tricks. Youtu-Agent is like giving that robot a smart, build-it-yourself toolkit and a way for it to improve by itself over time, without you having to redo its brain every week.

First, it splits the whole job into three parts: the place where it acts (the computer browser or a little sandbox to run code), the tiny tools it uses (like clicking a button or calculating something), and the brain that plans what to do (the language model). All of this is described in a simple recipe file called YAML, which tells the robot exactly how to set things up and what to try next.

There are two ways it can work. The Workflow mode is like following a recipe step by step for common tasks. The Meta-Agent mode is like having a seasoned planner who can design new tools and even write the missing pieces of the recipe if the task is tricky or unusual.

Even better, it can learn from experience without changing its brain. It runs small practice rounds, looks at what worked and what didn’t, and stores those lessons as smart, textual memory. Later, when the robot is working online, it reads those memories to make better choices, all without touching its underlying model weights. For tougher jobs that still need real learning, it has a full RL path that trains at scale using good engineering—think lots of computers working together without the usual chaos.

What they found is that this approach can reach solid performance on real tasks, while keeping everything open-source and flexible. The big takeaway is that you can automate both how you build an agent and how you make it better over time, which could make it much easier to deploy capable helpers in the real world.

Glossary
Architect Agent: The high-level orchestrator that drives generation and configuration, enabling multiturn planning and tool synthesis — "The Meta-Agent mode deploys a higher-level Architect Agent that treats generation capabilities as callable tools."\nAgent Practice: A low-cost, training-free experience accumulation module that enables agents to improve without gradient updates — "Agent Practice module enables low-cost experience accumulation on small datasets without parameter updates"\nAgent RL: An end-to-end reinforcement learning module that enables scalable and stable RL for Youtu-Agents via distributed training — "the Agent RL module provides a complete pipeline for end-to-end, production-grade reinforcement learning (see Figure 3)."\nE2B sandbox: A sandboxed code execution environment used for executing user code safely — "sandboxed code execution environments (e.g., E2B)."\nEnvironment Layer: The foundational execution context and low-level capabilities the agent uses to interact with backends — "Environment Layer provides the foundational execution context and low-level capabilities."\nMCP (Model Context Protocol): A protocol for exposing external tool services to LLMs, via MCP tools — "MCP tools that integrate external Model Context Protocol services for extended capabilities."\nMeta-Agent mode: The higher-level Architect Agent that handles generation tasks for complex, dynamic requirements — "The Meta-Agent mode deploys a higher-level Architect Agent that treats generation capabilities as callable tools."\nRESTful API wrapping: An infrastructure technique that wraps environments as standardized services for distribution — "RESTful API wrapping that encapsulates agent execution environments as standardized services, enabling seamless distribution and load balancing"\nRay-based concurrency: Distributed, parallel rollout collection across workers via Ray — "Ray-based concurrency for highly parallel rollout collection across distributed workers"\nTextual LoRA: A gradient-free memory extension that injects experiential knowledge as textual guidance — "textual LoRA" to guide reasoning without modifying model weights\nWorkflow mode: A deterministic four-stage pipeline for routine tasks — "The Workflow mode follows a deterministic, four-stage pipeline suitable for well-defined, routine agent construction tasks"\nYAML-based structured configuration system: A YAML-driven configuration framework for environment/tool/instruction/components — "YAML-based structured configuration system that facilitates component reuse and serves as the foundation for automated generation."

Knowledge Gaps
There is no extensive ablation comparing Workflow vs Meta-Agent modes across a broad set of task types; what task characteristics predict which mode is best, and how does performance degrade when the Architect Agent makes design choices?
The long-term stability and safety of automated tool synthesis are not deeply explored; what are failure modes when synthesized tools misbehave, and how can we detect and recover automatically?
Scalability beyond 128 GPUs and across diverse hardware environments is not tested; how does performance scale on different clusters, and what are the bottlenecks?
Generalization to multimodal inputs (images, video, audio) and real-time sensor data remains untested; can the same YAML-based configuration handle richer modalities with similar gains?
Dependency on specific tool libraries and open-source models: how sensitive are results to model quality, tool availability, and library updates?
Latency and cost analysis of automated generation (Workflow/Meta-Agent) versus manual configuration in production timelines is missing; what are practical cost-benefit thresholds?
Detailed error analysis and failure cases for AgentGen-80 and the 1–2% validity lapses in Meta-Agent mode need systematic documentation and remediation strategies.
The impact of Training-free GRPO across domains with varying data regimes (scarce vs. abundant labeled data) requires broader experimentation.
Real-world deployment Feedback: user-study data on reliability, trust, and safety of autonomous agents in consumer or enterprise settings is absent.
Reproducibility across different RL backbones (beyond Agent-Lightning and VeRL) and varying hyperparameter regimes needs exploration.
Practical Applications
Immediate Applications
Rapid deployment of domain-specific agents for enterprise knowledge workflows (e.g., finance or healthcare) by providing a high-level task description and using Workflow mode to synthesize tools, prompts, and YAML configurations; dependencies include open-source models, YAML-based configurations, and a library of tools.
Automated IT/DevOps tooling: generate agents that monitor systems, perform root-cause analysis, and remediate issues by composing environment tools and Python utilities; dependencies include accessible environment backends (browsers, shells), and MCP-enabled tool integrations.
On-device, multimodal desktop automation with Tip: an on-device GUI agent that can perform bash/file tasks offline, store workflows locally, and replay GUI skills; dependencies include on-device hardware and multimodal input handling.
Open-source end-to-end RL experimentation: train and deploy agents at scale using Agent RL with distributed frameworks (e.g., VeRL) across multiple GPUs; sector relevance includes robotics and autonomous software agents; dependencies include RESTful wrapping, Ray-based concurrency, and proper infrastructure.
Training-free GRPO-driven improvements for data-scarce domains: apply Training-free GRPO to small datasets (e.g., education or compliance tasks) to achieve measurable gains without gradient updates; dependencies include small task datasets and a capable LLM.
Rapid tool synthesis for software engineering: use automated tool generation (create_tool) to synthesize Python tools and integrate via MCP to accelerate building domain-specific assistants; dependencies include the Architect Agent and open-tool libraries.
Long-Term Applications
Fully autonomous, cross-domain agents that autonomously generate and evolve their own tools and configurations via Meta-Agent mode; enabling long-horizon, dynamic tasks but requiring richer tool libraries, safer tool execution, and robust evaluation environments.
Privacy-preserving, offline on-device agents (Tip-enabled) operating in sensitive domains (healthcare, finance) with data staying local; requires robust on-device multimodal capabilities and efficient models.
Cross-organization tool sharing and collaboration enabled by Model Context Protocol (MCP) that standardizes external tool access across enterprises; requires governance, security, and interoperability standards.
Large-scale, end-to-end RL deployment in real-world environments (industrial robotics, logistics) with stable, scalable infrastructure supporting 128+ GPUs; demands further stability improvements, artifact management, and safety measures.
Advanced multi-agent collaboration with dynamic role assignment and automated tool provisioning at scale, surpassing static multi-agent templates by leveraging automated generation and continuous optimization.
CONTENT
Executive Summary
Detailed Breakdown
Conceptual Simplification
Explain it Like I'm 14
Glossary
Knowledge Gaps
Practical Applications
PDF Preview
Preview PDF
Download PDF
AI Key Findings
Generated Jan 02, 2026

Methodology
The research introduces Youtu-Agent, a modular framework combining automated generation and hybrid policy optimization. It employs a structured configuration system, two generation paradigms (Workflow and Meta-Agent modes), and a hybrid policy optimization system with Agent Practice and Agent RL modules. Experiments are conducted on benchmarks like WebWalkerQA and GAIA using open-source models.

Key Results
Youtu-Agent achieves 71.47% accuracy on WebWalkerQA and 72.8% on GAIA using open-weight models.
Automated generation pipeline has over 81% tool synthesis success rate and 68.75% task completion rate.
Agent RL training achieves 40% speedup and improves coding/reasoning capabilities by 35% and 21% respectively on Maths and QA benchmarks.
Significance
This research addresses high configuration costs and static capabilities in LLM agents, providing a scalable framework for automated agent generation and continuous improvement. It establishes a competitive open-source baseline without requiring proprietary APIs.

Technical Contribution
Introduces a hybrid policy optimization system with Training-free GRPO for low-cost agent improvement and Agent RL for scalable reinforcement learning, along with a structured configuration system for flexible agent composition.

Novelty
Youtu-Agent distinguishes itself by automatically generating agent configurations, tool code, and optimized prompts, and providing a unified pipeline from automated generation to continuous optimization, unlike existing frameworks that rely on static roles and manual configuration.

Limitations
Performance may depend on the quality of training data and initial configurations.
The framework's effectiveness is evaluated primarily on specific benchmarks, which may limit generalizability.
Future Work
Expand the framework with more environment integrations and enhanced multi-agent collaboration capabilities.
Develop more sophisticated experience accumulation strategies for continuous optimization.
Explore applications in diverse domains beyond the current benchmarks.