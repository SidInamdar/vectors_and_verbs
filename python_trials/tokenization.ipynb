{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f2ffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c617b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "# Configuration for training the SentencePiece model\n",
    "# SentencePiece allows for subword tokenization, which helps handle out-of-vocabulary words.\n",
    "options = {\n",
    "    # The source text file used for learning the vocabulary\n",
    "    'input': 'train_data.txt',\n",
    "    # The base name for the generated model (.model) and vocabulary (.vocab) files\n",
    "    'model_prefix': 'bpe_model',\n",
    "    # Number of unique tokens in the final vocabulary\n",
    "    'vocab_size': 4000,\n",
    "    # 'bpe' (Byte Pair Encoding) merges frequent pairs of characters/sequences\n",
    "    'model_type': 'bpe',\n",
    "    # Percentage of characters covered by the model; 0.9995 is standard for languages with large character sets\n",
    "    'character_coverage': 0.9995,\n",
    "    # When enabled, unknown characters are decomposed into UTF-8 bytes to avoid 'unk' tokens\n",
    "    'byte_fallback': True,\n",
    "    # Treats digits individually (0-9), preventing large numbers from being treated as single tokens\n",
    "    'split_digits': True,\n",
    "    # Prevents adding a whitespace prefix to the first token; useful for fine-grained control\n",
    "    'add_dummy_prefix': False\n",
    "}\n",
    "\n",
    "try:\n",
    "    print(\"Starting the training process...\")\n",
    "    # SentencePieceTrainer.train takes the dictionary of options to build the BPE model\n",
    "    spm.SentencePieceTrainer.train(**options)\n",
    "    print(\"Training complete. 'bpe_model.model' and 'bpe_model.vocab' have been created.\")\n",
    "\n",
    "    # Initialize the processor and load the newly trained model\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load('bpe_model.model')\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(\"Model Metadata:\")\n",
    "    # Retrieve the total number of tokens in the vocabulary\n",
    "    print(f\"Total Vocab Size: {sp.get_piece_size()}\")\n",
    "    # Special tokens are used for sequence boundaries and handling unknown characters\n",
    "    print(f\"BOS (Beginning of Sentence) ID: {sp.bos_id()}\")\n",
    "    print(f\"EOS (End of Sentence) ID:       {sp.eos_id()}\")\n",
    "    print(f\"UNK (Unknown) ID:              {sp.unk_id()}\")\n",
    "    print(f\"PAD (Padding) ID:              {sp.pad_id()}\")\n",
    "    \n",
    "    # Test the tokenizer on sample strings to see how it breaks down text\n",
    "    test_sentences = [\n",
    "        'Hello World! 1234567890', \n",
    "        'This blog is the most uninstagrammable blog ever'\n",
    "    ]\n",
    "\n",
    "    for text in test_sentences:\n",
    "        print(\"\\n--- Tokenization Test ---\")\n",
    "        print(f\"Original Text: {text}\")\n",
    "        # encode_as_pieces: shows the actual subword units (tokens)\n",
    "        print(f\"Subword Tokens: {sp.encode_as_pieces(text)}\")\n",
    "        # encode_as_ids: shows the numerical mapping for each token\n",
    "        print(f\"Numerical IDs:  {sp.encode_as_ids(text)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training or processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7338019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "\n",
      "Training Unigram Model (SentencePiece Default)\n",
      "Starting Unigram training...\n",
      "Training complete. 'unigram_model.model' created.\n",
      "------------------------------\n",
      "Unigram Model Metadata:\n",
      "Total Vocab Size: 1200\n",
      "\n",
      "--- Tokenization Test (Unigram) ---\n",
      "Original Text: Hello World! 1234567890\n",
      "Subword Tokens: ['<0x48>', 'e', 'll', 'o', '▁', 'W', 'or', 'ld', '<0x21>', '▁', '1', '2', '3', '4', '5', '<0x36>', '7', '8', '<0x39>', '0']\n",
      "Numerical IDs:  [75, 268, 363, 340, 259, 473, 380, 1020, 36, 259, 283, 277, 536, 323, 348, 57, 316, 319, 60, 311]\n",
      "\n",
      "--- Tokenization Test (Unigram) ---\n",
      "Original Text: This blog is the most uninstagrammable blog ever\n",
      "Subword Tokens: ['T', 'his', '▁b', 'l', 'o', 'g', '▁is', '▁the', '▁m', 'o', 'st', '▁un', 'in', 'sta', 'g', 'ra', 'm', 'm', 'able', '▁b', 'l', 'o', 'g', '▁', 'e', 'ver']\n",
      "Numerical IDs:  [346, 1121, 344, 381, 340, 365, 293, 269, 811, 340, 562, 754, 815, 828, 365, 678, 388, 388, 412, 344, 381, 340, 365, 259, 268, 1128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: train_data.txt\n",
      "  input_format: \n",
      "  model_prefix: unigram_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1200\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 1\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 1\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 0\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: train_data.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 114 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x00>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x01>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x02>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x03>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x04>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x05>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x06>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x07>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x08>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x09>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x0A>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x0B>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x0C>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x0D>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x0E>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x0F>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x10>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x11>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x12>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x13>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x14>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x15>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x16>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x17>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x18>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x19>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x1A>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x1B>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x1C>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x1D>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x1E>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x1F>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x20>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x21>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x22>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x23>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x24>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x25>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x26>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x27>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x28>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x29>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x2A>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x2B>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x2C>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x2D>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x2E>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x2F>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x30>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x31>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x32>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x33>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x34>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x35>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x36>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x37>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x38>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x39>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x3A>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x3B>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x3C>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x3D>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x3E>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x3F>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x40>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x41>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x42>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x43>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x44>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x45>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x46>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x47>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x48>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x49>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x4A>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x4B>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x4C>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x4D>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x4E>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x4F>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x50>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x51>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x52>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x53>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x54>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x55>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x56>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x57>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x58>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x59>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x5A>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x5B>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x5C>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x5D>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x5E>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x5F>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x60>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x61>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x62>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x63>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x64>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x65>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x66>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x67>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x68>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x69>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x6A>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x6B>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x6C>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x6D>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x6E>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x6F>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x70>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x71>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x72>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x73>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x74>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x75>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x76>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x77>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x78>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x79>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x7A>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x7B>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x7C>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x7D>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x7E>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x7F>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x80>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x81>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x82>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x83>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x84>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x85>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x86>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x87>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x88>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x89>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x8A>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x8B>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x8C>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x8D>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x8E>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x8F>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x90>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x91>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x92>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x93>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x94>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x95>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x96>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x97>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x98>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x99>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x9A>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x9B>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x9C>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x9D>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x9E>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0x9F>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xA0>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xA1>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xA2>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xA3>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xA4>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xA5>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xA6>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xA7>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xA8>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xA9>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xAA>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xAB>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xAC>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xAD>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xAE>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xAF>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xB0>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xB1>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xB2>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xB3>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xB4>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xB5>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xB6>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xB7>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xB8>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xB9>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xBA>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xBB>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xBC>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xBD>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xBE>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xBF>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xC0>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xC1>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xC2>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xC3>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xC4>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xC5>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xC6>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xC7>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xC8>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xC9>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xCA>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xCB>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xCC>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xCD>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xCE>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xCF>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xD0>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xD1>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xD2>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xD3>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xD4>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xD5>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xD6>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xD7>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xD8>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xD9>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xDA>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xDB>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xDC>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xDD>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xDE>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xDF>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xE0>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xE1>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xE2>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xE3>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xE4>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xE5>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xE6>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xE7>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xE8>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xE9>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xEA>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xEB>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xEC>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xED>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xEE>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xEF>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xF0>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xF1>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xF2>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xF3>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xF4>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xF5>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xF6>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xF7>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xF8>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xF9>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xFA>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xFB>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xFC>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xFD>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xFE>\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <0xFF>\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=26109\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=76\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 114 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=14170\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 3418 seed sentencepieces\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 114\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 1402\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 1402 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1300 obj=14.974 num_tokens=3510 num_tokens/piece=2.7\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1167 obj=12.5086 num_tokens=3542 num_tokens/piece=3.03513\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: unigram_model.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: unigram_model.vocab\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "print(\"Training Unigram Model (SentencePiece Default)\")\n",
    "# Note: SentencePiece doesn't strictly have a 'wordpiece' model_type. \n",
    "# It supports 'unigram', 'bpe', 'char', and 'word'.\n",
    "# 'Unigram' is the default and usually recommended over BPE in SentencePiece.\n",
    "# Configuration for the Unigram model training.\n",
    "# Unigram is a probabilistic subword tokenization method that starts with a large vocabulary \n",
    "# and iteratively removes tokens that minimize the loss of the likelihood of the training data.\n",
    "options_unigram = {\n",
    "    'input': 'train_data.txt',        # Path to the raw text file for training\n",
    "    'model_prefix': 'unigram_model',  # Prefix for the output .model and .vocab files\n",
    "    'vocab_size': 1200,               # Desired size of the final vocabulary\n",
    "    'model_type': 'unigram',          # Specifies the Unigram language model algorithm\n",
    "    'character_coverage': 0.9995,     # Percentage of characters covered by the model (0.9995 is standard for Latin scripts)\n",
    "    'byte_fallback': True,            # Enables mapping unknown characters to UTF-8 bytes to avoid <unk> tokens\n",
    "    'split_digits': True,             # Treats each digit as an individual token (useful for numerical data)\n",
    "    'add_dummy_prefix': False         # Prevents adding a leading space (SentencePiece default is True)\n",
    "}\n",
    "\n",
    "try:\n",
    "    # 1. Train the SentencePiece model using the defined options\n",
    "    print(\"Starting Unigram training...\")\n",
    "    spm.SentencePieceTrainer.train(**options_unigram)\n",
    "    print(\"Training complete. 'unigram_model.model' created.\")\n",
    "\n",
    "    # 2. Load the trained model into a processor instance for inference\n",
    "    sp_unigram = spm.SentencePieceProcessor()\n",
    "    sp_unigram.load('unigram_model.model')\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(\"Unigram Model Metadata:\")\n",
    "    print(f\"Total Vocab Size: {sp_unigram.get_piece_size()}\")\n",
    "    \n",
    "    # 3. Define test cases to evaluate how the model handles common and rare words\n",
    "    test_sentences = [\n",
    "        'Hello World! 1234567890', \n",
    "        'This blog is the most uninstagrammable blog ever'\n",
    "    ]\n",
    "\n",
    "    # 4. Iterate through test sentences to visualize subword segmentation\n",
    "    for text in test_sentences:\n",
    "        print(\"\\n--- Tokenization Test (Unigram) ---\")\n",
    "        print(f\"Original Text: {text}\")\n",
    "        # encode_as_pieces: Converts text into subword strings (visual representation)\n",
    "        print(f\"Subword Tokens: {sp_unigram.encode_as_pieces(text)}\")\n",
    "        # encode_as_ids: Converts text into numerical indices for model input\n",
    "        print(f\"Numerical IDs:  {sp_unigram.encode_as_ids(text)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle potential errors during training or loading (e.g., missing input file)\n",
    "    print(f\"An error occurred with Unigram model: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a5a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Training complete. 'wordpiece.json' created.\n",
      "------------------------------\n",
      "WordPiece Model Metadata:\n",
      "Total Vocab Size: 2609\n",
      "\n",
      "--- Tokenization Test (WordPiece) ---\n",
      "Original Text: Hello World! 1234567890\n",
      "Subword Tokens: ['H', '##el', '##lo', 'W', '##or', '##ld', '[UNK]', '[UNK]']\n",
      "Numerical IDs:  [37, 180, 214, 52, 162, 418, 0, 0]\n",
      "\n",
      "--- Tokenization Test (WordPiece) ---\n",
      "Original Text: This blog is the most uninstagrammable blog ever\n",
      "Subword Tokens: ['This', 'b', '##lo', '##g', 'is', 'the', 'm', '##os', '##t', 'un', '##ins', '##ta', '##g', '##ra', '##m', '##ma', '##ble', 'b', '##lo', '##g', 'ever']\n",
      "Numerical IDs:  [691, 58, 214, 102, 248, 194, 69, 660, 96, 875, 350, 209, 102, 155, 108, 173, 510, 58, 214, 102, 1240]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "try:\n",
    "    # 1. Initialize the WordPiece Tokenizer\n",
    "    # We specify the [UNK] token for handling words not found in the vocabulary.\n",
    "    tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "    \n",
    "    # 2. Configure Pre-tokenization\n",
    "    # Before the subword algorithm runs, we need to split the raw text into words.\n",
    "    # Whitespace splitting is the standard first step for most English NLP tasks.\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    \n",
    "    # 3. Initialize the Trainer\n",
    "    # We define our target vocabulary size and the special tokens required for \n",
    "    # downstream tasks (like BERT's [CLS] for classification or [SEP] for separators).\n",
    "    trainer = WordPieceTrainer(\n",
    "        vocab_size=4000, \n",
    "        special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "    )\n",
    "    \n",
    "    # 4. Train the Model\n",
    "    # The tokenizer scans the training file to build a vocabulary of the most \n",
    "    # frequent subword units.\n",
    "    tokenizer.train(files=[\"train_data.txt\"], trainer=trainer)\n",
    "    \n",
    "    # 5. Persist the Model\n",
    "    # Save the configuration and vocabulary to a JSON file for future inference.\n",
    "    tokenizer.save(\"wordpiece.json\")\n",
    "    print(\"Training complete. 'wordpiece.json' created.\")\n",
    "    \n",
    "    # 6. Metadata Inspection\n",
    "    print(\"-\" * 30)\n",
    "    print(\"WordPiece Model Metadata:\")\n",
    "    print(f\"Total Vocab Size: {tokenizer.get_vocab_size()}\")\n",
    "    \n",
    "    # 7. Testing Subword Tokenization\n",
    "    # WordPiece shines at handling rare words by breaking them into meaningful chunks.\n",
    "    test_sentences = [\n",
    "        'Hello World! 1234567890', \n",
    "        'This blog is the most uninstagrammable blog ever'\n",
    "    ]\n",
    "\n",
    "    for text in test_sentences:\n",
    "        print(\"\\n--- Tokenization Test (WordPiece) ---\")\n",
    "        print(f\"Original Text: {text}\")\n",
    "        \n",
    "        # Encode converts raw text into a Tokenizer object containing tokens and IDs\n",
    "        output = tokenizer.encode(text)\n",
    "        \n",
    "        # 'tokens' shows the subword breakdown (e.g., 'un', '##insta', etc.)\n",
    "        print(f\"Subword Tokens: {output.tokens}\")\n",
    "        # 'ids' are the numerical indices mapped to the vocabulary\n",
    "        print(f\"Numerical IDs:  {output.ids}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred with WordPiece model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32bc934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
