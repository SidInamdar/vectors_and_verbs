{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a59ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple Continuous Bag of Words (CBOW) style model\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        # The embedding layer stores the word vectors we want to learn\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: tensor of word indices for the context\n",
    "        embeds = self.embeddings(inputs)\n",
    "        # Aggregate context by calculating the mean of the word embeddings\n",
    "        h = torch.mean(embeds, dim=1)\n",
    "        # Produce logits (raw scores) for each word in the vocabulary\n",
    "        logits = self.linear(h)\n",
    "        return logits\n",
    "\n",
    "# Setup vocabulary and mappings\n",
    "word_to_ix = {\"the\": 0, \"quick\": 1, \"brown\": 2, \"fox\": 3, \"jumps\": 4, \"over\": 5, \"lazy\": 6, \"dog\": 7}\n",
    "ix_to_word = {v: k for k, v in word_to_ix.items()}\n",
    "\n",
    "# Configuration constants\n",
    "EMBEDDING_DIM = 5 \n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Prepare dummy training data: context words and the target word\n",
    "# Context: [\"quick\", \"brown\", \"jumps\", \"over\"] -> Target: \"fox\"\n",
    "context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long)\n",
    "target_idxs = torch.tensor([3], dtype=torch.long)\n",
    "\n",
    "# Perform a single optimization step\n",
    "model.zero_grad()\n",
    "# Forward pass: we reshape context to (1, -1) to simulate a batch of size 1\n",
    "logits = model(context_idxs.view(1, -1))\n",
    "# Calculate loss against the target word index\n",
    "loss = loss_function(logits, target_idxs)\n",
    "# Backpropagate and update weights\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# Output results for the blog post\n",
    "print(\"=== Model Training Snapshot ===\")\n",
    "print(f\"Calculated Loss: {loss.item():.6f}\")\n",
    "\n",
    "print(f\"\\n=== Learned Vector for 'jumps' ===\")\n",
    "word_vec = model.embeddings(torch.tensor([word_to_ix['jumps']]))\n",
    "print(word_vec.detach().numpy())\n",
    "\n",
    "print(\"\\n=== Embedding Matrix (Weights) ===\")\n",
    "print(model.embeddings.weight.detach().numpy())\n",
    "\n",
    "print(\"\\n=== Linear Layer Weights ===\")\n",
    "print(model.linear.weight.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b41d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SkipGramNegativeSampling(nn.Module):\n",
    "    \"\"\"\n",
    "    Skip-Gram with Negative Sampling (SGNS) implementation.\n",
    "    \n",
    "    SGNS approximates the softmax over the entire vocabulary by instead \n",
    "    distinguishing between a real context word (positive) and K noise words (negative).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramNegativeSampling, self).__init__()\n",
    "        # Input embeddings: used when the word is the center word\n",
    "        self.in_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Output embeddings: used when the word is a context or negative sample\n",
    "        self.out_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Initialize weights with small values to prevent gradient saturation\n",
    "        initrange = 0.5 / embedding_dim\n",
    "        self.in_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        self.out_embed.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, center_words, target_words, negative_words):\n",
    "        \"\"\"\n",
    "        Computes the negative sampling loss.\n",
    "        \n",
    "        Args:\n",
    "            center_words: (batch_size)\n",
    "            target_words: (batch_size)\n",
    "            negative_words: (batch_size, K) where K is number of negative samples\n",
    "        \"\"\"\n",
    "        # Retrieve vectors\n",
    "        v_c = self.in_embed(center_words)      # (batch_size, embed_dim)\n",
    "        u_o = self.out_embed(target_words)     # (batch_size, embed_dim)\n",
    "        u_n = self.out_embed(negative_words)   # (batch_size, K, embed_dim)\n",
    "\n",
    "        # 1. Positive Score: log(sigmoid(v_c · u_o))\n",
    "        # Compute dot product: (batch, 1, dim) @ (batch, dim, 1) -> (batch, 1)\n",
    "        pos_score = torch.bmm(u_o.unsqueeze(1), v_c.unsqueeze(2)).squeeze(2)\n",
    "        pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-7)\n",
    "\n",
    "        # 2. Negative Score: sum(log(sigmoid(-v_c · u_n)))\n",
    "        # Compute dot products for all K samples: (batch, K, dim) @ (batch, dim, 1) -> (batch, K)\n",
    "        neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2)\n",
    "        neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-7), dim=1, keepdim=True)\n",
    "        \n",
    "        # Total loss is the negative of the objective function\n",
    "        loss = -(pos_loss + neg_loss)\n",
    "        return torch.mean(loss)\n",
    "\n",
    "# --- Configuration & Mock Data ---\n",
    "VOCAB_SIZE = 100\n",
    "EMBED_DIM = 10\n",
    "word_to_ix = {'fox': 0} # Example vocabulary mapping\n",
    "\n",
    "model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Mock inputs: 1 center word, 1 target word, 5 negative samples\n",
    "center_id = torch.tensor([0], dtype=torch.long)\n",
    "target_id = torch.tensor([1], dtype=torch.long)\n",
    "negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long)\n",
    "\n",
    "# Training Step\n",
    "model.zero_grad()\n",
    "loss = model(center_id, target_id, negative_ids)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# Output Results\n",
    "print(f\"Loss after one step: {loss.item():.6f}\")\n",
    "word_vec = model.in_embed(torch.tensor([word_to_ix['fox']]))\n",
    "print(f\"Vector for 'fox':\\n{word_vec.detach().numpy()}\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
