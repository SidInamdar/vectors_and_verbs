{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13a59ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Training Snapshot ===\n",
      "Calculated Loss: 1.863045\n",
      "\n",
      "=== Learned Vector for 'jumps' ===\n",
      "[[ 0.29590198 -0.48890254 -0.8542548  -0.9656708   0.36455676]]\n",
      "\n",
      "=== Embedding Matrix (Weights) ===\n",
      "[[-0.13485748  0.45796484  0.5082475  -0.9655638   0.22472069]\n",
      " [-1.0922837  -0.8376392  -1.9895118  -0.6761363   0.52810454]\n",
      " [-0.48990375 -1.7160931  -0.10055047 -0.21604995  0.25440407]\n",
      " [ 1.1479449   0.37760517 -1.2026685  -0.32337672 -1.864741  ]\n",
      " [ 0.29590198 -0.48890254 -0.8542548  -0.9656708   0.36455676]\n",
      " [-0.20498867 -0.29153195 -0.4145538   1.1002443  -0.29684016]\n",
      " [ 1.2364109  -1.3861771  -1.2367728   0.9709441  -0.3087463 ]\n",
      " [ 1.1985914   1.7510175   0.4587708  -1.8716775  -0.15679668]]\n",
      "\n",
      "=== Linear Layer Weights ===\n",
      "[[-0.30654982 -0.19728437  0.41693833  0.03748522 -0.38215488]\n",
      " [-0.188174   -0.16843402  0.26216137 -0.14779729 -0.08642971]\n",
      " [-0.30187497 -0.26113632 -0.19111304 -0.13131228  0.10903408]\n",
      " [-0.13401619 -0.09882405 -0.25202116  0.41303945  0.2886999 ]\n",
      " [-0.05993969 -0.3525516  -0.11821194  0.02525838  0.07857686]\n",
      " [ 0.06936981 -0.28830373 -0.42867166 -0.2098654   0.14071469]\n",
      " [-0.215357    0.01657986  0.05610387 -0.04846594  0.01466867]\n",
      " [-0.2605      0.11983841  0.34135845 -0.38784653 -0.39911577]]\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple Continuous Bag of Words (CBOW) style model\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        # The embedding layer stores the word vectors we want to learn\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # The linear layer maps the averaged embedding back to the vocabulary size to predict the target word\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: tensor of word indices for the context\n",
    "        embeds = self.embeddings(inputs)\n",
    "        # Aggregate context by calculating the mean of the word embeddings\n",
    "        h = torch.mean(embeds, dim=1)\n",
    "        # Produce logits (raw scores) for each word in the vocabulary\n",
    "        logits = self.linear(h)\n",
    "        return logits\n",
    "\n",
    "# Setup vocabulary and mappings\n",
    "word_to_ix = {\"the\": 0, \"quick\": 1, \"brown\": 2, \"fox\": 3, \"jumps\": 4, \"over\": 5, \"lazy\": 6, \"dog\": 7}\n",
    "ix_to_word = {v: k for k, v in word_to_ix.items()}\n",
    "\n",
    "# Configuration constants\n",
    "EMBEDDING_DIM = 5 \n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CBOWModel(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Prepare dummy training data: context words and the target word\n",
    "# Context: [\"quick\", \"brown\", \"jumps\", \"over\"] -> Target: \"fox\"\n",
    "context_idxs = torch.tensor([1, 2, 4, 5], dtype=torch.long)\n",
    "target_idxs = torch.tensor([3], dtype=torch.long)\n",
    "\n",
    "# Perform a single optimization step\n",
    "model.zero_grad()\n",
    "# Forward pass: we reshape context to (1, -1) to simulate a batch of size 1\n",
    "logits = model(context_idxs.view(1, -1))\n",
    "# Calculate loss against the target word index\n",
    "loss = loss_function(logits, target_idxs)\n",
    "# Backpropagate and update weights\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# Output results for the blog post\n",
    "print(\"=== Model Training Snapshot ===\")\n",
    "print(f\"Calculated Loss: {loss.item():.6f}\")\n",
    "\n",
    "print(f\"\\n=== Learned Vector for 'jumps' ===\")\n",
    "word_vec = model.embeddings(torch.tensor([word_to_ix['jumps']]))\n",
    "print(word_vec.detach().numpy())\n",
    "\n",
    "print(\"\\n=== Embedding Matrix (Weights) ===\")\n",
    "print(model.embeddings.weight.detach().numpy())\n",
    "\n",
    "print(\"\\n=== Linear Layer Weights ===\")\n",
    "print(model.linear.weight.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b41d148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.164300441741943\n",
      "Vector for is [[ 0.00032597  0.04756524 -0.0195841   0.03475741  0.0301356  -0.03958734\n",
      "   0.02311508 -0.00974941 -0.00969105  0.03378046]]\n"
     ]
    }
   ],
   "source": [
    "## Skip gram using NEgative Sampling\n",
    "\n",
    "class SkipGramNegativeSampling(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramNegativeSampling, self).__init__()\n",
    "        self.in_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.in_embed.weight.data.uniform_(-0.5/embedding_dim, 0.5/embedding_dim)\n",
    "        self.out_embed.weight.data.uniform_(-0.5/embedding_dim, 0.5/embedding_dim)\n",
    "\n",
    "    def forward(self, center_words, target_words, negative_words):\n",
    "        center_vectors = self.in_embed(center_words)\n",
    "        target_vectors = self.out_embed(target_words)\n",
    "        negative_vectors = self.out_embed(negative_words)\n",
    "\n",
    "        pos_score = torch.bmm(target_vectors.unsqueeze(1), center_vectors.unsqueeze(2))\n",
    "        pos_score = pos_score.squeeze(-1)\n",
    "        pos_score = torch.sigmoid(pos_score)\n",
    "\n",
    "        neg_score = torch.bmm(negative_vectors, center_vectors.unsqueeze(2))\n",
    "        neg_score = neg_score.squeeze(-1)\n",
    "        neg_score = torch.sigmoid(-neg_score)\n",
    "        \n",
    "        loss = -torch.log(pos_score + 1e-5) - torch.sum(torch.log(neg_score + 1e-5), dim=1)\n",
    "        \n",
    "        return torch.mean(loss)\n",
    "\n",
    "VOCAB_SIZE = 100\n",
    "EMBED_DIM = 10\n",
    "BATCH_SIZE = 1\n",
    "K_NEGS = 5 \n",
    "\n",
    "model = SkipGramNegativeSampling(VOCAB_SIZE, EMBED_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "center_id = torch.tensor([0], dtype=torch.long)\n",
    "target_id = torch.tensor([1], dtype=torch.long)\n",
    "negative_ids = torch.tensor([[50, 23, 99, 4, 12]], dtype=torch.long)\n",
    "model.zero_grad()\n",
    "loss = model(center_id, target_id, negative_ids)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "word_vec = model.in_embed(torch.tensor([word_to_ix['fox']]))\n",
    "print(f\"Vector for fox is {word_vec.detach().numpy()}\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
